article,brief,generated_brief
"[""‘2TScrum’: A Board Game to Teach Scrum  ABSTRACT  The need for software with quality, low cost and differential, the  constant changes in the mart and the need for fast and flexible  software development companies, led to the creation of a new  approach to software development, the agi le methods seek to  reduce the time devoted to documentation and to increase the  time of software development, besides being adaptive to  changes. Scrum is one of the agile methods mostly used in the  software development market. The interest for professional s  who have experience with agile methods, in specific Scrum, is  increasing. However, the teaching of these methods is carried  out theoretically, however, due to the complexity of the agile  methods, there is a need for practice to fix the content and  provide a first practical contact. One practical teaching method  that has shown good results is the use of games. In this context,  with the aim of contributing to the teaching of agile methods,  this work proposes a board game to assist the practical teaching  of Scrum. To validate the game an evaluation of educational  games was carried out with 50 undergraduate students in  Information Systems and 20 students of specialization in  Software Engineering. Students expressed positively that the  game was an efficient way  to learn and that they achieved the  goals by applying their skills. The data collected in this research  provided evidence that the game can have a positive effect on  Scrum teaching.1    CCS CONCEPTS  • Software and its engineering → Software creation and  management → Software development methods → Software  development methods → Agile software development    KEYWORDS  Agile Methods, Scrum, Games.                                                                © 2017 Association for Computing Machinery. ACM acknowledges that this  contribution was authored or co -authored by an employee, contractor or affiliate of  a national government. As such, the Government retains a nonexclusive, royalty - free right to publish or reproduce this article, or to allow others to  do so, for  Government purposes only.  SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil   © 2017 Association for Computing Machinery.  ACM ISBN 978 -1-4503-5326- 7/17/09…$15.00   https://doi.org/10.1145/3131151.3131177  ACM Reference format:  A. Brito, J. Vieira. 2017. SIG Proceedings Paper in Word Format.  In Proceedings of ACM SBES confe rence, Fortaleza, Ceará BR,  September 2017 (SBES), 10 pages.  https://doi.org/10.1145/3131151.3131177  1 INTRODUCÃO  Na década de 1960 o termo “crise de software” começou a ser  utilizado para definir um conjunto de problemas referentes a  construção, implantaç ão e manutenção de software, vividos  naquele período [ 1], como uma possível solução à crise, foram  estabelecidos modelos de processos formais para organizar o  desenvolvimento de software [ 2], entretanto muitos dos  problemas encontrados na crise ainda persistem [1].  Como alternativa a estes métodos de desenvolvimento, um  grupo de especialistas produziu um conjunto de valores e  princípios comuns para o desenvolvimento ágil de software,  conhecido como Manifesto Ágil [ 3], cujo objetivo é  propor um  maior foco nas pessoas ao invés dos processos, além de procurar  utilizar menos tempo com documentação e mais tempo  resolvendo problemas de forma iterativa [ 4]. Entre estes  métodos, o mais utilizado atualmente no mercado é o Scrum [ 5],  um framework para gerenciamento de projetos que se propõe a  maximizar a produtividade, entregar produtos de grande valor e  resolver problemas complexos e adaptativos [6].  O Scrum proporciona o aumento no comprometimento e  integração da equipe do projeto, resultando em uma maior  motivação, facilitando assim o gerenciamento do projeto [ 7].  Scrum é um dos métodos de gerenciamento de projetos mais  utilizadas do mundo, em pesquisa realizada por [ 8], 95% de 3.880"", 'motivação, facilitando assim o gerenciamento do projeto [ 7].  Scrum é um dos métodos de gerenciamento de projetos mais  utilizadas do mundo, em pesquisa realizada por [ 8], 95% de 3.880  organizações entrevistadas mundialmente utilizam métodos  ágeis, e que cerca de 58% (aproximadamente 2.137) delas utilizam  especificamente o framework Scrum para desenvolvimento de  software. Com isso, é possível entender que existe uma demanda  por profissionais que entendam conceitos e práticas do Scrum, de  modo a estarem preparados para o mercado de trabalho.  Para as empresas, a adesão do Scrum é o primeiro passo para  a agilidade, então como forma de preparar os alunos para a  crescente necessidade da indústria de software, o ensino de  Scrum é algo imprescindível [9]. Porém, há dificuldade no ensino  prático do Scrum a alunos, ocasionando problemas de adaptação  referente a transição da área acadêmica para o mercado de  trabalho, havendo assim um lapso entre o aluno e o mercado [ 5].  Essa dificuldade em formar bons profis sionais em gerenciamento   A. Brito  Universidade Federal do Ceará  Quixadá, Ceará                   Brasil  andrezadesouzabrito@gmail.c om   J. Vieira  Universidade Federal do Ceará  Quixadá, Ceará                                       Brasil  jefersonkenedy@ufc.br    279', 'de projetos pode levar a um alto índice de fracasso nos projetos  de software, de acordo com Chaos Manifesto [8] a ineficiência no  gerenciamento de projetos é um dos fatores que acabam  resultando no índice de 19% de projetos cancela dos antes de  serem concluídos. Gerentes de projetos experientes tendem a  obter mais sucesso do que os novatos, principalmente no que se  refere as dificuldades do cumprimento do cronograma, não  ultrapassar os custos e atingir as funcionalidades desejadas, p ois  as experiências obtidas em projetos anteriores podem ser  reutilizadas [17].   Uma forma de ensinar a atividade de gerenciamento de  projetos é a utilização de jogos de simulação [10, 11], assim essa  pesquisa propõe um jogo de tabuleiro chamado 2TScrum ( To  Teach Scrum ) que possibilita a prática de gerenciamento de  projetos, utilizando o Scrum.  Este artigo é composto, além da seção introdutória, por mais  8 seções. A seção 2 apresenta o método ágil Scrum, juntamente  com suas fases, eventos, equipe e artefato s. A seção 3 aborda a  classificação dos jogos, com foco em jogos de simulação, além de  ser exposto vantagens dos jogos no ensino. A seção 4 descreve  alguns trabalhos relacionados ao tema listados no decorrer do  levantamento bibliográfico. A seção 5 descrev e a metodologia  que foi utilizada nessa pesquisa, descrevendo as etapas ao qual a  pesquisa foi dividida. A seção 6 apresenta o jogo 2TSCRUM, com  a definição do projeto do jogo. A seção 7 contém o mapeamento  das atividades e artefatos presentes no jogo com o framework  Scrum. A seção 8 apresenta os resultados obtidos com a  aplicação do jogo. E finalmente, a seção 9 apresenta as  conclusões e trabalhos futuros.  2 O MÉTODO ÁGIL SCRUM  Scrum é um framework que proporciona o controle de  problemas adaptativos e com complexidade. Ele é baseado na  teoria empírica, ou seja, o conhecimento é proveniente de  experiências anteriores, e a tomada de decisão é realizada com  base neste conhecimento [12]. O Scrum é composto por fases,  time, eventos e artefatos [12], descritos a seguir.  As fases do Scrum são divididas em Pré -game, Game, e Pós - game [13]. A fase de Pré -game é destinada ao desenvolvimento  do Backlog do Produto, definição de funcionalidades a serem  inseridas em um incremento, realização de estimativa de custo,  avaliação e controle dos riscos, revisão dos itens de Backlog,  análise do domínio para construção do produto, e refinamento da  arquitetura. O Game é a fase iterativa de desenvolvimento, são  determinadas funcionalidades, tempo e qualidade, onde ocorrem  as Sprints (time-boxeds de um mês ou menos, em que um  incremento é criado). O Pós -game é a fase final em que são  realizados os testes, a documentação final e a integração ao  produto [13].  O Scrum estabelece a formação de uma equipe ou ‘Time  Scrum’ auto organizável e multifuncional. Times auto  organizáveis decidem a melhor forma de realizarem seu trabalho  e times multifuncionais possuem todas as competências  necessárias para completar o trabalho. O time Scrum é composto  pelo Product Owner , Equipe de Desenvolvimento e o  Scrum  Master. Esse Time Scrum é auto organizável e multifuncional [6].   O Scrum Master é responsável por garantir que o Scrum seja  entendido e aplicado por toda a equipe, assegura ainda que todos  os impedimentos do time sejam retirados para que consigam  alcançar os objetivos definidos. O Product Owner  procura  expandir o valor do produto a ser produzido e do trabalho do  time de desenvolvimento, é o responsável pelo gerenciamento do  Backlog do Produto. A equipe de desenvolvimento é composta  por profissionais responsáveis pela criação de versões utilizáveis  a serem incrementadas no produto [12].  O Scrum também estabelece eventos como forma de criar  uma rotina e reduzir a necessidade de reuniões. A Reunião de  Planejamento da Sprint é um evento para a definição de objetivos  da Sprint. A Reunião Diária tem o objetivo de acompanhar as', 'uma rotina e reduzir a necessidade de reuniões. A Reunião de  Planejamento da Sprint é um evento para a definição de objetivos  da Sprint. A Reunião Diária tem o objetivo de acompanhar as  atividades realizadas, planejar atividades a serem executadas no  decorrer do dia, e analisar possíveis obstáculos que impossibilite  que a equipe alcance a meta da Sprint. E, finalmente, a Revisão e  Retrospectiva da Sprint são duas reuniões informais realizadas no  fim da Sprint com a intenção de apresentar o resultado produzido  no decorrer da mesma, e do time se auto avaliar propondo  melhorias ao processo [12].   Como resultado dos event os do Scrum são gerados artefatos  que permitem transparência, a obtenção do controle do projeto e  sua adaptação [12]. O Backlog do Produto é uma lista do que o  produto final necessita, organizados de acordo com sua  prioridade. E o Backlog da Sprint é uma l ista de funcionalidades  com itens selecionados do Backlog do Produto para o  desenvolvimento durante uma Sprint.  3 JOGOS PARA O ENSINO DE  GERENCIAMENTO DE PROJETOS  Para Fullerton et al. [ 14] os jogos podem ser classificados, de  acordo com seu gênero, em: jogos de ação, de estratégia, de  esportes, de corrida/condução, de voo e outros simuladores, de  aventura, RPG ( Role-Playing Games ) e jogos de  simulação/construção.   O jogo proposto nesse trabalho é classificado como um  jogo de simulação, por simular um ambiente de desenvolvimento  de software utilizando Scrum e possibilitar que o jogador  gerencie um projeto, controlando tempo e orçamento.  O jogo de simulação é uma forma híbrida de desempenho  das atividades de um jogo de forma simulada, unindo as  características de um jogo (competição, cooperação,  participantes, regras, etc.) com as de uma simulação (incorporar  características críticas da realidade) [15]. Os jogos de simulação  podem ser dispostos de diversas maneiras, como um jogo de  computador, manual, de cartas ou mesmo de tabuleiro [16].  Os jogos possibilitam um ambiente simulado, onde os  alunos podem pôr em prática dif erentes soluções de um  problema e constatar suas consequências de forma segura, pois  essas consequências, boas ou ruins, estão no ambiente simulado  [18], possibilitando que o aluno reveja suas estratégias e aprenda  com seus erros [19]. Para a utilização de  jogos de simulação  como ferramenta de ensino, há a necessidade da definição de  regras para o jogador, tendo em vista que estes confrontarão  280', 'desafios na tomada de decisões dentro do ambiente simulado  [20].  Uma vantagem de utilizar jogos como forma de ensin o é  que estes estimulam e motivam o aluno por meio de simulações  de situações reais, além de possibilitar um aprendizado de baixo  custo e em menos tempo [21]. Os jogos no ensino proporcionam  várias outras vantagens, tais como [22]: utilização de conceitos  de difícil compreensão; criar estratégias de resolução de  problemas; aprender a tomar decisões e conseguir avaliá -las;  possibilitar interdisciplinaridade; proporcionar ao aluno a  construção do seu próprio conhecimento, a partir da sua  participação; favorec er a integração social entre os alunos e o  trabalho em equipe; é interessante para os alunos; aumento da  criatividade, do senso crítico, da participação, da competição, da  observação, das formas do uso da linguagem e retomar o prazer  em aprender; desenvolv er habilidades necessárias; e  possibilidade de identificar dificuldades dos alunos.  Nos cursos de Tecnologia da Informação o ensino de  gerenciamento de projetos muitas vezes segue uma abordagem  teórica, faltando assim referências práticas [23]. Em especifi co,  métodos ágeis precisam ser postos em prática, estes métodos  evoluíram a partir das experiências e só podem ser totalmente  compreendidos com a sua prática, pois algumas atividades não  são plenamente inferidas na teoria [24].   4 TRABALHOS RELACIONADOS  O ensino de métodos ágeis é necessário devido ao seu crescente  uso, porém limitações de tempo, escopo e ambiente dificultam a  experiência prática dos alunos na utilização desses métodos [25].  Diversos jogos têm sido propostos para a poiar o ensino de  gerenciamento de projetos e métodos ágeis, como os, [10], [25],  [5], [23], [11]. Para a execução dessa pesquisa foi realizada uma  revisão não -sistemática da literatura, com isso foram  identificados trabalhos publicados relacionados ao tem a, os  principais serão apresentados nesta seção.  O jogo The Incredible Manager foi desenvolvido por [26]  como forma de prover experiência de forma divertida e com  baixo custo, visando complementar o ensino de gerência de  projetos tradicional. As conclusões dessa pesquisa apontam que  os participantes consideraram que suas habilidades em  gerenciamento aumentaram, além de terem considerado a  experiência divertida. O trabalho desenvolvido por [ 23] sugere  uma dinâmica de ensino que consiste em iniciar, planejar,   executar, controlar e encerrar um projeto cujo produto é uma  refeição a base de pizza. Os alunos mostraram que a dinâmica foi  muito proveitosa (91% de aprovação) e uma evolução na  aprendizagem (cerca de 35%).  Os jogos desenvolvidos por [23] e [ 26] são destinados a  auxiliar no gerenciamento de projetos de forma generalista,  enquanto o 2TScrum tem foco no Scrum.  Uma alternativa proposta por [25] para fornecer  conhecimento sobre o Scrum é o Virtual Scrum, um ambiente de  trabalho onde há um Product Owner, Scrum Master e Equipe. O  Product Owner é representado pelo professor, dentre os alunos é  escolhido um para o papel de Scrum Master e o restante atua  como equipe, aprendendo com o Scrum Master. Foi possível  constatar, que o jogo evoluiu a comunicação entre o ti me,  aumentou a compreensão sobre Scrum, ajudou nas reuniões.  Outra forma de ensinar Scrum utilizando simulador pode ser  encontrada em [11], esta proposta tem o foco na comunicação da  equipe e na tomada de decisões em conjunto, de forma a cumprir  um objetivo imposto pelo cliente (simulado pelo professor).  Os jogos definidos por [11] e [ 25] dependem diretamente  do professor para definir detalhes do projeto a ser desenvolvido,  o 2TScrum só precisa que o participante leia as regras para  começar a jogar.  Outro jogo para ensino de Scrum pode ser encontrado em  [10] chamado Scrumia, um jogo manual de criação de barcos de  papel, para o ensino Scrum a gerentes de projetos. O jogo foi  considerado, pelos alunos, como uma dinâmica motivadora,', '[10] chamado Scrumia, um jogo manual de criação de barcos de  papel, para o ensino Scrum a gerentes de projetos. O jogo foi  considerado, pelos alunos, como uma dinâmica motivadora,  como uma forma eficiente de aprendizado. O 2TScrum se  diferencia do Scrumia por ser um jogo de tabuleiro.  Todos os trabalhos apresentados nessa seção possuem o  foco no ensino de gerenciamento de projetos, entretanto apenas  os trabalhos desenvolvidos por [ 30], [ 25] e [ 11] e este traba lho  tem o foco no método ágil Scrum. Os trabalhos desenvolvidos  por [26], [25] e [11] propuseram um jogo digital de simulação, o  objetivo desse trabalho também é propor um jogo de simulação,  mas na forma de um tabuleiro não-digital. Apenas este trabalho e  o proposto por [26] não necessitam da presença do professor.  5 METODOLOGIA  De acordo com Wazlawick [ 27], do ponto de vista de sua  natureza, esta pesquisa é classificada como um trabalho original,  uma forma de apresentar um novo conhecimento partindo de  observações e teorias para explicá -las. Do ponto de vista dos  objetivos, Wazlawick [ 27] classifica esta pesquisa como  descritiva, buscando obter dados coesos sobre uma realidade,  sem intervenção do pesquisador ou intenção de explicar um  determinado acontecimento, apenas a descrição de como algo é.  De acordo com Gil [34], do ponto de vista de seus procedimentos  técnicos, esta pesquisa é classificada como experimental, de  forma a escolher um objeto de estudo, variáveis que possam  influenciá-los e formas de controle e observação do resultado.  Esta pesquisa foi realizada em 6 etapas. Na etapa 1 foi  realizado o mapeamento das práticas do Scrum como forma de  estabelecer os componentes do jogo. Na etapa 2 foi criado o  projeto do jogo utilizando o game design e design instrucional, de  acordo com [31] o processo game design e o design instrucional  se completam para a criação de jogos educacionais de qualidade.   Na etapa 3 foi criado um protótipo como forma de avaliar a  jogabilidade e validar que o processo simulado consistia no  Scrum.  Em seguida foi realizada a aplicação de um tes te piloto.  Teste piloto é descrito por [28] como sendo uma atividade  necessária para avaliar a qualidade do produto gerado, onde é  analisado se o participante compreendeu corretamente o produto  apresentado, se o tempo da tarefa está dentro do viável e se a   partir das tarefas demandadas o participante consegue avaliar os  critérios desejados.  O teste piloto foi realizado por três profissionais que  trabalham em ambiente de desenvolvimento de software com  Scrum a mais de dois anos. Com esta versão deu -se início a etapa  281', '4, em que 50 (cinquenta) alunos de graduação em Sistemas de  Informação e Engenharia de Software e 20 (vinte) alunos de pós - graduação latu senso em Engenharia de Software foram  convidados a participar da aplicação do jogo.   Em paralelo a aplicação do jogo a etapa 5 foi iniciada, onde  os alunos foram acompanhados utilizando o método de  observação, definido por [29] como uma técnica de coleta de  dados que possibilita ao pesquisador apontar e obter provas  sobre objetivos que orientam o comportamento do  usuário,  mesmo que inconscientemente. Após a aplicação do jogo, os  alunos responderam um questionário, que é definido por [30]  como uma lista de questões a serem respondidas por pessoas  sujeitas a pesquisa. O questionário utilizado foi uma adaptação  do qu estionário desenvolvido por [30] para avaliação de jogos  educacionais, que propõe uma avaliação através da percepção  dos alunos sobre os níveis motivacionais, de experiência e de  aprendizagem estimulado pelo jogo.  Por fim, a etapa 6 consistiu na análise de  dados, onde foram  analisados os questionários e as notas tomadas durante a  observação.  6 O JOGO 2TSCRUM  O objetivo desse trabalho é facilitar o ensino do framework  Scrum através de um jogo de simulação, para isso foi construído  um jogo de tabuleiro denomi nado 2TScrum. O formato de jogo  de tabuleiro foi escolhido devido sua facilidade de aplicação em  empresas e salas de aula, não necessitando de computadores para  sua execução, apenas do material impresso. O projeto do jogo  (desing instrucional e game design) é apresentado nessa seção.   O design instrucional identifica os princípios educacionais  para o jogo [31]. Os principais pontos do design instrucional são:  análise do público alvo, conhecimento prévio necessário, e  objetivos educacionais [19]. O público a lvo do 2TScrum são  pessoas que buscam conhecimento prático no Scrum. Com  relação aos conhecimentos prévios é necessário que o jogador  tenha pelo menos conhecimentos teóricos sobre Scrum, seus  eventos e artefatos, pois esses conceitos serão utilizados no jo go.  E quanto aos objetivos educacionais, espera -se que ao final do  jogo o aluno tenha vivenciado a prática do método ágil Scrum.  O game design contém a descrição do jogo, sua classificação  de acordo com o gênero, descrição da mecânica do jogo  (jogabilidade), funcionalidades da interface com o jogador,  sistema de pontuação, plataforma utilizada e características  técnicas do jogo, ou seja, tecnologias utilizadas no  desenvolvimento [19]. Por este ser um jogo de tabuleiro, não  serão especificadas as características técnicas do jogo.  Gênero do jogo:  O 2TScrum é um jogo de simulação, este  tipo de jogo simula situações do cotidiano, além de desenvolver  habilidades de gerenciamento e construção do jogador, podendo  também ter características estratégicas [32]. O 2TSc rum simula  um ambiente de desenvolvimento de software utilizando o   framework Scrum, além de permitir que o jogador realize  decisões que afetem o ambiente simulado.  Plataforma do jogo: 2TScrum é um jogo de tabuleiro, não  digital, necessitando então do tabul eiro, cartas e pino para ser  jogado.   Descrição do jogo: O objetivo do jogo é a realização do  gerenciamento de um projeto de um sistema, aplicando os  conhecimentos sobre Scrum, em busca de finalizar o  desenvolvimento do projeto dentro do prazo e orçamento  estabelecido. Como resultado, o jogador calculará a sua  colocação no ranking geral de jogadores.  Narrativa: para atingir o objetivo do jogo o jogador seguirá  a seguinte narrativa: “Um novo projeto chega a empresa e você é  alocado para gerenciá -lo utilizand o o framework Scrum, este  projeto se trata de um sistema web para uma biblioteca, com um  orçamento e prazo fixos que você deve cumprir, mas no decorrer  do tempo há diversos acontecimentos que você deve gerenciar.  Além desses acontecimentos, durante a const rução do produto, o  seu cliente acaba entendendo melhor o que necessita e mudanças', 'do tempo há diversos acontecimentos que você deve gerenciar.  Além desses acontecimentos, durante a const rução do produto, o  seu cliente acaba entendendo melhor o que necessita e mudanças  podem acontecer. Você deve manter o projeto no prazo e no  orçamento estabelecido, mas será que é possível? Você aceita  esse desafio?”.  Funcionalidades da interface com jogad or: o jogador  também terá pontos de interação com o jogo. Abaixo são  descritos os artefatos e pontos de interação do jogador presentes  no jogo:  \uf0b7\uf020 Carta do cliente: artefato que representa o cliente do  projeto, com ela, o jogador terá conhecimento sobre o  sistema que o cliente deseja adquirir, além do orçamento e  prazo do projeto.  \uf0b7\uf020 Cartas de Backlog: estes artefatos contêm itens que o  jogador escolherá para fazer parte do Backlog do Produto.  \uf0b7\uf020 Carta de validação: permite que o jogador valide os itens de  Backlog do Produto.  \uf0b7\uf020 Desenvolvedores: elementos que representam  desenvolvedores de software a serem alocados na equipe  pelo jogador, servindo assi m como equipe de  desenvolvimento do produto. Cada carta de desenvolvedor  irá gerar um custo de tempo e monetário ao projeto.  \uf0b7\uf020 Cartas Surpresas: simulam modificações no escopo do  projeto feitas pelo cliente, modificando o projeto que o  jogador gerencia. O conteúdo de algumas cartas surpresas  está disposto na Tabela 1.  \uf0b7\uf020 Cartas de evento: estas cartas, referentes aos eventos do  Scrum, buscam aproximar o ambiente simulado de um  ambiente real, simulando possíveis acontecimentos no  projeto que o jogador deverá gerenciar. Estas cartas  possuem problemas que necessitam de tomada de decisões,  afetando custo e prazo do projeto e problemas que devem  apenas ser aceitos pelo jogador, gerando mudanças no  ambiente. Estas são separadas por evento Scr um, 7 (sete)  cartas do tipo planejamento da Sprint, 10 (dez) cartas da  reunião diária, 7 (sete) da reunião de revisão, e 7 (sete) da  reunião de retrospectiva. Exemplos dessas cartas são  apresentados na Tabela 1.  Mecânica do Jogo:  O 2 TScrum é um jogo em que uma  pessoa ou um grupo de pessoas devem realizar o gerenciamento  do projeto de um software utilizando o método ágil Scrum. A Fig.  1 apresenta o tabuleiro do jogo que indica o fluxo a ser seguido  pelo jogador.  282', 'Tabela 1: Conteúdo das cartas  Tipo de carta Conteúdo da Carta  Planejament o   da Sprint  Os seus desenvolvedores descobriram uma  nova linguagem de programação que pode  acelerar a construção do produto, mas para  isso eles precisam fazer um treinamento.  Realizar o treinamento: Perde R$ 500,00 e  ganha 4 dias  Não realizar: Perde 5 dias  Os desenvolvedores verificaram que um item  do backlog não está muito claro, por isso  precisarão de mais tempo para visitar o  cliente e entender melhor o que precisa ser  desenvolvido. Você perdeu 3 dias.   Surpresa Fazendo uma análise das pesquisas realizadas  na biblioteca, as empresas Wayne decidiram  que as pesquisas devem ser mais refinadas,  assim os livros devem conter mais dados,  como editora, ano de publicação e tipo  literário.  Verifique o Backlog do Produto e inclua o  item b17 e retire o item b16. Para retirar um  item implementado você perde metade o  tempo que levou para desenvolver.  O sistema em construção deve permitir que o  usuário tenha acesso a lista de reservas que  ele realizou.  Verifique o Backlog do Produto e inclua os  itens b32 e b34.  Reunião  diária      A internet está fora do ar e seus  desenvolvedores estão de braços cruzados. Ou  você contrata um técnico para tentar resolver  o problema mais rápido ou espera a empr esa  provedora de internet resolver. Perdeu 1 dia.  Contratar técnico: Perde R$ 500,00 e ganha 1  dia.  Esperar solução da empresa: Perde 3 dias.  O desenvolvedor tinha horas extras para  pagar e acabou mais cedo uma de suas tarefas.  Você ganhou 2 dias.  Revisão   da Sprint  O cliente gostou da interface do sistema e  decidiu aplicar mais no projeto, você ganhou  R$ 500,00.      O cliente não conseguiu utilizar a  ferramenta, ele quer que a torne mais fácil.      Ajustar: Perde 3 dias.      Contratar um profissional de IHC: Perde R$  900,00 e perde 1 dia.  Reunião   De  Retrospectiva  Sua equipe não está muito segura com a  tecnologia utilizada no projeto, eles sugerem a  contratação de uma consultoria.  Contratar a consultoria: Perde R$ 300,00 e  ganha 2 dias.  Não contratar a consultoria: Ganha R$ 100,00  e perde 7 dias.  Sua equipe anda muito desestimulada, seria  legal você oferecer um happy hour na sexta.  Realizar o Happy Hour: Perde R$ 1000,00.  Não realizar o Happy Hour: Perde 7 dias.      Figura 1: Tabuleiro do jogo 2Tscrum  Primeiramente o jogador agrupa as cartas de acordo com o  tipo (cartas surpresas, reunião diária, planejamento da Sprint,  reunião de revisão e retrospectiva da Sprint), embaralha e  posiciona o pino no quadro de início (Fig. 1 - A).   Com o pino posicionado, o jogador pode então dar início ao  jogo, avançando seu pino para a próxima etapa, carta cliente  (Fig. 1 - B), o jogador então fará a leitura da carta que o cliente o  enviou, especificando através de um text o corrido, o produto  desejado, orçamento e prazo disponíveis para o projeto.  Havendo realizado a leitura da carta, o jogador segue para a  criação do Backlog do Produto (Fig. 1 - C), o jogador conta com  36 (trinta e seis) itens de Backlog e deverá escolher entre estes  itens apenas o que o cliente especificou em sua carta. Os itens  são dispostos com um identificador, descrição do item e  estimativa de tempo para o desenvolvimento.  Com o Backlog do Produto pronto, o jogador segue para a  reunião com o cliente (Fig. 1 - D), nesse momento o jogador pega  a carta de validação e adequa seu Backlog de acordo com essa  carta, descartando os itens que não estão na carta, e incluindo os  que estão na carta, mas não estão em seu Backlog.  Tendo validado o Backlog do Produto o jogador alocará  desenvolvedores para sua equipe ( Fig. 1 - E), ou seja, escolherá 3  (três) desenvolvedores para sua equipe de desenvolvimento.  Cada desenvolvedor possui uma classificaç ão, além de possuir  também um custo monetário por Sprint e um bônus.  Com a equipe estabelecida, o jogador fará o planejamento da  Sprint (Fig. 1  - F), onde criará o Backlog da Sprint, o jogador', 'também um custo monetário por Sprint e um bônus.  Com a equipe estabelecida, o jogador fará o planejamento da  Sprint (Fig. 1  - F), onde criará o Backlog da Sprint, o jogador  deverá escolher no máximo 7 (sete) iten s para desenvolvimento.  Com o Backlog da Sprint criado, é somado o tempo estimado para  cada item presente no Backlog da Sprint e decrementado do prazo  do projeto.  Ainda no planejamento ( Fig. 1 - F) o jogador retira 2 (duas)  cartas d o tipo planejamento da Sprint e 2 (duas) cartas do tipo  283', 'surpresa. As cartas, com exceção das surpresas, retornam ao  baralho e são embaralhadas novamente.  Ao lidar com as modificações e acontecimentos presentes nas  cartas de planejamento e surpresa, o jogad or parte para o  desenvolvimento (Fig. 1 - G), reduzindo do orçamento o custo de  cada desenvolvedor, além de acrescentar ou reduzir dias no  projeto, dependendo do bônus do desenvolvedor alocado.  Durante o desenvolvimento, o jogador t ambém retira 2 (duas)  cartas do tipo reunião diária.  Ao ter gerenciado os acontecimentos presentes na reunião  diária, o jogador segue para a revisão da Sprint (Fig. 1  - H),  retirando 1 (uma) carta de reunião de revisão, essas cartas   apresentarão dificuldades e soluções identificadas pelo cliente ao  ser apresentado ao incremento do sistema na reunião de revisão.  Após a resolução do acontecimento da carta, o jogador parte  para a retrospectiva da Sprint (Fig. 1 - I) e retira 1 (uma) carta de  reunião de retrospectiva, essas cartas apresentarão  acontecimentos positivos ou negativos da Sprint que terão que  ser gerenciados pela equipe. Ao final da retrospectiva, o jogador  realiza o incremento do produto ( Fig. 1 - J), debitando 1 (um) dia  no seu prazo, pela junção e/ou implantação do novo incremento.  Finalmente, o jogador se depara com uma pergunta ‘Produto  Finalizado?’ (Fig. 1 - K), ou seja, se todos os itens do Backlog do  Produto foram desenvolvidos, caso a resposta seja negativa, o  jogador retorna para o planejamento da Sprint (Fig. 1  - F)  repetindo o processo descrito anteriormente, caso a resposta seja  positiva, o jogador finaliza o jogo.  Sistema de Pontuação: o sistema de pontuação do jogo  2TScrum, realizado através de um ranking, tem como objetivo  dar um feedback ao participante, além de instigar os demais  jogadores. O feedback é realizado ao final do jogo, levando em  consideração o orçamento e  tempo resultantes ao final do  projeto, jogadores com essas variáveis positivas e com menor  variação possível entre elas, tem uma colocação superior no  ranking. O ranking é exposto em um mural acessível aos  jogadores.  7. MAPEAMENTO DO JOGO COM O  FRAMEWORK SCRUM  O jogo foi mapeado de acordo com o Guia do Scrum [5], com  relação aos eventos, artefatos, e papéis Scrum. Para o  mapeamento foi seguida uma escala ordenada de três categorias.  As três categorias são:   A: Atendido  – O jogo atende ao evento/artefato do   framework Scrum.  PA: Parcialmente Atendido  – O jogo atende  parcialmente ao evento/artefato do framework Scrum.  NA: Não atendido – O jogo não atende ao evento/artefato  do framework Scrum.  O mapeamento entre os eventos, artefatos e papéis do Scrum  com o jog o 2TScrum são apresentados nas Tabela 2, Tabela 3, e  Tabela 4 respectivamente.   Tabela 2: Mapeamento entre o jogo 2TScrum e os eventos  Scrum   Scrum 2TScrum Categori a  Reunião de  Planejament o da Sprint  Figura 1 – Planejamento da Sprint  (Etapa ‘F’)  Cartas Tipo Reunião de  Planejamento  A  Reunião  Diária  Figura 1 – Desenvolvimento  (Etapa ‘G’)  Cartas Surpresa  Cartas Tipo Reunião Diária  A  Revisão da  Sprint  Figura 1 – Revisão da Sprint  (Etapa ‘H’)  Cartas Tipo Reunião de  Revisão  A  Retrospectiva  da Sprint  Figura 1 – Retrospectiva da  Sprint (Etapa ‘F’)  Cartas Tipo Retrospectiva de  Revisão  A  Tabela 3: Mapeamento entre o jogo 2TScrum e os artefatos  Scrum  Scrum 2TScrum Categori a  Backlog  do  Produto  Figura 1 – Desenvolvimento  (Etapa ‘B’, ‘C’ e ‘D’)  Carta do Cliente  Cartas de Backlog  Carta de Validação  A  Backlog  da Sprint  Figura 1 – Desenvolvimento  (Etapa ‘F’)  A  Tabela 4: Mapeamento entre o jogo 2TScrum e os papéis  Scrum  Scrum 2TScrum Categori a  Product Owner Simulado pelo jogo (Carta  do Cliente, Cartas de  Backlog, Carta de  Validação)  PA  Equipe de  Desenvolvimento  Papel realizado pelo  jogador  Cartas de Desenvolvedor  PA  Scrum Master Papel realizado pelo  jogador  PA    Essa análise mostra que todos os iten s referentes aos', 'PA  Equipe de  Desenvolvimento  Papel realizado pelo  jogador  Cartas de Desenvolvedor  PA  Scrum Master Papel realizado pelo  jogador  PA    Essa análise mostra que todos os iten s referentes aos  artefatos e eventos do Scrum encontrados no seu Guia [5] são  atendidos pelo 2TScrum . Entretanto, o time Scrum está presente  no jogo, mas não há uma separação clara dos papéis do Scrum.  8 RESULTADOS E DISCUSSÕES  8.1 Aplicação do Jogo  284', 'Em um primeiro momento, realizou-se um teste piloto com 3  (três) profissionais que trabalham como Scrum Master em  ambiente de desenvolvimento de software com Scrum há mais de  2 (dois) anos. Nesse teste piloto,  foi constatado que o processo  simulado corresponde as práticas encontradas no Scrum.  A utilização do jogo por estes profissionais gerou as  seguintes modificações: aumento da quantidade de cartas com  bugs; aumento de cartas que possibilitem a tomada de dec isões;  reajuste da quantidade de tempo e orçamento retirado nas cartas;  criação de uma quantidade maior de cartas para o evento reunião  diária; e definição de tempo e/ou atividades por Sprint.  Após as modificações sugeridas pelos profissionais chegou-se  a uma versão estável do jogo contendo 1(um) tabuleiro, 1 (uma)  carta do cliente, 1 (uma) de validação, 36 (trinta e seis) cartas de  Backlog, 9 (nove) de desenvolvedores, 7 (sete) de planejamento  da Sprint, 10 (dez) de reunião diária, 7 (sete) de revisão da S print,  8 (oito) de retrospectiva da Sprint, 10 (dez) surpresas, além do  orçamento de R$ 10.000 e tempo de 70 dias e tempo médio da  partida de 1 (uma) hora.   A aplicação do jogo e a coleta dos resultados ocorreram entre  os dias 26 de agosto e 30 de outubro de 2016, com 20 alunos de  especialização em Engenharia de Software da disciplina de  Gestão Ágil de Projetos, e 50 (cinquenta) alunos de graduação  dos cursos de Sistemas de Informação e Engenharia de Software,  com conhecimento teórico sobre Scrum.  Inicialmente, foi realizada uma breve apresentação sobre a  mecânica do jogo, em seguida os alunos jogaram o 2TScrum e na  medida em que iam terminando eram convidados a responder o  questionário sobre a experiência vivenciada.  O questionário aplicado segue o modelo d e avaliação de  jogos educacionais descrito por [30] e é composto pela variável  reação dos alunos ao jogo educacional que é dividida em 3 (três)  subcomponentes (motivação, experiência do usuário e  aprendizagem) e cada subcomponente é dividido em dimensões.  O subcomponente motivação é dividido em 4 (quatro)  dimensões: atenção, relevância, confiança e satisfação. O  subcomponente experiência do usuário é composto por 6 (seis)  dimensões: imersão, desafio, competência, divertimento, controle  e interação social. O  subcomponente aprendizagem é composta  por 5 (cinco) dimensões: aprendizagem de curto termo e  aprendizagem de longo termo, conhecimento, compreensão,  aplicação. As dimensões conhecimento, compreensão e aplicação   são avaliadas através de uma lista de objeti vos educacionais para  que os alunos indiquem o nível de aprendizagem, antes e depois  de utilizarem o jogo. O questionário possui 27 (vinte e sete)  perguntas para avaliar os subcomponentes e objetivos de  aprendizagem.  Com os dados coletados foi possível ger ar gráficos que  representam a frequência das respostas, sendo considerado os  valores de ( -2) a (+2), adaptado da escala Likert [33], sendo (-2)  equivalente a “discordo fortemente”, ( -1) equivalente a  “discordo”, (0) correspondente a “não concordo nem disco rdo”,  (1) equivalente a “concordo” e (+2) equivalente a “concordo  fortemente”. Além disso, as perguntas foram agrupadas em  dimensões dentro dos subcomponentes analisados: motivação,  experiência do usuário e aprendizagem.  Para o subcomponente motivação, observa-se que o jogo teve  um efeito positivo, pois de acordo com os resultados obtidos Fig.  2, é possível verificar um alto nível de concordância em todas as  dimensões avaliadas (todos os itens receberam nota +1 ou +2 por  pelo menos 8 0% dos alunos). A seguir são apresentados  comentários específicos sobre as dimensões deste  subcomponente.    Figura 2: Avaliação da motivação do jogador no jogo  2TScrum.  \uf0b7\uf020 Dimensão de satisfação (Questões 1 e 2) – Para esta  dimensão os alunos foram perguntados se foi por causa do  seu próprio esforço que eles conseguiram avançar no jogo,  80% deram nota +1 ou +2. Foram perguntados também se', 'dimensão os alunos foram perguntados se foi por causa do  seu próprio esforço que eles conseguiram avançar no jogo,  80% deram nota +1 ou +2. Foram perguntados também se  estavam satisfeitos por saber que terão a oportunidade de  utilizar na prática o que aprendeu com o jogo, 87,1% deram  nota +1 ou +2.  \uf0b7\uf020 Dimensão confiança (Questões 3 e 4) – Para esta dimensão  os alunos foram perguntados se ao passar pelas etapas do  jogo sentiram confiança de que estavam aprendendo, 91,4%  deram nota +1 ou +2. Foram perguntados se foi fácil  entender o jogo e começar a utilizá -lo como material de  estudo, 82,8% deram nota +1 ou +2.  285', '\uf0b7\uf020 Dimensão relevância (Questões 5 a 7) – Para esta dimensão  os alunos foram perguntados se o conte údo do jogo está  conectado com outros conhecimentos que possuíam, 94,2 %  deram nota +1 ou +2. Foram perguntados se o  funcionamento do jogo está adequado ao seu jeito de  estudar, 91,4% deram nota +1 ou +2. E se o conteúdo do  jogo é relevante para seu interesse, 95,7% deram nota +1 ou  +2.  \uf0b7\uf020 Dimensão atenção (Questões 8 a 10) – Para esta dimensão  os alunos foram perguntados se a variação de forma,  conteúdo ou atividades ajudou a mantê -los atento ao jogo,  95,7% deram nota +1 ou +2. Foram perguntados se houve  algo interessante no início do jog o que capturou a atenção,  92,9% deram nota +1 ou +2. E se o design do jogo é  atraente, 80% deram nota +1 ou +2.  Com relação ao subcomponente experiência do usuário, o  jogo proporcionou uma experiência positiva aos alunos, como  pode ser visualizado na Fig. 3 . A seguir são apresentados  comentários específicos sobre as dimensões do subcomponente  experiência do usuário.  \uf0b7\uf020 Dimensão competência (Questões 11 e 12) – Para esta  dimensão os alunos foram perguntados se tiveram  sentimentos positivos de eficiência no desenrolar do jogo,  81,4% deram nota +1 ou +2. Foram perguntados se  conseguiram atingir os objetivos do jogo por me io de sua  habilidade, 70% deram nota +1 ou +2.  \uf0b7\uf020 Dimensão diversão (Questões 13 a 16) – Para esta  dimensão os alunos foram perguntados se gostariam de  utilizar o jogo novamente, 90% deram nota +1 ou +2. Foram  perguntados se recomendariam o jogo para colega s, 97,1%  deram nota +1 ou +2. Se quando interrompido ficaram  desapontados que o jogo tinha acabado, 58,6% deram nota  +1 ou +2. E se se divertiram com o jogo, 91,5% deram nota  +1 ou +2.  \uf0b7\uf020 Dimensão desafio (Questões 17 e 18) – Para esta dimensão  os alunos fo ram perguntados se o jogo evolui num ritmo  adequado e não fica monótono, 94,3% deram nota +1 ou +2.  Foram perguntados se o jogo é adequadamente desafiador  para eles, as tarefas não sendo fáceis nem difíceis, 87,1%  deram nota +1 ou +2.  \uf0b7\uf020 Dimensão imersão (Questões 22 a 24) – Para esta dimensão  os alunos foram perguntados se se sentiram mais no  ambiente do jogo do que no mundo real, 78,5% deram nota  +1 ou +2. Foram perguntados se não perceberam o tempo  passar enquanto jogava, 87, 1% deram nota +1 ou +2. Se  temporariamente esqueceram as preocupações do dia -a- dia, 80% deram nota +1 ou +2.  Referente ao subcomponente aprendizagem, é avaliado a  contribuição do jogo para a aprendizagem. A seguir são  apresentados comentários específicos sobre as dimensões  aprendizagem (Fig. 4) e objetivos de aprendizagem (Fig. 5).  \uf0b7\uf020 Dimensão aprendizagem de longo termo (Questão 25) –  Para esta dimensão os alunos foram perguntados se a  experiência com o jogo vai contribuir para seu  desempenho na vida profissional, 92,9% deram nota +1 ou  +2.  \uf0b7\uf020 Dimensão aprendizagem de curto termo – Para esta  dimensão os alunos foram perguntados se o jogo  contribuiu para a aprendizagem na disciplina e se o jogo  foi eficiente para a sua aprendizagem, em compa ração com  outras atividades da disciplina, 91,4% deram nota +1 ou +2  para as duas perguntas.    Figura 3: Avaliação da experiência do usuário no jogo  2TScrum.  Ainda no subcomponente de aprendizagem, os objetivos de  aprendizagem avaliados foram: Scrum, backlog do produto,  planejamento da Sprint, reunião diária, revisão da Sprint e  retrospectiva da Sprint, por serem conceitos utilizados no jogo.  Scrum foi escolhido como um objetivo de aprendizagem, por ser  286', 'o foco deste trabalho. Planejamento da Sprint, reunião  diária,  revisão da Sprint e retrospectiva da Sprint foram escolhidos  como objetivo de aprendizagem por serem eventos Scrum e  etapas do jogo. Por fim, backlog do produto foi escolhido como  objetivo de aprendizagem por ser um artefato Scrum e ser  subsídio para o backlog da Sprint.    Figura 4: Avaliação de aprendizagem no jogo 2TScrum.  Em relação aos objetivos de aprendizagem são avaliadas três  variáveis Fig. 5 : “lembrar”, “compreender” e “aplicar”. A  variável, lembrar tem o objetivo d e analisar se o aluno recorda  das informações sobre um determinado conteúdo. A variável  compreender busca entender se o aluno possui a compreensão  sobre uma informação ou fato, de forma a saber como utiliza -lo  em diferentes contextos. Por fim, a variável a plicar avalia a  possibilidade de o aluno colocar em prática o conhecimento  adquirido no jogo em situações concretas.  Levando em conta as variáveis “lembrar”, “compreender” e  “aplicar”, cada aluno atribuiu uma nota de 1,0 (um) a 5,0 (cinco)  de acordo com o seu nível de conhecimento em cada objetivo de  aprendizagem, antes e depois do jogo. Quanto maior a nota  estabelecida pelo aluno, maior o seu nível de conhecimento.  A  partir das notas estabelecidas pelos alunos, foi possível constatar  um aumento do nível d e conhecimento em todos os objetivos de  aprendizagem como pode ser observado na Fig. 5.  Todos as variáveis de cada objetivo de aprendizagem  obtiveram um aumento após utilização do jogo. Os maiores  saltos ocorreram nos itens aplicar o Scrum, com um salto de 1,85,  aplicar Backlog do Produto com 1,75 e aplicar a Retrospectiva da  Sprint com 1,63. Entre os itens, a maior variação está relacionada  a aplicar os conceitos abordados no jogo, ou seja, pôr em prática  os conhecimentos referentes  ao que está sendo ensinado, um  resultado bastante positivo, pois demonstra que o jogo está  oferecendo a oportunidade para os alunos praticarem os  conceitos estudados.  Durante a aplicação do jogo foi possível observar que alguns  alunos confundiam o conceit o de Backlog do Produto e Backlog  da Sprint, pois quando foi necessário montar o Backlog da Sprint,  estes afirmavam ter realizado esta atividade, mostrando o  Backlog do Produto.   Outra dificuldade relacionada ao Backlog do Produto, foi em  relação a quais i tens deveriam compô -lo, alguns dos alunos  perguntaram se o Backlog deveria conter o que o cliente desejava  ou que eles achavam que o produto deveria ter. Alguns alunos  optaram por inserir no Backlog do Produto os itens que estes  acharam que o produto dever ia ter e outros optaram por inserir  apenas os itens que o cliente solicitava, de acordo com a carta.  Poucos alunos pediram mais informações sobre o produto. Ainda  com relação ao produto, uma dúvida demonstrada pelos alunos é  se era necessário a priorização  dos itens para ser desenvolvido,  alguns realizaram esta priorização.       Figura 5: Avaliação dos objetivos de aprendizagem.  8.2 Analise dos resultados  Os jogos quando utilizados para o ensino proporcionam várias  vantagens, como por exemplo: a possibilidade que o aluno  construa o seu próprio conhecimento a partir da participação;  jogar é algo interessante para os alunos; possibilit a o  desenvolvimento de habilidade necessárias; possibilidade de  identificação de dificuldades dos alunos, além dos demais  benefícios citados [22]. Essas vantagens podem ser confirmadas  no jogo 2TScrum através dos dados coletados.  De acordo com a percepção dos alunos, o jogo os motivou a  estudarem, e possibilitou o avanço por esforço próprio, que de  conforme [10] é algo essencial de um jogo educativo. Percebeu - se também que o jogo acompanha adequadamente o processo de  aprendizagem e que este capturou a atenç ão dos alunos, porém  requer melhorias no design.  Os alunos expressaram positivamente que o jogo foi uma  maneira eficiente de aprender e que eles alcançaram os objetivos', 'requer melhorias no design.  Os alunos expressaram positivamente que o jogo foi uma  maneira eficiente de aprender e que eles alcançaram os objetivos  aplicando as suas competências. Ainda de acordo com os alunos,  o jogo ajudou a reforçar  o conteúdo, a compreensão e aplicação  de Scrum, de forma divertida, desafiadora e imersiva. Porém, o  287', ""jogo requer ajustes, de forma a exigir que o jogador utilize ainda  mais suas habilidades, e despertar um desejo maior de  continuidade após uma interrupção.   A utilização do jogo permitiu a constatação de dificuldades  dos alunos, como a diferenciação dos conceitos de backlog do  produto e backlog da Sprint e insegurança em tomar decisões,  sobre a necessidade de priorização de atividades e sobre os itens  do backlog do produto. Por fim, esses dados coletados  forneceram evidências de que o jogo pode produzir um efeito  positivo no ensino de Scrum.  9 CONCLUSÕES  O jogo 2TScrum, apresentado como uma das principais  contribuições desse trabalho, foi concebido fundamentado na  base teórica apresentada. O trabalho propõe uma ferramenta que  gamefica o processo de ensino -aprendizagem do método ágil  Scrum, possibilitando aos participantes uma experiência prática  e divertida em um projeto simulado de desenvolvimento de  software.  Após a construção do jogo, foram realizadas avaliações sobre  a dificuldade em utilizá-lo, de acordo com as dimensões definidas  no questi onário (experiência do usuário, motivação e  aprendizagem), buscando verificar se o jogo atingiu seu objetivo.  Com os resultados obtidos, foi possível observar que o objetivo  apresentado desta pesquisa, de facilitar o ensino de Scrum por  meio de um jogo de simulação disposto em forma de tabuleiro,  foi atingido.   Um ponto que pode afetar as conclusões desta pesquisa é o  fato de que o jogo não possui a separação clara das tarefas de  acordo com o time Scrum, outras conclusões ou um  melhoramento desta pesquisa p oderia ter sido alcançados com  esta separação.  Atualmente, o 2TScrum possui apenas um cenário de  desenvolvimento simulado, que é o desenvolvimento de um  sistema web para uma biblioteca. Como trabalho futuro é  possível melhorar o design do jogo a partir das  possibilidades de  melhorias identificadas na sua aplicação, tais como a utilização  de níveis de dificuldade e adicionar novos cenários simulando  outros projetos. Uma outra forma de evoluir esse trabalho é  tentar aplicar o jogo com mais participantes, pois  a partir disso  novas conclusões poderão ser extraídas.  REFERÊNCIAS  [1] D. Rezende. 2005. Engenharia de software e sistemas de informação (3  ed.). Brasport, Rio de Janeiro, BR.  [2] R. Pressman. 2011. Engenharia de software: uma abordagem profissional  (7 ed.). McGraw Hill, Porto Alegre, BR.  [3] K. Beck, M. Beedle, A. Bennerkum, A. Cockburn, W. Cunningham, M.  Fowler, J. Grenning, J. Highsmith, A. Hunt, R. Jeffries, J. Kern, B. Marick,  R. Martin, S. Mellor, K. Schwaber, J. Sutherland e D. Thomas. 2001.  Manifesto for Agile Software Development.   [4] M. Soares. 2004. Metodologias ágeis extreme programming e scrum para  o desenvolvimento de software. Revista Eletrônica de Sistemas de  Informação ISSN 1677-3071 DOI: 10.21529/RESI, 3, 1.  [5] F. Siller, e J. Braga. 2013. Software educacional para prática do scrum.  Anais dos Workshops do Congresso Brasileiro de Informática na  Educação, 2, 1, 152-161.  [6] Schwaber, K. and Sutherland, J.  Guia do Scrum. 2011. Retirado de  http://www.scrum.org/Portals/0/Documents/Scrum%20Guides/Scrum%20 Guide%20-%20Portuguese%20BR.pdf  [7]  P. Pereira, P. Torreão, e A. S. Marçal. 2007. Entendendo Scrum para  gerenciar projetos de forma ágil. Mundo PM, 1, 3-11.  [8] S. Hastie, e S. Wojewoda. 2015. Standish Group 2015 Chaos Report.  Chaos, Londres, UK.  [9] V. Mahnic. 2010. Teaching Scrum through team-project work: Students'  perceptions and teacher's observations. International Journal of  Engineering Education, 26, 1, 96 páginas.   [10] V. Wangenheim, C. Gresse, R. Savi, e A. Borgatto. 2013. SCRUMIA - An  educational game for teaching SCRUM in computing courses.  Journal of  Systems and Software, 86, 10, 2675-2687.  [11] P. Gestal, e R. Barros. 2014. Proposta de um simulador para auxiliar no  processo de ensino do Scrum. Anais do X Simpósio Brasileiro de Sistemas  de Informação, 723-736."", ""[11] P. Gestal, e R. Barros. 2014. Proposta de um simulador para auxiliar no  processo de ensino do Scrum. Anais do X Simpósio Brasileiro de Sistemas  de Informação, 723-736.  [12] K. Schwaber, e J. Sutherland. 2013. The Scrum Guide. Retirado de  http://www. scrum. org/Scrum-Guides>.  [13] K. Schwaber. 1995. Scrum development process. OOPSLA'95 Workshop  on Business Object Design and Implementation, Austin, USA.  [14] T. Fullerton, C. Swain, e S. Hoffman. 2004. Game design workshop:  Designing, prototyping, & playtesting games. CRC Press, Flórida, USA.  [15] C. Greenblat, e R. Duke. 1981. Principles and practices of gaming- simulation. SAGE Publications, California, USA.  [16] V. Ruohomäki. 1995. Teamwork Game. Simulation Games and Learning  in Production Management, 82-90.  [17] M. Barros, e C. M. L. Werner. 2001. Gerenciamento de Projetos Baseado  em Cenários: uma abordagem de Modelagem Dinâmica e Simulação. I  Simpósio Brasileiro de Qualidade de Software, 213-224.   [18] C. Aldrich. 2005. Learning by doing: A comprehensive guide to  simulations, computer games, and pedagogy in e-learning and other  educational experiences. John Wiley & Sons, São F rancisco, CA.  [19] A. Silva. 2010. Jogo educacional para apoiar o ensino de técnicas para  elaboração de testes de unidade. Tese de Mestrado. Universidade do Vale  do Itajaí, Santa Catarina, Brasil.   [20] M. Gramigna. 2007. Jogos de Empresas (2 ed.). Pearson, Londres, UK.  [21] M. Averill, e K. David. 200 Simulation modeling and analysis. Mc‐Graw  Hill, Nova Iorque, USA.  [22] R. Grando. 2004. O jogo e a matemática no contexto da sala de aula.   Paulus, São Paulo, BR.  [23] P. Schoeffel. 2014. PizzaMia: Dinâmica Vivencial para Apoio ao Ensino de  Gerenciamento de Projetos Baseado no PMBOK. CSBC-Congresso da  Sociedade Brasileira de Computação.  [24] D. Lübke, e K. Schneider. 2005. Agile hour: teaching XP skills to students  and IT professionals. International Conference on Product Focused  Software Process Improvement, 1-12.  [25] G. Rodriguez, A. Soria, e M. Campo. 2015. Virtual Scrum: A teaching aid  to introduce undergraduate software engineering students to  Scrum. Computer Applications in Engineering Education, 23, 1, 147-156.  [26] A. Dantas, M. Barros, e C. Werner. 2004. Treinamento experimental com  jogos de simulação para gerentes de projeto de software. XVIII Simpósio  Brasileiro de Engenharia de Software, 23-38.  [27] R. Wazlawick. 2014. Metodologia de Pesquisa para Ciência da  Computação (2 ed.). Elsevier, Rio de Janeiro, BR.  [28] R. Prates, e S. Barbosa. 2003. Avaliação de Interfaces de Usuário – Conceitos e Métodos. Jornada de Atualização em Informática do  Congresso da Sociedade Brasileira de Computação, 28 páginas.  [29] M. Marconi, e E. Lakatos. 2007. Técnicas de Pesquisa (6 ed.). Editora Atlas  S.A, São Paulo, BR.  [30] T. Savi, C. Wangenheim, V. Ulbrich, e T. Varzin. 2011. Avaliação de jogos  voltados para a disseminação do conhecimento. Tese de Doutorado.  Univerdidade Federal de Santa Catarina, Santa Catarina, Brasil.   [31] N. Iuppa, e T. Borst. 2012. End-to-end game development: creating  independent serious games and simulations from start to finish. CRC  Press, Burlington, USA.  [32] R. Barton. 1973. Manual de simulação e jogo. Vozes, Rio de Janeiro, BR.  [33] A. Filardi, e A. Traina. 2008. Montando questionários para medir a  satisfação do usuário: avaliação de interface de um sistema que utiliza  técnicas de recuperação de imagens por conteúdo. Proceedings of the VIII  Brazilian Symposium on Human Factors in Computing Systems , 176-185.  [34] A. Gil. 2002. Como elaborar projetos de pesquisa (4 ed.). Editora atlas  S.A., São Paulo, BR.        288""]","[""‘2TSCRUM’: A BOARD GAME TO TEACH SCRUM This  briefin  reports  scieitfc  evideice oi the beiefts of teach Scrum throunh names. FINDINGS \uf0b7 The practce of the game made  possible to confrm that the game  allows the constructon of knowledge  and that is something interestng to  students. \uf0b7 2TSCRUM motiated the users to  study more about Scrum and made  possible their adiance. \uf0b7 Was also notce that the game  adequately monitors the learning  process and that it captured students'  attenton. \uf0b7 The game is something interestng to  students, was an efcient way to learn and enable players to deielops  necessaries skills and achieie goals by  applying them. \uf0b7 The game helps to reinforce content,  an understanding of the applicaton of  Scrum, in a fun, challenging and  immersiie way. \uf0b7 The use of the game allows see  difcultes that the players haie about the difference of rroduct backlog and  Sprint backlog. \uf0b7 The game showed that students are  insecure to making decisions about  prioritie actiites and which items  should not be in the product backlog.    Who is this briefin  or? Software engineering practtoners who want to make decisions about ways to  haie practce on the framework Scrum  based on scientfc eiidence. Where the fidiins come  rom? All fndings of this briefng were  extracted from the game applicatons  and questonaries’ conducted by Brito  et al.   What is iicluded ii this briefin? The main fndings of the use of a game  board to teach scrum. What is iot iicluded ii this briefin? Additonal informaton not supported  by the fndings of the use of the game  as well as descriptons about it. To access other evideice briefins  oi software einiieeriin: http:////www.lia.ufc.br//ccbsoft2017//xxxi -sbes//""]","**Title: Enhancing Scrum Learning through Board Games**

**Introduction:**
This evidence briefing presents findings from a study that explores the effectiveness of a board game, named 2TScrum, designed to teach the Scrum framework in an engaging and practical manner. The goal is to provide educators and software engineering practitioners with insights into how gamification can enhance the learning experience of agile methodologies.

**Main Findings:**
The study involved 70 participants, including undergraduate and postgraduate students, who engaged with the 2TScrum board game. Key findings include:

1. **Engagement and Motivation**: Students reported high levels of motivation, with 95.7% indicating that the game’s content was relevant to their interests. The interactive nature of the game kept them engaged, with 94.3% agreeing that the game maintained an appropriate pace and level of challenge.

2. **Learning Outcomes**: The game significantly enhanced students' understanding of Scrum concepts. Participants showed marked improvement in their ability to apply Scrum practices, particularly in creating and managing a product backlog, planning sprints, and conducting retrospectives. The average increase in knowledge levels across various learning objectives was substantial, with the highest gains observed in applying Scrum principles in practical scenarios.

3. **Practical Application**: The board game allowed students to simulate real-world project management scenarios using Scrum, which facilitated experiential learning. Players had to make decisions that affected project timelines and budgets, mimicking the complexities of actual software development.

4. **Feedback and Iteration**: Initial testing with experienced Scrum practitioners led to refinements in the game, such as the inclusion of more decision-making scenarios and adjustments to time and budget constraints. This iterative development process ensured that the game accurately reflected Scrum practices.

5. **Challenges Identified**: Some students struggled with distinguishing between the product backlog and sprint backlog, highlighting the need for clearer instructional support. Additionally, there was uncertainty about prioritizing backlog items, indicating areas for further instructional focus.

**Who is this briefing for?**
This briefing is intended for educators, software engineering practitioners, and curriculum developers who are interested in innovative teaching methods for agile methodologies, particularly Scrum.

**Where the findings come from?**
The findings are derived from an experimental evaluation of the 2TScrum board game conducted with 50 undergraduate students in Information Systems and 20 postgraduate students in Software Engineering at the Federal University of Ceará.

**What is included in this briefing?**
This briefing includes insights into the design and implementation of the 2TScrum board game, its impact on student engagement and learning outcomes, and the iterative process of refining the game based on feedback from participants.

To access other evidence briefings on software engineering:
[http://www.lia.ufc.br/~cbsoft2017/](http://www.lia.ufc.br/~cbsoft2017/)

For additional information about the Group of Computer Networks, Software Engineering, and Systems (GREat):
[http://www.great.ufc.br](http://www.great.ufc.br)

**Original Research Reference:**
A. Brito, J. Vieira. 2017. ‘2TScrum’: A Board Game to Teach Scrum. In Proceedings of ACM SBES Conference, Fortaleza, Ceará, Brazil, September 2017. https://doi.org/10.1145/3131151.3131177"
"['A Strategy for Functional Defect Prediction in Homogenous  Datasets: A case study in the SIGAA academic system   A. Pontes  Universidade Federal da Paraíba  58058-600  Brazil  alan@sti.ufpb.br  C. Siebra  Universidade Federal da Paraíba  58058-600  Brazil  clauirton@ci.ufpb.br  M. Bittencourt  Universidade Federal da Paraíba  58058-600  Brazil  mchl.bittencourt@gmail.com     ABSTRACT  The optimization of test sequence s is an important  resource to  improve the test efficiency of complex software  systems. This  optimization ca n be carried out by means of defect  prediction  techniques, which are able to identify modules with a  higher  chance to present problems, so that these modules can be  first  evaluated. The current literature brings some proposals of   algorithms with high accur acy for defect prediction. However   they present a poor generalization power, since problems of   overfitting are hidden due to the nature of the evaluation methods  that are  used. The aim of this work is to propose a modelling  strategy based on  more homogeneo us datasets to trainee defect  prediction models aimed at  functional bugs . The object of study  for evaluation of our proposal is  a complex system for academic  management (SIGAA), which  is used in several Brazilian  universities. The application in  successive versions of this system  shows that our proposal is  able to identify the best approach for  defect prediction, which  in fact indicates the most problematic  modules, supporting in this way the construction of optimal test  sequences.  CCS CONCEPTS  •Software an d its engineering  → Software verification and  Validation → Software defect analysis  KEYWORDS  Defect prediction, software test, test automation, learning  algorithms.    ACM Reference format:  A. Pontes, C. Siebra, and M. Bittencourt. 2017. A Strategy for Functional  Defect Prediction in Homogeneous Datasets: A case study in the SIGAA  academic system . In Proceedings of 2nd Brazilian Symposium on  Systematic and Automated Software Testing , Fortaleza, Ceará, Brazil,  September2017 (CBSOFT’17), 10 pages.  1 INTRODUÇÃO  Os sistemas computacionais estão cada vez mais presentes em  nosso dia a dia e a tecnologia de software tem proporcionado um  grande avanço no modo de vida humana. Tarefas antes nunca  imaginadas podem ser atualmente executadas pelo homem com  auxílio de siste mas comput acionais. Mas, para se alcançar   objetivos satisfatórios, os sistemas devem estar livres de erros  (bug free) [1]. Este s erros devem ser evitados e/ou encontrados e  corrigidos o quanto antes. Assim, o principal motivador do estudo  proposto está relacionado aos erros.  Os erros podem ser entendidos como algo que não é certo ou  correto. Para alguns autores [2], um erro de software é uma falha  ou defeito em um programa de computador ou sistema que produz  um resultado incorreto ou inesperado, ou faz com que ele se  comporte de maneira inesperada . Sabe -se que os erros podem  estar presentes e podem ser encontrados em qualquer fase do  desenvolvimento de um software, tornando-se a principal causa de  todos os problemas que podem vir a afetar o sistema como um  todo.   Vários danos podem ser causados por erros, como a perda de  tempo e dinheiro [3]. Ademais, eles podem levar à entrega de um  sistema que não atenda as expectativas do cliente. Os resultados  esperados podem também divergir dos resultados reais e o sistema  pode gerar resultados imprevisí veis ou incorretos. Então, para  tentar evitar as situações descritas, os erros devem ser evitados, ou  ainda, encontrados o mais cedo possível. A identificação  antecipada de falhas é conhecida como  previsão de falhas de  software e este tema de pesquisa investiga as características de  segmentos de códigos individuais para identificar aqueles  segmentos que são propensos a falhas ou para prever o número de  falhas em cada segmento [4,8].  Permission to make digital or hard copies of all or part of this work for', 'segmentos que são propensos a falhas ou para prever o número de  falhas em cada segmento [4,8].  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM  must be honored. Abstracting with credit is permitted. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.    SAST, September 18–19, 2017, Fortaleza, Brazil   © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503-5302-1/17/09…$15.00   https://doi.org/10.1145/3128473.3128474', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.    2    Neste contexto, u ma proposta para auxil iar o time de testes  consiste em tentar prever os defeitos de um software antes da  realização dos  testes. A  técnica de  predição de defeitos de  software (SDP – Software Defect Prediction) consiste no processo  de localização de potenciais módulos defeituosos em software [2].  A literatura atual discute diversas propostas de algoritmos com  alta acurácia para predição de defeitos [9,10]. Contudo, eles  apresentam um fraco poder de generalização, uma vez que  problemas de overfitting ficam escondidos devido à natur eza dos  métodos de avaliação utilizados. O objetivo deste trabalho é  propor uma estratégia de modelagem baseada em bases de  treinamento mais homogêneas, as quais são utilizadas na criação  de modelos preditivos direcionados a bugs funcionais. A  avaliação da nossa proposta é realizada em um complexo sistema  de gerenciamento acadêmico, conhecido como SIGAA, o qual é  utilizado em d iversas universidades brasileir as. O principal  objetivo é que os modelos possam ser generalizados, não estando  presos ao domínio responsável por sua criação.  O restante do artigo está es truturado da seguinte forma. A  Seção 2 apresenta a abordagem utilizada na pesquisa, descrevendo  os algoritmos e classes de predição utilizadas, assim como a  técnica de validação. A Seção 3 descreve os d ados utilizados na  pesquisa, os quais s ão obtidos do sistema SIGAA. A S eção 4  apresenta as métricas  utilizadas para a caracterização das  instâncias de software  retiradas do SIGAA . A Seção 5 discute o  processo de identificação de bugs de código, o qual indi ca quais  instâncias devem ser utilizadas na construção do modelo de  predição seguinte, associado aos bugs funcionais, de modo que a  base se torne mais homogênea . Em adição, o próprio processo de  predição de bugs funcionais é descrito. A Seção 6 descreve os   principais trabalhos relacionad os a esta pesquisa, enquanto a  Seção 7 conclui este trabalho ressaltando os principais pont os  abordados.  2 MÉTODO DE PESQUISA  Este trabalho está baseado no uso dos conceitos de  SDP  juntamente com técnicas de Inteligência Artificial (IA) no auxílio  ao processo de automatização de testes. De fato, diversas técnicas  de IA já  vêm sendo utilizadas n o processo de automa ção de teste  [17], de forma que alguns autores [18] consideram a área de testes  de software  como uma das mais apr opriadas da engenharia de  software para o uso de IA.  Para o trabalho tem -se a avaliação empírica da acurácia da   previsão de defeitos usando a té cnica de aprendizado  de máquina  e usando também mé todos estatísticos. Para  isso, foi utilizada  inicialmente a abordagem da validação cruzada (cross validation).  Em um segundo momento, a validação utilizou as versões do  sistema SIGAA , Figura 1 (a) , onde versões passadas eram  utilizadas no treinamento de um modelo que pudesse prever os  erros de uma versão futura . Os  atributos de entrada (dados de  entrada) s ão tratados como  valores numéricos e contí nuos,  enquanto que a saída assume basicamente valores discretos.  Com rela ção aos classificadores , Figura 1 (e) , foram  utilizadas as seguintes abordagens: Ár vores de Decis ão (J48), o   Algoritmo do Vizinho Mais Pró ximo (iBk), o classificador de  NaiveBayes (NB) e ainda o OneRule (OneR). Utilizamos também  um dos métodos da Máquina de Vetores de Suporte conhecido  como Sequential minimal optimization (SMO) que pode  manipular tanto en tradas contí nuas como entradas discre tas. A  Rede Neural (RNA) foi outra técnica utilizada, uma vez que ela  pode aceitar entradas e produzir saí das em ambos os modos  contínuos e discretos. Assim todos os classificadores atenderam  aos requisitos. A Tabela 1 mostra o resultado  (validação cruzada)  da execu ção dos algoritmos com 5 diferentes op ções para  classificação das instâncias de entrada.   Uma caracterí stica encontrada no SQ consiste na', 'da execu ção dos algoritmos com 5 diferentes op ções para  classificação das instâncias de entrada.   Uma caracterí stica encontrada no SQ consiste na  identificação de classes J ava que possuem bugs, além de também  permitir a identificação d a quantidade de bugs dessas classes.  Assim, foi possí vel verificar classes que n ão possuem erros, que  no caso s ão a maioria das cla sses do sistema. Também pudemos  diferenciar as classes com apenas um bug e com diversos bugs, as  quais eram minoritárias. Devido a essa disparidade na quantidade   de bugs, verificou-se que para utiliza ção dos algoritmos de  classificação, era necessá rio atentar para a escolha da forma para  representar os dados em cada conjunto, e q ue quando criada  conseguisse um bom desempenho nesses conjuntos.  A Tabela 1 mostra na primeira coluna  o agrupamento em três  níveis (zero, um e m aior igual a dois); na segunda, em cinco    Figura 1: Processo de extração e análise de dados.', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.     3  níveis (zero, um, entre dois e tr ês, entre quatro e nove, e igual  maior que 1 0); na terceira coluna, c inco níveis (zero, um, entre  dois e cinco, entre seis e dez, maior que dez); na quarta, em quatro  (zero, um, entre dois e nove, igual maior que dez); e na quinta  coluna, dois níveis (zero e um). Segundo o experiment o, pode-se  concluir que a melhor forma de  classificação apresentada foi a  abordagem em dois níveis. Porém esta escolha foi descartada uma  vez que se coloca em um mesmo  patamar as classes que possuem  apenas um erro com outra classe  que possui diversos erros. Por  exemplo, classes que possuem  30 erros. Desta forma, o segundo   melhor entre os ní veis testados foi eleito. Como é possível  perceber, o que apresentou o segundo melhor desempenho, para o  mesmo conjunto de  algoritmos e mesmo conjunto de dados, foi a  abordagem 0, 1 e >=2. A principal mé trica ut ilizada na  comparação foi a MTPR.  Tabela 1: Resultados da execução dos algoritmos selecionados.  * CC: Instâncias Classific adas Corretamente, MAE: Erro Mé dio Absoluto, RMSE:  Erro Quadrático Médio, MTPR: Média Aritmética da Taxa de Verdadeiros Positivos.   Para a montagem da Tabela 1, através da validação cruzada,   foi utilizada a vers ão 2-15-0 do sistema (Tabela 2), a qual  possui  6131 inst âncias. Esta ver são foi escolhida por possuir o maior  número de inst âncias dentre todas as versões disponí veis. O  atributo de classificação utilizado foi à métrica bugs. A partir da  tabela foi possí vel identificar o  agrupamento que apresentou o  melhor desempenho, e assim indicar o caminho a ser seguido na  pesquisa.  3 DADOS PARA PESQUISA  Para a realização da pesquisa foi necessário a extração de métricas  obtida do sistema acad êmico da UFPB, mas especificamente do  projeto SIGAA. Este sistema informatiza todas as tarefas  inerentes à atividade acad êmica da instituição. Além das métricas  extraídas diretamente do c ódigo-fonte do sistema, tamb ém foram  utilizados os dados hist óricos de cadastros de bugs funcionais  reportados pelo time de desenvolvimento. A Tabela 2 descreve os  dados que  são objetos da pesquisa e foram extraído s conforme  verificado na Figura 1 em (1).  A primeira coluna da Tabela 2 indica  as respectivas ver sões  do projeto SIGAA em determinado perí odo. Cada  número de  versão indica a release no qual o sistema foi para produção . Para  cada uma destas vers ões o SonarQube (SQ) fez a análise de  código e foi possí vel extrair as informa ções referentes à   quantidade de classes do projeto  (segunda coluna) , bem como  a  quantidade de bugs de código -fonte indicados na ferramenta e  mostrada na terceira coluna da tabela. A quarta  coluna representa  o levantamento/mapeamento de bugs funcionais cadastrados pela  equipe separados por versão. A ú ltima coluna demonstra que o  projeto é majoritariamente em J ava e que o objeto de estudo s ão  estas classes Java.  Tabela 2: Conjunto de dados usados na pesquisa  Versão  Sistema  Quantidade  de classes  Bugs de  código  Bugs  funcionais  Ling.  2-12-0 5980 709 21 Java  2-13-0 6100 730 08 Java  2-14-0 6104 731 17 Java  2-15-0 6131 734 14 Java  2-16-0 5527 665 25 Java  2-17-0 5531 665 01 Java  2-18-0 5531 666 11 Java  2-19-0 5590 669 11 Java  4 MÉTRICAS UTILIZADAS  As mé tricas s ão u ma importante ferramenta de auxílio para  engenharia de software. O termo métrica é definido em [13] como  a medida da extensão ou grau em que um produto possui  ou exibe  certa característica. Mé tricas podem surgir como u ma resposta  para a necessidade de avaliar um artefato de forma objetiva e elas  são utilizadas no desenvolvimento de sistemas  computacionais  tendo como i ntuito assegurar fatores como a manutenibilidade,  suportabilidade e disponibilidade  [11,12]. Segundo Jan et al [14],  as mé tricas padr ões medem as caracterí sticas objetivas do  software e podem indicar um direcionamento da qualidade dos', 'suportabilidade e disponibilidade  [11,12]. Segundo Jan et al [14],  as mé tricas padr ões medem as caracterí sticas objetivas do  software e podem indicar um direcionamento da qualidade dos  produtos implementados.  Para obter as mé tricas desejadas, nos apoiamos no software   SonarQube (SQ) o qual está disponível em  http://www.sonarqube.org. O SQ consiste de uma plataforma de   inspeção contínua e que é capaz de realizar o cá lculo de diversas  métricas. Segundo  Ferencet al  [15], o SQ é  uma plataforma de   código aberto p ara gerenciar a qualidade do có digo, o qual  possibilita a extração de informações importantes sobre o projeto.  Neste contexto , uma sele ção inteligente de mé tricas de software  antes de construir um modelo de predi ção de defeito é importante,  pois é  provável que se obtenha um melhor  resultado final  [16].  Algumas métricas disponibilizadas pelo SQ  e extraídas conforme  Figura 1 (b) são listadas na Tabela 3.', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.    4    Tabela 3: Métricas disponibilizadas pelo SQ e utilizadas na  pesquisa  Tipo de  Métrica  Métricas  Complexity class-complexity, complexity,  complexity-in-funciotns, function- complexity  Documentation comment-lines, comment-lines-density,  public-documented-api-density,  public-undocumented-api  Duplications duplicated-blocks, duplicated-files,  duplicated-lines, duplicated-lines-density  Issues blocker-violations, critical-violations, info- violations,major-violations, minor- violations, violations  Maintainability effort-to-reach-maintainability-rating-a,  code-smells, sqale-debt-ratio, sqale-index,  sqale-rating  Reliability reliability-remediation-effort  Security security-rating,  security-remediation-effort, vulnerabilities  Size accessors, classes,functions, lines, ncloc,  public-api, statements  5 PROPOSTA DE PREDIÇÃO  A predição está dividida em duas partes: predição para os bugs de  código-fonte indicado s pelo SQ e a predi ção de defeitos  de  software para bugs funcionais reportados pelo time.  5.1 Predição para os Bugs de Código-Fonte   Conforme ilust rado na Figura 1, em (a) tem -se a obtenção  de  informações do sistema acadêmico da instituição. O dispositivo de  controle de vers ão possibilita o acesso a todo có digo-fonte do  projeto e também faz o controle das versões do sistema. O sistema  está dividido por versões com suas respectivas tags de produção.  Já na etapa (b), de vido ao uso da ferramenta SQ em cada  versão, é possível a  extração das diversas m étricas usadas  na  pesquisa. É importante destacar que o SQ trouxe as informa ções  de mé tricas do projeto apenas referentes as classes Java d o  sistema. Isto é, foram excluídas da análise do SQ as .jsp, os .jsf, os  .jrxml, os javascripts, etc. As mé tricas utilizadas estão  apresentadas na Tabela 3.  Uma das métricas disponíveis e que serviu como atributo para  a classificação foi a mé trica bugs. O SQ ao fazer a varredura do  código identificou a quantidade de erros encontrados nas classes  analisadas. N o que diz respeito as outras mé tricas, foi realizado  um pré-processamento (d), ou seja, foi realizada uma limpeza dos  dados extraí dos, pois o SQ disponibiliza várias mé tricas e nem  todas foram preenchidas pela ferramenta. Além disso, percebeu-se  que em determinados conjuntos de métricas existia depend ências  que fizeram com que algumas métricas fossem preenchidas com o  mesmo valor de outra s, fazendo com que existisse repeti ção das  métricas que são usadas como atributos na predição. Isto interferia  no preditor  devido à  alta correlação entre as métricas . E ntão foi  necessário fazer uma averigua ção para a escolha do  conjunto de  métricas na delineação do problema.  Além das escolhas das métricas, conforme Tabela 3, foi  necessário verificar a repeti ção dos dados extraí dos pela  ferramenta ao preencher as mé tricas em diferentes ver sões, pois  devido ao curto espaço  de tempo entre algu mas ver sões do  sistema, percebeu-se repeti ção destes dados. Com isso, fez -se  necessário a té cnica de remo ção dessas repetições. Determinadas  versões do sistema não sofreram grandes mudanças e com isso os  mesmo err os encontrados foram propagados  nas vers ões  seguintes. Inicialmente utilizamos as sete primeiras ver sões do  sistema (Tabela 2) para a cria ção da base a ser utilizada na  predição. Com os modelos criados (e), realizou -se a valida ção da  predição (f) na ú ltima versão do SIGAA (2 -19-0). Esta vers ão  desempenhou a função  da ba se de teste. Para isto, também foi  utilizada a abordagem d e remoção de dados repetidos nesta  versão.   Tanto as bases de treinamento e teste criadas apresentaram  problemas de desbalanceamento. De fato, este problema é clássico  neste ti po de domínio, uma vez que o número de instâncias da  classe sem erros é muito maior do que as demais (com um ou  mais erros). Para resolver esse problema, fizemos um', 'neste ti po de domínio, uma vez que o número de instâncias da  classe sem erros é muito maior do que as demais (com um ou  mais erros). Para resolver esse problema, fizemos um  undersampling (diminuição) da classe majoritária.  Ou seja, na  classe sem erros a qual possu ía 5302 instâncias na base de  treinamento e 4643 na base de testes. A ideia foi que tais números  ficassem perto dos números apresentados abaixo:  \uf0b7 Base de treinamento: 546 instâncias com um erro e 256  instâncias com dois ou mais erros;  \uf0b7 Base de testes:  483 instâncias com um erro e 186  instâncias com dois ou mais erros.  As Tabelas de 4 a 7 mostram o s resultados para 4 versões do  undersampling, o qual gerou subconjuntos de instâncias da classe  sem erros. A formação de tais subconjuntos foi feito através de  uma e scolha aleatória, sendo o tamanho de tais subconjuntos   (treinamento/teste) igual a 356 /229 (Tabela 4), 401/334 (Tabela  5), 566/505 (Tabela 6) e 802/669 (Tabela 7). Tais tabelas mostram  os resultados da previsão de bugs do SQ para aversão 2 -19-0 do  sistema (g).  Tabela 4: Resultados da execução dos algoritmos de predição  usando undersampling (356/229) – Bugs SQ.   J48 OneR Ibk(5) SMO RNA NB  CC 76,95% 93,76% 36,64% 54,90% 89,09% 39,31%  MAE 0,1957 0,0416 0,4119 0,3484 0,0866 0,4028  RMSE 0,3251 0,2039 0,5458 0,4474 0,2441 0,6228  TPR 0 0,310 1,000 0,026 0,044 0,987 0,886  TPR 1 0,973 0,983 0,342 0,959 0,925 0,201  TPR \uf0b32 0,806 0,742 0,849 0,108 0,683 0,285  MpTPr 0,769 0,938 0,366 0,549 0,891 0,393  MTPR 0,6963 0,9083 0,4056 0,3703 0,8650 0,4573  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, TPR \uf0b32: Taxa  de Verdadeiros P ositivos para a Cla sse Com Dois ou Mais Bugs, MpTPr: Média  Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da Taxa de  Verdadeiros Positivos.', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.     5  Os resultados descritos nas tabelas mostram que o algoritmo  OneR foi o que apresentou o melhor desempenho, sendo q ue ele  foi capaz de classificar corretamente mais do que 93% das  instâncias em todos os experimentos realizados. Outra conclusão é  que a variabilidade da acurácia é insignificante ( 93,76%; 94,42%;  95,23%; 95,81%) quando aumentamos o número de instâncias na   classe majoritária, mas foi percebido que tal acurácia tende a  aumentar com o aumento de tais instâncias. Tal fato indica que o  sistema tem maior facilidade de acertar a classificação das  instâncias da classe sem erro.  Tabela 5: Resultados da execução dos algoritmos de predição  usando undersampling (401/334) – Bugs SQ.   J48 OneR Ibk(5) SMO RNA NB  CC 70,69% 94,42% 34,80% 50,35% 89,13% 45,36%  MAE 0,2285 0,0372 0,4035 0,3627 0,0841 0,3624  RMSE 0,352 0,1929 0,5369 0,4632 0,2384 0,5914  TPR 0 0,266 1,000 0,123 0,069 0,925 0,925  TPR 1 0,977 0,983 0,325 0,952 0,957 0,193  TPR \uf0b32 0,796 0,742 0,812 0,102 0,661 0,285  MpTPr 0,707 0,944 0,348 0,503 0,891 0,454  MTPR 0,6796 0,9083 0,420 0,3767 0,8476 0,4676  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Mé dio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, TPR \uf0b32: Taxa  de Verdadeiros P ositivos para a Classe Com Dois ou Mais Bugs, MpTPr: Média  Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da Taxa de  Verdadeiros Positivos.  Tabela 6: Resultados da execução dos algoritmos de predição  usando undersampling (566/505) – Bugs SQ.   J48 OneR Ibk(5) SMO RNA NB  CC 68,91% 95,23% 31,69% 50,34% 92,67% 51,96%  MAE 0,2519 0,0318 0,4101 0,3665 0,0644 0,3205  RMSE 0,3732 0,1783 0,5335 0,4672 0,1997 0,5569  TPR 0 0,430 1,000 0,111 0,236 0,990 0,907  TPR 1 0,925 0,983 0,342 0,938 0,988 0,205  TPR \uf0b32 0,780 0,742 0,812 0,102 0,597 0,285  MpTPr 0,689 0,952 0,317 0,503 0,927 0,520  MTPR 0,7116 0,9083 0,4216 0,4253 0,8583 0,4656  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de V erdadeiros Positivos para a Classe Com Bugs, TPR \uf0b32: Taxa  de Verdadeiros P ositivos para a Classe Com Dois ou Mais Bugs, MpTPr: Média  Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da Taxa de  Verdadeiros Positivos.  Tabela 7: Resultados da execução dos algoritmos de predição  usando undersampling (802/669) – Bugs SQ.   J48 OneR Ibk(5) SMO RNA NB  CC 79,52% 95,81% 38,49% 55,97% 94,17% 55,98%  MAE 0,2058 0,0279 0,4055 0,3411 0,0517 0,2933  RMSE 0,3092 0,167 0,5247 0,4391 0,1796 0,5331  TPR 0 0,789 1,000 0,288 0,888 1,000 0,910  TPR 1 0,834 0,983 0,362 0,284 0,959 0,180  TPR \uf0b32 0,715 0,742 0,790 0,097 0,688 0,285  MpTPr 0,795 0,958 0,385 0,560 0,942 0,560  MTPR 0,7793 0,9083 0,48 0,423 0,8823 0,4583  Outro fat o que chama a atenção é o valor da Taxa de  Verdadeiros Positivos  (0,742) para a classe com dois ou mais  bugs (TPR\uf0b32). Tal fato ocorreu em todos os 4 experimentos, onde  a percentagem de instâncias de treinamento desta classe era  sempre menor. Ou seja, esse número menor influiu no  aprendizado do sistema para esta classe em específico. A Figura 2  apresenta a Matriz de Confusão ( Confusion Matr ix) para os  resultados da Tabela 7 do algoritmo OneR.  Esta matriz é lida da seguinte forma.  A diagonal principal  mostra os acertos para cada uma das classes. Ou seja, 138  instâncias da classe “c”, que representa instâncias com dois ou  mais erros, foram cor retamente classificadas em tal classe,  enquanto 48 foram erroneamente classificadas como sendo da  classe b (classes com um erro). Ou seja, o sistema mostra que é  difícil diferenciar a quantidade de erros de uma classe entre um ou  mais. Note também que nenh uma instância erroneamente', 'classe b (classes com um erro). Ou seja, o sistema mostra que é  difícil diferenciar a quantidade de erros de uma classe entre um ou  mais. Note também que nenh uma instância erroneamente  classificada foi indicada como da classe sem erros. Isso mostra  que se o sistema fosse binário, indicando apenas se a classe possui  ou não erros, ele teria uma acurácia aparentemente de 100% neste  experimento.    a b c  \uf0acclassificado como  669 0 0 | a = zero  0 475 8 | b = um  0 48 138 | c = doisEmaior  Figura 2: Matriz de Confusão do OneR, na Tabela 7.    Outra tentativa foi refazer a predição, mas desta vez utilizando  a base completa para criar os modelos.  Ou seja, não foi realizado   nenhum tipo de balanceamento de modo que temos uma  majoração da classe com ausência de bugs (Figura 3).             Figura 3: Distribuição das instâncias nas três classes do  domínio, mostrando o alto nível de desbalanceamento.    Na base de  treinamento dos modelos foram colocadas as sete  primeiras vers ões do sistema retirando apenas os dados  duplicados. A base de  teste foi criad a conforme os dados foram      Sem erro            Um erro                         2 ou mais erros', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.    6    extraídos pelo SQ  da ú ltima ver são apenas. Desta base  também  foram retiradas  as possí veis duplica ções. A quantidade de  instâncias encontradas foi 6104 para criação dos modelos (base de  treinamento) e 5154 para a base de teste.  Os gráficos da Figura 3  mostram a distribuição das instâncias nas classes, em cada uma  das bases, e destaca o grau de desbalanceamento das mesmas.  O resultado da  execução é apresentado  na Tabela 8.  Como  pode ser observado, o uso de toda a base com dados  desbalanceados não influencia na acurácia do algoritmo OneR,  Pelo contrário, tal acurácia foi aumentada em todos os a lgoritmos,  ficando acima dos 98 % no algoritmo OneR. A literatura [19]  mostra que trabalhos que possuem este nível de acurácia com  dados desbalanceados possuem uma baixa taxa de verdadeiros  positivos para as classes minoritárias . Porém, isso n ão é  observado no nosso modelo, onde TPR1 tem um valor de  0,981 e  TPR\uf0b32 é mantido em 0,742.   Tabela 8: Resultados da execução dos algoritmos de predição  para versão 2-19-0 usando todas as instâncias – Bugs SQ   J48 OneR Ibk(5) SMO RNA NB  CC 93,05% 98,91% 87,60% 88,41% 98,29% 85,68%  MAE 0,0863 0,0072 0,1693 0,2555 0,0138 0,0958  RMSE 0,1975 0,0851 0,2783 0,3276 0,0945 0,3036  TPR 0 0,9930 1,0000 0,9150 1,0000 1,0000 0,9450  TPR 1 0,4090 0,9810 0,5060 0,0000 0,9740 0,1620  TPR \uf0b32 0,5860 0,7420 0,7630 0,0750 0,5860 0,2900  MpTPr 0,931 0,989 0,876 0,884 0,983 0,857  MTPR 0,6626 0,9076 0,7280 0,3583 0,8533 0,4656  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, TPR \uf0b32: Taxa  de Verdadeiros P ositivos para a Classe Com Dois ou Mais Bugs, MpTPr: Média  Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da Taxa de  Verdadeiros Positivos.    A análise da Matriz de Confusão também comprova este fato.  Podemos observar (Figura 4) que das 186 instâncias da classe com  dois ou mais erros, 48 foram classificadas c omo sendo da class e  com um erro; enquanto apenas 8 instâncias que são da classe com  um erro foram erroneamente clas sificadas na classe de dois ou  mais erros. Ou seja, mesmo a base estando desbalanceada, o nosso  modelo foi capaz de aprender  e classificar as instâncias existentes  com um bom nível de acurácia. Desta forma, nossa hipótese é que  mesmo a base desbalanceada p ode ser utilizada na predição de  classes com bugs identificados pelo SQ.    a b c  \uf0acclassificado como  4543 0 0 | a = zero  0 417 8 | b = um  0 48 138 | c = doisEmaior  Figura 4: Matriz de Confusão do algoritmo OneR utilizando a  base de dados ilustrada na Figura 3.  5.2 Predição de Defeitos para Bugs Funcionais  Com rela ção aos erros rep ortados pela equipe, observa -se na  Figura 1 que a etapa (c) consiste na tarefa de rastreabilidade  dos  bugs e tem o intuito de determinar os defeitos  de sistema  reportados pelo time. Foi realizada, então, a ligação do erro a uma  ou mais classes Java do sistema. Este tipo  de bug foi denominado  de mé trica de bug funcional. Assim,  cada classe modificada  devido à correção do erro recebeu um rótulo bug. Percebeu-se que  parte dos erros cadastrados estavam ligados apenas a uma classe  do sistema, mas existiam ainda situações que um bug reportando e  corrigido alterava mais de  uma classe. Assim, em nosso  mapeamento, o rótulo  bug foi atribu ído a uma ou m ais classes  Java que tenha m sido altera das devido à correção do erro  reportado pela equipe.  Na fase de pré -processamento (d), realizou -se a adi ção dos  bugs funcionais a serem usados  na predi ção. Ou seja, foram  inseridos os bugs funcionais encontrado s pela equipe em cada  versão do sistema , ligando estes a classe que  foi alterada. As  etapas seguintes ( e, f e g) seguiram de forma aná loga os mesmos', 'inseridos os bugs funcionais encontrado s pela equipe em cada  versão do sistema , ligando estes a classe que  foi alterada. As  etapas seguintes ( e, f e g) seguiram de forma aná loga os mesmos  procedimentos realizados nas etapas da Seção 5.1. Porém, a tarefa  foi modificada já que o objetivo neste momento era  prever os  erros funcionais. Ou seja, a classificação das instâncias deveria ser  realizada em apenas duas classes: com erros e sem erros.  Diferentemente do  que ocorre com a predição dos bugs do  SQ, o uso de uma base desbalanceada não retorna uma boa  predição, como mostrado na Tabela 9. As seis primeiras versões  (2-12-0 até  2-17-0) foram utilizadas para compor a base de  treinamento, sendo que apenas os dados duplicados foram  retirados. Deste modo foram utilizadas 6070 instâncias, das quais  86 possuíam bugs funcionais. As duas últimas versões (2 -18-0 e  2-19-0) foram utilizadas para compor a base de testes, também  com a retirada dos dados duplicados. Neste caso tivemos um total  de 5369 instâncias, das quais 22 possuíam bugs funcionais. De  acordo com a Tabela 9, alguns preditores (J48, OneR, Ibk e SMO)  não foram capazes de fazer uma predição mínima na base de teste  proposta. Ou seja, parte dos algoritmos erraram todas as instâncias  da classe minoritária.  Tabela 9: Resultados da execução dos algoritmos de predição  para as duas últimas versões do sistema usando o total de  instâncias – Bugs Funcionais   J48 OneR Ibk(5) SMO RNA NB  CC 99,59% 99,59% 99,57% 99,59% 99,51% 95,66%  MAE 0,0181 0,0041 0,0165 0,0041 0,0187 0,0439  RMSE 0,0647 0,0640 0,0809 0,0640 0,0704 0,2033  TPR 0 1,0000 1,0000 1,0000 1,0000 0,9999 0,9590  TPR 1 0,0000 0,0000 0,0000 0,0000 0,1360 0,3640  MpTPr 0,9960 0,996 0,996 0,996 0,995 0,957  MTPR 0,5000 0,5000 0,5000 0,5000 0,5679 0,6615  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, T PR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.    Para corrigir este p roblema, podemos utilizar o modelo de  classificação utilizado na predição de bugs de código (SQ) de  forma que apenas as instâncias classificadas nas classes b e c  (as  quais podem ou não ter bugs funcionais) e as instâncias que  possuem bugs funcionais (mas não pertencem à s classes b e c )', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.     7  serão utilizadas no treinamento de um modelo para a previsão de  bugs funcionais. Note que se o objetivo do experimento anterior  fosse apenas indicar se a instância não possui (classe a) ou possui  (classe b e c) erro, ou seja , a classe b e c fosse m unidas, o  classificador OneR teriam uma acurácia máxima. Para avaliarmos  esta abordagem, fixamos os números de instâncias com bugs  funcionais na base de treinamento (86) e na base de testes (22) e  realizamos experimentos  (Tabela 10)  apenas com instâncias que  possuíam bugs, os quais foram indicados pelo modelo anterior.  Tabela 10: Número de instâncias com bugs de código na base  de treinamento e de testes para a execução dos experimentos  de previsão de bugs funcionais. Os resultados para cada  combinação se encontram nas tabelas de 11 a 16.   Tabela com os resultados dos experimentos  11 12 13 14 15 16  Treinamento 534 381 300 250 200 100  Teste 472 337 300 250 200 100    O primeiro experimento indicado na Tabela 10 utiliza o maior  número de instâncias com bugs de código nas bas es de  treinamento e testes (534/ 472). O resultado é mostrado na Tabela  11. Os demais experimentos, respectivamente, fazem uma seleção  aleatória das instâncias ( undersampling) de modo a se obter cada  vez mais um núme ro próximo a um melhor balanceamento. O  objetivo é verificar se a questão do balanceamento tem uma  conotação significante neste cenário, onde apenas instâncias com  bugs são utilizadas. Os resultados são mostrados da Tabela 12 a  16.  Tabela 11: Resultados da execução dos algoritmos de  predição, experimento (534/472) – Bugs Funcionais.   J48 OneR Ibk(5) SMO RNA NB  CC 95,55% 98,18% 95,55% 95,55% 97,17% 71,66%  MAE 0,1691 0,0182 0,1073 0,0445 0,3390 0,3244  RMSE 0,2259 0,1350 0,2250 0,2110 0,1555 0,4720  TPR 0 1,0000 1,0000 0,9960 1,0000 0,9890 0,7330  TPR 1 0,0000 0,5910 0,0910 0,0000 0,5910 0,3640  MpTPr 0,955 0,9820 0,9550 0,9550 0,9720 0,7170  MTPR 0,5000 0,7955 0,5435 0,5000 0,7900 0,5480  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto , RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.  Tabela 12: Resultados da execução dos algoritmos de  predição, experimento (381/337) – Bugs Funcionais.   J48 OneR Ibk(5) SMO RNA NB  CC 91,36% 97,49% 92,76% 93,87% 90,81% 82,17%  MAE 0,2167 0,0251 0,1438 0,0613 0,1164 0,2179  RMSE 0,2835 0,1583 0,2690 0,2476 0,2639 0,4098  TPR 0 0,9670 1,0000 0,979 1,0000 0,9290 0,858  TPR 1 0,0910 0,5910 0,136 0,0000 0,5910 0,273  MpTPr 0,914 0,975 0,928 0,939 0,9080 0,822  MTPR 0,5290 0,7955 0,5575 0,5000 0,7600 0,565  * CC: Inst âncias Classific adas Corretament e, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.    Tabela 13: Resultados da execução dos algoritmos de  predição, experimento (300/300) – Bugs Funcionais.   J48 OneR Ibk(5) SMO RNA NB  CC 90,68% 97,21% 91,93% 93,17% 94,41% 69,56%  MAE 0,2329 0,028 0,1507 0,0683 0,0836 0,3686  RMSE 0,2969 0,1672 0,2796 0,2614 0,2209 0,4984  TPR 0 0,9600 1,000 0,9770 1,0000 0,9670 0,7130  TPR 1 0,1820 0,591 0,1360 0,0000 0,6360 0,4550  MpTPr 0,907 0,972 0,919 0,9320 0,944 0,6960  MTPR 0,571 0,7955 0,5565 0,5000 0,8015 0,5840  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros P ositivos, MTPR: Média Aritmé tica da', 'Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros P ositivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.    Tabela 14: Resultados da execução dos algoritmos de  predição, experimento (250/250) – Bugs Funcionais.   J48 OneR Ibk(5) SMO RNA NB  CC 86,76% 96,69% 90,81% 91,54% 90,81% 83,82%  MAE 0,2219 0,0331 0,1789 0,0846 0,0939 0,1745  RMSE 0,3540 0,1819 0,2970 0,2908 0,2764 0,3958  TPR 0 0,9240 1,0000 0,9720 0,9960 0,9240 0,8840  TPR 1 0,2270 0,5910 0,1820 0,0000 0,7270 0,3180  MpTPr 0,868 0,967 0,908 0,915 0,9080 0,838  MTPR 0,5755 0,7955 0,5770 0,4980 0,8255 0,601  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Classe Com Bugs, MpTPr:  Média Pon derada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.    Tabela 15: Resultados da execução dos algoritmos de  predição, experimento (200/200) – Bugs Funcionais.   J48 OneR Ibk(5) SMO RNA NB  CC 88,74% 95,95% 81,08% 88,74% 84,68% 81,53%  MAE 0,2874 0,0405 0,2351 0,1126 0,1581 0,2003  RMSE 0,3477 0,2013 0,3540 0,3356 0,3521 0,4268  TPR 0 0,9500 1,0000 0,8800 0,9800 0,8600 0,8700  TPR 1 0,3180 0,5910 0,1820 0,0450 0,7270 0,3180  MpTPr 0,887 0,959 0,811 0,887 0,8470 0,8150  MTPR 0,634 0,7955 0,531 0,5125 0,7935 0,5940  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Verd adeiros Positivos para a Cl asse Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.    8    Tabela 16: Resultados da execução dos algoritmos de  predição, experimento (100/100) – Bugs Funcionais.   J48 OneR Ibk(5) SMO RNA NB  CC 64,75% 92,62% 69,67% 65,57% 78,69% 69,67%  MAE 0,3761 0,0738 0,3184 0,3443 0,2312 0,3067  RMSE 0,4984 0,2716 0,4456 0,5867 0,4247 0,5160  TPR 0 0,6600 1,0000 0,7800 0,6700 0,8000 0,7800  TPR 1 0,5910 0,5910 0,3180 0,5910 0,7270 0,3180  MpTPr 0,648 0,926 0,697 0,656 0,787 0,697  MTPR 0,6255 0,7955 0,549 0,6305 0,7635 0,549  * CC: Inst âncias Classific adas Corretamente, MAE: Erro Médio Absoluto, RMSE:  Erro Quadrático Mé dio, TPR 0: Taxa de Verdadeiros Positivos para  a Classe Sem  Bugs, TPR 1: Taxa de Ve rdadeiros Positivos para a Classe Com Bugs, MpTPr:  Média Ponderada da Taxa de Verdadeiros Positivos, MTPR: Média Aritmé tica da  Taxa de Verdadeiros Positivos.  5.3 Discussão  Como mostrado na Tabela 9, a alta porcentagem de acurácia dos  algoritmos não reflete  a real eficiência de tais abordagens, uma  vez que o TPR das classes minoritárias são extremamente baixos.  As possíveis melhorias indicadas na literatura atuam sobre  modificações dos atributos de qualificação das instâncias,  melhorias na normalização dos d ados, balanceamento via  processos de oversampling e undersampling, e melhoria da  qualidade da base. No nosso trabalho escolhemos esta última  opção, criando uma base que contém apenas classes com uma alta  probabilidade de conter bugs de código. A racionalid ade desta  abordagem vem do fato que desejamos exatamente prever quais  instâncias possuem bugs funcionais. Desta forma, filtrar a base e  utilizar apenas instâncias com esta alta probabilidade de possuir  bugs torna o conjunto de treinamento mais homogêneo e propício  às abordagens de treinamento.  A Tabela 11 mostra que essa abordagem surte efeito, de modo  que o TPR1 do  algoritmo (OneR) salta de 0 para 0,591. Um  detalhamento da execução deste algoritmo é apresentado na  Figura 5, onde a Matriz de Confusão mostra  que das 22 instâncias  com bugs funcionais, o modelo classificou 13 corretamente e 9 de  forma incorreta.   O próximo passo da pesquisa foi realizar experimentos  (Tabelas 12 a 16) para entender se a questão do balanceamento  estava influenciando no TPR da cla sse minoritária . Porém, tais  experimentos indicam uma pequena influência do processo de  balanceamento, principalmente para o algoritmo OneR, o qual  permaneceu com o seu TPR da classe minoritária constante. Deste  modo podemos destacar as seguintes conclusões:  \uf0b7 A rede neural e o algoritmo OneR foram as abordagens   que apresentaram  o melhor desempenho geral nos  nossos experimentos. Em relação ao OneR, e sse fato é  interessante porque o mesmo não é citado em trabalhos  relacionados (ver Seção 6). Ou seja, tal algor itmo não é  bem explorado na literatura, a qual prioriza outras  abordagens de aprendizagem de máquina;  \uf0b7 O processo de tornar a base de treinamento mais  homogênea de fato aumenta a acurácia do modelo,  tornando-o mais imune aos problemas enfrentados pelo  desbalanceamento dos dados;  É difícil comparar esta abordagem com outras indicadas na  literatura, uma vez que valores de TPR ou a Matriz de Confusão  não são geralmente mostrados. Desta forma, a acurácia total (CC)  é o principal parâmetro utilizado pelas pesquis as, sendo que o  mesmo não reflete de fato a eficiência da abordagem.  A próxima  seção traz uma melhor discussão sobre este último ponto com  exemplos práticos da literatura. Mais detalhes sobre a questão de  replicações de experimentos e uso correto de métric as para a  qualificação de preditores no domínio da engenharia de software  podem ser encontrados em [19].      Figura 5: Execução do algoritmo OneR para predição de bugs  funcionais. Detalhamento do experimento da Tabela 11.  6 TRABALHOS RELACIONADOS  Diferentes métodos estatí sticos e métodos de aprendizagem de  máquinas têm sido pesq uisados e propostos para apoiar o uso de', '6 TRABALHOS RELACIONADOS  Diferentes métodos estatí sticos e métodos de aprendizagem de  máquinas têm sido pesq uisados e propostos para apoiar o uso de  predição de defeitos de software. Poré m, grande parte destes  estudos se apoia apenas na utiliza ção das métricas disponí veis  diretamente na fer ramenta WEKA para  identificação da acurá cia  do preditor. Diversos trabalhos pesquisados  se basearam, p or  exemplo, na métrica de acurá cia de  classificação de inst âncias  corretas (CC) e encontraram valores com altas porcentagens de  acerto. Ou tros trabalhos u tilizaram as mé tricas de erro MAE ou  mesmo o RMSE. Porém, acreditamos que as métricas CC, MAE e  RMSE devem ser utilizadas com  certeza cautela, haja vista o  desbalanceamento das classes e  consequente majora ção, desta  forma, possibilitando a tais classes  tenderem a elevar o valor da  acurácia e també m permitir diminuição do valor das mé tricas de  erro. Como citado por alguns autores  [19], a precis ão preditiva  pode não ser apropriada quando  os dados est ão desbalanceados.', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.     9  Em seguida são discutidos alguns  trabalhos que  utilizaram as  métricas clássicas para mensuração da qualidade dos modelos  preditores.  No trabalho de Gayathri [2] fica  evidente que o modelo  de  rede neural MLP propost o obteve os melhores resultados  quando  comparados com outros mé todos previamente es tudados por ele.  Em seus estudos, para o conjunto de dados KC1,  extraído do  repositório da NASA, a rede neural proposta MLP obteve acurácia  de classifica ções corretas de 98,2% contra  94,5% da árvore  randômica e 95,6% para regress ão logística. O RMSE encontrado  foi de 0,29 para rede neural MLP;  0,43 para árvore randômica e  0,37 para regressão logística.  Malhotra et al [20] analisaram a Árvore de Decis ão (DT) e a  Regressão Logística (LoR ) dentro do domínio de  predição de  defeitos. Toda a pesquisa feita por ele e sua equipe apontaram que  as técnicas de aprendizagem de máquina são bem melhores do que  as técnicas LoR. Em suas pesquisas eles alcançaram, por exemplo,  para um mesmo conjunto de dados do experimento  o percentual  de acerto de 55% no L oR e 88,84% para o  DT. Já em outro  conjunto de dados eles obtiveram 53% no L oR contra 83.16% da  árvore de decisão . Para o estudo deste autor, os  modelos que  utilizam os métodos de aprendizagem de máquina, tais como Rede  Neural, Máquina de Vetores de Suporte e Ár vore de Decisão têm  uma ótima capacidade preditiva para prever mó dulos defeituosos  e não defeituosos.  Nos estudos de Callagula  [21] foram compar ados diferentes  modelos de aprendizagem de má quina para identifica ção de  defeitos de mó dulos de software de tempo real. Segu ndo sua  análise, ficou evidente  que n ão existe uma té cnica que  tenha o  melhor desempenho para todos os conjuntos de dados.  Mas,  segundo seus estudos, a IBL e 1R mostraram melhor  consistência  na precis ão quando comparado com os outros  métodos  pesquisados.  Sankar em seus estudos  [22] citou a exist ência de várias  técnicas disponí veis para a previ são de defeitos de  software.  Porém, ele se ateve a  estudar basicamente duas delas:  o  classificador de Má quina de Vetores de Suporte (SVM)  e a  técnica de Naive  Bayes. S egundo os resultados por ele   encontrados, a abordagem  de Naive  Bayes mostrou -se mais  válida.  Já no trabalho desenv olvido por Selvaraj  [23] foi discutido o  conceito de Máquina de Vetores de Suporte (SVM) e verificado a  capacidade deste classificador em prever defeitos de software com  relação ao modelo de Naive Bayes. Para o conjunto de dados KC1  da NASA, ele obteve para Naive  Bayes o valor de 83,77% de  instâncias classificadas corretamente e  o RMSE foi 0,3967 ;  enquanto que o SVM -PolyKernel obteve 86,05% de i nstâncias  classificadas corretamente e valor de erro  RMSE de 0,3735. Os  resultados encontrados por ele mostraram  que a execução do   SVM o bteve resultados mais satisfat órios que o outro modelo  estudado.  Thwin em suas pesquisas [24] usou um modelo de rede neural  para prever defeitos e descobriu  que esta red e neural particular é   mais precisa em predizer  defeitos quando comparado a outros  modelos de regress ão. Ele obteve o valor de métrica de acurá cia  de erro MAE  aproximadamente 0, 78 para a rede neural e  aproximadamente 0,85 para a regressão.  Tao [25] verificou o conceito de Naive Bayes juntamente com  diferentes métodos para o Naive  Bayes, de forma  a compará-los.  Os resultados obtidos mostraram que Naive Bayes Multivariante  Gauss se apresentou mais eficaz do que  outros tipos de m étodos  de Naive Bayes e inclusive foi melhor que o algoritmo J48.  Boetticher et al [26] estudaram o algoritmo do K-vizinho mais  próximo (2 -NN) para o processo de predição e em seus  experimentos constataram que o desempenho deste algoritmo não  foi satisfatório. Segundo os estudos deles , o algoritmo 2 -NN fica  com certa dispersão quando utilizado em grande conjuntos de  dados.', 'foi satisfatório. Segundo os estudos deles , o algoritmo 2 -NN fica  com certa dispersão quando utilizado em grande conjuntos de  dados.  Os estudos de Handi [27] foi realizado no sentido de comparar  duas abordagens de predi ção de defeitos: a M áquina Vetor de  Suporte (SVM) e o Perceptron NN (PNN). Na pesquisa  desenvolvida por ele, os cinco conjuntos de dados foram extraídos  do repositó rio da NASA. De acordo com os seus resultados,  o  modelo PNN forneceu o melhor desempenho  na maior parte  dos  experimentos. Ou seja, segundo o trabalho por ele desenvolvido, a  PNN teve desempenho melhor que o SVM.  Erturk [28] propôs um novo mé todo de Neur ônio Adaptativo  de Sistema de Inferência Fuzzy (ANFIS) para a predição de falhas  de software . Ele se valeu das mé tricas McCabe e  também da  utilização da métrica de acurácia de á rea ROC para  permitir a  abrangência do esforço de programação. Os resultados obtidos por  ele foram 0,7795 para SVM; 0, 8685 para as Redes Neurais e  0.8573 para o ANFIS.  Vemos depois de todos estes experimento s que  definitivamente não existe uma concordância sobre o melhor  algoritmo ou abordagem a ser utilizada neste domínio.  Porém,  independente deste fato, é importante que os trabalhos descrevam  seus experimentos de forma que eles possam ser replicados. Neste   sentido, é nossa intenção tornar público todos os dados utilizados  neste projeto, de modo que outros grupos possam realizar  replicações e comparações entre diferentes abordagens.  7 CONCLUSÃO  A utilização de métricas disponí veis no WEKA é importante para  demonstrar a acurácia do preditor. P orém, em  determinadas  situações algumas mé tricas mostradas n ão são suficientes para  indicar o valor real de acur ácia. Haja vista, em situações em que  classe ou classes se identif ique majoração em detrimento a outras  classes, fica percep tível que a classe  majorada tende a el evar o  valor da acurá cia vindo a sugerir que  o modelo criado tem uma  porcentagem de acerto elevada. Porém, tal fato acaba sendo uma  inverdade. Esse cenário de não balanceamento é crítico em  sistemas de predição de defeitos, uma vez que as classes com  erros tendem a representar um número bem menor de instâncias  do que as classes corretas. Tal fato pode ser observado no nosso  experimento, onde tivemos que utilizar a inter seção de instâncias  da classe de erro de diversas versões do sistema para fazer o  treinamento do sistema de predição. Ou seja, a utiliza ção das  métricas de acurácia clássicas cumpre seu papel quando as classes  que são objetos de estudo estão balanceadas.', 'SATS’17, September 2017, Fortaleza, Ceará -Brazil A. Pontes et al.    10    Para evitar este problema de métrica, o nosso projeto tomou  como base  o uso da matriz de confu são. Conforme  sua  demonstração de classifica ção das instâncias  foi possível obter  valores mais consistentes sobre a corretud e das instâncias que  foram ou não classificadas corretamente.  Na aná lise da matriz de confus ão sugerimos uma forma  de  cálculo que reú na a acu rácia de todas as classes que est ão sendo  utilizadas na predi ção em um ú nico valor. Para isso podemos  nos  valer da taxa de verdadeiros positivos (TP Rate) de cada classe, ou  seja, podemos adotar a média aritmé tica desta métrica extraída de  cada classe. C om isso teremos o MTPR  (ou a mé dia da taxa de  verdadeiros positivos). Ferramentas como o WEKA  disponibilizam a informação referente ao TP Rate de cada classe,  e também a mé dia ponderada das taxas de verdadeiro positivo .  Porém a utiliza ção desta média ponderada també m tende a   aumentar a porcentagem da acurá cia elevando o valor da classe   majorada. Assim nos val eremos da si mples média aritmé tica dos  TP Rate . A ideia empregada em usar o MTPR consiste  em, por  exemplo, uma matriz de confu são 4x4 ser atribuí da para cada  classe 25% dos 100% disponíveis. E para cada uma dessas classes  verifica-se a porcentagem de acerto com relação ao total da classe.  Ao final somam -se cada porcentagem de  acerto de cada classe  obtendo-se um número para ser usado como valor de acurácia, no  caso o MTPR. Acreditamos ser  um valor mais preciso do que  aquele mostrado no WEKA com  relação à porcentagem de  instâncias classificadas corretamente, por exemplo.  REFERENCES  [1] KritikaVerma and Pradeep Kumar Singh. 2015. An insight to soft computing   based defect prediction techniques in software . International Journa lof Modern  Education and Computer Science, 7(9):52.  [2] M Gayathri and A Sudha.  2014 Software defect prediction system using   multilayer perceptron neural network with data mining . International Journal of  Recent Technology and Engineering, 3:54–59.  [3] K Punitha and S Chitra. 2013. Software defect pre diction using software   Metrics: a survey. In  Proceedings of the 2013 International Conference on  Information Communication and Embedded Systems, pages 555–558.  [4] D Kerana Hanirex and KP Kaliyamurthie.  2013. Multi-classification approach   for detecting th yroid attacks. International Journal of Pharma and Bio   Sciences.  [5] Aditya P. Mathur.  2007.Foundation of Software Testing . Pearson Education,   first edition edition.  [6] Antonia Bertolino. 2007. Software testing research: Achievements, challenges,   dreams. In Future of Software Engineering, 2007. FOSE ’07, pages 85–103.  [7] Praveen Ranjan Srivastava and Km Baby.  2010. Automated software testing   using metahurestic technique based on an ant colony optimization. In   Proceedings of the 2010 International Sympo sium on Electronic System Design   (ISED), pp. 235–240.  [8] Er Rohit Mahajan, Dr Sunil Kumar Gupta, and Rajeev Kumar Bedi.   2014.Comparison of various approaches of software fault prediction: Areview.  International Journal of Advanced Technology & Engineerin g Research  (IJATER).  [9] Yasutaka Kamei, Shinsuke Matsumoto, Akito  Monden, Ken -ichi Matsumoto,   Bram Adams, and Ahmed E Hassan.  2010. Revisiting common bug  prediction  findings using effort -aware models. In 2010 IEEE International Conference on  Software Maintenance (ICSM), pp 1–10.  [10] Tian Jiang, Lin Tan, and Sunghun Kim. 2013. Personalized defect prediction .In  Proceedings of the IEEE/ACM 28th International Conference on Automated  Software Engineering (ASE), pp. 279–289.  [11] Pooja Paramshetti and DA Ph alke 2014. Survey on software defect prediction   using machine learning techniques. International Journal Of Science And  Research, 3(12):1394–1397.  [12] Peng He, Bing Li, Xiao Liu, Jun Chen, and Yutao Ma.  2015. An empirical', 'using machine learning techniques. International Journal Of Science And  Research, 3(12):1394–1397.  [12] Peng He, Bing Li, Xiao Liu, Jun Chen, and Yutao Ma.  2015. An empirical   study on software defect predict ion with a simplified metric set.  Information  and Software Technology, 59:170–190.  [13] Barry W Boehm, John R Brown, and Mlity  Lipow. 1976. Quantitative  evaluation of software quality. In Proceedings of the 2nd international   Conference on Software engineering, pp. 592–605.  [14] Syed Roohullah Jan, Syed Tauhid Ullah Shah, Zia Ullah Johar, Yasin Shah, and  Fazlullah Khan.  2016. An innovative approach to investigate various  software  testing techniques and strategies. International Journal of Scientific Research in  Science, Engineering and Technology (IJSRSET), pp. 2395–1990.  [15] Rudolf Ferenc, Laszlo Lang o, Istvan Siket, Tibor Gyim othy, and Tibor Bakota.  2014. Source meter sonar  qube plug -in. In IEEE 14th International Working  Conference on Source Code Analysis and Manipulation (SCAM), pp. 77–82.  [16] Huanjing Wang, Taghi M Khoshgoftaar, Jason Van Hulse, and KehanGao.   2011. Metric selection for software defect prediction. International Journal  of  Software Engineering and Knowledge Engineering, 21(02):237–257.  [17] Phil McMinn. 2004. Search-based software test data generation: A survey.   Software Testing Verification and Reliability, 14(2):105–156.  [18] Mark Harman.  2007. The current state and future of search based software   engineering. In 2007 Future of Sof tware Engineering , p p. 342–357.IEEE  Computer Society.  [19] Clauirton A Siebra and Michael AB Mello.  2015. The importance of   replications in software engineering: a case study in defect prediction.  In  Proceedings of the 2015 Conference on research in adaptive and  convergent  systems, pp. 376–381. ACM, 2015.  [20] Ruchika Malhotra.  2014. Comparative analysis of statistical and machine  learning methods for predicting faulty modules. Applied Soft  Computing,21:286–297.  [21] Venkata Udaya B Challagulla, Farokh B Bastani, I -Ling Yen, and  Raymond A  Paul. 2008. Empirical assessment of machine learning based  software defect  prediction techniques. International Journal on Artificial  Intelligence Tools ,  17(02):389–400.  [22] K Sankar, S Kannan, and P Jennifer. 2014. Prediction of code fault using naive  bayes and svm classifiers. Middle-East Journal Of Scientific  Research,20(1):108–113.  [23] PA Selvaraj and Dr P Thangaraj.  2013. Support vector machine for software   defect prediction. International Journal of Engineering & Technology   Research, 1(2):68–76.  [24] Mie Mie Thet Thwin and Tong-Seng Quah. 2002. Application of neuralnetwork  for predicting software development faults using object -oriented design metrics.  In Proceedings of the 9th International Conference on Neural Information  Processing, 5:2312–2316.  [25]      [26]  Tao Wang and Wei-hua Li. 2010. Naive bayes software defect prediction model  .In Proceedings of the 2010 International Conference on Computational  Intelligence and Software Engineering (CiSE), pp. 1–4.  Gary   D  Boetticher. Nearest   neighbor sampling   for   better defect  prediction.  ACM  SIGSOFT  Software  Engineering  Notes,  30(4):1–6, 2005.  [27] Hamdi A Al -Jamimi and Lahouari  Ghouti. 2011. Efficient prediction of   software fault proneness modules using support vector machines and   probabilistic neural networks. In Proceedings of the 5th Malaysian Conference  in Software Engineering (MySEC), pp. 251–256.  [28] Ezgi Erturk and Ebru  Akcapinar Sezer. 2015. A comparison of some soft   computing methods for software fault prediction. Expert Systems with   Applications, 42(4):1872–1879.']","['A STRATEGY FOR FUNCTIONAL DEFECT PREDICTION IN HOMOGENOUS DATASETS:  A CASE STUDY IN THE SIGAA ACADEMIC SYSTEM  Este  briefin  reporta  evidêicias cieitfcas sobre a aplica\x15ão da predi\x15ão de defeitos de softare com iituito  de ideitfcar  buns  fuicioiais  utliizaido bases homonêieas. FINDINGS   \uf0b7 O  trabalho  aborda  uma  estratégia  de modelagem  baseada  em  bases  de treinamento mais homogêneas, as quais são utilizadas  na  criaãão  de  modelos  preditivos direcionados a bugs funcionais. \uf0b7 O trabalho utiliza as versões mais iniciais do sistema e que foram para produãão para criar os  modelos  para  que  sejam  validados  nas versões mais recentes e que com os modelos seja  possível  identiicar  os  defeitos  de software nas versões futuras. \uf0b7 A avaliaãão da proposta é realizada em um complexo  sistema  de  gerenciamento acadêmico, conhecido como SIGAA, o qual é utilizado  em  diversas  universidades brasileiras.  O  principal  objetivo  é  que  os modelos  possam  ser  generalizados,  não estando presos ao domínio responsável por sua criaãão. \uf0b7 No estudo é constatado que o uso de toda a base  com  dados  desbalanceados  não influencia  na  acurácia  do  algoritmo  OneR, icando  este  algoritmo  com  acurácia  acima dos 98%. A literatura mostra que trabalhos que  possuem  este  nível  de  acurácia  com dados  desbalanceados  possuem  uma  baixa taxa de verdadeiros positivos para as classes minoritárias. Porém, isso não é observado no modelo proposto na pesquisa, em que o TPR1 tem um valor de 0,981 e TPR\uf0b32 em 0,742. \uf0b7 Na pesquisa também foi trabalhada dentre as técnicas  existentes,  a  técnica  de undersampling. Assim criou-se uma base que contém  apenas  classes  com  uma  alta probabilidade  de  conter  bugs  de  código.  A racionalidade desta abordagem vem do fato de desejar prever  quais instâncias possuem bugs funcionais. Desta forma, iltrar a base e utilizar  apenas  instâncias  com  esta  alta probabilidade  de  possuir  bugs  tornou  o conjunto de treinamento mais homogêneo e propício às abordagens de treinamento para ser  uma  estratégia  de  modelagem  baseada em bases de treinamento mais homogêneas, as  quais  foram  utilizadas  na  criaãão  de modelos  preditivos  direcionados  a  bugs funcionais.   \uf0b7 A rede neural e o algoritmo OneR foram as abordagens  que  apresentaram  o  melhor desempenho  geral  nos  experimentos.  Em relaãão  ao  OneR,  esse  fato  é  interessante porque  o  mesmo  não  é  muito  citado  em trabalhos relacionados. Ou seja, tal algoritmo não  é  bem  explorado  na  literatura,  a  qual prioriza outras abordagens de aprendizagem de máquina.   \uf0b7 O processo de tornar a base de treinamento mais homogênea de fato aumenta a acurácia do  modelo,  tornando-o  mais  imune  aos problemas  enfrentados  pelo desbalanceamento dos dados.   \uf0b7 Algumas  métricas  mostradas  no  WEKA  não são  suicientes  para  indicar  o  valor  real  de acurácia.  Haja  vista,  em  situaãões  em  que classe ou classes se identiique majoraãão em detrimento a outras classes, ica perceptvel que a classe majorada tende a elevar o valor da  acurácia  vindo  a  sugerir  que  o  modelo criado  tem  uma  porcentagem  de  acerto elevada.  Porém,  tal  fato  acaba  sendo  uma inverdade.  \uf0b7 A utilizaãão das métricas de acurácia clássicas cumpre seu papel quando as classes que são objetos  de  estudo  estão  balanceadas.  No caso  de  desbalanceamento,  um  método promissor  de  veriicar  a  acurácia  de  um modelo  é  através  do  estudo  das  métricas extraídas da matriz de confusão. \uf0b7 O cenário de não balanceamento é crítico em sistemas  de  prediãão  de  defeitos,  uma  vez que  as  classes  com  erros  tendem  a representar  um  número  bem  menor  de instâncias do que as classes corretas.   Keywords Defect prediction Software test Test Automation Learning Algorithms Who is this briefin  or? Para pesquisadores e proissionais com  interesse em testes de software e que  querem aplicar ou mesmo desenvolver', 'Software test Test Automation Learning Algorithms Who is this briefin  or? Para pesquisadores e proissionais com  interesse em testes de software e que  querem aplicar ou mesmo desenvolver  novas pesquisas sobre o processo de  prediãão no âmbito de automaãão de  testes de software Where the fidiins come  rom? All indings of this brieing were  extracted from practical experiments  conducted by Pontes, Siebra e  Bittencourt.  What is iicluded ii this briefin? The main indings regarding the use of  defect prediction for identiication of  functional bugs For additoial ii ormatoi about  this briefin: http:////www.lia.ufc.br//ccbsoft2017//  ORIGINAL RESEARCH REFERENCE Pontes Alan B, Siebra Clauirton A, Bittencourt Michael A. A Strategy for Functional Defect Prediction in Homogenous Datasets: A case study in the SIGAA academic system.      2nd Brazilian Symposium on Systematic and Automated Software Testing, Fortaleza- CE, Brazil — September 18 - 19, 2017']","**Title: Enhancing Defect Prediction in Software Testing through Homogeneous Datasets**

**Introduction:**
This evidence briefing summarizes findings from a study aimed at improving defect prediction in software testing, specifically focusing on functional bugs within the SIGAA academic management system. The goal is to provide actionable insights for practitioners seeking to optimize their testing processes and enhance software quality.

**Key Findings:**
1. **Homogeneous Datasets Improve Prediction Accuracy:** The study demonstrates that using more homogeneous datasets for training defect prediction models leads to better generalization and accuracy. By focusing on functional bugs in the SIGAA system, the researchers were able to create models that effectively identified problematic modules, allowing for prioritized testing.

2. **Algorithm Performance Varies:** Among the various machine learning algorithms tested, the OneR algorithm exhibited the highest accuracy in classifying instances with functional defects, achieving over 93% accuracy in multiple experiments. This highlights the importance of selecting appropriate algorithms based on the specific characteristics of the dataset.

3. **Addressing Class Imbalance:** The researchers found that class imbalance (where instances without defects vastly outnumber those with defects) negatively impacts prediction accuracy. Techniques such as undersampling were employed to balance the dataset, which significantly improved the model's ability to predict classes with defects.

4. **Limitations of Traditional Metrics:** The study critiques the reliance on traditional metrics like accuracy, mean absolute error (MAE), and root mean square error (RMSE), suggesting that these may not adequately reflect predictive performance, especially in imbalanced datasets. Instead, using the average true positive rate (MTPR) provides a more nuanced understanding of model effectiveness.

5. **Implications for Testing Strategies:** The findings suggest that practitioners should consider using homogeneous datasets and appropriate algorithms like OneR for defect prediction. Additionally, balancing training datasets can enhance the reliability of predictions, ultimately leading to more efficient testing sequences.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, especially those involved in software testing and quality assurance, who are looking to implement or improve defect prediction strategies in their projects.

**Where the findings come from?**
The findings are derived from a case study conducted on the SIGAA academic management system, as presented in the paper by Pontes, Siebra, and Bittencourt (2017).

**What is included in this briefing?**
This briefing includes a summary of the methodology, key findings related to defect prediction accuracy, algorithm performance, and practical implications for software testing strategies.

**To access other evidence briefings on software engineering:**
http://www.lia.ufc.br/~cbsoft2017/

**For additional information about the research group:**
http://www.great.ufpb.br

**Original Research Reference:**
Pontes, A., Siebra, C., & Bittencourt, M. (2017). A Strategy for Functional Defect Prediction in Homogeneous Datasets: A case study in the SIGAA academic system. In Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing (CBSOFT’17), Fortaleza, Ceará, Brazil. https://doi.org/10.1145/3128473.3128474"
"[""Analysing Requirements Communication Using Use Case  Specification and User stories  Ana Carolina Oran, Elizamary Nascimento   Universidade Federal do Amazonas  (UFAM) Manaus – AM – Brazil  {ana.oran, elizamary.souza}  @icomp.ufam.edu.br  Gleison Santos  Universidade Federal do Estado do Rio  de Janeiro (UNIRIO)   Rio de Janeiro –  RJ – Brazil  gleison.santos@uniriotec.br  Tayana Conte  Universidade Federal do Amazonas  (UFAM) Manaus – AM – Brazil  tayana@icomp.ufam.edu.br  ABSTRACT  Effective requirements communication is essential in software  development project s due to the importance of understanding  the requirements throughout the software development cycle.  Software requirements can be specified in different formats, for  instance using free texts or more structured fo rms, such as use  cases and user stories  used in Behavior Driven Development  (BDD). We present a comparative analysis on the requirements  communication dynamics using use case specification and user  stories as the basis for mockups creation. We carried out an  exploratory empirical study involving 16 students. Th e study  comprised 3 steps: requirements specification, mockups  construction, and inspection to investigate whether the mockups  were in accordance with the specifications. Results show that  there is no  significant difference in using use case specification  or user stories  to communicate software requirements. Our  findings suggest that different specification formats can provide  similar results while communicating requirements, nonetheless  the human factor should not be neglected.1  CCS CONCEPTS  • Software notations and tools → System description  languages; Unified Modeling Language (UML); Specification  languages.  KEYWORDS  Requirements communication, requirements specification, use  case, Behavior Driven Development, experimental study.     ACM Reference format:  A. C. Oran, E. Nascimento, G. Santos, T. Conte. 2017. Analysing  Requirements Communication Using Use Case Specification and  User stories. In Proceedings of 31st Brazilian Symposium on  Software Engine ering, Fortaleza, Ceará, Brazil, September 2017  (XXXI SBES), 10 pages.   https://doi.org/10.1145/3131151.3131166                                                                    Publication rights licens ed to ACM. ACM acknowledges that this contribution was  authored or co -authored by an employee, contractor or affiliate of a national  government. As such, the Government retains a nonexclusive, royalty -free right to  publish or reproduce this article, or to allow others to do so, for Government  purposes only.  SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil   © 2017 Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM ISBN 978-1-4503-5326-7/17/09…$15.00   https://doi.org/10.1145/3131151.3131166  1 INTRODUÇÃO  A comunicação dos requisitos é essencial em todos os projetos de  software, uma vez que existe necessidade de entendimento das  informações durante todo o ciclo do processo de  desenvolvimento de software  [29]. A comunicação de requisitos  é iniciada com a elicitação de requisitos com o cliente e continua  ao longo de todo o projeto de desenvolvimento, envolvendo  diferentes papéis [33]. Na fase de elicitação de requis itos, os  clientes devem ser capazes de comunicar as suas necessidades  para os analistas de requisitos e os analistas precisam ser capazes  de transmitir essas necessidades para todos os demais membros  da equipe de desenvolvimento de forma clara e eficaz . Al-Rawas  e Easterbrook [1], Tu et al. [33] e Fernández et al. [11] afirmam  que tanto usuários finais quanto profissionais de software têm  dificuldades relacionadas à validaçã o e compreensão das  informações resultantes da especificação dos requisitos. Uma boa  comunicação não significa apenas emitir os requisitos para todo  o grupo, mas também é preciso garantir o entendimento destes  na sua recepção. Segundo Hoisl et al. [13], a forma como os"", 'comunicação não significa apenas emitir os requisitos para todo  o grupo, mas também é preciso garantir o entendimento destes  na sua recepção. Segundo Hoisl et al. [13], a forma como os  requisitos são descritos pode influenciar no esforço requerido  para que o receptor desta informação entenda o que deve ser  desenvolvido. É importante representar os requisitos de forma  que permita que  todos os stakeholders envolvidos no projeto  estabeleçam um entendimento comum sobre o funcionamento do  sistema para que o produto final  desenvolvimento pela equipe   esteja de acordo com as expectativas do cliente.   Há diferentes formas de se representar os requisitos de um  sistema, desde uso de textos livres a formas mais estruturadas.   Problemas com comunicação de requisitos podem surgir devido  ao modelo de especificação escolhida para o processo de  desenvolvimento do Sistema [15]. Uma forma muito difundida de  se descrever requisitos são os casos de uso, bastante utilizados  pela indústria  [3, 9, 22]. Casos de uso , além de descreverem os  requisitos, são utilizados também como meio de comunicação  entre os membros da equipe de projeto e outros stakeholders  [8]  e também como insumos em diferentes atividades não  diretamente relacionadas a um processo de Engenharia de  Requisitos, como atividades de planejamento, análise, projeto,  desenvolvimento, teste e manutenção do sistema [ 2]. Outra  forma que tem despertado interesse nas comunidades  acadêmicas e industriais é a user story usada em Behavior Driven  Development (BDD ou Desenvolvimento Dirigido a  Comportamento) [8]. BDD é uma metodologia ágil preocupada  em utilizar um vocabulário comum para entendimento de  214', 'requisitos por todos os stakeholders do projeto  [7]. S egundo  Silva [27], BDD impulsiona as equipes de desenvolvimento para  uma especificação de requisitos baseada em user stories [19] em  um formato abrangente de linguagem natural . A s user stories  consistem em descrições breves, na perspectiva do usuário final,  das funcionalidades desejadas [35] e englobam vários aspectos da  especificação de requisitos representados em linguagem natural ,  tais como: cenários alternativos, de exceção  e interação do ator  com o sistema.   Com o intuito de compara r a dinâmica da comunicação de  requisitos utilizando especifica ção de caso de uso e user story  como base para a criação de mockups (representações dos  aspectos da interface do usuário [18]), este artigo apresenta os  resultados de um estudo experimental exploratório onde foram  verificadas a emissão e recepção dos requisitos utilizando caso de  uso e user story, avaliando e comparando o grau de correção das  especificações e dos mockups criados. Para direcionar este  estudo, foi definida a seguinte questão de pesquisa : “O uso de  casos de uso ou user stories afeta a comunicação dos requisitos  para a construção de mockups de forma diferente? ”.  O restante deste artigo está organizado da seguinte forma: a  Seção 2 aborda conceitos sobre proces so de engenharia de  requisitos, caso de uso , user story utilizada no BDD e trabalhos  relacionados; a Seção 3 descreve o planejamento e execução do  estudo experimental; a Seção 4 apresenta a análise dos resultados  encontrados neste estudo ; a Seção 5  as discussões e ameaças à  validade do estudo . Por fim, a Seção 6 apresenta as conclusões ,  limitações e trabalhos futuros.  2 ESPECIFICAÇÃO DE REQUISITOS DE  SOFTWARE   O documento de especificação de requisitos é uma declaração  dos requisitos para os stakeholders. Este documento fornece base  para a análise e validação de requisitos junto aos stakeholders,  definição do que os projetistas têm que construir, bem como a  verificação da qualidade do produto final no momento da  entrega [5,19]. Através de um mapeamento sistemático  da  literatura sobre comunicação de requisitos de software,  identificou-se que existem diferentes formas para representar os  requisitos de software e a escolha por uma dessas formas  dependerá de fatores como, a experiência da equipe  e a  visibilidade destes requisitos [31]. Entre muitas formas de  representar requisitos, duas das mais utilizadas pela indústria  são: casos de uso [3, 17] e user stories, devido ao crescimento do  desenvolvimento ágil  [34]. A seguir é apresentada uma breve  descrição destas formas de representar os requisitos.   2.1 Caso de uso   Um dos artefatos uti lizados pelos engenheiros de software para   descrever e docume ntar requisitos do software é a especificação  de Caso de Uso (UC)  [31]. Originalmente proposto por Jacobson  et al.  [16] como uma forma dos profissionais de software  obterem uma melhor compreensão dos requisitos de um sistema .  Segundo Bezerra  [6] e Cockburn [9], caso de uso é a  especificação da sequência de interações  entre um sistema e um  ou mais atores (agentes ex ternos a esse sistema) de forma  sequencial, representando um relato de utilização de alguma  funcionalidade do sistema em questão, sem revelar a estrutura e  o comportamento internos desse sistema.   A estrutura amplamente utilizada pelos engenheiros de  requisitos em geral utiliza a seguinte estrutura  [24]: Nome:   descreve o nome do caso de uso, capturando a sua essência.  Descrição: uma descrição sucinta do caso de uso, descrevendo o  objetivo do caso de uso. Pré-condições (opcional): descrição de  condições que devem ser atendidas antes de iniciar o caso de uso.  Fluxo Principal: descreve os passos do caso de uso realizados em  situações normais, ou seja, sem considerar nenhum erro. Fluxos  Alternativos: descreve m formas al ternativas de realizar certos  passos do caso de uso. Fluxos de Exceção: descrevem formas de', 'situações normais, ou seja, sem considerar nenhum erro. Fluxos  Alternativos: descreve m formas al ternativas de realizar certos  passos do caso de uso. Fluxos de Exceção: descrevem formas de  recuperação de erros que podem ocorrer em certos passos do  caso de uso. Pós-condições (opcional): descrição de condições que  devem ser atendidas após a execução do caso de uso,  considerando que o fluxo de eventos normal seja realizado com  sucesso. Regras de Negócio:  são políticas, procedimentos ou  restrições que devem ser levadas em consideração durante a  execução do caso de uso.   Esta estrutura é voltada principalme nte para descrever o s  fluxos principal e alternativo de eventos. O uso dessa estrutura  facilita o entendimento da funcionalidade por utilizar uma  estrutura pré-definida [4] e permite aos engenheiros de software  flexibilidade na especificação dos requisitos  [31] ajudando assim  na comunicação de requisitos dentr o das equipes de  desenvolvimento.  2.2 User story usada em Behavior Driven  Development (BDD)  Behavior-Driven Development é uma metodologia  ágil que  auxilia grupos a construir e entregar software de maior valor e  qualidade de forma mais rápid a [28]. BDD tem como objetivo  promover o entendimento comum do domínio do negócio entre  os stakeholders do projeto, além de apoiar a automação de outras  atividades do ciclo de desenvolvimento e manter a  documentação atualizada [10, 12, 28].  A especificação dos requisitos em BDD é feita por todos os  stakeholders do projeto no formado de user stories (US). Onde  cada user story é instancia da com múltiplos cenários BDD, que  representam um comportamento esperado para uma  determinada situação.  Os cenários BDD possuem um template,  escrito em uma linguagem independente de domínio, de forma a  ser u tilizado em qualquer situação [7]. Segundo Silva  [27], as  user stories  utilizada em BDD são desenvolvidas sob uma  perspectiva de comportamento no ponto de vista do usuário. Este  método promove uma descrição de linguagem natural  semiestruturada, de forma não ambí gua, além de promover a  reutilização de comportamentos de negócios que podem ser  compartilhados para vários recursos no sistema.   Um cenário BDD é composto por uma parte inicial q ue  representa uma user story [7]: Como: descreve o papel/ator que  utilizará a  funcionalidade. Eu quero: descreve o objetivo, desejo  ou característica da funcionalidade. Para que: descreve o motivo,  justificativa ou benefício da funcionalidade.   215', '3  A segunda parte do cenário descreve o comportamento em si  e u tiliza-se palavras chaves: Dado que: descreve um contexto  inicial, descrito por  várias características,  são as pré -condições  para executar o cenário . Quando: descreve o que pode  eventualmente, ocorrer uma mudança no domínio,  os testes  (passos) para execução do cenário . Então: descreve o estado final  pretendido, o resultado esperado da execução dos passos.   Esta estrutura  utiliza uma linguagem ubíqua que permite  especificar o comportamento do sistema ao invés de detalhes  técnicos, o que possibilita a comunicação, colaboração e  compreensão dos requisitos do sistema por todos os envolvidos  no desenvolvimento do software [7, 8, 10, 12, 27, 28].  2.3 Trabalhos relacionados  Lauesen e Kuhail [17] compararam a técnica de descrição de  tarefas com casos de uso em projetos reais  e verificaram que  a  utilização de casos de uso torna o requisit o muito restritivo por  duas razõ es: 1. Força o analista a criar um diá logo muito cedo,  projetando assi m uma solução em vez de especificar uma  necessidade do cliente. 2. O template util izado para especificação  pode induzir o analista a criar regras, pré-condições e fluxos que  muitas vezes não refletem a necessidade do cliente e pode  prejudicar no desenvolvimento do s istema. Por outro lado, na  descrição de tarefas não existe o template para ser seguido  pelo  analista, a descrição é criada  ao long o do desenvolvimento e  descreve-se apenas a interação do   usuário com o sistema.   Tu et al.  [33] afirmam que o uso de documentos mais  transparentes, ou seja, com uma maior visibilidade de  informações para as partes interess adas, possa contribuir para  uma comunicação mais eficaz. Eles realizaram um  experimento  que mostrou evidências de que uma maior transparência dos  documentos de requisitos leva a uma comunicação mais eficaz.   Al-Rawas e Easterbrook [ 1] descrevem  um estudo s obre os  problemas de comunicação de requisitos em projetos de  desenvolvimento de software. Os resultados mostraram que as  questões organizacionais e sociais têm grande influência na  eficácia das atividades de comunicação. Além disso, apontam que  os usuário s finais têm dificuldades em entender e validar os  requisitos devido às notações utilizadas. Por outro lado, os  profissionais de software relatam que o uso de notações que são  legíveis para os seus clientes, muitas vezes, gera documentos  extensos e que apresentam ambiguidade.    Os trabalhos apresentados focaram n a comunicação de  requisitos entre cliente e analista . Estes não  tiveram como  objetivo verificar qual a melhor forma de representar os  requisitos para que todos da equipe tenham o entendimento do  que será desenvolvido.   3 ESTUDO EXPERIMENTAL  Com o prop ósito de comparar especificações  de casos de uso  e  user stories foi realizado um estudo  experimental para comparar  as abordagens em termos de  comunicação de requisitos entre os  membros da equipe de desen volvimento. Esta s eção descreve o  plano de estudo e sua execução. A versão completa do pacote  experimental encontra-se disponível no relatório técnico [23].  3.1 Planejamento do Estudo  Para orientar o projeto do estudo, a partir da questão de pesquisa  (O uso de casos de uso ou user stories afeta a comunicação dos  requisitos para a construção de mockups de forma diferente? ),  foi definido o objetivo da seguinte forma:  “Analisar as especificações de caso de uso e user story usadas  em BDD com a finalidade de compar á-las em relação a  comunicação de requisitos dentro de grupos de desenvolvimento,  do ponto de vista de pesquisadores em engenharia de software  no contexto de  especificação de requisitos e con strução de  mockups por estudantes de graduação e pós-graduação”.   3.1.1 Participantes. O estudo foi realizado em um ambiente  acadêmico, com 16 participantes, sendo 11 alunos de graduação e  5 alunos de pós-graduação do curso de Ciência da Computação e', '3.1.1 Participantes. O estudo foi realizado em um ambiente  acadêmico, com 16 participantes, sendo 11 alunos de graduação e  5 alunos de pós-graduação do curso de Ciência da Computação e  Sistema de Informação da Universidade Federal do Amazonas  (UFAM). Os alunos haviam cursado as disciplinas de Engenharia   de Software e Análise e Projeto de Sistemas e estavam cursando  a disciplina de Engenharia de Software Experimental.   Os alunos foram caracterizados como novatos,  pois tinham  apenas experiência acadêmica com a especificação de caso de uso  e user story. Somente um participante da pós -graduação já havia  trabalhado user story e com caso de uso na indústria.   3.1.2 Artefatos. Todos os participantes assinaram um  formulário de consentimento - Termo de Consentimento Livre e  Esclarecido (TCLE), em que concordaram em fornecer seus  resultados para análise. Os artefatos utilizados neste estudo  experimental foram: a) descri ção textual de quatro  cenários de  aplicações com níveis  de complexidade semelhantes ; b) modelo  de especificação de caso de uso e user story; c) artefatos para  criação da especificação; d) artefatos para criação dos mockups ;  e) artefatos para a inspeção dos  mockups; f) artefato para  descrição de e-mail de comunicação ; g)  questionários de  avaliação de especificação, avaliação de mockup e avaliação  geral. Os 4 cenários utilizados no estudo foram:  Cenário 1 : Um sistema de  restaurante para pessoas com  restrição alimentar onde é possível clientes procurarem  restaurantes que oferecem comidas diferenciadas e também  realizar o pedido sem sair de casa. As funcionalidades do sistema  são: cadastro de cliente, pesquisar restaurantes especializados em  algum tipo de restrição alimentar e realização de pedido online.  Cenário 2: Um sistema de planejamento de viagens para as  pessoas que criam seus roteiros de viagens. As funcionalidades  do sistema  são: cadastro de roteiros de viagens, cadastro de  informações de voos, reserva de hotéis e controle de dinheiro.  Cenário 3: Um sistema de acompanhamento de rotinas de  idosos para as famílias e para os idosos terem seus dados de  rotina disponível para consulta. As funcionalidades do sistema   são: cadastro de consultas médicas, cadastro e lembrete  de  remédios utilizados e registro de informações do idoso.   Cenário 4: Um sistema para facilitar a troca de livros entre  seus usuários. As funcionalidades do sistema  são: cadastro de  livros, disponibilizar livros para troca e gerenciar de troca.  3.1.3 Treinamento. Todos os participantes receberam  treinamento de 2 h30min em um mesmo ambiente sobre os dois  tipos de especificação. Durante o trei namento também foram  realizados exercícios práticos de especificação em casos de uso e  216', 'user stories.  3.1.4 Inspeção dos mockups. Para identificar os defeitos, foi  criado um formulário de inspeção baseado em Travassos et al.  [32] e no Método de Avaliação de Comunicabilidade (MAC) [25],  conforme apresentado na Tabela 1. Foram adotadas essas  nomeclaturas diferenciadas do MAC para d espertar o interesse  dos alunos na atividade de inspeção dos mockups.  Tabela 1: Classificação de defeitos entre especificação e  Mockups  Categoria Descrição Exemplo  Onde tá?  Informações que  foram descritas na  especificação e não  foram inseridas no  mockup  Na especificação tem a  descrição: “O cliente  seleciona opção Salvar”. No  mockup não tem o botão  Salvar   Não era  assim!   Informações que  foram descritas na  especificação e  inseridas no  mockup de outra  forma  Na especificação tem a  descrição: “O cliente  seleciona a opção Salvar”  No mockup o nome do  botão é Inserir   O que é isso? Informações que  foram inseridas no  mockup e que não  estavam descritas  na especificação  No mockup tem a opção  Salvar e Cancelar. Na  especificação só tem a  descrição da opção Salvar.   Falta de  dependência!  Informações que  foram descritas  como um cálculo ou  informação  derivada de outra  que no mockup não  obedeceram essa  dependência  No mockup tem que mostrar  um campo idade e preço da  entrada do cinema, ambos  dependentes da data de  nascimento. Ou seja, campo  idade e preço deve ser  calculado a partir da data de  nascimento e não inseridos.     3.1.5 Inspeção da especificação. Para avaliar as especificações  de casos de uso e user stories geradas pelos grupos, foi criado um  checklist de inspeção. A técnica de checklist foi escolhida por ser  uma das mais utilizadas para inspecionar casos de uso [ 4] e fo i  adaptada para inspecionar também as user stories . Com a  inspeção é possível reduzir esforços em relação ao retrabalho na  correção de defeitos em artefatos durante o desenvolvimento de  software [20]. Desse modo, f oi elaborado um checklist de  inspeção baseado em Travassos et al. [32] e Anda et al. [3] com o  objetivo de identificar defeitos na especificação de casos de uso e  user stories, como, omissão de regras de negócio, o missão de  fluxos alternativos e fluxo de exceções para caso de uso e  omissão de cenários para user stories . Travassos et al.  [32]  apresentam as classes de defeitos que comumente são  encontrados em artefatos de soft ware: Omissão (Informações  necessárias foram omitidas do caso de uso), Fato Incorreto  (Algumas informações no caso de uso contradizem a lista de  requisitos ou do conhecimento geral do domínio do sistema),  Inconsistência (As informações em uma parte do caso  de uso  estão inconsistentes com outras no caso de uso), Ambiguidade  (As informações no caso de uso são ambíguas, isto é, é possível  ao cliente, desenvolvedor ou testador interpretar as informações  de diferentes maneiras podendo não levar a uma implementaç ão  correta) e Informação Estranha (As informações fornecidas não  são necessárias para o caso de uso). A técnica de Anda et al. [3]  apresenta itens de verificação que auxiliam de forma mais  detalhada quais informações as especificações de caso de uso  devem conter para atender aos requisitos de qualidade de um  caso de uso, esses itens foram adaptados para a user story. Desta  forma os itens de verificação foram organizados seguindo a  taxonomia de Travassos et al. [32] como mostra a Tabela 2, itens  de verificação para especificação de caso de uso e itens de  verificação para especificação de user story. Os checklists foram  revisados por três pesquisadores antes de serem utilizados para a  inspeção das especificações.   Tabela 2: Exemplos dos Itens de Verificação para Caso de  Uso (UC) e User story (US)  Categoria  Código Itens de Verificação  Omissão UC_OMI _01 Faltou identificar/descrever o  nome do(s) ator(es)?  US_OMI _01 Faltou descrever a user story do  cenário?  Fato incorreto UC_FIN _01 O nome dado ao caso de uso', 'Omissão UC_OMI _01 Faltou identificar/descrever o  nome do(s) ator(es)?  US_OMI _01 Faltou descrever a user story do  cenário?  Fato incorreto UC_FIN _01 O nome dado ao caso de uso  expressa corretamente seu  objetivo?  US_ FIN_ 05 A descrição no campo “Dado que”  do cenário BDD está  incompleta/incorreta?  Inconsistência UC_INC_03 O sequenciamento nos fluxos  principal, alternativos e de  exceções estão coerentes?  US_INC_02  A descrição do cenário está  inconsistente com o  comportamento da funcionalidade  do sistema?  Ambiguidade UC_AMB_01 Os nomes de atores refletem seus  papéis? Ou estão ambíguos  (podem levar a dupla  interpretação)?  US_AMB_01 O nome do cenário permite  interpretações diferentes do seu  objetivo?  Informação  estranha  UC_ IES_02  Informações descritas nas regras  de negócio fazem parte do  contexto do caso  US_IES_01 Informações descritas nos  cenários não fazem parte do  contexto das funcionalidades do  sistema?    Foram criados graus de severidade para cada tipo de defeito.   Por exemplo, a severidade do tipo “Grave” foi utilizada para  classificar defeitos de omissão, ou seja, informações que não  foram descritas no caso de uso e na user story. Os defeitos de  severidade “Média” foram utilizados para classificar informações  que não foram descritos por completo ou que foram descritos de  217', '5  forma incorreta. Os defeitos de severidade “Baixa” identificaram  aqueles defeitos que não prejudicavam a compreensão e  entendimento do caso de uso e da user story.  3.2 Execução do Estudo  Este experimento foi executado em 3 etapas: 1) especificação dos  requisitos, 2) construção dos mockups e 3) inspeção para  investigar se os mockups estavam de acordo com o que  foi  especificado. Na etapa 1  os participantes foram divididos  aleatoriamente em quadro grupos (1 a 4)  com 4 pessoas cada  e  receberam os cenários para fazerem as especificações com user  story e caso de uso e modelos para serem seguidos . Neste ponto,  destaca-se que apesar de cada cenário conter várias  funcionalidades, cada grupo especificou apenas duas com grau  de complexidade equivalentes entre si.  A Tabela 3  apresenta a   divisão dos grupos, cenários e especificações durante o estudo.  Tabela 3: Distribuição dos grupos, cenários e especificações  Grupos Cenários Tipos de especificação  1 1 User story  2 2 User story  3 3 Caso de uso  4 4 Caso de uso    Na etapa 2 a principal atividade foi a construção dos mockups  baseados nas especificações  criadas. Destaca-se q ue cada grupo  recebeu uma especificação de caso de uso  e user story diferente  da que especificou na etapa 1, por exemplo, equipe 1 recebeu um  cenário e fez a especificação de requisitos  na etapa 1, na etapa 2  de construção de mockups, a equipe  1 recebeu as especificações  da equipe 2 e 3 para construir o mockup , conforme mostrado na  Tabela 4. Durante a construção dos mockups, os grupos podiam  tirar dúvidas com o grupo especificador através de bilhetes. Para  isto, foi utilizado uma folha de papel onde o grupo de construção  escrevia sua dúvida , o  moderador entregava -o para o grupo  especificador responder o questionamento e, por fim, o papel era  devolvido para a grupo de construção.  Esse procedimento foi  inserido no estudo com o intuit o de evitar a propagação de   defeitos inseridos na especificação para os mockups.  Tabela 4: Distribuição de construção dos mockups  Grupos Construção de Mockups dos cenários  1 3 e 2  2 4 e 1  3 1 e 4  4 2 e 3    Nesta etapa os grupos não fizeram o mockup das  especificações que tinham criado no anterior, havendo o cuidado  de garantir o rodízio da especificação. Por exemplo, quem  especificou com user story  construiu o primeiro mockup  utilizando uma especificação de caso de uso e depois construiu  um mockup com user story. Dessa forma, pro curou-se reduzir o  viés de aprendizado no tipo de especificação.  Na etapa 3 foi realizada a inspeção dos mockups pelos gru pos  que fizeram a especificação . Os grupos que criaram as  especificações receberam os mockups criados pel os outro s  grupos para identificar os defeitos  entre o que foi especificado  por eles e o que foi criado nos mockups pel os grupos de  construção. Dessa forma, o  grupo 1 verificou os defeitos dos  mockups criados pel os grupos 3 e 2. O grupo  2 verificou os  defeitos dos mockups criados pel os grupos 4 e 1. O grupo 3  verificou os defeitos dos mockups criados pelos grupos 1 e 4. E o  grupo 4 verificou os defeitos dos mockups dos grupos 2 e 3.  4 ANÁLISE DOS RESULTADOS  4.1 Resultados Quantitativos   Nesta seção, são apresentados  os resultados  quantitativos  referentes à análise, feita pelos pesquisadores, das especificações  de caso de uso e user stories criada pelos participantes , análise  dos defeitos cometidos ao criar os mockups a partir das  especificações e a relação entre as especificações e mocku ps  criados, visando à comunicação dos requisitos dentro dos grupos.  4.1.1 Análise da especificação . Para avaliar a corretude  das  especificações, criadas pelos participantes do estudo, os  pesquisadores realizaram inspeções nas especificações usando os  checklists de inspeção para auxiliar na verificação de defeitos da  especificação de casos de uso e user stories de forma guiada. Cada  item identificado como incorreto, impactou no grau de corretude', 'especificação de casos de uso e user stories de forma guiada. Cada  item identificado como incorreto, impactou no grau de corretude  da mesma. O processo de avaliação das especificações foi  realizado em duas etapas. Na primeira etapa dois , pesquisadores  avaliaram as es pecificações que os participantes elaboraram a  partir dos cenários entregues (duas especificações de caso s de  uso e duas especificações user stories). Na segunda etapa, três  pesquisadores revisaram por completo a  avaliação realizada e  retiraram as discrepâncias. A Tabela 5 apresenta uma parte dessa  avaliação.   A avaliação das especificações de caso de uso e user story  resultou em 45 defeitos, dos quais, 24 defeitos foram  identificados nas especificações de user stories dos grupos 1 e 2 e  21 defeitos foram identificados nas especificações de casos de uso  dos grupos 3 e 4. A Fig. 1  mostra o resultado quantitativo dos  diferentes tipos de defeitos identificados nas especificações.      Figura 1:  Números de defeitos encontrados nas  especificações de user stories e caso de uso.  218', 'Tabela 5: Classificação da avaliação das especificações  Cód. do  defeito  Descrição do defeito  encontrado  Nome do  UC  Elem. do  template  UC_AMB_04  Na pré-condição ficou  estranho à frase: “Estar  logado” – O usuário  deve estar logado,  como descrito no caso  de uso Cadastrar um  livro  Solicitar  livro  Pré- condição  UC_OMI_03  Faltou escrever o fluxo  de exceção livro já  cadastrado  Cadastrar  livro  Fluxo de  exceção  UC_INC_03  O fluxo de exceção 1  deveria retornar para o  passo 2 do fluxo  principal, já que já  atingiu o limite de  solicitações  Solicitar  livro  Fluxo de  exceção  UC_FIN_03  No passo 3 faltou  descrever o ator que  faz a ação  Cadastrar  remédio  Fluxo de  exceção    O maior número de defeitos encontrados nas especificações  de user stories  foram de Fato Incorreto (9), seguido por  Ambiguidade (6),  Omissão (5), Inconsistência (2) e Informação  estranha (2). Nas especificações de caso de uso os defeitos mais  encontrados foram Omissão (9), Ambiguidade (9) seguido por  Fato incorreto (2) e Inconsistência (1). Não foram encontrados  defeitos de Informação entranha. A Fig. 2  mostra o número de  defeitos encontrados nas especificações de user stories e casos de  uso distribuídas por grau de severidade. A especificação de user  story teve maior número de defeitos do tipo Médio (17), seguido  de Grave (5) e Baixo ( 2). Vale ressaltar que o tipo Grave na user  story foi a omissão de cenários nas especificações. Na  especificação do grupo 2 foram encontrados 8 defeitos, onde 4  eram omissões de cenários importantes para o desenvolvimento  da funcionalidade. A especificação  de caso de uso teve maior  número de defeitos do tipo Médio (10), seguido de Grave (9) e  Baixo (2). Os defeitos do tipo grave em caso de uso estão  relacionados a omissão de fluxo de exceção e regras de negócio.    Figura 2: Número de defeitos por severidade  4.1.2 Análise dos mockups.  A avaliação dos mockups em  relação à especificação resultou em 25 defeitos, dos quais, 11  foram identificados nos mockups construídos a partir de  especificação de user story  e 14 identificados nos mockups  construídos a partir de espec ificação de caso de uso. A Tabela 6  mostra o número dos diferentes tipos de defeitos identificados na  inspeção dos mockups.   Tabela 6: Defeitos encontrados nos mockups  Especificação Onde  tá?  Não era  assim!  Falta de  dependência!  O que é  isso?  User story 2 4 1 4  Caso de uso 4 4 4 2    Nos mockups criados a partir de especificação de user story, o  maior número de defeitos encontrados foram do tipo “Não era  assim!” (4) e “O que é isso?” (4), seguido de “Onda tá?” (2) e  “Falta de dependência” (1). Nos mockups c riados a partir de  especificação de caso de uso, o maior número de defeitos foram  do tipo “Onde tá?”, “Não era assim!” e “Falta de dependência”  com 4 defeitos cada, seguido de “O que é isso?”, com 2.  Além disso, foi realizada uma avaliação da completude do s  mockups criados pelos grupos em relação ao que foi  especificado. Para avaliar a completude dos mockups, foi  verificado se cada informação definida na especificação estava  representada de forma completa no mockup. Cada item  identificado como incompleto im pactou na completude do   mesmo, os resultados são apresentados na Fig. 3.       Figura 3: Corretude dos mockups criados  Percebe-se que não houve diferença significativa quanto à  completude dos mockups. A completude do s mockups criados  baseados em user stories pelos grupos 2 e 4 foi um pouco maior  do que utilizando caso de uso. Já os grupos 1 e 3 tiveram um  desempenho melhor utilizando o caso de uso como base para a  construção do mockup.   4.1.3 Análise de propagação de defeitos. Quanto a questão de  propagação de defeitos, ou seja, a quantidade de defeitos  inseridos nos mockups devido a defeitos inseridos previamente  nas especificações, destaca -se que enquanto os mockups criados  a partir de user stories apresentaram 36% de defeitos nesta', 'inseridos nos mockups devido a defeitos inseridos previamente  nas especificações, destaca -se que enquanto os mockups criados  a partir de user stories apresentaram 36% de defeitos nesta  situação, os mockups criados a partir de casos de uso  apresentaram 0% de defeitos nesta situação. Nesta seção, serão  apresentadas algumas respostas para esta questão.  219', 'Figura 4: Defeitos especificações x defeitos mockups  Defeitos inseridos na especificação de user stories: com o intuito  de prover uma visão geral sobre defeitos inseridos nos mockups  e defeitos inseridos na especificação, a Fig. 4  apresenta um  gráfico com a frequência de cada defeito, bem como, a relação  entre eles. A área do gráfico com fundo cinza destaca os defeitos  existentes nos mockups e especificações que não possuem  relação entre si.  Quanto aos defeitos que possuem relação com propagação de  defeitos, destaca -se que, dos 11 defeitos encontra dos nos  mockups, 4 (36%) foram gerados a partir de defeitos nas  especificações de user stories, os quais estão detalhados na Fig. 5.      Figura 5: Defeitos propagados para mockups   Quanto a propagação de defeitos dos mockups e  especificações, a área do gráf ico com fundo cinza da Fig. 4 ,  mostra que enquanto os mockups possuem 7 (74%) defeitos nesta  situação – “Onde está?” (1), “Não era assim” (4), “O que é isso?”  (2), as especificações possuem 20 (83%) defeitos nesta situação –  Omissão (5), Fato incorreto (8) , Inconsistência (2) e Ambiguidade  (5). Estes resultados indicam que a maioria dos defeitos não  possuem relação de propagação.  Devido à especificação do cenário 2 ter sido simplista,  possuindo um alto grau de omissões de user stories, os mockups  apresentaram poucos defeitos de construção, pois a simplicidade  da especificação deixava pouca margem para defeitos. Neste  caso, os grupos não inseriram nenhum defeito ao construir os  mockups.   Defeitos inseridos na especificação de Caso de Uso:  com o  intuito de pro ver uma visão geral sobre defeitos inseridos nos  mockups e defeitos inseridos na especificação, a Fig. 4 apresenta  um gráfico com a frequência de cada defeito.  O gráfico mostra,  também, que não houve relação de propagação de erro ao utilizar  especificação de caso de uso.   Nenhum defeito encontrado nos mockups foram gerados  pelos defeitos na especificação de caso de uso. Os defeitos  encontrados nos mockups criados pelos grupos foram  relacionados a: não inserções de informações que estavam  descritas, alteração de mensagem no mockup, inserção de botões  não especificados; não indicação da navegabilidade entre as telas  e apresentação dos campos de forma incorreta.  Foi observado que os defeitos dos mockups não estavam  relacionados aos defeitos inseridos na especificação dos cenários,  assim como mostra a Fig. 4 . Os defeitos com maior frequência  nas especificações foram: Omissão (9) e Ambiguidade (9).  Não houve propagação do defeito de Omissão inserido na  especificação, pois como não havia informação especificada, os   mockups construídos não consideraram tais informações,  portanto não houve como errar. Assim, os grupos não inseriram  nenhum defeito ao construir o mockup.   Quanto aos defeitos nas especificações relacionados a   Ambiguidade, destaca -se que uma das técnicas definidas na  Seção 3.2 Execução do estudo – utilização de bilhete de  comunicação para tirar dúvidas – quando utilizada evitou a  inserção dos defeitos do tipo “O que é isso?” (3) e “Não era  assim” (1) nos mockups. Esta situação indica a importância da  equipe esclarecer defeitos do tipo “Ambiguidade” com o emissor  da especificação e não tomar decisões por conta própria. Em  projetos reais pode-se substituir os bilhetes por e -mails, reuniões  ou videoconferências.   220', 'No contexto deste artigo, o termo emissor foi u tilizado para  representar o participante que especifica os requisitos dos  clientes no formato de caso de uso ou user story . E para o  participante que recebe a especificação para desenvolver sua  atividade foi utilizado o termo receptor [21].  4.2 Resultados Qualitativos  Nesta seção, são apresentados  os resultados  de uma análise  qualitativa das respostas dos participantes (P) aos questionários  aplicados ao longo da execução do estudo, com o objetivo de  avaliar a utilização das especificações de cas o de uso e  user story  para a comunicação de requisitos.  Os métodos qualitativos  segundo Seaman [26] apoiam uma melhor compreensão das  questões que necessitam de uma análise mais específica e  detalhada, permitindo ao pesquisador considerar o  comportamento humano e entender completamente o objeto  estudado. A análise qualitativa realizada neste trabalho baseia -se  em procedimentos d e codificação de Grounded Theory [30], mais  detalhes no relatório técnico [23].  Enquanto eram analisados os dados contidos no questionário,  foram criados códigos associados  com fragmentos de texto.  Outro pesquisador analisou os códigos  relacionados com as  citações em cada transcrição do questionário. Este pesquisador  verificou os códigos e categorias para validar o processo de  codificação e, portanto, mitigar o viés eventualmente causado  pela participação de um único pesquisador no processo de  codificação.   4.2.1 Análise dos resultados sobre especificação. Foram  identificadas as dificuldades encontradas pelos participantes  (P)  na utilização dos modelos de especificação:  - Dificuldade com o campo ‘quando’ da user story (ver citação  de P04 e P05 abaixo).  “(…) Um pouco de dificuldade para especificar as pré -condições  que algumas vezes têm semelhança com os passos do ""quando"" ” –  P04  “(…) esqueço as vezes a função do campo ""quando""” – P05   - Dificuldade com os fluxos do caso de uso  (ver citação de  P10 e P16 abaixo).   “(…) quando apareceu uma condição (se seria um fluxo  alternativo ou um fluxo de exceção)” – P10  “(…) nos momentos de especificar os fluxos alternativos (como  verificar o passo em que este será chamado)” – P16  Os participantes que apresentaram dificuldades na utilização  da estrutura de especificação das user stories e casos de uso   indicaram que pode haver um problema no modelo de  especificação ou a inexperiência nos modelos pode atrapalhar na  especificação dos requisitos.  Quanto à comunicação de requisitos a través de user stories,  um participante afirmou que user story é capaz de comunicar  requisitos para todos os integrantes da equipe de  desenvolvimento (P01). Os participantes P03 e P04 afirmaram  que depende do tamanho do projeto e equipe, como mostram as  citações abaixo:   “(…) cada cenário mostra as regras que devem ser  seguidas/inseridas no sistema” – P01  “(…) user story  é super eficaz para projetos pequenos de  desenvolvimento de software” – P03  “(…) depende muito do tamanho da equipe e da forma de  trabalho” – P04  No entanto, os participantes P02 e P05 afirmaram que user  story não é suficiente para comunicar requisitos, pelas seguintes  razões:  “Não, pois existem vários outros passos que não ficam  totalmente claros para seguir” – P02  “Não é tão bom para representar os fluxos de exceção” – P05  Quanto a comunicação de requisitos utilizando a  especificação de caso de uso somente um participante (P16)  discordou que caso s de uso são capazes de comunicar requisitos  para todos os integrantes da equipe de desenvolvimento, como:  “a especificação não detalha exatamente a consequência das  ações tomadas pelo usuário, por exemplo, além de agrupar muitas  informações em um único local ou que pode gerar confusão ao  seguir os fluxos” – P16  Ao verificar se os participantes que u tilizaram user story para  especificar utilizariam outro artefato complementar¸ foram  listados os seguintes artefatos que poderiam tornar a', 'Ao verificar se os participantes que u tilizaram user story para  especificar utilizariam outro artefato complementar¸ foram  listados os seguintes artefatos que poderiam tornar a  especificação mais completa: Protótipo (P01), Detalhes do  usuário (P02), Caso de uso (P03, P05 e P06) e Personas (P04, P07 e  P08). Os que utilizaram a especificação de caso de uso apontaram  os seguintes artefatos que poderiam tornar a especificação mais  completa: Diagrama de caso de uso (P11), Protótipos (P15) e User  stories (P09, P12 e P16).   4.2.2 Análise dos resultados sobre criação de mockups . Quanto  à utilização das especificações de caso de uso e user story para a  criação dos mockups foram encontradas três categorias de  problemas de comunicação: problema de especificação por parte  do emissor, p roblema no modelo de especificação e problema de  entendimento do modelo de especificação.  A Tabela 7 mostra  algumas citações referentes a estes problemas.  Todas as citações  relacionadas são mostradas em [23].  Tabela 7: Exemplos de códigos sobre problemas  Categorias Citações   Problema de  especificação  por parte do  emissor  “(…) alguns fluxos pareciam que levavam para  duas telas ao mesmo tempo” – P05 - UC  “Na construção do mockup percebemos que  algumas informações estavam faltando” – P10   - US  Problema no  modelo de  especificação  “(…) excesso de fluxos, algum elemento pode  passar batido, pois as informações ficam  espalhadas” – P16 - UC  “a especificação é falha em algumas  informações que são necessárias em cenários  adjacentes” – P09 - US  Problema de  entendimento  do modelo de  especificação    “(…) como tinha fluxo alternativos, foi  necessário buscar informações fora do fluxo  principal e isso dificulta a extração” - P11 - UC  “tive dificuldade na parte de ""dado que"" pois  fiquei em dúvida se os dados eram pré-condições  ou se eram para ser inseridos” - P05 - US    4.2.3 Análise dos resultados gerais.  Os resultados do  221', '3  questionário final, verificou -se na percepção dos participantes  qual das especificações, caso de uso ou user story, detalhava mais  as informações par a a construção dos mockups. Os argumentos  relacionados às preferências por user story foram:  “(…) user story mostra melhor as interações das ações” - P02  “(…) user story foi mais detalhista, pela possibilidade de criar  cenários separados” – P04  Os argumentos relacionados às preferências por caso de uso  foram:  “(…) a especificação com mais detalhes de informação para  construção de mockup é o caso de uso, porque as informações de  regra de negócio, por exemplo, deixam mais detalhada” – P10  “(…) casos de uso de talham melhor todos os fluxos, fornecendo  maiores detalhes das ações” – P16  Vale ressaltar que dos 8 participantes que especificaram com  user story somente 3 acham que caso de uso detalha mais as  informações para a construção dos mockups e dos 8  participantes que especificaram com caso de uso, somente um  acredita que user story detalha mais para construção do mockup.  5 DISCUSSÃO  Durante a análise d a totalidade de defeitos encontrados, a user  story totalizou 24 defeitos contra 21 de caso s de uso. Apesar da  especificação de caso de uso ter apresentado 9 omissões contra 5  da user story, os problemas gerados com as omissões da user story  foram mais impactantes, pois tratava-se de informações  essencias para a construção dos mockups.   Comparando o número de defeitos encontrados nos mockups  identificados na inspeção, foi observado o total de 11 defeitos nos  mockups criados util izando user story e 14 defeitos nos mockups  criados utilizando UC. Os mockups criados com caso de uso  tiveram mais defeitos do tipo “Onde tá? ”, “Não era assim! ” e  “Falta de dependência! ”. Esses defeitos podem ter sido  ocasionados por problema de entendimen to do modelo de  especificação.  Como mostrado na Tabela 7, os problemas de comunicação de  requisitos encontrados foram: 1) Problema de especificação por  parte do emissor, 2) Problema no modelo de especificação e 3)  Problema de entendimento do modelo de espe cificação. Estes  problemas podem ter sido ocasionados pela falta de experiência  dos participantes nestes tipos de especificação . Sugerindo assim  que o fator humano não deve ser negligenciado na dinâmica da  comunicação de requisitos dentro de equipes de  desenvolvimento.   Neste estudo, existiram algumas ameaças que podem afetar a  validade dos resultados e que foram mitigadas quando possível.  As principais ameaças foram:  (1) Efeitos de treinamento: os  participantes receberam treinamento equivalente nos dois ti pos  de especificação, incluindo atividades teóricas e exercícios  práticos em sala de aula; (2) Uso de cenários: foi minimizada  utilizando cenários escritos em linguagem natural, onde os  requisitos deste cenário estavam explícitos, de forma similar aos  exercícios realizados durante o treinamento ; (3)  Representatividade dos participantes: embora os sujeitos deste  estudo fossem estudantes, Höst [14] afirma que estudantes  podem representar uma população de profissionais  da indústria;  (4) Tamanho e h omogeneidade da amostra: devido ao número  limitado de participantes, também uma vez que todos os  participantes eram estudantes da mesma instituição, os  resultados deste estudo não podem ser considerados conclusivos.   6 CONCLUSÃO  Este artigo apresentou um estudo experimental exploratório, que  comparou a dinâmica da comunicação de requisitos e seus  resultados utilizando especificação de caso de uso e user stories  como base para a criação de mockups. A questão de pesquisa  investigada foi avaliar se a comunicação de requisitos para a  construção de mockups é afetada de forma diferente ao se usar  as especificações de casos de uso ou user stories.  De acordo com a análise realizada sobre os resultados  alcançados, foi identificad o que a especificação de caso de uso  gerou mais defeitos na parte da construção dos mockups e', 'De acordo com a análise realizada sobre os resultados  alcançados, foi identificad o que a especificação de caso de uso  gerou mais defeitos na parte da construção dos mockups e  menos defeitos na parte de especificação. Entretanto, destaca-se  que a quantidade e o impacto desses defeitos no resultado final  não são suficientes para determin ar qual das duas especificações  é melhor ou pior para a comunicação de requisitos entre equipes  de desenvolvimento de software.  Os resultados mostraram que  não há diferença significativa que sustente escolher por uma  forma de especificação em detrimento da  outra. Então, sob esta  ótica, as equipes de desenvolvimento de software que estiver em  com dúvida em qual das formas de especificação adotar, podem  optar tanto pela utilização de user story quanto caso de uso.  Percebe-se, também, que vários defeitos encont rados nos  mockups não foram causados por defeitos na especificação, mas  originaram-se por fatores relacionados  à proatividade do  receptor sem considerar o que estava especificado e sem  considerar a necessidade de validação por parte do emissor. Este  tipo d e proatividade deve ser desencorajado, pois pode gerar  mais problemas do que benefícios.  No entanto, foi observado que  a partir da análise geral dos  resultados, indícios que o fator humano foi uma das causas de  geração e propagação dos defeitos, além da pa rte técnica. Para  minimizar esses tipos de problemas, deve-se identificar quais os  defeitos possuem natureza essencialmente técnicas e quais  defeitos são advindos de fatores humanos. Os defeitos de  natureza técnicas estão relacionados com o artefato utiliz ado  para especificação dos requisitos e poderão ser evitados ou  eliminados através de treinamentos, definições de padrões  (nomenclaturas, escritas , templates ) e inspeção nas  especificações. Os defeitos de natureza humana  estão  relacionados com o profission al que utilizada o artefato em suas  atividades, podem estar relacionados à falta de conhecimento do  domínio do problema, falta de experiência, falta de compromisso  e deve-se procurar meios de minimizar esses tipos  de  interferência.   Neste estudo, existiram  algumas limitações que devem ser  consideradas. As principais limitações foram: (1)  utilização de  cenários com níve is de complexidade baixo s - foi adotado esse  nível de complexidade pois  o estudo foi realizado em ambiente  acadêmico e o tempo para execução das atividades foi limitado;  (2) aspectos avaliados - tratou-se a representação das  funcionalidades de interação  e não o aspecto de interferência  222', 'entre requisitos;  (3) avaliação da comunicação  - a comunicação  entre diferentes stakeholders não foi abordada, pois o foco foi  apenas na comunicação dentro da equipe de desenvolvimento.   Essas limitações são pontos a serem melhorados em estudos  futuros.  Como trabalhos futuros, pretende-se reaplicar este estudo  com novos alunos e na indústria para identificar outras causas de  problemas de comuni cação de requisitos . E, com o objetivo de  verificar como ocorre a comunicação de requisitos na fase de  implementação e testes, pretende -se entender o experimento  para outras fases do ciclo de vida do desenvolvimento de  software. Pretende-se, também, investigar quais fatores humanos  são capazes de influenciar a propagação de defeitos na  comunicação de requisitos em equip es de desenvolvimento de  software e como podemos minimizar esse tipo de problema .  Além disso , pretende-se realizar um levantamento sobre as  principais técnicas que utilizam linguagem d o contexto para  comparar o resultado com as técnicas deste trabalho.  AGRADECIMENTOS  Os autores agradecem o apoio financeiro do CNPq processo  423149/2016-4, CAPES processo 175956/201 3, da FAPEAM ,  processo 062.00578/2014, da  FAPERJ (projetos E- 26/010.000883/2016, E -211.174/2016), UNIRIO (PQ -UNIRIO  01/2016 e 01/2017) e também aos participantes do estudo.  REFERÊNCIAS  [1] A. Al-Rawasa and  S. Easterbrook. 1996. Communication problems in  requirements engineering: A field study, In  Proc. of Conf. on Professional on  Awareness in Software Engineering, London, 47–60.   [2] B. Anda, H. Dreiem, D. Sjøberg and M. Jørgensen. 2001. Estimating software  development effort based on use cases – experiences from industry. In UML  2001 - The Unified Modeling Language. Modeling Languages, Concepts, and  Tools. Springer, Berlin Heidelberg, v. 2185, 487–502.  [3] B. Anda, K. Hansen and G. Sand. 2009. An investigation of use case quality  in a large safety -critical software development project. In Information and  Software Technology , v . 51 , n. 12, 1699 –1711. DOI:  https://doi.org/10.1016/j.infsof.2009.04.005.  [4] B. Anda and D. Sjøberg. 2002. Towards an inspection technique for use case  models. In Proc. of the  14th Intl . Conf. on Software Engineering and  Knowledge Engine ering, ACM, NY, USA,  127–134. DOI:  http://dx.doi.org/10.1145/568760.568785.  [5] A. Belgamo and Luiz E. G. Martins. 2000. Estudo comparativo sobre as  técnicas de elicitação de requisitos do soft ware. In XX Congresso Brasileiro  da Sociedade Brasileira de Computação (SBC).  [6] E. Bezerra.  2007. Princípios de Análise e Projeto de Sistemas com UML,  Campus, 2ª edição.  [7] A. Ceverino e   Fernando P. Nascimento. 2016. Utilização da técnica de  desenvolvimento orientado por comportamento (BDD) no levantamento de  requisitos. Revista Interdisciplinar Científica Aplicada, v.10, n.3, 40-51, TRIII  2016. ISSN 1980-7031.  [8] D. Chelimsky , D. Astels, B. Helmkamp , D. North, Z. Dennis and A.   Hellesoy. (2010). Th e RSpec Book: Behaviour Driven Development with  Rspec. Cucumber, and Friends, Pragmatic Bookshelf.   [9] A. Cockburn. 2001. Writing Effective Use Cases, Vol. 1, Addison -Wesley,  Boston.  [10] E. Evans. 2003. ""Domain-Driven Design: Tacking Complexity In the H eart  of Software"". Boston, MA, USA: Addison-Wesley.  [11] Daniel M. Fernández , S. Wagner, M. Kalinowski , M. Felderer, P. Mafra, A.  Vetrò, T. Conte, et al.. 2016. Naming the pain in requirements engineering.  Empirical Software Engineering , 1 -41.DOI: http://dx.doi.org/10.1007/s10664- 016-9451-7.  [12] K. Gohil, N. Alapati and S. Joglekar. 2011. Towards behavior driven  operations (bdops). In: Intl. Conf. on Advances in Recent Technologies in  Communication and Computing, 262 -264.  [13] B. Hoisl, S. Sobernig and M.  Strembeck. 2014. Comparing Three Notations  for Defining Scenario -based Model Tests: A Controlled Experiment In 9th  Intl. Conf. on the Quality of Information and Communications Technology', 'for Defining Scenario -based Model Tests: A Controlled Experiment In 9th  Intl. Conf. on the Quality of Information and Communications Technology  (QUATIC 2014), 180-189. DOI: https://doi.org/10.1109/QUATIC.2014.62.  [14] M. Höst, B. Regnell and C. Wohlin . 2000.  Using Students as Subjects – A  Comparative Study of Students and Professionals in Lead -Time Impact  Assessment. In: Empirical Software Engineering. v. 5, n. 3, 201–214.  [15] I. Ibriwesh, Sin-Ban Ho, I. Chai and Chuie-Hong Tan. 2017. A Controlled  Experiment on Comparison of Data Perspectives for Software Requirements  Documentation. Arabian Journal for Science and Engineering, 1-15.  [16] I. Jacobson1987. Object-oriented development in na industrial e nvironment.  In Conf. on Object -Oriented Programming, Systems, Languages &  Applications, 183– 191.  [17] S. Lauesen and Mohammad A. Kuhail.  2012. Task descriptions versus use  cases. Requirements Engineering, v. 17, n. 1, 3 -18. DOI:  https://doi.org/10.1007/s00766-011-0140-1  [18] Esteban R. Luna, José I. Panach , J. Grigera, et al.  2010. Incorporating  usability requirements in a test/model -driven web engineering  approach.  In: Journal of Web Engineering, v. 9, n. 2, 132 - 156.  [19] M. Cohn. 2004. User stories  applied: For agile software development.  Addison-Wesley Professional.  [20] Rafael M. Mello, Eldânae N. Teixeira, M. Schots, Cláudia M. L. Werner and  Guilherme H. Travassos. 2014.  Verification of Software Product Line  Artefacts: A Checklist to Support Fe ature Model Inspections, Journal of  Univ. Comp. Science, v. 20, 720-745.  [21] V. Mikulovic and M. Heiss. 2006. How do I know what I have to do?: the  role of the inquiry culture in requirements communication for distributed  software development projects. I n Proc. of the 28th Intl. Conf. on Software  engineering, 921 - 925. DOI: https://doi.org/10.1145/1134285.1134453.  [22] P. Mohagheghi, B. Anda, R. Conradi. 2005. Effort estimation of use cases for  incremental large -scale software development. In 27th Intl.  Conf. on  Software Engineering , 303 –311. DOI:  https://doi.org/10.1109/ICSE.2005.1553573  [23] Ana C. Oran, E. Nascimento , G. Santos e  T. Conte. 2017. Relatório técnico:  Uma Análise sobre Comunicação de Requisitos Utilizando Especificação de  Caso de Uso e User story . TR -USES-2017-0010. Disponível em:  http://uses.icomp.ufam.edu.br/relatorios-tecnicos/.  [24] Keith T. Phalp, J. Vincent and K. Cox. 2007. Assessing the quality of use  case descriptions. In Software Quality Journal , v. 15, n. 1, 69 –97. DOI:  http://dx.doi.org/10.1007/s11219-006-9006-z.  [25] Raquel O. Prates, Clarisse S. de Souza and Simone D. Barbosa. 2000.  Methods and tools: a method for evaluating the communicability of user  interfaces interactions, v. 7, n. 1, 31-38.  [26] Carolyn B. Seaman. 1999. Qualitative methods in empirical studies of  software engineering. IEEE Transactions on software engineering, v. 25, n.  4, 557-572.   [27] Thiago R. Silva. 2016. Definition of a behavior -driven model for  requirements specification and testing of interact ive systems. In  Requirements Engineering Conf ., IEEE 24th Intl ., 444 -449. DOI:  https://doi.org/10.1109/RE.2016.12  [28] J. Smart. 2014. BDD in Action: Behavior -Driven Development for the  Whole Software Lifecycle. New York, USA: Manning Publications.  [29] K. Stapel , E. Knauss and K. Schneider. 2009. Using flow to improve  communication of requirements in globally distributed software projects. In  Requirements: Communication, Understanding and Softskills . Collaboration  and Intercultural Issues, 5 -14. IEEE. DO I:   http://dx.doi.org/10.1109/CIRCUS.2009.6  [30] A. Strauss and J. Corbin.   2014. Basics of Qualitative Research: Techniques  and Procedures for Developing Grounded Theory , in Thousand Oaks, CA,  SAGE publications.  [31] S. Tiwari and A.  Gupta. 2015. A syst ematic literature review of use case  specifications research. In Information and Software Technology, v. 67, 128 –', 'SAGE publications.  [31] S. Tiwari and A.  Gupta. 2015. A syst ematic literature review of use case  specifications research. In Information and Software Technology, v. 67, 128 – 158. DOI: https://doi.org/10.1016/j.infsof.2015.06.004.  [32] Guilherme H. Travassos, F. Shull,  M. Fredericks and V.  Basili. 1999.  Detecting de fects in object -oriented designs: using reading techniques to  increase software quality, In Proc. of XIV ACM SIGPLAN conf. on Object - oriented programming, systems, languages, and applications, v. 34, n. 10, 47 - 56. DOI: https://doi.org/10.1145/320384.320389.  [33] Yu-Cheng Tu, E. Tempero and C. Thomborson. 2016 . An experiment on the  impact of transparency on the effectiveness of requirements documents.  Empirical Software Engineering, v. 21, n. 3, 1035 -1066. DOI:   http://dx.doi.org/10.1007/s10664-015-9374-8.  [34] X. Wang, L. Zhao, Y. Wang and J. Sun.  2014. The Role of Requirements   Engineering Practices in Agile Development: An Empirical Study. In  Proc.  of the Asia Pacific Requirements Engineering Symposium, ser. CCIS. Springer,  v. 432, 195–209  [35] A. Zeaaraoui, Z. Bougroun, M. G. Belkasmi and T. Bouchentouf. 2013. User  stories template for object -oriented applications. In Innovative Computing  Technology (INTECH), 407–410.    223']","['Para quem é esse briefin? Profiisionasii de engenharsia de  ioftware que deieaam toomar  decsiiõei iobre o uio de caio de  uio ou user story para  eipecsifcaação de requsiisitooi com  baie em evsidêncsiai csientfcaia De oide vêm os resultados? Todai oi reiultoadoi deitoe brsiefng  foram extoraídai do eitoudo  expersimentoal exploratoórsio  conduzsido por Aa Ca Oran et ala O que está iicluído ieste  briefin? Oi prsincsipasii reiultoadoi do eitoudo  expersimentoala O que ião está iicluído ieste  briefin? Deicrsiaçõei detoalhadai iobre a  execuação do eitoudo e artoefatooi  utilsizadoia Para acessar outros briefins de  evidêicias sobre eineiharia de  software: http:////wwwalsiaaufcabr//ccbioft2017 //en//xxxsi-ibei// Para obter iiformações  adicioiais sobre USES: http:////uieiasicompaufamaeduabr//   ANALYSING REQUIREMENTS COMMUNICATION This briefin reports scieitfc evideice oi comparatve aialysis oi the requiremeits commuiicatoi  dyiamics  usiin  use  case specifcatoi aid  user stories  as the basis for mockups creatoin FINDINGS O  artigo  apreientoa  um  eitoudo  expersimentoal exploratoórsio,  comparando  a  dsinâmsica  da comunsicaação de requsiisitooi utilsizando eipecsifcaação de caio de uio e user story como baie para a crsiaação de mockupia A queitoão de peiqusiia sinveitigada fosi avalsiar ie a comunsicaação de requsiisitooi para a conitoruação de mockupi é afetoada de forma dsiferentoe ao ie uiar ai eipecsifcaaçõei de caioi de uio ou user storiesa Resultados  Quaittatvos :  ião  apreientoadoi  oi reiultoadoi quantitoativoi referentoei à análsiie dai eipecsifcaaçõei de caioi de uio e user stories crsiadai peloi  particsipantoei  e  a  análsiie  doi  defesitooi cometidoi  ao  crsiar  oi  mockupi  a  partir  dai eipecsifcaaçõeia Além dsiiio, sinveitigou-ie a relaação entore ai eipecsifcaaçõei e mockupi crsiadoi, vsiiando versifcar qual eipecsifcaação é capaz de paiiar melhor oi requsiisitooi para a equsipe de deienvolvsimentooa Análsiie  da  eipecsifcaação:  a  avalsiaação  dai eipecsifcaaçõei de caio de uio e user story  reiultoou em 45 defesitooi no tootoala Ai eipecsifcaaçõei no formatoo de user stories  apreientoaram 24 defesitooi contora 21 defesitooi dai eipecsifcaaçõei de caioi de uioa Apeiar dai  eipecsifcaaçõei  de  caioi  de  uio  toerem apreientoado 9 omsiiiõei contora 5 dai user stories, oi problemai geradoi com ai omsiiiõei nai user stories foram  masii  simpactoantoei,  posii  toratoava-ie  de sinformaaçõei  eiiencsiasii  para  a  conitoruação  doi mockup, como: deicrsiação de cenársioi, regrai de negócsio, nome de campoi e deicrsiação de meniagenia Análsiie doi mockupi: comparando o número de defesitooi encontoradoi na sinipeação doi mockupi, foram obiervadoi 11 defesitooi noi mockupi crsiadoi utilsizando  user  story e 14 defesitooi noi mockupi crsiadoi utilsizando caio de uioa Oi mockupi crsiadoi com caio de uio tiveram masii defesitooi do tipo “Onde toá?”, “Não era aiisim!” e “Faltoa de dependêncsia!”a Eiiei defesitooi podem toer isido ocaisionadoi por problema  de  entoendsimentoo  do  modelo  de eipecsifcaaçãoa Análsiie de propagaação de defesitooi: quantoo aoi defesitooi siniersidoi prevsiamentoe nai eipecsifcaaçõei de caioi de uio e user  stories e propagadoi para oi mockupi, deitoaca-ie que enquantoo oi mockupi crsiadoi a partir de user stories apreientoaram 36% de defesitooi neitoa isitouaação, oi mockupi crsiadoi a partir de caioi de uio apreientoaram 0% de defesitooi neitoa isitouaação,  ou  ieaa,  oi  defesitooi  siniersidoi  nai eipecsifcaaçõei  de  caioi  de  uio  não  foram propagadoi para oi mockupia Resultados  Qualitatvos :  ião  apreientoadoi  oi reiultoadoi de uma análsiie qualsitoativa dai reipoitoai doi particsipantoei aoi queitionársioi aplsicadoi ao longo da execuação do eitoudo, com o obaetivo de avalsiar a utilsizaação dai eipecsifcaaçõei de caio de uio e user story  para a comunsicaação de requsiisitooia Análsiie doi reiultoadoi iobre eipecsifcaação: ai', 'avalsiar a utilsizaação dai eipecsifcaaçõei de caio de uio e user story  para a comunsicaação de requsiisitooia Análsiie doi reiultoadoi iobre eipecsifcaação: ai dsifculdadei encontoradai na utilsizaação da eitorutoura de eipecsifcaação dai user stories  e caioi de uio foram decorrentoei à problemai noi  modeloi  de  eipecsifcaação  ou  a sinexpersiêncsia por partoe doi particsipantoeia Análsiie doi reiultoadoi iobre crsiaação de mockupi:  quantoo  à  utilsizaação  dai eipecsifcaaçõei de caio de uio e user  story para  a  crsiaação  doi  mockupi  foram encontoradai torêi catoegorsiai de problemai de comunsicaação: problema de eipecsifcaação por partoe do emsiiior (particsipantoe que eipecsifca oi requsiisitooi doi clsientoei no formatoo de caioi de uio ou user stories ), problema no modelo de eipecsifcaação e problema de entoendsimentoo do modelo de eipecsifcaaçãoa  Eitoei problemai podem toer isido ocaisionadoi pela faltoa de expersiêncsia doi particsipantoei neitoei tipoi de eipecsifcaaçãoa Iitoo iugere que o fatoor humano não deve ier neglsigencsiado na dsinâmsica da comunsicaação de requsiisitooi dentoro de equsipei de deienvolvsimentooa  Análsiie doi reiultoadoi gerasii: quantoo à análsiie  geral  fosi  sidentifcado  que  a eipecsifcaaçõei de caioi de uio geraram masii defesitooi na partoe da conitoruação doi mockupi e menoi defesitooi na partoe de eipecsifcaaçãoa Entoretoantoo, deitoaca-ie que a quantidade e o simpactoo deiiei defesitooi no reiultoado fnal não ião iufcsientoei para detoermsinar qual dai duai eipecsifcaaçõei é melhor ou psior para a comunsicaação de requsiisitooi entore equsipei de deienvolvsimentoo de ioftwarea Oi  reiultoadoi  moitoraram  que  não  há dsiferenaça isignsifcativa que iuitoentoe eicolher por  uma  forma  de  eipecsifcaação  em detorsimentoo da outoraa Entoão, iob eitoa ótica, ai equsipei de deienvolvsimentoo de ioftware que eitiverem com dúvsida em qual dai formai de eipecsifcaação adotoar, podem optoar toantoo pela utilsizaação de user story quantoo caio de uioa Percebe-ie, toambém, que vársioi defesitooi encontoradoi  noi  mockupi  não  foram cauiadoi por defesitooi na eipecsifcaação, mai orsigsinaram-ie por fatoorei relacsionadoi à proativsidade do receptoor (particsipantoe que recebe a eipecsifcaação para deienvolver iua ativsidade) iem conisiderar o que eitoava eipecsifcado noi cenársioi e iem conisiderar a neceiisidade de valsidaação  por  partoe  do emsiiiora Além dsiiio, foram obiervadoi sindícsioi que o fatoor humano fosi uma dai cauiai de geraação e propagaação doi defesitooi, além da partoe toécnsicaa  Para  msinsimsizar  eiiei  tipoi  de problemai, deve-ie sidentifcar quasii defesitooi poiiuem natoureza eiiencsialmentoe toécnsicai (relacsionadoi com o artoefatoo utilsizado para eipecsifcaação doi requsiisitooi) e quasii defesitooi ião  advsindoi  de  fatoorei  humanoi (relacsionadoi com o profiisional que utilsizada ORIGINAL RESEARCH REFERENCE Aa Ca Oran, Ea Naicsimentoo, Ga Santooi, Ta Contoea 2017a Analyising Requsirementoi Communsication  ising  ie Caie Specsifcation and User storiesa In Proceedsingi of 31ito Brazsilsian  Sympoisium on Software Engsineersing (XXXI SBES), Fortoaleza, Ceará, Brazsil, 10 pageia httpi:////dosiaorg//10a1145//3131151a3131166a']","**Title: Comparing Requirements Communication: Use Cases vs. User Stories**

**Introduction:**
This Evidence Briefing summarizes findings from a study that compares the effectiveness of two popular requirements communication formats—use case specifications and user stories—in software development projects. Understanding how these formats impact communication dynamics can enhance the clarity and effectiveness of requirements throughout the software development lifecycle.

**Main Findings:**
1. **No Significant Difference**: The study revealed no significant difference in the effectiveness of use case specifications compared to user stories in communicating software requirements. Both formats yielded similar outcomes in terms of the accuracy of mockups created based on the specifications.

2. **Human Factor Matters**: While the technical aspects of both formats provided comparable results, the human factor plays a critical role. Participants exhibited varying degrees of understanding and ability to communicate requirements effectively, indicating that training and experience significantly influence outcomes.

3. **Common Defects Identified**: A total of 45 defects were identified in the specifications, with user stories having slightly more issues (24) compared to use cases (21). However, the impact of defects in user stories was often more pronounced, particularly regarding essential information needed for mockup construction.

4. **Mockup Evaluation**: The mockups created from user stories had 11 identified defects while those based on use cases had 14. The types of defects varied, with user stories encountering more issues related to ambiguity and incompleteness, while use cases had more instances of information being misrepresented.

5. **Propagation of Defects**: Notably, mockups created from user stories exhibited a 36% propagation of defects originating from the specifications, while those from use cases showed no such propagation. This suggests that the clarity of the initial requirements may influence the quality of the resulting mockups.

6. **Implications for Practice**: The findings imply that software development teams can choose either format based on their preferences and context. However, attention should be paid to the human factors involved in requirements communication, such as training and clarity in specifications, to minimize misunderstandings and defects.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and educators who are involved in requirements gathering and communication within software development teams.

**Where the findings come from?**
The findings are based on an exploratory empirical study conducted by Ana Carolina Oran, Elizamary Nascimento, Gleison Santos, and Tayana Conte, which involved 16 students from the Universidade Federal do Amazonas and Universidade Federal do Estado do Rio de Janeiro.

**What is included in this briefing?**
This briefing includes insights into the comparative effectiveness of use case specifications and user stories, their impact on mockup creation, common defects encountered in both formats, and the influence of human factors on requirements communication.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the authors and their research:**
Contact Ana Carolina Oran at ana.oran@icomp.ufam.edu.br

**Original Research Reference:**
Oran, A. C., Nascimento, E., Santos, G., & Conte, T. (2017). Analysing Requirements Communication Using Use Case Specification and User Stories. In Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES 2017), Fortaleza, Brazil. https://doi.org/10.1145/3131151.3131166"
"['Challenges to the Development of Smart City Systems:   A System-of-Systems View  Everton Cavalcante, Nélio Cacho, Frederico Lopes, Thais Batista  Federal University of Rio Grande do Norte  Natal, Brazil  {everton, neliocacho}@dimap.ufrn.br, fred@imd.ufrn.br, thais@ufrnet.br      ABSTRACT  The technological empowerment of cities as a way of facing  challenges to their sustainability and quality of life of population  has gave rise to the realization of the smart city concept. Smart  cities typically encompass several distributed systems engaged in  complex relationships, including the integration and interaction  with other systems towards providing new functionalities. How- ever, these systems are highly heterogeneous, developed with  different technologies and data formats, and owned by distinct  organizations and agencies within the system, thereby challeng- ing the development of value-a dded applications. A way of ad- dressing these issues is by holistically viewing a smart city as a  system-of-systems (SoS), a set of complex systems with their  own purpose and collaborating with each other to fulfill com- mon global missions. In this pers pective, we herein discuss how  smart city systems could be holistically viewed as a complex SoS.  We also shed light on some challenges related to the develop- ment of this type of systems in the context of smart cities.  CCS CONCEPTS  • Computer systems organization → Embedded and cyber- physical systems  • Software and its engineering  → Designing  software  KEYWORDS  Smart cities, systems-of-systems, system development    ACM Reference format:  E. Cavalcante, N. Cacho, F. Lopes, and T. Batista. 2017. Challenges to the  development of smart city systems: A system-of-systems view. In Pro- ceedings of the 31st Brazilian Symposium on So ftware Engineering, Fortale- za, CE, Brazil, September 2017 (SBES’17), 6 pages.  DOI: 10.1145/3131151.3131189  1 INTRODUCTION  The world is in the midst of a po pulation shift from rural to ur- ban areas. Since 2009, more than ha lf of the entire world’s popu- lation lives in cities and this nu mber may grow to nearly 70% by  2050. With the population growth, many challenges arise and  hamper cities to become more sustainable, pleasant, and effi- cient. This unprecedented stress on the city infrastructure and  resources has led governments, businesses, and community to  rely on Information and Communication Technologies (ICTs) to  overcome the challenges posed by such a rapid urbanization.  A way of facing the issues ahead is by realizing the smart city  concept, which envisions enhancing resource utilization and in- frastructure available at a city in a sustainable fashion while im- proving the quality of life of its population. A city can be viewed  as smart when investments on human and societal capital as well  as on modern ICTs harmoniously contribute to sustainable eco- nomic development and improvements in citizens’ quality of life  through participative, engaged actions. ICTs can be regarded as  key elements to achieve these goals as they can be employed to  collect and analyze large amounts of information generated by  several sources, such as sensor networks, traffic systems, and  citizens’ devices. Such data can be used in innovative and crea- tive ways to develop applications and systems able to profoundly  change the daily lives of people by improving the services of the  city and contributing to economic growth, environmental sus- tainability, and quality of life of individuals and society. At the  same time, it is possible to foster decision-making processes  based on reliable information while providing novel and better  services, saving time, and wisely using natural, financial, materi- al, and human resources.  Nowadays, ICTs are already at the core of many solutions for  urban development while empowering the development of es- sential services for transportatio n, health, public safety, building', 'Nowadays, ICTs are already at the core of many solutions for  urban development while empowering the development of es- sential services for transportatio n, health, public safety, building  management, and smart governan ce. The technological devel- opment in the recent years and the emergence of concepts such  as ubiquitous computing, cyber-physical systems, Internet of  Things, and many others have brought the notion of smart cities  as fully-connected environments where almost every conceiva- ble element within the city is integrated into some sort of auto- mated computing system. Indeed, it can be said that the old city  of concrete, glass, and steel now conceals a huge underworld  from small to large devices and software [1].  __________________________________        Permission to make digital or hard copies of part or all of this work for personal o r classroom use is granted without fee provided that copies are not made or distrib- uted for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).   SBES’17, September 20-22, 2017, Fortaleza, CE, Brazil   © 2017 Association for Computer Machinery.  ACM ISBN 978-1-4503-5326-7/17/09. . . $15.00   https://doi.org/10.1145/3131151.3131189  244', 'SBES’17, September 2017, Fortaleza, CE, Brazil E. Cavalcante et al.        Looking forward to increase smartness, today’s cities are  evolving into environments with a myriad of distributed systems  engaged in complex relationships including the integration and  interaction with other systems towards providing new function- alities. These relationships lead smart cities to become systems- of-systems (SoS), a widespread set of independent, heterogeneous  complex systems (the so-called constituent systems) that have  their own purpose and collaborate  with each other to fulfill  common global missions (goals or purposes) [ 2]. SoS arise from  the interaction among these different constituent systems, but  the result of such an interaction is said to be more than the sum  of the constituents as it enables an SoS to offer new functionali- ties not provided by any of the constituents working alone.  The SoS concept has emerged as an active domain of research  in recent years and it is of cr ucial importance for the societal  challenges facing the world. Co nsequently, it has been possible  to find diverse SoS in several fields, such as energy, transporta- tion, healthcare, environmental monitoring, disaster manage- ment, and public safety and security, just to name a few. Notic- ing that systems in these domains can be part of a smart city, it  is at least reasonable to view a smart city as an ultimate SoS con- stituted of a plethora of complex systems, which may be SoS  themselves and that may result in dynamic and unpredictable  interactions. In this perspective, the main goal of this paper is to  present how a smart city itself can be holistically viewed as a  complex SoS with increasingly entangled dependencies and in- teractions among systems as well as stakeholders from multiple  d i f f e r e n t  d o m a i n s .  B y  o f f e r i n g  s u c h  a  v i e w ,  w e  a l s o  i n t e n d  t o   shed light on some challenges rela ted to the development of this  type of system in the context of smart cities.  The remainder of this paper is structured as follows. Section 2  introduces the main distinguishing features of SoS aiming at  providing the reader with a clearer understanding of what this  type of system stands for. Section 3 presents a view of smart cit- ies as SoS. Section 4 discusses some relevant issues to be ad- dressed when developing smart city systems (-of-systems). Final- ly, Section 5 contains some concluding remarks.  2 SYSTEMS-OF-SYSTEMS  A single system and an SoS are both systems comprising con- stituents arranged together to provide functionalities and fulfill  missions. Nevertheless, SoS have been distinguished from other  complex and large-scale systems due to a set of inherent charac- teristics, and all of them must be fulfilled to regard a system as  an SoS [2, 3]. In the following, we briefly elicit these distinguish- ing features that undoubtedly make SoS different from tradition- al single systems and hence call for a paradigm shift when de- veloping these systems. These characteristics have already made  the research community to acknowledge that traditional meth- ods to engineer systems are of li mited applicability or no longer  valid to SoS, thus making the de velopment of this class of sys- tems a significant challenge to the software engineering com- munity [4, 5].   Operational independence of constituent systems.  Con- sistent systems in an SoS are operationally independent in the  sense that they provide their own functionalities and fulfill their  own missions even when they do  not cooperate with other sys- tems within the scope of the SoS. This is not observed in single  systems since their constituent components operate as designed  to provide their functionality.   Managerial independence of constituent systems.  Con- stituent systems participating wi thin an SoS are often developed  by different organizations, with their own stakeholders, devel-', 'Managerial independence of constituent systems.  Con- stituent systems participating wi thin an SoS are often developed  by different organizations, with their own stakeholders, devel- opment teams, processes, and resources, besides presenting a life  cycle on their own and having autonomy for managing their  own resources. This is not the case for single systems as their  constituent components are bound to it and all decisions are  made at the system.  Geographical distribution of constituent systems.  This  characteristic refers to the extent to which constituent systems  are dispersed so that some form of connectivity allows for com- munication or information sharing [ 6]. Therefore, constituent  systems in an SoS are physically decoupled and exchange only  information among them.  Evolutionary development of the SoS.  An SoS can con- stantly evolve as a response to changes, whether related to the  environment, to its constituent systems, or to its functions and  purposes. Due to their managerial independence, constituent  systems of may evolve by their own without control of the SoS,  which must in turn absorb such  changes and minimize possibly  undesired effects.   Emergent behaviors in the SoS. In single systems, behavior  and interactions of their constituent components are often pre- dictable and controllable from the system perspective. However,  even though constituent systems in an SoS may be predictable,  their operational and managerial independence and their local  interactions make the behavior of  the SoS to emerge at runtime.  This poses a cumbersome challenge to the development of this  class of systems since SoS largely depend on emergent behaviors  to fulfill their missions and provide new functionalities. These  functionalities result from the collaboration among its constitu- ent systems and they cannot be achieved by any of them.   Last but not least, an additional characteristic of SoS is in- herent dynamicity, which makes the system be subjected to  changes, i.e., they can be composed, operated, and reconfigured  at runtime, mostly without any planning. The concrete constitu- ent systems to compose an SoS at runtime are often partially  known or even unknown at design time. Therefore, these con- crete constituents must be discovered, selected, and composed at  runtime to identify proper arrangements of these systems that  contribute to the accomplishment of the global goals of the SoS  based on their capabilities [7].  245', 'Challenges to the Development of Smart City Systems SBES’17, September 2017 , Fortaleza, CE, Brazil      3 SMART CITIES AS SYSTEMS-OF-SYSTEMS  To date, systems in smart cities have been often developed  and deployed in significant frag mentation and isolation, each  one responsible for tackling specific areas, concerns, and prob- lems in the city. This might be explained by the fact that these  systems are often complex enough in their own right, even be- fore starting the exploration on how they can interact with each  other in the daily life of a city. As a result, some implementa- tions of the smart city concept in urban agglomerations around  the world have been done in a bottom-up approach [8]. In this  sense, cities would become smarter through decentralized initia- tives and gradual implementation  of successive projects, each  one focusing on a specific objective. Although these applications  and systems may be relatively mature in some specific fields, it is  easy to observe that collaboration and coordination among them  are missing. This type of situation may lead to an unmanageable  and unsustainable sea of systems, thus preventing solutions of  becoming more efficient, scalable, and suitable to support new  generations of systems and services that are not envisaged yet  [9]. These are some important issues begging attention:   1. How to integrate these differ ent projects (and possibly  applications and systems) towards a holistic view for a  smart city?   2. How to make them to collabo rate to provide citizens  with new services different fields?   Our premise is that viewing a smart city as an SoS can be  helpful to address such issues, even though it raises many signif- icant challenges (see Section 4). The main thrust behind such a  view is that it is possible to provide a city with higher capabili- ties, performance, and other important concerns than would be  possible with a traditional system view. Advances in technology  as well as in the understanding of complex systems are making  that now possible, although it is still in the very early stages. An  immediate benefit of having this integrated, holistic view simply  comes from the ability of differen t city agencies to coordinate  their actions. For instance, during a crisis due to a natural disas- ter, all responders will have a common view of the situation and  will be able to make better decisions about where and how to  apply the available resources [10].  Naphade et al. [ 11] argue that three main city processes can  surely benefit from the integration among different systems  forming an SoS, namely planning, management, and operations.  According to these authors, a holi stic view  of t he city can pro- vide urban planners with mean s of exploiting several infor- mation sources about human behavior and environmental varia- bles to better allocate resources (land, water, transportation, etc.)  as the city evolves. In terms of management, such a system inte- gration can open space to coordinate infrastructure management  activities and provide cross-agen cy visibility of planned inter- ventions. At last, a smart city should also be able to integrate  multiple data sources to repres ent the interdependence of urban  domains at real-time.  Figure 1 provides a general picture of the main elements typi- cally involved in a smart city. These elements can encompass  both public and private heterogeneous and independent systems  that must be integrated to achieve effectiveness and efficiency  for citizens, besides being producers and consumers of infor- mation to each other. Therefore, smart cities as SoS rely on col- laborating smart systems (or even SoS) that interact, communi- cate, and share information with each other, allowing for cross-  domain usages of services and their synergistic integration [12].  To a general extent, these systems shall be interconnected and  must be treated as such. Understanding one system and making', 'domain usages of services and their synergistic integration [12].  To a general extent, these systems shall be interconnected and  must be treated as such. Understanding one system and making  it work better means that cities must comprehend the bigger pic- ture and how the various systems collaborate with each other  [13]. Connecting these systems can surely deliver greater effi- ciency/effectiveness and address interrelated, long-term threats  to the city sustainability.  As an example on how multiple operationally and manageri- ally independent systems within a city can be integrated and col- laborate with each other towards SoS, consider a system intend- ed to improve traffic conditions in the city roads relying on intel- ligent vehicle systems (q.v. Figure 2). This complex system is  highly relevant in a smart city as monitoring, controlling, and  optimizing urban traffic is of crucial interest because of the in- creasing growth on the number of circulating vehicles. This  comes as a response to the traffic jams that have struggled sev- eral cities around the world, directly degrading air quality due to  the emission of pollutants, and affecting the level of stress and  anxiety of drivers. In this scenario, roadside units can include  road sensors, tolls, traffic lights, etc., and intelligent vehicles can  be equipped with devices associated  to light, navigation, fueling,  and other electrical control modules. A system managed by the  city authority responsible for mobility can therefore control the  roadside units automatically and at real-time to reduce traffic  jams and incidents, as well as to improve the overall quality of  the traffic based on data provided by sensors. Furthermore, data  collected by the external sensor systems can be used by the in- ternal systems of a vehicle to set up vehicle parameters (such as  fuel efficiency and consumption, carbon emissions, and engine  performance) or to plan routes by considering fastest/shortest  paths and/or avoiding roads under work, intense traffic or acci-   Figure 1: A smart city SoS can integrate both public and  private heterogeneous, independent systems across dif- ferent domains.  246', 'SBES’17, September 2017, Fortaleza, CE, Brazil E. Cavalcante et al.        dents. The systems of vehicles in a road can also interact with  each other to mitigate accidents when a driver wants to change  lanes, for example.  4 CHALLENGES TO THE DEVELOPMENT OF  SMART CITY SYSTEMS  As in SoS, systems in smart cities are independently devel- oped, operated, managed, evolved,  and eventually retired. While  viewing smart cities as SoS is interesting to better understand  the relationships among these constituent systems, important  challenges arise in terms of their design, engineering, and opera- tion, most of them coming from the very nature of SoS. In this  section, we briefly discuss a non- exhaustive list of relevant is- sues to be addressed when develo ping SoS targeting smart cities.  This set of challenges is based on (i) an ad-hoc literature search  on the deployment of SoS targetin g smart cities and (ii) our ex- periences in integrating different systems in Natal, a city in  Northeastern Brazil where a significant synergy between gov- ernment parties and academia is paving the way for the devel- opment of emerging applications  and integration of value-added  systems [ 14]. The challenges elicited hereinafter are mostly  based the required effort to integrate systems related to public  safety, emergency and non-emergency services, land infor- mation, traffic monitoring, and city management.  Scale and inherent complexity. Advances in embedded  systems, mobile and ubiquitous computing, and wireless sensor  and actuator networks, culminating in the today’s Internet of  Things paradigm have been fundamental elements to make the  smart city vision a reality. Physical devices in use nowadays  have already reached dozens of billions, whether personal devic- es connecting people or sensing devices spread over the city.  This has made several complex systems to emerge in several  domains, but the expectation is that such systems and their func- tionalities will be more complex than they are in the present, due  to market opportunities and the huge potential of producing a  considerable impact in the daily lives of human beings. In smart  cities, these different systems need to be integrated to form a  larger, more complex system (an SoS) aiming at bringing sensi- ble benefits to government, society, economy, and environment,  as well as providing a complete, holistic view of the city. At the  same time, it is important to emphasize that each of these con- stituents might be SoS themselves, thereby increasing matters of  scale and complexity in the design, engineering, and operation of  these systems. Managing the complexity of SoS is not easy and  demands complex organizational procedures.  Multitude of stakeholders. To allow for sustainable, sup- ply-secure, and future-proof planni ng that can keep up with to- day’s rapid city growth and urbani zation, it is vital to enable  stakeholders to make well-informed decisions [ 15]. Single sys- tems often encompass a clear, predefined set of stakeholders  concerned with the system under production. In turn, SoS envi- ronments embraces a broader range of stakeholders, including  the ones of the individual constituent systems and the ones of  the overall SoS. This gives rise to: (i) conflicts resulting from the  fact that different groups of stakeholders may have their own  interests in the SoS; (ii) conflicts in the relationship between  constituent systems and the SoS due to their managerial inde- pendence; (iii) conflicts arisen from interactions among constitu- ent systems due to their operational independence; and (iv) the  fact that a given constituent system might simultaneously be- long to more than one SoS. Smart city SoS must hence deal with  such a variety of stakeholders with conflicting interests, re- quirements, and priorities, requiring strategies and solutions to  tackle these different scenarios.  Multiple disciplines and domains. It is possible to notice', 'quirements, and priorities, requiring strategies and solutions to  tackle these different scenarios.  Multiple disciplines and domains. It is possible to notice  that many SoS designed and running today are tailored to specif- ic domains, such as the ones found in defense, health care, and  crisis management. However, a smart city SoS embracing the  different sectors of the city must inherently involve constituent  systems from different domains. This poses a challenge in the  sense that different types of information, models, notations, etc.  from different disciplines and wi th different purposes must be  considered at the same time, thus providing the city with holistic  planning and management capabi lities. Schleicher et al. [ 16] re- fer to such a relationship among multiple disciplines and do- mains as a multi-domain expert network. The idea is to establish  a dynamic interaction of different domain experts, providing an- alytics and models about important aspects of the city, thereby  allowing for a holistic view of the city that can foster relevant,  innovative decision-making support for stakeholders in industry,  government, and society. At the same time, integrating ae diver- sity of actors from multiple disciplines and domains can leverage  the so-called Medici Effect 1, stepping into an intersection of                                                                    1 This expression comes from the Frans Johansson’s book of the same name  (https://goo.gl/M52RBR). Between the 13 th and 17 th centuries, the Medici family  patronaged poets, philosophers, scientists, architects, painters and sculptors from  all over Europe and elsewhere to Florence, Italy. By attracting talented souls from  many different fields and cultures, this Fiorentina family put all these creative  people in contact with each other to trade ideas and collaborate, so that such an  intersection of concepts and diverse backgrounds kicked off the Renaissance, one  of the most innovative eras in human history.    Figure 2: Integration of different heterogeneous systems  to support smart traffic management.    247', 'Challenges to the Development of Smart City Systems SBES’17, September 2017, Fortaleza, CE, Brazil      fields, disciplines or cultures to  combine existing concepts into  many new ideas and drive innovation.  Heterogeneity and interoperability. In a smart city, reli- a n c e  m i g h t  b e  p l a c e d  o n  t h e  d e l i v e r y  o f  a  s e r v i c e  c o m p o s i n g   preexisting systems (including legacy systems) forming a city- wide SoS. Nevertheless, a challenge to the engineering of this  type of system is the high heterogeneity of its constituent sys- tems, which are distributed, independent, developed with differ- ent technologies and data formats, and owned by distinct organ- izations and agencies within the city. For example, the successful  treatment of a patient in an emergency results from the interac- tion of several operationally and managerially independent sys- tems, including at least telephony, ambulance assignment, and  hospital management. Therefore, a successful smart city SoS  must imperatively have effective means of promoting interoper- ability among the existing constituent systems while preserving  their operational and managerial independences. One of the ma- jor challenges here resides in the fact that these constituent sys- tems are mostly not designed to communicate with each other,  thereby making an interoperability solution harder to build. A  way of mitigating these issues is through the development of  sophisticated SoS-centric middleware services to support the  management and execution of SoS in smart city environments in  a dynamic, transparent, and scalable way while supporting in- teroperability among the different systems operating in the city  [17]. Middleware refers to a software layer between applications  and the underlying communication, processing, and sensing in- frastructure aimed to offer different levels of transparency, in- teroperability, and services for end-users and applications. This  solution seems to be quite promising in the context of smart cit- ies mainly due to its potential for making system development  easier, besides tackling the high heterogeneity and dynamicity of  the environment and providing important functionalities, such  as the management of large volumes of data, data gathering and  analysis, monitoring, scalability, privacy policies, etc.  Effect of emergent behaviors. As previously mentioned,  one of the main features that makes SoS distinct from other clas- ses of systems (and at the same time the most difficult one to  handle) is the so-called emergent behavior. Emergent behaviors  are associated to the interactions among constituent systems and  hence they only appear when such systems interact with each  other within the scope of the SoS. Some behaviors can be indeed  unpredictable, i.e., they dynamically appear in the context of the  SoS, and they may be desirable or even undesirable, so that the  result of the interactions among constituent systems within an  SoS can be respectively positive or negative over its operation  [18]. Due to these different natures of emergent behavior that  may occur within SoS, these syst ems need to (i) maximize desir- able behaviors, thus fostering the accomplishment of the mis- sions of the SoS through interactions among its constituent sys- tems, and (ii) minimize undesirable behaviors, which may affect  the accomplishment of the missions  of the SoS and have a nega- tive impact on important quality attributes, such as performance,  safety, and reliability. This is quite important in smart city SoS  since the interactions among the different constituent systems  might not have the desired effect or even imply in damages to  life and safety of citizens. More over, contextual information of  SoS managed at runtime can enhance the system behavior and  better supports evolution for unforeseen scenarios, as new func- tionalities can be dynamically a dded, removed, or changed over', 'SoS managed at runtime can enhance the system behavior and  better supports evolution for unforeseen scenarios, as new func- tionalities can be dynamically a dded, removed, or changed over  time [19]. Failing to understand how smart city systems are re- lated to and interact with each ot her is that these oversights can  lead to catastrophic and cascading failures across multiple criti- cal systems, besides consequences ranging from effects on oper- ability, performance degradation or even death and costly dis- ruptions in the worst case. It is no longer affordable ignoring the  impact that one system might have on another.  Unification on information. The main challenges in man- aging smart city data are th e need for common information  models and the ability to safely share information across multi- ple agencies within a city and among multiple cities in a metro- politan region, thus making it possible to obtain a more complete  picture of urban activity [ 12]. For instance, information related  to roads is scattered across many agencies and their respective  systems, including transportation, urban planning, public work,  emergency services, and environmental management. Directly  related to means of promoting interoperability is the adoption of  uniform models for representing information handled and  shared by different applications, services, and systems in a smart  city SoS. In the literature about SoS, it has been possible to no- tice some sort of tendency to the use of ontologies and semantic- based techniques as a way of seamlessly integrating constituent  systems to be dynamically discovered [7, 20]. By using an ontol- ogy, a given constituent system can be described in an unambig- uous way, thus avoiding misint erpretation about the provided  functionalities while improving accuracy and automation of sys- tem discovery and composition processes. However, it is still  important to have in mind that both operational and managerial  independences of constituent systems pose a challenge on the  use of ontology-based techniques as each of these systems may  have their own ontology and respective meaningful information.  Addressing this heterogeneity is important as it can contribute  to the achievement of full interoperability among constituent  systems [17].  Data granularity. Beyond sharing, proceeding and trans- forming produced into value-added information for a variety of  stakeholders and systems in a smart city, another challenge is  related to the granularity level of information. End-users, appli- cations, and systems might be more interested in aggregated and  qualitative (coarse-grained) information about each constituent  system of a smart city SoS instead of fine-grained measurements  of every single device deployed in the city. For instance, there  may be smart buildings equipped with several sensing devices  able to provide low-granularity information, such as the temper- ature of a given part of a room in the building. In this scenario,  each system should also provide aggregated information based  on data provided by its sensors, such as the average temperature  and lighting of each room and of the whole building, current  electrical consumption, number of  people in the building, etc.  Therefore, the provision of ag gregated and qualitative infor- mation by each system within the smart city SoS relieves end- users, applications, and decision-making systems of processing  248', 'SBES’17, September 2017, Fortaleza, CE, Brazil E. Cavalcante et al.        such data to produce relevant information. Nonetheless, there  may be applications and systems that require storing both raw  and historical data, especially when aggregated information has  an anomaly or atypical behavior that cannot be well-understood  or when it is necessary to perform analyses based on historical  data. In all cases, it is necessary to deal with such a huge mass of  data with cross-domain dependenci es in the presence of signifi- cant uncertainty and variability.  Data analytics. Data analytics, especially predictive analyt- ics, can play a critical role in  making gathered data more mean- ingful within smart city SoS and hence help to identify potential  problems before they occur. It is not solely about the volume of  data generated by sensors and/or other connected devices or sys- tems, but the valuable concern lies in the ability of analyzing  large volumes of data created by a myriad of information sources  in near real-time to generate actionable insights to streamline  operational workflows to make be tter operational decisions. The  ultimate value for public sector agencies is business intelligence  and predictive analysis that is timely and actionable to respond  faster to evolving trends and improve citizen services. As an ex- ample, predictive analytics in public safety can help to prevent  serious incidents before they occur and enable law enforcement  to intercept dangerous trends. This typically requires complex,  integrated systems to not only collect large amounts of city data  through different sensors and other information sources, but also  to aggregate such data. Additionally, presenting data in different  formats, statistics, and correlations via dashboards can provide  far greater, holistic insight into what is happening in the city.  5 CONCLUDING REMARKS  Next-generation cities must prepare for change that will be  revolutionary rather than evolutionary, as they can put in place  systems that work in entirely new ways. This is indeed a jour- ney, not an overnight transformation. At a holistic level, smart  cities led to a transformation on today’s cities into complex SoS  with a plethora of increasingly complex interactions among in- dependent heterogeneous systems. Having many stakeholders  from multiple different domain s pose complex requirements on  the design, engineering, and operation of these systems, also giv- ing rise to requirements that mi ght be conflicting and subjected  to changes over time.   In this paper, we have presented how a smart city can be ho- listically viewed as a complex SoS with interactions among dif- ferent systems. Our main purpose was shedding light on an SoS  perspective for these systems, l ooking forward to the provision  of efficient, scalable, and suitable services and applications. Our  studies on both SoS and smart cities have revealed that many  approaches employed in some smart city projects are indeed  fragmented, non-integrated (or integrated in a limited way), thus  creating island solutions far fr om an SoS approach. Of course,  many important methods, strategies, etc. generally used for SoS  could be employed in the development of these systems target- ing smart cities. Upon such a view, we have also elicited and dis- cussed some important challenges that might commission fur- ther research and open opportunities on system design, engi- neering, and operation in smart cities.  REFERENCES  [1] A. M. Townsend. 2014. Smart cities: Big Data, civic hackers, and the quest for a  new utopia. W. W. Norton & Company, Inc., USA.  [2] M. W. Maier. 1998. Architecting principles for systems-of-systems. Systems  Engineering 1, 4 (Feb. 1998), 267–284.  [3] D. Firesmith 2010. Profiling systems using the defining characteristics of systems  of systems (SoS). Technical Report. Software Engineering Institute, Carnegie  Mellon University, USA.', '[3] D. Firesmith 2010. Profiling systems using the defining characteristics of systems  of systems (SoS). Technical Report. Software Engineering Institute, Carnegie  Mellon University, USA.  [4] C. Azani. 2009. An open systems approach to System of Systems Engineering.  In System of Systems Engineering: Innovations for the 21 st Century, M. Jashidi  (Ed.) John Wiley & Sons, Inc., USA, 21–43.  [5] C. A. Lana, N. M. Souza, M. E. Delamaro, E. Y. Nakagawa, F. Oquendo, and J. C.  Maldonado. 2016. Systems-of-systems de velopment: Initiatives, trends, and  challenges. In Proceedings of the XLII Latin American Computing Conference  (CLEI 2016). IEEE, USA, 585–596.  [6] C. B. Nielsen, P. G. Larsen, J. Fitzge rald, J. Woodcock, and J. Peleska. 2015.  Systems of Systems Engineering: Basic concepts, model-based techniques, and  research directions. ACM Computing Surveys 48, 2 (Nov. 2015).  [7] P. Gomes, E. Cavalcante, P. Maia, T. Batista, and K. Oliveira. 2015. A systemat- ic mapping on discovery and composition mechanisms for systems-of-systems.  In Proceedings of the 41st Euromicro Conference on Software Engineering and  Advanced Applications (SEAA 2015). IEEE Computer Society, USA, 191–198.  [8] N. Komninos, P. Tsarchopoulos, and C. Kakderi. 2014. New services design for  smart cities: A planning roadmap for user-driven innovation. In Proceedings of  the 2014 ACM International Workshop on Wireless and Mobile Technologies for  Smart Cities (MobiHoc’14). ACM, USA, 29–38.  [9] J. Hernández-Muñoz, J. B. Vercher, L. Muñoz, J. A. Galache, M. Presser, L. A.  H. Gómez, and J. Petterson. Smart cities at the forefront of Future Internet. In  The Future Internet, J. Domingue et al. (Eds). Lecture Notes in Computer Sci- ence, Vol. 6656. Springer Berlin-Heidelberg, Germany, 447–462.  [10] C. Harrison and I. A. Donnelly. 2011. A theory of smart cities. In Proceedings of  the 55th Annual Meeting of the International Society for the Systems Sciences. In- ternational Society for the Systems Sciences, United Kingdom.  [11] M. Naphade, G. Banavar, C. Harrison, J. Paraszczak, and R. Morris. Smarter  cities and their innovation challenges. Computer 44, 6 (Jun. 2011), 32–39.  [12] V. Javidroozi, H. Shah, A. Cole, and A. Amini. 2015. Towards a city’s system  integration model for smart city development: A conceptualization. In Proceed- ings of the 2015 International Conference on Computational Science and Compu- tational Intelligence (CSCI’15). IEEE Computer Society, USA, 312–317.  [13] S. Dirks and M. Keeling. 2009. A vision of smart cities: How cities can lead the  way into a prosperous and sustainable future . Executive report. IBM Institute  for Business Value.  [14] N. Cacho, F. Lopes, E. Cavalcante, and I. Santos. 2016. A smart city initiative:  The case of Natal. In Proceedings of the Second IEEE International Smart Cities  Conference (IEEE ISC2 2016). IEEE, USA, 242–248.  [15] J. M. Schleicher et al. 2016. A holistic, interdisciplinary decision support sys- tem for sustainable smart city design. In Proceedings of the First International  Conference on Smart Cities (Smart-CT 2016), E. Alba, F. Chicano, and G. Luque  (Eds.) Lecture Notes in Computer Science, Vol. 9704. Springer International  Publishing, Switzerland, 1-10.  [16] J. M. Schleicher, M. Vögler, C. Inzinger, and S. Dustdar. 2015. Towards the  Internet of Cities: A research roadmap for next-generation smart cities. In Pro- ceedings of the ACM First International Workshop on Understanding the City  with Urban Informatics (UCUI’15). ACM, USA, 3–6.  [17] F. Lopes, S. Loss, A. Mendes, T. Batis ta, and R. Lea. 2016. SoS-centric middle- ware services for interoperability in smart cities systems. In Proceedings of the  2nd International Workshop on Smart Cities: People, Technology and Data  (SmartCities’16). ACM, USA.  [18] O. T. Holland. 2007. Taxonomy for the modeling and simulation of emergent  behavior systems. In Proceedings of the 2007 Spring Simulation Multiconference', '(SmartCities’16). ACM, USA.  [18] O. T. Holland. 2007. Taxonomy for the modeling and simulation of emergent  behavior systems. In Proceedings of the 2007 Spring Simulation Multiconference  (SpringSim’07), Vol. 2. Society for Computer Simulation International, USA,  28–35.  [19] E. Y. Nakagawa, R. Capilla, F. J. Díaz, and F. Oquendo. 2014. Towards the dy- namic evolution of context-based systems-of-systems. In Proceedings of the VIII  Workshop on Distributed Software Development, Software Ecosystems and Sys- tems-of-Systems (WDES 2014). SBC, Brazil, 45–52.  [20] G. Rubio, J. F. Martínez, D. Gómez, and X. Li. 2016. Semantic registration and  discovery system of subsystems and services within an interoperable coordi- nation platform in smart cities. Sensors 16, 7 (Jun. 2016).  249']","['CHALLENGES TO THE DEV ELOPMENT OF  SMART CITY SYSTEMS     This briefing elicits some challenges to the  development of smart city systems under  a system-of-systems (SoS) perspective.      FINDINGS  As an active domain of research in recent years,  systems-of-systems (SoS) arise from interactions  among widespread independent, heterogeneous  complex systems with their own purposes and that  collaborate with each other to fulfill common  global missions. Inherent characteristics of SoS are  the operational and managerial independences of  constituent systems, geographical distribution,  evolutionary development, emergent behaviors,  and dynamicity.    Smart cities typically encompass several systems  engaged in complex relationships, including the  integration and interaction with other systems  towards providing new functionalities. This makes  a smart city to be holistically viewed as an ultimate  SoS.    Although an SoS perspective seems to be a natural  (even obvious) choice to develop smart city  systems, many existing approaches are indeed  fragmented, non-integrated (or integrated in a  limited way), thus creating unmanageable and  unsustainable island solutions.    Developing smart city systems under an SoS  perspective can be helpful to provide a city with  higher capabilities, performances, new services to  citizens, and other important concerns than would  be possible with a traditional system view. Many  city processes can hence benefit from the  integration of different systems forming an SoS,  e.g.:     • means of exploiting several information  sources about human behavior and  environmental variables to better allocate  resources as the city evolves  • coordination of infrastructure management  activities  • cross-agency visibility of planned interventions  • integration of multiple data sources to  represent the interdependency of urban  domains at real-time    Shedding light on an SoS perspective for urban  systems is important to look forward to the  provision of efficient, scalable, and suitable  services and applications to citizens, thereby  contributing to fully realize the smart city concept.  While viewing smart cities as SoS is interesting to  better understand the relationships among these  constituent systems, important challenges arise in  terms of their design, engineering, and operation:  • Scale and inherent complexity. Besides  integrating different smart city systems to  achieve a larger, more complex system (an  SoS), each of these constituents might be SoS  themselves. This hence increases matters of  scale and complexity in the design,  engineering, and operation of these systems.  • Multitude of stakeholders. SoS environments  embrace a broader range of stakeholders,  including the ones of the individual constituent  systems and the ones of the overall SoS. Smart  city SoS must hence deal with such a variety of  stakeholders with conflicting interests,  requirements, and priorities.  • Multiple disciplines and domains. A smart city  SoS embracing the different sectors of the city  must inherently involve constituent systems  from different domains. Different types of  information, models, notations, etc. from  different disciplines and with different purposes  need to be considered at the same time.  • Heterogeneity and interoperability. A  challenge to the engineering a city-wide SoS is  the high heterogeneity of its constituent  systems. Therefore, a successful smart city SoS  must imperatively have effective means of  promoting interoperability among the existing  constituent systems while preserving their  operational and managerial independences.  • Effect of emergent behaviors. Emergent  behaviors associated to the interactions among  constituent systems may be desirable (to be  maximized) or even undesirable (to be  minimized). Addressing these concerns is quite  important in smart city SoS since this may not  have the desired effect or even imply in', 'maximized) or even undesirable (to be  minimized). Addressing these concerns is quite  important in smart city SoS since this may not  have the desired effect or even imply in  damages to life and safety of citizens.  • Unification on information. The main  challenges in managing smart city data are the  need for common information models and the  ability to safely share information across  multiple agencies within a city and among  multiple cities in a metropolitan region, thus  making it possible to obtain a more complete  picture of urban activity.  • Data granularity. Beyond sharing, proceeding  and transforming produced into value-added  information for a variety of stakeholders and  systems in a smart city, another challenge is  related to the granularity level of information.  • Data analytics. There is a need of analyzing  large volumes of data created by a myriad of  information sources in near real-time to  generate actionable insights to streamline  operational workflows to make better  operational decisions.      Keywords    Smart cities  Systems-of-systems    Who is this briefing for?    Software engineering researchers and  practitioners who want to reflect on  the challenges and opportunities on the  development of smart city systems.    Where the findings come from?    All findings of this briefing result from  (i) an ad-hoc literature search on the  development of SoS targeting smart  cities and (ii) experiences in integrating  different systems in the smart city  initiative of Natal, Brazil.      What is included in this briefing?    A holistic view of smart city systems as  a complex SoS.    A discussion about challenges to the  development of smart city systems   (-of-systems).    What is not included in this briefing?    Description of other studies and  initiatives having the same perspective  of viewing smart cities as SoS and how  they address challenges to software  development.    For additional information about the  Smart Metropolis Project at UFRN  (Natal, Brazil):    http://smartmetropolis.imd.ufrn.br/  ?lang=en      ORIGINAL RESEARCH REFERENCE  Everton Cavalcante, Nélio Cacho, Frederico Lopes, Thais Batista. Challenges to the development of smart city systems: A system-of-systems view. Proceedings of  the 31st Brazilian Symposium on Software Engineering (SBES 2017), Fortaleza, CE, Brazil. New York, NY, USA: ACM, 2017, DOI:  https://dx.doi.org/10.1145/3131151.3131189']","**Title:** Overcoming Challenges in Developing Smart City Systems

**Introduction:**  
This Evidence Briefing summarizes the findings from the paper ""Challenges to the Development of Smart City Systems: A System-of-Systems View"" by Everton Cavalcante et al. The goal is to provide insights into the complexities and challenges of developing smart city systems, which are increasingly viewed as systems-of-systems (SoS). This perspective is crucial for urban planners, software engineers, and policymakers aiming to enhance city functionality and quality of life through integrated technological solutions.

**Main Findings:**  
1. **Complexity and Integration:** Smart cities are composed of various independent systems that need to collaborate to provide enhanced services. This integration is complicated by the diversity of technologies, data formats, and ownership structures among these systems. A holistic SoS approach can help manage this complexity by fostering collaboration among different systems, thus enabling them to function synergistically rather than in isolation.

2. **Stakeholder Diversity:** The development of smart city systems involves numerous stakeholders with potentially conflicting interests, including government agencies, private companies, and citizens. Effective communication and negotiation strategies are essential to align these diverse interests and ensure that the systems serve the broader goals of the city.

3. **Emergent Behaviors:** Smart city systems often exhibit emergent behaviors that arise from the interactions of constituent systems. These behaviors can be unpredictable and may lead to either beneficial or detrimental outcomes. Understanding and managing these emergent behaviors is critical for ensuring the reliability and effectiveness of smart city applications.

4. **Interoperability Challenges:** The high heterogeneity of constituent systems poses significant interoperability challenges. Many systems are not designed to communicate with one another, complicating the integration process. Developing middleware solutions can facilitate interoperability by providing a common platform for data exchange and system interaction.

5. **Data Management and Analytics:** Smart cities generate vast amounts of data from various sources. Effective data management strategies are necessary to process this information, ensuring it is accessible and actionable for decision-making. Predictive analytics can enhance the ability to respond to urban challenges proactively, improving service delivery and resource management.

6. **Dynamic Evolution:** Smart city systems must be capable of evolving in response to changing urban environments and stakeholder needs. This requires flexible design approaches that allow for the addition or modification of functionalities over time, ensuring that the systems remain relevant and effective.

**Who is this briefing for?**  
This briefing is intended for urban planners, software engineers, government officials, and stakeholders involved in the development and management of smart city initiatives.

**Where the findings come from:**  
All findings in this briefing are derived from the paper by Everton Cavalcante et al., which discusses the challenges of smart city systems from a systems-of-systems perspective based on empirical observations and literature review.

**What is included in this briefing?**  
The briefing covers the main challenges related to the development of smart city systems, including complexity, stakeholder diversity, emergent behaviors, interoperability, data management, and dynamic evolution.

**What is NOT included in this briefing?**  
This briefing does not provide detailed technical specifications or case studies of specific smart city projects. It focuses on broad challenges and considerations rather than specific implementations.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**ORIGINAL RESEARCH REFERENCE:**  
Cavalcante, E., Cacho, N., Lopes, F., & Batista, T. (2017). Challenges to the development of smart city systems: A system-of-systems view. In Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES’17), Fortaleza, CE, Brazil. DOI: [10.1145/3131151.3131189](https://doi.org/10.1145/3131151.3131189)"
"['Characterizing Big Data Software Architectures: A Systematic Mapping Study Bruno Sena University of São Paulo São Carlos, Brazil brunosena@usp.br Ana Paula Allian University of São Paulo São Carlos, Brazil ana.allian@usp.br Elisa Yumi Nakagawa University of São Paulo São Carlos, Brazil elisa@icmc.usp.br ABSTRACT Big data is a broad term for large, dynamic, and complex data sets that have brought great challenges to be addressed by traditional software systems. It has also demanded advanced software archi- tectures (i.e., the big data software architectures) prepared to deal with the continuous expansion of the volume of data as well as to take advantage of new technologies for big data context. How- ever, the main characteristics, basic requirements, and modules and organization of big data architectures are not still widely known. Besides that, no detailed overview about them is available. The main contribution of this paper is to present the state of the art related to big data software architectures; for this, we conducted a Systematic Mapping Study. As results, an essential set of eight requirements for big data architectures was identified, besides a collection of five modules that are fundamental to adequately en- able the data flow. We also intend these results can guide architects in the development of software systems for this new challenging scenario of big data management. CCS CONCEPTS • Information systems → Data management systems ; • Soft- ware and its engineering → Software architectures; KEYWORDS Big Data System, Software Architecture, Reference Architecture, Systematic Mapping Study ACM Reference format: Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa. 2017. Charac- terizing Big Data Software Architectures: A Systematic Mapping Study. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017, 10 pages. https://doi.org/10.1145/3132498.3132510 1 INTRODUCTION There is no doubt that we live in the era of big data, which refers to data sets whose size is beyond the ability of current methods and tools to capture, store, process and analyze [53]. The growth in the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5325-0/17/09. . . $15.00 https://doi.org/10.1145/3132498.3132510 amount of data produced has exponentially increased. Currently, more than 2.5 billion gigabytes of data are produced every day and it is likely to increase along the years [ 30]. All this data has been employed for real time decisions such as operational intelli- gence, customer intelligence, business innovation, and competitive advantage [10]. Data are collected from many different domains including mod- ern commercial airlines, which produce approximately 0.5 terabyte of operational data per flight. Besides that, this data is applied in a variety of contexts, such as diagnose faults or predict maintenance [20] and healthcare [27], which support to extract new insights for disease treatment and prevention, and reduce costs by improving patient outcomes and operational efficiency. In addition, along with the growth of technologies, such as Internet of Things and Smart Systems [50], data has been used not only for analysis, but also to impact the system behavior. With the rapid increase in data generation, the current software', 'Systems [50], data has been used not only for analysis, but also to impact the system behavior. With the rapid increase in data generation, the current software and hardware resources became overloaded and require new in- novative approaches to effectively manage and use the data [ 1]. The variety, velocity, and volume with which data is ingested by a system often reduce its efficiency causing failures. Due to the difficulty of managing and analyzing such large data sets, attention to software architecture is required to support big Data innovations. Particularly, big Data architecture should be well documented and it should define requirements that meet the big data systems [6]. However, it is observed that industrial practition- ers have faced challenges to select technologies to design big data architectures, i.e., selecting each component affects the selection of architecture patterns, data models, programming languages, query languages, and access methods. These consequently affect many quality attributes, such as system performance, latency, scalabil- ity, availability, consistency, and modifiability [ 10]. Additionally, there is a lack of studies that investigate software architectures that address big data. The main contribution of this study is to present the state of the art about how big data has been addressed in the architectural level. Our goal is to gather the main characteristics, requirements, quality attributes, design patterns and architecture modules for big data architectures. Aiming to achieve our goal, we conducted a Systematic Mapping Study (SMS). Furthermore, our intention is that our work can contribute to better architect software systems that manage big data. This paper is organized as follows: Section 2 introduces the main concepts regarding software architecture and the principles of big data. Section 3 presents the research method including research questions, search strategy, and selection criteria. Section 4 discusses the results of our SMS. Section 5 states the main threats to validity', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa of this study. Finally, Section 6 presents the main conclusions and future work. 2 BACKGROUND 2.1 Software architecture As the size and complexity of software systems increase, the design, specification, and analysis of overall system structure becomes a critical issue [21]. Software architecture is the structure or struc- tures of systems, which comprise software components, the exter- nally visible properties of those components, and the relationships among them [5]. The importance of software architecture is evi- denced by representing earlier design decisions and to promote a systematic reuse [40]. Software architecture has a positive impact on, at most, seven aspects [21] of software development such as: (i) understanding: implying that software architecture simplifies our ability to compre- hend large systems by presenting them at a high level of abstraction [22, 43]; (ii) reuse and integration: architectural design supports both reuse of large components and also frameworks into which components can be integrated [21]; (iii) construction: an architec- tural description provides a planning for development by indicating the major software components and dependencies among them [21]; (iv) evolution: software architecture can expose the variability along which a system is expected to evolve [21]; (v) analysis: archi- tectural descriptions provide new opportunities for analysis such as system consistency [2, 36], conformance to quality attributes [12] and dependent analysis [49]; (vi) management: critical evaluation of an architecture typically leads to a much clearer understanding of requirements, implementation strategies, and potential risks [8]; and (vii) communication: an architectural description often serves as a vehicle for communication among stakeholders [21]. Reference architecture is a special type of software architecture which is used to represent systems in a higher level of abstraction [3, 13, 37, 39]. The main contribution of using reference architec- ture is to facilitate and guide design of concrete architectures for new systems, new versions or extensions of similar products. In general, reference architecture identify abstract solutions of a prob- lem promoting reuse of design expertise by achieving solid and well-recognized understanding of a specific domain [17]. Considering its relevance, it is observed that different domains have already understood the need of encapsulating knowledge into this infrastructure. Examples of reference architectures are AUTOSAR (AUTomotive Open System ARchitecture) [ 4] for the automotive domain, and Continua [14], and UniversAAL [51], for Ambient Assisted Living (AAL) [ 24]. In the similar perspective, reference model represents a minimal set of unifying concepts, axioms and relationships within a particular problem domain, inde- pendently of specific patterns, technologies and implementations [41]. In other words, reference model is a model of goals and ideas, which focuses on level of concepts [38] and it is more abstract than a reference architecture. 2.2 Big Data Big data is a broad term for large and complex data sets that can not be addressed by the traditional data processing applications [48]. Besides that, the term big data often refers to applying some tech- nique or method to extract value from data. Beyer et al. [7] define big data as “high volume, high velocity and/or high variety informa- tion assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization"". In 2001, Laney [ 35] proposed the 3’V’s model to characterize big data based on three aspects of data growth: (i) volume: the data size determines the value and potential of the information under consideration, and whether it can actually be considered big data or not; (ii) velocity: it means how fast the data is generated', 'data size determines the value and potential of the information under consideration, and whether it can actually be considered big data or not; (ii) velocity: it means how fast the data is generated and processed to meet the demands and the challenges that lie in the path of growth and development; and (iii) variety: it refers to the wide range of data types and sources. Later, in 2012, this model was expanded to a 5-V’s model through addition of two more characteristics [7]: (i) value: this refers to inconsistency the data can show at times in which the process of handling and managing the data effectively; and (ii) veracity: the quality of data captured can vary greatly and an accurate analysis depends on the veracity of source data. The challenges of this scenario include analysis, capture, data curate, search, sharing, storage, transfer, visualization, and informa- tion privacy [ 48]. The analysis of data sets can find new correlations to identify new trends, predict future behaviors, and so on. Besides that, the development of big data systems is dramatically different from development of small data systems [ 10]. Big data systems require distributed design principles to create scalable solutions and a special attention to select and adopt technologies that can provide the required quality attributes [26]. Due to the exponential growth of data over the last decade and, consequently, the increase of big data systems developed, it is necessary to have architectural patterns for the development of these systems. Moreover, the soft- ware architectures of systems that manage big data can directly impact the outcome of these challenges. Hence, the characterization of these architectures is fundamental. 3 RESEARCH METHOD SMS can provide a detailed overview of a research area by catego- rizing and investigating studies related to the topics of interest [44]. We followed the guidelines recommended by Kitchenham et al. [31] and Petersen et al [44] to conduct our SMS. This research method involves: i) research objectives; ii) research questions; iii) search strategy; iv) selection criteria; v) quality assessment; vi) selection of studies; and vii) analysis of results, as follows. 3.1 Research Objectives This SMS aims to present a first overview of the state of the art of how big data has been addressed at an architectural level. We are focused on understanding what are the main requirements to develop such systems, including quality attributes, and identify the main modules required in software architectures from these systems including architecture patterns and styles. 3.2 Research Questions Aiming to support the research objective, three research questions (RQs) were created to address requirements and main modules of', 'Characterizing Big Data Software Architectures: A Systematic Mapping Study SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil software architectures proposed to manage big data systems. RQ1: What are the main requirements for building software archi- tectures for systems that address big data? RQ2: What are the main modules present in software architectures of big data systems? RQ3: What requirements and modules are found in the reference architectures of big data systems already proposed? 3.3 Search Strategy Aiming to perform the search to answer our RQs, we defined a search string including terms from the research objective, which is the primary purpose of this mapping; and terms from research questions, as suggested by Kitchenham et al [31]. Following this process, we defined the search string as presented in Figure 1. As the purpose of this SMS is to find requirements and modules of software architectures for big data systems, we used the terms big data and Software Architecture in our search string. TermsReference Architecture and Reference Model were also applied, because they describe software architectures in a more abstract level and so can contribute by presenting a general representation of whole big data systems. Figure 1: Search string Big Data AND (Software Architecture OR Reference Architecture OR Reference Model) We performed searches for primary studies using electronic search engines and digital libraries recommended to software engi- neering research. In particular, we adopted those ones presented in Table 1, chosen for being the most relevant sources in the computer science area [19, 31]. The search string described previously in Fig- ure 1 was used against metadata (titles, keywords, and abstracts). Table 1: Search databases Source URL ACM Digital Library http://portal.acm.org El Compendex http://www.engineeringvillage.com IEEE Digital Library http://ieeexplore.ieee.org ISI Web of Science http://www.isiknowledge.com Science@Direct http://www.sciencedirect.com Scopus http://www.scopus.com Springer Link http://link.springer.com We had a control group of studies that were suggested by experts in big data and software architecture to validate our search string including the following studies: [25, 26, 32, 33]. This control group was used against our search string to verifiy if it was properly defined, i.e., whether our string was able to find these studies in the search sources. All these studies were returned using our search string. Hence, we can say that the string is representative to to cover the relevant studies related to big data and software architecture. 3.4 Selection Criteria The selection criteria were used to assess each primary study ob- tained from the search databases. It allowed us to include relevant studies to answer our RQs, and to exclude non-relevant studies. The inclusion criteria (IC) of our SMS are: IC1 The primary study addresses software architectures of big data systems. IC2 The primary study proposes a reference architecture or a reference model for big data systems. As well, the following indicate the exclusion criteria (EC). EC1 The primary study is a table of contents, short course description, or summary of a conference/workshop. EC2 The primary study is written in a language other than English. EC3 The study is a previous version of a more complete one on the same research or of the same authors. EC4 The full text of the primary study is not available. EC5 The primary study is out of scope of our SMS. 3.5 Quality Assessment Criteria We adopted the quality assessment criteria, as defined in [31], aim- ing to assess the quality of studies selected in our SMS with respect to their ability to answer our RQs. We defined five quality criteria questions: (1) Is there a rationale why the study was undertaken? (2) Is it an overview about the state of the art of the area in which the study is developed presented? (3) Is there a clear justification about the methods used during', '(2) Is it an overview about the state of the art of the area in which the study is developed presented? (3) Is there a clear justification about the methods used during the study? (4) Are the perspectives on future works discussed? (5) Is there a validation of what was proposed in the study? For each question, the following scale-point was applied: (i) 1 point if the study fully meets a given quality criterion; (ii) 0.5 point if the study meets the quality criterion partially; and (iii) no point if the study does not meet any quality criterion. Table 6 shows the quality classification according to each punctuation, i.e., the sum of points for each question. Only studies with a punctuation equal or above the average were included in our SMS, aiming to consider only studies with medium or high quality. Table 2: Quality classification of primary studies Punctuation Classification of Primary Studies 4.1 - 5.0 Excellent 3.1 - 4.0 Good 2.1 - 3.0 Average 1.1 - 2.0 Poor 0 - 1.0 Very poor 3.6 Selection of Studies Our SMS was conducted from March to June 2017. During the conduction phase, primary studies were identified, selected, and evaluated by the first two authors of this paper using the search', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa Figure 2: Search conduction results. 493 studies  remained 39 studies remained 19 studies selected920 studies Removal of Duplicated  Studies First Selection Second Selection 4 Recovery of Studies from Databases strategy previously defined. For each selected study, data were ex- tracted and synthesized. To support the management of the primary studies, we used Parsifal1. Figure 2 shows the results for each step described below. • Recovery of Studies and Exclusion of Duplicates: The search string was applied to the selected databases, presented previously in Section 3.3. As a result, we found 920 studies, divided into databases as shown in Table 3. From these stud- ies, 427 were duplicated and, therefore, were removed. In this activity, 493 primary studies were selected for the next step. Table 3: Number of studies per database Database Amount ACM Digital Library 48 El Compendex 271 IEEE Digital Library 189 ISI Web of Science 61 Science@Direct 13 Scopus 200 Springer Link 138 • First Selection: We read the title, abstract and keywords of each one of the 493 primary studies and applied the inclusion and exclusion criteria. When the abstract did not provide enough information, we also read the introduction and the conclusion sections of each study. As a result, we selected a set of 39 potentially relevant studies. • Second selection: We performed a full reading on the 39 se- lected studies. Such studies were analyzed again considering the inclusion and exclusion criteria. Divergences about in- cluding or excluding a study were discussed between the two authors and solved by the last one. As a result, 19 primary studies were included and 20 studies were excluded. • Quality Assessment: For each study, we calculated the quality score answering questions presented in Section 3.5. 11 studies out of 19 present excellent quality, i.e., S2, S5, S7, S8, S9, S10, S13, S14, S16, S15 and S19; six studies were con- sidered as of good quality, i.e., S1, S3, S4, S11, S17 and S18; and two studies were classified as average quality, i.e., S6 and S12. Therefore, we considered all 19 studies to extract infor- mation to answer our RQs. Table 4 summarizes the studies selected, where column ""IC"" describes the criterion used to 1Parsifal is a reference management tool that allows storing information on the primary studies (e.g., title, authors, venue, and abstract), as well as the set the inclusion/exclusion criteria applied to select each primary study. Available in: https://parsif.al/. include the studies and ""QA"" indicates the quality assessment criteria for each study selected. • Data Extraction: To guide the extraction, we created a form using Google Sheets2 with the following items: title of study, authors, bibliographic reference, type of system, architec- tural requirements, architectural modules, and domain appli- cation. All these information was extracted of each primary study during the reading. 4 RESULTS In this section, we present an overview of the primary studies found, including year of publication, application domains and type of systems. Then, we present a discussion of the main results extracted from the 19 studies, aiming to answer our RQs. 4.1 Overview of Studies First of all, we present a temporal distribution of the studies, as shown in Figure 3, classified by the two inclusion criteria. Studies selected by IC1 address software architecture of big data systems; and studies selected by IC2 established a reference architecture or a reference model for such systems. We can observe that the concern with software architectures of big data systems has been concentrated in the last three years, giving us evidence that it is a quite new research area. Figure 3: Number of studies per year IC1 IC2 2 25 1 1 1 1 2 2012 2013 2014 2015 2017 2 22016 2https://www.google.com/sheets', 'Characterizing Big Data Software Architectures: A Systematic Mapping Study SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Table 4: Final list of primary studies ID Title Authors/Reference IC QA S1 A big data analytics architecture for Industry 4.0 Santos et al. (2017) [46] IC1 4.0 S2 A modular software architecture for processing of big geospatial data in the cloud Kramer et al. (2015) [34] IC1 4.5 S3 A proposal for a reference architecture for long-term archiving, preservation, and retrieval of big data Viana (2015) [52] IC2 4.0 S4 A reference architecture for big data solutions introducing a model to perform predictive analytics using big data technology Geerdink (2013) [23] IC2 4.0 S5 A reference architecture for big data systems in the national security domain Klein et al. (2016) [32] IC2 5.0 S6 A reference architecture for supporting secure big data analytics over Cloud- Enabled relational databases Cuzzocrea (2016) [15] IC2 3.0 S7 An architecture to support the collection of big data in the Internet of Things Cecchine et al. (2014) [9] IC1 4.5 S8 Architecture knowledge for evaluating scalable databases Gorton et al. (2015) [26] IC1 4.5 S9 Big data analytics architecture for Agro Advisory systems Shah et al. (2016) [47] IC1 4.5 S10 Building a big data platform for Smart Cities: Experience and lessons from Santander Cheng et al. (2015) [11] IC1 5.0 S11 Cloud Computing infrastructure for data intensive applications Demchenko et al. (2017) [18] IC1 3.5 S12 Characteristics and requirements of big data analytics applications Al-Jaroodi et al. (2016) [1] IC1 3.0 S13 Design principles for effective knowledge discovery from big data Begoli et al. (2012) [6] IC1 4.5 S14 Distribution, data, deployment: Software architecture convergence in big data systems Gorton et al (2015). [25] IC1 4.5 S15 Expanding global big data solutions with innovative analytics Dayal et al. (2014) [16] IC2 4.5 S16 Managing cloud-based big data platforms: A reference architecture and cost perspective Heilig et al. (2017) [28] IC2 5.0 S17 Reference architecture and classification of technologies, products and services for big data systems Pääkkönen et al. (2015) [42] IC2 4.0 S18 Runtime, performance challenges in big data systems Klein et al. (2015) [33] IC1 4.0 S19 SmartLAK: A big data architecture for supporting learning analytics services Rabelo et al. (2014) [45] IC1 5.0 With respect to application domains addressed by these studies, Figure 4 shows that seven of the studies are related to a specific application domain, namely: Industry 4.0 (S1), smart campus (S7), smart cities (S10), learning services (S19), agriculture (S9), national security (S5) and geospatial (S2). This set of domains found in the primary studies usually presents large, complex and intense systems, which certainly will require attention to their architectures to be developed. We also found 12 studies that address big data general solutions. These studies (S3, S4, S6, S8, S11, S12, S13, S14, S15, S16, S17, and S18) are focused on diverse aspects of big data, such as data retrieval, data storage and data analytics. We were also interested in finding the size of big data systems addressed in each study. Most studies do not clearly state such size, but we identified three studies that mention Systems-of-Systems (S5), Cyber-Physical Systems (S7), and Smart Ecosystems (S10). Due to the inherent complexity and size of these systems together with the continuous growth of data volume, a trend could be an increase of attention for the design architectural of these systems, when also addressing big data. 4.2 Architectural Requirements for big data Systems It is crucial to be able to design and to develop well-designed soft- ware systems capable of effectively and efficiently satisfying the Figure 4: Application domain of each study various requirements, related to the need of data analysis, process- ing, and storage [1]. Due to the complexity of big data systems and', 'Figure 4: Application domain of each study various requirements, related to the need of data analysis, process- ing, and storage [1]. Due to the complexity of big data systems and their dynamic capabilities that must be met at runtime, the concern to precisely define the requirements increases. In order to answer RQ1 (What are the main requirements for building software ar- chitectures for systems that address big data?), requirements were extracted from each study identified in our SMS. As shown in Table 5, nine main requirements were identified. It is important to emphasize that some requirements, such as scalability, performance and consistency, were found in most studies. Study', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa S10 did not present any requirement, because its main goal is to instantiate a software architecture for big data system. Following, we present a brief description of each architectural requirement. Table 5: Architectural requirements for big data systems R1 R2 R3 R4 R5 R6 R7 R8 S1 ✕ ✕ ✕ S2 ✕ ✕ ✕ ✕ S3 ✕ ✕ ✕ ✕ S4 ✕ ✕ ✕ ✕ ✕ S5 ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ S6 ✕ ✕ ✕ ✕ S7 ✕ ✕ S8 ✕ ✕ ✕ ✕ ✕ ✕ S9 ✕ ✕ ✕ ✕ ✕ S10 S11 ✕ ✕ ✕ ✕ ✕ ✕ S12 ✕ ✕ ✕ ✕ ✕ S13 S14 ✕ ✕ ✕ S15 ✕ ✕ ✕ ✕ S16 ✕ ✕ ✕ ✕ ✕ ✕ S17 ✕ ✕ ✕ ✕ S18 ✕ S19 ✕ ✕ ✕ Total 13 13 11 10 8 7 6 5 R1 Scalability: Big data systems must be scalable, that is, must be able to increase and support different amounts of data, processing them uniformly, allocating resources without impacting costs or efficiency. To meet this requirement, it is required to distribute data sets and their processing across multiple computing and storage nodes; R2 High Performance Computing: Big data systems must be able to process large streams of data in a short period of time, thus returning the results to users as efficiently as possible. In addition, the system should support computation intensive analytics, i.e,. support diverse workloads, mixing request that require rapid responses with long-running requests that perform complex analytics on significant portions of the data collection [25]; R3 Modularity: Big data systems must offer distributed services, divided into modules, which can be used to access, process, visualize, and share data, and models from various domains, providing flexibility to change machine tasks; R4 Consistency: Big data systems must support data consis- tency, heterogeneity, and exploitation. Different data for- mats must be also managed to represent useful information for the system [ 9]. Besides that, systems must be flexible to accommodate this multiple data exchange formats and must achieve the best accuracy as possible to perform these operations; R5 Security: Big data systems must ensure security in the data and its manipulation in the architecture, supporting integrity of information, exchanging data, multilevel policy-driven, access control, and prevent unauthorized access [18]; R6 Real-time operation: Big data systems must be able to man- age the continuous flow of data and its processing in real time, facilitating decision making; R7 Interoperability: Big data systems must be transparently in- tercommunicated to allow exchanging information between machines and processes, interfaces, and people [46]; R8 Availability: Big data systems must ensure high data avail- ability, through data replication horizontal scaling, i.e., spread a data set over clusters of computers and storage nodes, avoiding bottlenecks [26]. For this, system should allow repli- cation and it should handle failures. All these requirements are non-functional ones, i.e., quality re- quirements and therefore, they are architecturally significant re- quirements (ASR) and should be considered in the architectural design of big data systems. There are more requirements for these systems addressed by the studies such as Data as a service, con- sidering service oriented architecture (SOA) [ 9], reconfiguration [9] and sensors board [50], when dealing with Internet of Things, Cyber-Physical Systems or Smart Systems, among others. However, these are application domain specific requirements, which are out of scope of this study to be analyzed in details. Since all these architectural requirements are classified as non- functional requirements, we mapped them onto a known quality model, specifically ISO/IEC 25010 [29]. Table 6 shows a brief sum- mary of which quality attributes proposed by ISO/IEC 25010 are related to the requirements and which were not found. Table 6: Mapping of requirements to quality attributes of ISO/IEC 25010 Requirement Quality Attribute (also quality char- acteristics and sub characteristics of ISO/IEC 25010)', 'Table 6: Mapping of requirements to quality attributes of ISO/IEC 25010 Requirement Quality Attribute (also quality char- acteristics and sub characteristics of ISO/IEC 25010) Consistency Not covered Scalability Portability: adaptability Real time operation Performance Efficiency: time-behavior High Performance Performance Efficiency: time-behavior and resource utilization Security Security: confidentiality, integrity and authenticity Availability Reliability: availability and recoverabil- ity Modularity Maintainability: modularity Interoperability Compatibility: interoperability One attribute not covered by the quality model is consistency, which represents the ability of all integrated information, whether from one data source or many, to be cohesive and usable. We believe that the non-coverage of this attribute is due to the fact that ISO/IEC 25010 represents a quality model for systems, while consistency represents information quality. However, due to the complexity of the current information and the need to persist it, it is expected that consistency become an essential attribute for any systems. Regarding the requirements that are directly related to quality attributes found in ISO/IEC 25010, we identified that some of them could be mapped onto more than one attribute and some of them', 'Characterizing Big Data Software Architectures: A Systematic Mapping Study SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil is defined with a different concept. The definitions presented in the selected studies and in ISO/IEC 25010 are detailed below. Scalability is one of the main requirements of a big data system implying that the system must support different amounts of data, allocating resources efficiently to maintain the system performance without causing any impact in the accomplishment of specified tasks and objectives or costs. This attribute is not directly defined by ISO/IEC 25010, but it is categorized as part of adaptability, i.e., degree to which a product or system can effectively and efficiently be adapted for different or evolving hardware, software or other operational or usage environments [29]. Real time operation can be characterized as the sum of high scal- ability and high performance, enabling operations to be carried out in real time, allowing rapid analysis and decision making. De- spite meeting some quality requirements, such as quality in use, performance, and functional suitability, there is no formal defini- tion for real-time operation in ISO/IEC 25010. However, due to its need to perform the operations at runtime, we mapped it as time- behavior, i.e., the degree to which the response and processing times and throughput rates of a product or system, when performing its functions, meet requirements [29]. Considering high performance, it is important that even with a variety of complex techniques to make possible the integration, process, and data analysis, the system is able to continue its execu- tion accomplishing its objectives efficiently. According to ISO/IEC 25010 [29], high performance can be mapped onto two quality at- tributes: (i) resource utilization, which is the degree to amounts and types of resources used by a product or system, when performing its functions; and (ii) time behavior, related to the response time to perform tasks. Comparing both definitions, big data systems must perform the tasks within a stipulated execution time and yet, for that execution, correctly distribute the resources among the different clusters so that this combination implies in an acceptable performance. When dealing with a large amount of data that travels among modules and is handled by them, security becomes a key require- ment for the systems. The studies found in our SMS treat security as a single problem that can be addressed as a unique attribute. However, according to ISO/IEC 25010 [29], security can be subdi- vided into several segments and some of these can be mapped as essential for big data systems, such as (i) confidentiality: to ensure that the data is available only when necessary and for the subjects who have access to it; (ii) integrity: to guarantee the security when modifying the date; and (iii) authenticity: to prove the identity of subjects or resources to access or change the information. According to ISO/IEC 25010 [29], availability is the degree to which a system is operational and accessible when required for use. In the definition found in the selected studies, they also define failure recovery as part of availability, thus, we mapped it onto recoverability, which is the degree in the event of an interruption or a failure, where a product or system can recover the data directly affected and re-establish the desired state of the system [29]. Finally, the requirements for modularity and interoperability were mapped directly into ISO/IEC 25010, since their definitions are the same ones of ISO/IEC 25010 and big data systems. Therefore, when analyzing the requirements found and the necessary quality attributes, it was possible to identify a total of eight requirements and map them in eleven quality attributes. 4.3 Architectural Modules for big data Systems A module represents a set of functionalities that together can meet', 'and map them in eleven quality attributes. 4.3 Architectural Modules for big data Systems A module represents a set of functionalities that together can meet specific goals of software architecture systems. The objective of the RQ2 (What are the main modules present in software architectures of big data systems?) is to elicit the main modules found in the architectures of big data systems. To answer this question, modules that are domain specific were not analyzed, as they could not represent part of an architectural pattern for big data systems. We found five common modules (M1 to M5) that can compose a software architecture that addresses big data, as shown in Table 7. Studies S8, S13, S14, and S18 do not present any architectural modules. We present a brief description of each module as follows: Table 7: Architectural modules for big data Systems M1 M2 M3 M4 M5 S1 ✕ ✕ ✕ ✕ S2 ✕ ✕ ✕ ✕ S3 ✕ ✕ S4 ✕ ✕ ✕ ✕ ✕ S5 ✕ ✕ ✕ ✕ S6 ✕ ✕ S7 ✕ S8 S9 ✕ ✕ ✕ ✕ ✕ S10 ✕ ✕ ✕ S11 ✕ ✕ ✕ ✕ ✕ S12 ✕ S13 S14 S15 ✕ ✕ ✕ ✕ ✕ S16 ✕ ✕ ✕ ✕ S17 ✕ ✕ ✕ ✕ ✕ S18 S19 ✕ ✕ ✕ ✕ Total 11 9 12 13 9 M1 Data Sources: This module identifies all sources of data and all possible expected formats and structures. The specifica- tions of data sources must be clearly described and docu- mented [34], detailing information of the different databases and files. These sources can generate data with high or low velocity and concurrency [11]; M2 Data Integration: This module supports the process of in- tegration of multiple data and knowledge representing the same real-world object into a consistent, accurate, and use- ful representation [47]. During execution, some data will be available from other sources, but some data will be acquired by the integration. This module should implement several technologies to achieve an efficient data integration; M3 Data Storage: This module supports mechanisms to orga- nize and optimize storage space usage, localization, availabil- ity, and reliability [45]. Collected data needs to be stored on', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa facilities that can handle their huge sizes [1], whether they are stored locally or in distributed multiple platforms; M4 Data Analytics: This module processes and analyze data [47]. Analysis is a very computing intensive and time con- suming process and must be optimized for best performance [34]. This module includes components, such as for data mining, querying, and reports, to facilitate analysis; and M5 Data Visualization: It must support data visualization by making it more accessible, understandable, and usable for users to identify patterns and trends [18, 47]. Several require- ments on the accuracy, timeliness, and delivery methods of the results must be achieved to find what data needs to be presented, how data is stored, and how soon the data needs to be reported. Finally, these essential modules of big data architectures could be organized into a pipeline structure, contemplating from the stage in which the data is generated by the sources to its processing, analysis, and visualization. 4.4 Reference architectures for big data systems With respect to the 19 studies found in this SMS, seven of them present reference architectures for big data systems (S3, S4, S5, S6, S15, S16, and S17) and were used to answer RQ3 (What requirements and modules are found in the reference architectures of big data systems already proposed?). Considering these studies, our interest was also to verify whether these reference architectures actually meet the requirements found in RQ1 and the modules found in the RQ2. This section presents an overview about these reference architectures and whether they encompass such requirements and modules. Regarding the requirements, six of seven reference architectures (S4, S5, S15, S16, and S17) detail scalability and high performance as essential requirements, implying that scalable systems running in a high performance must be the main concern when building software architectures for big data systems. Furthermore, security, which is addressed by four studies (S3, S5, S6, and S16), is a main requirement and general concern that affects all modules. Modular- ity is also one of the requirements found in our SMS. Six studies (S3, S4, S5, S6, S15, and S16) separate the main functionalities into well-defined modules that communicate to ensure data flow. Four of these architectures (S3, S4, S5, and S15) indicate communication among modules, i.e., interoperability as a main concern, presenting technologies such as the use of middlewares. Besides that, four of these studies (S3, S4, S5, and S16) address consistency among the data, three of them (S5, S16, and S17) treat real-time operation, and two studies (S5 and S17) treat availability requirement. Considering the essential modules, it is interesting to highlight the importance of data analytics module, which is presented in all reference architectures. Besides that, data storage module is presented in six studies (S4, S5, S6, S15, S16, and S17), five studies presented modules for managing the different sources (S3, S4, S15, S16, and S17), five studies (S4, S5, S15, S16, and S17) address visu- alization module, and four studies detail a module for integration (S4, S5, S15, and S17). As an important characteristic found in the implementation of these modules, we identified the use of the cloud Table 8: Architectural layers and patterns for big data sys- tems L1 L2 P1 P2 S1 ✕ ✕ S2 S3 ✕ ✕ S4 ✕ ✕ S5 ✕ ✕ ✕ ✕ S6 ✕ S7 S8 S9 ✕ S10 S11 ✕ ✕ S12 S13 ✕ ✕ S14 S15 ✕ S16 S17 ✕ S18 ✕ S19 ✕ Total 5 5 6 4 for storage as a trend, given its various advantages for big data systems, such as speed, cost, capacity, and even elasticity. Additionally to those modules and requirements, it is interesting to mention that most of the architectures for big data systems are designed over the layered architecture pattern [21], which focuses', 'Additionally to those modules and requirements, it is interesting to mention that most of the architectures for big data systems are designed over the layered architecture pattern [21], which focuses on grouping related functionalities required in a system applica- tion into distinct layers that are stacked on top of each other. The main layers found are related to security, management, and com- munication of modules. Moreover, we identified a layer related to communication with systems that represent the input and output of data, i.e., producers and consumers. The use of layered architectures was mainly identified in the studies that propose reference architec- tures, since they better detail the construction of the architecture, all the modules and communication among them. The following are brief definitions of layer responsibilities, and Table 8 presents a mapping of their presence in the studies. L1 Producers and Consumers: This layer is responsible for rep- resenting all big data producers and consumers [ 45], i.e., customers, suppliers, managers, among others; L2 Security, management, and monitoring: This layer includes components that provide base functionalities needed in the other layers, ensuring the proper operation of the whole infrastructure [46]. Some components needed are destined to monitoring, bottlenecks detection, performance improve- ment by adjusting parameters of the adapted technologies, authentication, data protection, among others. We also identified two different architectural styles for big data systems: P1 Pipe and filters: This pattern deals with how streams of data are successively processed or transformed by components', 'Characterizing Big Data Software Architectures: A Systematic Mapping Study SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil [23]. Its importance is justified because the sequence of data analysis is essential, for example, it is not possible to process a data that has not been still integrated, Such pattern is present in S4, S5, S9, S13, S17, and S18. P2 Orchestration: It configures and combines other modules by integrating activities into a cohesive system [32]. It was found in four studies (S4, S5, S13, and S15) and includes an optimizer for managing the data flow in all pipeline stages. [16]. In addition, considering the communication among modules in these two architectural styles, for the data flow through a pipeline structure, we identified the use of middlewares from studies of our SMS. While middlewares should not be a bottleneck for the data flow [9], it should assure quality in the data exchange. In general, the middleware architecture is organized in layers, including an API for sending data and an API for interacting collected data sets. 5 THREATS TO VALIDITY The main threats to validity of this SMS are described as follows: Missing of important primary studies: A threat to the va- lidity is that important studies were not found through the systematic search in databases. To minimize this threat, the result was validated with a control group of studies, as ex- plained in Section 3.3. Furthermore, we followed a predefined protocol proposed by Kitchenham2 et al. [31] and Petersen et al. [ 44] to avoid bias in the study search. Additionally, we searched in seven most important digital databases in software engineering area aiming to find all studies avail- able about software architectures in the context of big data systems. Study Selection: As the first author selected the studies by reading each one, it is possible that some studies were erro- neously assessed. To mitigate this threat, a set of 100 studies were randomly chosen and the title, abstract, and metadata analysis were again analyzed. After that, the results of both analyses were compared and no difference between them was found. Quality assessment: Aiming to assess the quality of primary studies, we defined seven criteria, as presented in Section 3.3. We evaluated each primary study using such criteria and we considered that all studies had enough quality to be considered in our SMS. However, it is possible that the assignment of scores had been influenced by opinion of the authors. Data Extraction: During data extraction, we created a data sheet to fill and save all answers from each study. However, since not all information was explicitly available in the stud- ies, some data had to be interpreted. To minimize this threat, discussions among the authors were developed. 6 CONCLUSION AND FUTURE WORKS The current advances in information and communication technolo- gies have propitiated the development of a variety of software systems that need to manage huge sets of data. In this scenario, attention and efforts to adequately architect these systems become fundamental for their success and long-living. To better understand and characterize software architectures of big data systems, we performed an SMS. In particular, we identified a set of eight quality requirements that could directly impact such architectures, where scalability and high performance were identified in 15 of a total of 19 studies selected for our SMS. This could imply that both require- ments should receive attention during the architectural design of big data systems. We also identified a set of five main modules nec- essary for their architectures and the ones responsible for storage and analytics are highlighted by addressing two main problems of the big data research area. Furthermore, we identified that the five main modules are usually arranged in a pipeline structure. Our SMS was also able to identify reference models and reference ar-', 'of the big data research area. Furthermore, we identified that the five main modules are usually arranged in a pipeline structure. Our SMS was also able to identify reference models and reference ar- chitectures, together with architectural styles/patterns, for big data domain, what shows a concern of the community to become more standardized the development and evolution of these systems. Being a first SMS carried out to gather knowledge about big data at architectural level, we believe this SMS will provide a first step towards systematizing the building of new big data systems and even evolution the existing ones. For future work, motivated by the results obtained in this study, we intend to conduct a more specific, deeper investigation, including the refinement of the set of requirements, and modules found. Moreover, we intend to use diagrams to detail how the modules communicate, investigate more patterns required to meet requirements and consider architectures and technologies developed in the industry, aiming to provide a more complete guide for architects of these quite important systems for big data management. ACKNOWLEDGMENTS This work is supported by Brazilian funding agencies FAPESP (Grant: 2016/15634-3, 2014/02244-7, and 2017/06195-9), CNPq, and CAPES. REFERENCES [1] Jameela Al-Jaroodi and Nader Mohamed. 2016. Characteristics and Requirements of Big Data Analytics Applications. In 2nd International Conference on Collabora- tion and Internet Computing, (CIC). IEEE Computer Society, Pittsburgh, PA, USA, 426–432. [2] Robert Allen and David Garlan. 1997. A Formal Basis for Architectural Con- nection. ACM Transactional Software Engineering Methodology 6, 3 (jul 1997), 213–249. [3] Samuil Angelov, Paul W. P. J. Grefen, and Danny Greefhorst. 2009. A classification of software reference architectures: Analyzing their success and effectiveness. In Joint Working IEEE/IFIP Conference on Software Architecture and European Conference on Software Architecture, (WICSA/ECSA). IEEE, Cambridge, UK, 141– 150. [4] AUTOSAR. 2017. AUTOSAR (AUTomotive Open System ARchitecture). URL: http://www.autosar.org/. (2017). Accessed on 05 June 2017. [5] Len Bass, Paul Clements, and Rick Kazman. 2012.Software Architecture in Practice (3rd ed.). Addison-Wesley Professional, New Jersey, USA. [6] Edmon Begoli and James Horey. 2012. Design principles for effective knowledge discovery from big data. In 6th Joint Working Conference on Software Architecture and European Conference on Software Architecture, (WICSA/ECSA). IEEE, Helsinki, Finland, 215 – 218. [7] Mark A Beyer and Douglas Laney. 2012. The importance of big data: a definition. (2012). [8] Barry Boehm, Prasanta Bose, Ellis Horowit, and Ming June Lee. 1995. Software Requirements Negotiation and Renegotiation Aids: A Theory-W Based Spiral Approach. In 17th International Conference on Software Engineering, (ICSE). IEEE Computer Society, Seattle, Washington, USA, 243–253. [9] Cyril Cecchinel, Matthieu Jimenez, Sebastien Mosser, and Michel Riveill. 2014. An Architecture to Support the Collection of Big Data in the Internet of Things.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa In IEEE World Congress on Services (SERVICES). IEEE, Anchorage, Alaska, USA, 442–449. [10] Hong-Mei Chen, Rick Kazman, Serge Haziyev, and Olha Hrytsay. 2015. Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm. In 1st IEEE/ACM International Workshop on Big Data Software Engineering, (BIGDSE). IEEE Computer Society, Florence, Italy, 44–50. [11] Bin Cheng, Salvatore Longo, Flavio Cirillo, Martin Bauer, and Ernoe Kovacs. 2015. Building a Big Data Platform for Smart Cities: Experience and Lessons from Santander. In IEEE International Congress on Big Data (IEEE BigData). IEEE, Santa Clara, CA, USA, 592–599. [12] Paul Clements, Len Bass, Rick Kazman, and Gregory Abowd. 1995. Predicting Software Quality by Architecture-Level Evaluation. In5th International Workshop on Software Quality, (WoSQ). SEI, Austin, Texas, USA, 485–497. [13] Robert J. Cloutier, Gerrit Muller, Dinesh Verma, Roshanak Nilchiani, Eirik Hole, and Mary A. Bone. 2010. The Concept of Reference Architectures. System Engineering 13, 1 (Feb 2010), 14–27. [14] Continua. 2017. Continua Health Alliance. URL: http://www.continuaalliance.org/. (2017). Accessed on 05 June 2017. [15] A. Cuzzocrea. 2016. A Reference Architecture for Supporting Secure Big Data Analytics over Cloud-Enabled Relational Databases. In 40th Annual Computer Software and Applications Conference, (COMPSAC). IEEE Computer Society, At- lanta, GA, USA, 356–358. [16] Umeshwar Dayal, Masaharu Akatsu, Chetan Gupta, Ravigopal Vennelakanti, and Massimiliano Lenardi. 2014. Expanding global big data solutions with innovative analytics. Hitachi Review 63, 6 (2014), 333–339. [17] Lucas Bueno Ruas de Oliveira, Katia Romero Felizardo, Daniel Feitosa, and Elisa Yumi Nakagawa. 2010. Reference Models and Reference Architectures Based on Service-Oriented Architecture: A Systematic Review. In 4th European Conference on Software Architectur, (ECSA). Springer Berlin Heidelberg, Copen- hagen, Denmark, 360–367. [18] Yuri Demchenko, Fatih Turkmen, Cees de Laat, Ching-Hsien Hsu, Christophe Blanchet, and Charles Loomis. 2017. Cloud Computing Infrastructure for Data Intensive Applications. In Big Data Analytics for Sensor-Network Collected Intelli- gence, Hui-Huang Hsu, Chuan-Yu Chang, and Ching-Hsien Hsu (Eds.). Academic Press, Taiwan, 21–62. [19] Tore Dyba, Torgeir Dingsoyr, and Geir K. Hanssen. 2007. Applying Systematic Reviews to Diverse Study Types: An Experience Report. In First International Symposium on Empirical Software Engineering and Measurement, (ESEM). ACM / IEEE Computer Society, Madrid, Spain, 225–234. [20] Matthew Finnegan. 2013. Boeing 787s to create half a terabyte of data per flight, says Virgin Atlantic. URL: https://goo.gl/lnNXhl. (2013). Accessed on 05 June 2017. [21] David Garlan. 2000. Software Architecture: A Roadmap. In 22nd International Conference on on Software Engineering, Future of Software Engineering Track, (ICSE). ACM, Limerick, Ireland, 91–101. [22] David Garlan and Dewayne Perry. 1994. Software Architecture: Practice, Potential, and Pitfalls. In16th International Conference on Software Engineering, (ICSE). IEEE Computer Society Press, Sorrento, Italy, 363–364. [23] Bas Geerdink. 2015. A reference architecture for big data solutions - introducing a model to perform predictive analytics using big data technology. International Journal of Big Data Intelligence (IJBDI) 2, 4 (2015), 236–249. [24] Christian Wehrmann Ger van den Broek, Filippo Cavallo. 2010. AALIANCE Ambient Assisted Living Roadmap. Ambient Intelligence and Smart Environments, Vol. 6. IOS Press, Amsterdam, The Netherlands. 136 pages. [25] Ian Gorton and John Klein. 2015. Distribution, data, deployment: Software architecture convergence in big data systems. IEEE Software 32, 3 (2015), 78 – 85. [26] Ian Gorton, John Klein, and Albert Nurgaliev. 2015. Architecture Knowledge for', 'architecture convergence in big data systems. IEEE Software 32, 3 (2015), 78 – 85. [26] Ian Gorton, John Klein, and Albert Nurgaliev. 2015. Architecture Knowledge for Evaluating Scalable Databases. In 12th Working IEEE/IFIP Conference on Software Architecture, (WICSA). IEEE Computer Society, Montreal, QC, Canada, 95–104. [27] Peter Groves, Basel Kayyali, David Knott, and Steven Van Kuiken. 2013. The Big Data revolution in healthcare. Center for US Health System Reform. Business Technology Office 1 (2013), 1–22. [28] Leonard Heilig and Stefan Voß. 2017. Managing Cloud-Based Big Data Platforms: A Reference Architecture and Cost Perspective. InBig Data Management. Springer International Publishing, Cham, Switzerland, 29–45. [29] ISO/IEC. 2010. ISO/IEC 24765 Systems and software engineering - Vocabulary. Technical Report. Institute of Electrical and Electronics Engineers, Inc. Accessed on 05 June 2017. [30] Mikal Khoso. 2016. How Much Data is Produced Every Day? URL:http://www.northeastern.edu/levelblog/2016/05/13/how-much-data- produced-every-day/. (2016). Accessed on 13 July 2017. [31] Barbara Kitchenham and Stuart Charters. 2007. Guidelines for performing System- atic Literature Reviews in Software Engineering. Technical Report. Keele University and Durham University Joint Report. 57 pages. [32] John Klein, R. Buglak, D. Blockow, T. Wuttke, and B. Cooper. 2016. A Reference Architecture for Big Data Systems in the National Security Domain. In 2nd International Workshop on BIG Data Software Engineering, (BIGDSE@ICSE). ACM, Austin, Texas, USA, 51–57. [33] John Klein and Ian Gorton. 2015. Runtime performance challenges in big data systems. In ACM/SPEC Workshop on Challenges in Performance Methods for Soft- ware Development, in Conjunction with ICPE. ACM, Austin, TX, United states, 17 – 22. [34] Michel Kramer and Ivo Senner. 2015. A modular software architecture for pro- cessing of big geospatial data in the cloud. Computers and Graphics (Pergamon) 49 (2015), 69 – 81. [35] Doug Laney. 2001. 3D data management: Controlling data volume, velocity and variety. META Group Research Note 6 (2001), 70. [36] David C. Luckham, John J. Kenney, Larry M. Augustin, James Vera, Walter Mann, Walter Mann, Doug Bryan, and Walter Mann. 1995. Specification and Analysis of System Architecture Using Rapide. IEEE Transactions on Software Engineering 21, 4 (1995), 336–355. [37] Silverio Martínez-Fernández. 2013. Towards supporting the adoption of software reference architectures: An empirically-grounded framework. In 11th Interna- tional Doctoral Symposium on Empirical Software Engineering (IDoESE). IEEE, Baltimore, Maryland USA, 1–8. [38] JD Meier. 2011. Reference Models, Reference Architectures, and Reference Imple- mentations. URL: https://blogs.msdn.microsoft.com/jmeier/2011/02/16/reference- models-reference-architectures-and-reference-implementations/. (2011). Ac- cessed on 13 June 2017. [39] Elisa Yumi Nakagawa, Flavio Oquendo, and Martin Becker. 2012. RAModel: A Reference Model for Reference Architectures. In Joint Working IEEE/IFIP Confer- ence on Software Architecture and European Conference on Software Architecture, (WICSA/ECSA). IEEE, Helsinki, Finland, 297–301. [40] Linda Northrop. 2003. The importance of software architecture. URL: http://csse.usc.edu/GSAW/gsaw2003/s13/northrop.pdf. (2003). Accessed on 05 June 2017. [41] OASIS. 2006. Reference Model for Service Oriented Architecture 1.0. URL: http://docs.oasis-open.org/soa-rm/v1.0/soa-rm.pdf. (2006). Accessed on 13 June 2017. [42] Pekka Pääkkö. 2015. Reference Architecture and Classification of Technologies, Products and Services for Big Data Systems. Big Data Research 2, 4 (2015), 166 – 186. [43] Dewayne E. Perry and Alexander L. Wolf. 1992. Foundations for the Study of Software Architecture.SIGSOFT Software Engineering Notes 17, 4 (Oct. 1992), 40–52. [44] Kai Petersen, Sairam Vakkalanka, and Ludwik Kuzniarz. 2015. Guidelines for', 'Software Architecture.SIGSOFT Software Engineering Notes 17, 4 (Oct. 1992), 40–52. [44] Kai Petersen, Sairam Vakkalanka, and Ludwik Kuzniarz. 2015. Guidelines for conducting systematic mapping studies in software engineering: An update. Information & Software Technology 64, C (2015), 1–18. [45] Thomas Rabelo, Manuel Lama, Ricardo R. Amorim, and Juan Carlos Vidal. 2015. SmartLAK: A big data architecture for supporting learning analytics services. In 2015 IEEE Frontiers in Education Conference, (FIE). IEEE Computer Society, El Paso,TX, USA, 1–5. [46] Maribel Yasmina Santos, Jorge Oliveira e Sá, Carlos Costa, João Galvão, Carina Andrade, Bruno Martinho, Francisca Vale Lima, and Eduarda Costa. 2017. A Big Data Analytics Architecture for Industry 4.0. In Recent Advances in Information Systems and Technologies (WorldCIST). Springer, Porto Santo Island, Madeira, Portugal, 175–184. [47] Purnima Shah, Deepak Hiremath, and Sanjay Chaudhary. 2016. Big Data Ana- lytics Architecture for Agro Advisory System. In 23rd International Conference on High Performance Computing Workshops, (HiPCW). IEEE Computer Society, Hyderabad, India, 43–49. [48] Chris Snijders, Uwe Matzat, and Ulf-Dietrich Reips. 2012. Big data: Big gaps of knowledge in the field of internet science.International Journal of Internet Science 7, 1 (2012), 1–5. [49] Judith A. Stafford and Alexander L. Wolf. 1998. Architecture-level Dependence Analysis in Support of Software Maintenance. In 3rd International Workshop on Software Architecture, (ISAW). ACM, Orlando, Florida, USA, 129–132. [50] Vernon Turner. 2014. The Digital Universe of Opportunities: Rich Data and the Increasing Value of the Internet of Things. URL:https://www.emc.com/leadership/digital-universe/2014iview/index.htm. (2014). Accessed on 05 June 2017. [51] UniversAAL. 2017. The UniversAAL Reference Architecture. URL: http://www.universaal.org/. (2017). Accessed on 05 June 2017. [52] Phillip Viana and Liria Sato. 2015. A proposal for a reference architecture for long-term archiving, preservation, and retrieval of big data. In 13th International Conference on Trust, Security and Privacy in Computing and Communications, (TrustCom). IEEE Computer Society, Beijing, China, 622–629. [53] Lichen Zhang. 2014. Designing big data driven cyber physical systems based on AADL. In IEEE International Conference on Systems, Man, and Cybernetics, SMC. IEEE, San Diego, CA, USA, 3072–3077.']","['CHARACTERIZING BIG D ATA SOFTWARE ARCHITE CTURES     This briefing reports scientific evidence on  the state of the art related to big data   software architectures addressing basic  requirements, modules, and architectural  patterns.      FINDINGS    Findings are grouped into three different groups.    Architectural Requirements for Big Data Systems    \uf0b7 Scalability: Big data systems  must be able to  increase and support different amounts of data,  processing them uniformly, allocating resources  without impacting costs or efficiency.     \uf0b7 High Performance Computing: Big data systems  must be able to process large streams of data in  a short period of time, thus returning the results  to users as efficiently as possible.     \uf0b7 Modularity: Big data systems must offer  distributed services, divided into modules,  which can be used to access, process, visualize,  and share data, and models from various  domains, providing flex ibility to change  machine tasks.    \uf0b7 Consistency: Big data systems must support  data consistency, heterogeneity, and  exploitation. Moreover, systems  must be  flexible to accommodate data exchange formats  and achieve the best accuracy  to perform these  operations;    \uf0b7 Security: Big data systems must ensure security  in the data and its manipulation in the  architecture, supporting integrity of  information, exchanging data, multilevel policy- driven, access control, and prevent  unauthorized access.    \uf0b7 Real-time operation: Big data systems must be  able to manage the continuous flow of data and  its processing in real time, facilitating decision  making;    \uf0b7 Interoperability: Big data systems must be  transparently intercommunicated to allow  exchanging information between machines and  processes, interfaces, and people [46];    \uf0b7 Availability: Big data systems must ensure high  data availability, through data replication  horizontal scaling, i.e., spread a data set over  clusters of computers and storage nodes,  avoiding bottlenecks.    Architectural Modules for Big Data Systems    \uf0b7 Data Sources: This module identifies all sources  of data and all possible expected formats and  structures. The specifications of data sources  must be clearly described and documented,  detailing information of the different databases  and files.     \uf0b7 Data Integration:  This module supports the  process of integrat ion of multiple data and  knowledge representing the same real -world  object into a consistent, accurate, and useful   representation.     \uf0b7 Data Storage:  This module supports  mechanisms to organize and optimize storage  space usage, localization, availability, and   reliability.     \uf0b7 Data Analytics:  This module processes and  analyze data. Analysis is a very computing  intensive and time consuming process and must  be optimized for best performance.     \uf0b7 Data Visualization:  It must support data  visualization by making it more  accessible,  understandable, and usable for users to identify  patterns and trends.     Architectural Patterns for Big Data Systems    \uf0b7 Pipe and filters: This pattern deals with  how streams of data are successively  processed or transformed by  components. Its importance is justified  because the sequence of data analysis is  essential, for example, it is not possible to  process a data that has not been still  integrated.    \uf0b7 Orchestration: It configures and combines  other modules by integrating activities  into a cohesive system. It includes an  optimizer for managing the data flow in  all pipeline stages.    \uf0b7 Layered Architectural Pattern:  It focuses  on g rouping related functionalities  required in a system application into  distinct layers that are stacked on top of  each other. Following are the main layers:    o Producers and Consumers: This layer is  responsible for representing all big  data  producers and consumers , i.e.  customers, suppliers, managers,  among others.  o Security, management, and', 'o Producers and Consumers: This layer is  responsible for representing all big  data  producers and consumers , i.e.  customers, suppliers, managers,  among others.  o Security, management, and  monitoring: It includes components  that provide base functionalities  needed in the other layers, ensuring  the proper operation of the whole  infrastructure. Some components  needed are destined t o monitoring,  bottlenecks detection, performance  improvement by adjusting parameters  of the adapted technologies,  authentication, data protection,  among others.    Middlewares are used to assure quality in the data  exchange and improve the communication among  modules. However, it should not be a bottleneck.      Who is this briefing for?    Software engineering practitioners  who want to make decisions about  architectural patterns, selection of  components and quality attributes  for big data systems based on  scientific evidence.    Where the findings come from?    All findings of this briefing were   extracted from the systematic  mapping study  conducted by Sena  et al.      What is included in this briefing?    The main findings of the original  systematic mapping.    Evidence characteristics through a  brief description about the original  systematic review and the studies it  analyzed.      What is not included in this briefing?    The systematic steps to achieve  these findings.    Details about the selected studies  that describe these findings.      To access other evidence briefings  on software engineering:    http://cin.ufpe.br/eseg/evidence- briefings      For additional information about  START:    http://www.labes.icmc.usp.br      ORIGINAL RESEARCH REFERENCE  Bruno Sena et al. Characterizing Big Data Software Architectures: A Systematic Mapping. SBCARS, 2017.']","**Title: Understanding Big Data Software Architectures**

**Introduction:**  
This Evidence Briefing summarizes key findings from a systematic mapping study conducted by Bruno Sena and colleagues, which characterizes big data software architectures. The goal is to provide insights into the essential requirements, modules, and organization of architectures that effectively manage large and complex data sets, enabling practitioners to design robust software systems for big data applications.

**Main Findings:**  
The systematic mapping study identified eight critical requirements for big data architectures, which are essential for ensuring that these systems can handle the unique challenges posed by big data:

1. **Scalability:** The architecture must efficiently manage varying data volumes without degrading performance.
2. **High Performance:** Systems should process large data streams quickly to deliver timely insights.
3. **Modularity:** Architectures should consist of distinct modules that facilitate flexibility and ease of updates.
4. **Consistency:** The architecture must ensure data integrity and coherence across different formats and systems.
5. **Security:** Robust security measures are needed to protect data integrity and prevent unauthorized access.
6. **Real-time Operation:** Systems should support continuous data processing to enable real-time decision-making.
7. **Interoperability:** The ability to communicate and exchange information seamlessly between different systems and components is crucial.
8. **Availability:** High data availability must be ensured through strategies like data replication and fault tolerance.

In addition to these requirements, the study identified five essential modules that comprise a big data architecture:

- **Data Sources:** Identifies and documents various data input formats and structures.
- **Data Integration:** Facilitates the merging of data from multiple sources into a consistent representation.
- **Data Storage:** Organizes and optimizes the storage of large data sets.
- **Data Analytics:** Provides tools for processing and analyzing data effectively.
- **Data Visualization:** Ensures that data insights are presented in an accessible and understandable manner.

The findings suggest that these modules are often organized in a pipeline structure, emphasizing the flow from data generation to processing and visualization. Furthermore, the study highlighted that reference architectures for big data systems are increasingly incorporating these requirements and modules, promoting standardization in the field.

**Who is this briefing for?**  
This briefing is intended for software architects, developers, and practitioners involved in the design and implementation of big data systems. It aims to provide actionable insights to guide the development of effective architectures that meet the evolving demands of big data management.

**Where the findings come from:**  
The findings presented in this briefing are derived from a systematic mapping study conducted by Bruno Sena, Ana Paula Allian, and Elisa Yumi Nakagawa, published in the proceedings of the SBCARS 2017 conference.

**What is included in this briefing?**  
This briefing includes an overview of the main requirements and modules for big data architectures, along with practical implications for software design in the context of big data.

**To access the original research article:**  
Sena, B., Allian, A. P., & Nakagawa, E. Y. (2017). Characterizing Big Data Software Architectures: A Systematic Mapping Study. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017. https://doi.org/10.1145/3132498.3132510"
"['A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform Casimiro Conde Marco Neto Universidade Federal do Estado do Rio de Janeiro Av. Pasteur, 458 – Urca Rio de Janeiro, RJ casimiro.neto@uniriotec.br M´arcio de O. Barros Universidade Federal do Estado do Rio de Janeiro Av. Pasteur, 458 – Urca Rio de Janeiro, RJ marcio.barros@uniriotec.br ABSTRACT Issue tracking systems help so/f_tware development teams in iden- tifying problems to be solved and new features to be added to a so/f_tware system. In this paper, we replicate and extend a study carried out in 2013 on the usage of the issue tracking system provi- ded by the GitHub platform. /T_he replication aims at determining whether the results observed four years ago are still valid. /T_he extension seeks to analyze how o/f_ten issues are terminated by com- mits to the version control system and understand whether this feature allows developers to relate an issue to the source code mo- dules that were changed to resolve it. We conclude that the results of the previous study remain valid and that issues closed by com- mits are uncommon (about 4% of our sample) and o/f_ten linked to technical aspects of the project. CCS CONCEPTS •So/f_tware and its engineering→So/f_tware evolution; KEYWORDS GitHub,issues, issue management, issue tracking systems ACM Reference format: Casimiro Conde Marco Neto and M´arcio de O. Barros. 2017. A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017,10 pages. DOI: 10.1145/3132498.3134110 1 INTRODUC ¸ ˜AO O desenvolvimento de so/f_tware´e uma atividade complexa e ´e co- mum que as partes envolvidas tenham quest ˜oes a serem analisadas e respondidas sobre o so/f_tware em construc ¸˜ao. Essas quest ˜oes s˜ao genericamente chamadas de issues e indicam a existˆencia de defei- tos, problemas, riscos, oportunidades de melhoria, pendˆencias ou d´uvidas que devem ser avaliadas pela equipe de desenvolvimento. Desenvolvedores e usu ´arios do so/f_tware identi/f_icamissues com frequˆencia e s ˜ao encorajados a registr´a-los em um issue tracking system para facilitar a comunicac ¸˜ao entre os grupos respons´aveis pelo desenvolvimento, operac ¸˜ao e uso do sistema [1]. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or aﬃliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SBCARS 2017, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5325-0/17/09. . . $15.00 DOI: 10.1145/3132498.3134110 Os estudos sobre issue tracking systemsanalisam como este tipo de ferramenta pode apoiar a equipe durante o desenvolvimento de um projeto de so/f_tware. O maior estudo sobre o assunto at´e o mo- mento foi realizado em 2013 por BISSYANDE et al. [1]. Os autores identi/f_icaram que poucos projetos de c´odigo aberto usam issue trac- king systemse que estas ferramentas s˜ao geralmente utilizadas por projetos mais antigos, com maior base de c´odigo-fonte, com equipes maiores e com l´ıderes mais populares. Tamb´em identi/f_icaram que n˜ao h´a uma diferenc ¸a clara entre colaboradores que participam em um projeto apenas registrando issues e outros que apenas desen- volvem o c ´odigo-fonte, mas que grande parte dos colaboradores cumprem os dois pap´eis nos projetos. Um t´opico pouco abordado nos estudos anteriores ´e a relac ¸˜ao entre os issues e o c´odigo-fonte envolvido em sua resoluc ¸˜ao. Esta relac ¸˜ao permite evidenciar que alterac ¸˜oes foram realizadas em um projeto para encerrar umissue, possibilitando a rastreabilidade entre os m´odulos de c´odigo-fonte e osissues e serve de base para a criac ¸˜ao de modelos de predic ¸˜ao que permitam identi/f_icar os m´odulos de c´odigo-fonte que precisar˜ao ser alterados para resolver os novos', 'de modelos de predic ¸˜ao que permitam identi/f_icar os m´odulos de c´odigo-fonte que precisar˜ao ser alterados para resolver os novos issues que ser˜ao registrados no futuro. Por outro lado, esta relac ¸˜ao n˜ao ´e identi/f_icada de forma expl´ıcita no GitHub, visto que n˜ao existe uma funcionalidade para relacionar um issue com os trechos do c´odigo-fonte que ele afeta. A funcionalidade de encerramento de issues por commits oferecida pelo issue tracking systemda plata- forma GitHub pode ser utilizada para analisar esta relac ¸˜ao de forma indireta. Atrav´es desta funcionalidade, sempre que uma mensagem com o padr ˜ao “Closed #X”´e encontrada em um commit enviado para o sistema de controle de vers˜oes, a plataforma marca oissue de n´umero X como fechado, indicando que o problema nele descrito foi resolvido pelas alterac ¸˜oes realizadas nos m´odulos de c´odigo-fonte que participam do commit. Em decorrˆencia da ausˆencia de estudos sobre o assunto, n˜ao sabemos se esta funcionalidade ´e utilizada com frequˆencia e para resolver que tipo de issue. O objetivo desse artigo ´e compreender as caracter´ısticas dosis- sues de projetos de so/f_tware e determinar com que frequˆencia estes issues podem ser relacionados com o c´odigo-fonte utilizado para re- solvˆe-los atrav´es o recurso de encerramento porcommit. Para tanto, projetamos um estudo dividido em duas partes e realizado sobre uma amostra de 76.909 projetos resultantes de um /f_iltro aplicado sobre 220.000 projetos da plataforma GitHub. Na primeira parte do estudo foram replicadas as an´alises realizadas por BISSYANDE et al. [1] com o intuito de veri/f_icar se as conclus˜oes do estudo original se mantˆem passados quatro anos desde a sua divulgac ¸˜ao. A replicac ¸˜ao de estudos ´e uma tarefa importante para garantir que os resul- tados encontrados anteriormente continuam observ´aveis, se eles se mant´em idˆenticos ou seu comportamento apresentou alguma', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros mudanc ¸a com os novos dados dispon ´ıveis. Por´em, a replica c ¸˜ao ´e pouco disseminada devido a di/f_iculdades de realizac ¸˜ao: muitos artigos n˜ao apresentam todos os dados necess´arios para que seus es- tudos sejam replicados, muitas replicac ¸˜oes n˜ao chegam a resultados semelhantes aos estudos originais e seus autores tˆem di/f_iculdade para publicar resultados negativos e dissonantes [9]. Na segunda parte do estudo foram introduzidas novas quest˜oes de pesquisa que analisam a rela c ¸˜ao entre issues e o c ´odigo-fonte alterado para resolvˆe-los. Consideramos que um issue est´a relacio- nado aos m´odulos de c´odigo-fonte afetados por um commit quando o issue for encerrado por este commit. Se os issues fossem fre- quentemente encerrados por commits e os mesmos m ´odulos de c´odigo-fonte fossem alterados para tratar issues similares, talvez fosse poss´ıvel usar oscommits para prever os m´odulos de c´odigo- fonte que ser˜ao afetados por novosissues. No entanto, identi/f_icamos que a funcionalidade de encerramento de issues por commits n˜ao ´e amplamente difundida e buscamos caracterizar osissues encerrados desta forma de acordo com os desenvolvedores envolvidos, o uso de r´otulos e seu tempo de correc ¸˜ao. Enquanto a primeira parte do estudo pretende adicionar evid ˆencias atualizadas aos resultados encontrados por BISSYANDE et al. [1], a segunda parte pretende agregar novos conhecimentos e trazer informac ¸˜oes sobre o uso de issue tracking systemse como suas informac ¸˜oes podem ser utilizadas em pesquisas na ´area de Engenharia de So/f_tware. Este artigo est´a organizado em seis sec ¸˜oes, comec ¸ando por esta introduc ¸˜ao. A sec ¸˜ao 2 apresenta uma revis˜ao bibliogr´a/f_ica compre- endendo trabalhos que abordam o uso de issue tracking systems. A sec ¸˜ao 3 apresenta a metodologia utilizada para coleta de dados e os dados utilizados no estudo proposto. A se c ¸˜ao 4 apresenta a an´alise de dados e as conclus˜oes alcanc ¸adas. A sec ¸˜ao 5 consolida os resultados da se c ¸˜ao anterior e os discute de forma conjunta. Finalmente, a sec ¸˜ao 6 apresenta as conclus ˜oes ap´os a an ´alise, as limitac ¸˜oes, ameac ¸as`a validade e propostas para trabalhos futuros. 2 TRABALHOS RELACIONADOS O uso de issue tracking system´e um assunto abordado de diver- sos pontos de vista, principalmente buscando identi/f_icar qual a importˆancia do registro e correc ¸˜ao de issues para o ciclo de vida e sucesso de um projeto ou so/f_tware. Nesta sec ¸˜ao, concentraremos a discuss˜ao nos trabalhos relacionados com a plataforma GitHub. KALLIAMVAKOU et al. [5] descrevem o GitHub como um ambi- ente colaborativo de suporte ao desenvolvimento de c´odigo, cons- tru´ıdo sobre o sistema de controle de vers˜ao GIT. Em maio de 2016 a plataforma possu´ıa mais de 12 milh˜oes de usu´arios e 38 milh˜oes de projetos, sendo assim o maior reposit´orio de c´odigo-fonte online do mundo. A plataforma integra recursos de redes sociais `as funci- onalidades de desenvolvimento, permitindo que desenvolvedores “sigam” outros desenvolvedores ou projetos [6]. O GitHub tem sido alvo de estudos para entender o comportamento dos seus usu´arios e como ele pode in/f_luenciar no sucesso dos projetos. DABBISH et al. [4] identi/f_icam que a transparˆencia no ambiente colaborativo permite que os usu´arios do GitHub conhec ¸am outros desenvolvedores, possibilitando o reconhecimento, aproximac ¸˜ao e feedback frequente. LEE et al. [6] identi/f_icam uma grande in/f_luˆencia dos usu´arios muito seguidos no comportamento dos demais, apon- tando que as funcionalidades sociais oferecidas pelo GitHub criam diversas maneiras da comunidade colaborar e in/f_luenciar nas ativi- dades dos seus membros. XAVIER et al. [8] analisam as diferenc ¸as entre os usu´arios que registram issues e aqueles que os corrigem. Os autores identi/f_icaram', 'dades dos seus membros. XAVIER et al. [8] analisam as diferenc ¸as entre os usu´arios que registram issues e aqueles que os corrigem. Os autores identi/f_icaram que os primeiros tendem a n ˜ao participar na equipe de nenhum projeto, enquanto muitos usu´arios que corrigem issues participam na equipe de desenvolvimento de pelo menos um projeto. WEICHENG et al. [7] exploram padr˜oes nos commits feitos pelos desenvolvedores no GitHub buscando uma relac ¸˜ao entre os padr˜oes e a evoluc ¸˜ao do so/f_tware. Os autores identi/f_icaram dois padr˜oes na evoluc ¸˜ao do so/f_tware: (i) um arquivo alterado em uma vers˜ao tende a afetar um arquivo dependente dele em uma vers˜ao futura; e (ii) o tempo m´edio para a ocorrˆencia de um “grandecommit”, de/f_inido como um commit em que mais de cinco arquivos tiveram mais de cinco linhas alteradas, ´e trˆes vezes maior que o tempo entrecommits normais. Usando estes padr ˜oes, os autores a/f_irmam poder prever o pr´oximo commit do projeto e que arquivos ser˜ao alterados nesse commit, gerando assim informac ¸˜oes para apoiar o planejamento da evoluc ¸˜ao dos so/f_twares hospedados na plataforma. CABOT et al. [2] conclu´ıram que apenas 3,25% de trˆes milh˜oes de projetos avaliados no GitHub possu´ıam algumissue classi/f_icado em algum grupo. Os principais grupos utilizados foram melho- ria, defeito, quest˜ao, funcionalidade e documentac ¸˜ao. Os autores identi/f_icaram que os projetos que classi/f_icavam seusissues apresen- tavam um maior envolvimento de pessoas na soluc ¸˜ao e discuss˜ao dos mesmos, al´em de um aumento do n´umero de issues fechados. BISSYANDE et al. [1] analisaram a ferramenta de issue tracking da plataforma GitHub buscando identi/f_icar as caracter´ısticas que di- ferenciam os projetos que adotaram issue tracking systemsdaqueles que n˜ao utilizam essa ferramenta. Os autores utilizaram uma amos- tra de 100.000 projetos de c´odigo aberto, dispon´ıveis na plataforma. De posse dessa amostra, realizaram um estudo visando responder diversas quest˜oes de pesquisa, conforme ser´a detalhado na pr´oxima sec ¸˜ao. Identi/f_icaram que poucos projetos de c´odigo aberto utilizam issue tracking systemse que esta ferramenta ´e mais utilizada por projetos mais antigos, com maior base de c´odigo-fonte, com equipes de desenvolvimento maiores e com l´ıderes mais populares. 3 METODOLOGIA E COLETA DE DADOS Para colher os dados utilizados em nosso estudo da plataforma GitHub utilizamos o framework Egit, dispon´ıvel para o ambiente Eclipse, que possibilita a captura de dados do sistema de controle de vers˜oes GIT por programas escritos na linguagem de programac ¸˜ao Java. Foram utilizadas duas m´aquinas para a coleta de dados, que demorou 75 dias. Ap´os a coleta e o tratamento dos dados, o pacote estat´ıstico R v3.3 foi utilizado para realizar as an´alises estat´ısticas. 3.1 Entidades e dados coletados As an´alises desse estudo foram executadas sobre uma amostra de 220.010 projetos coletados entre os 38 milh˜oes de projetos registra- dos no GitHub. Para isso, coletamos os identi/f_icadores de todos os projetos ativos em maio/2016 e sorteamos aleatoriamente aqueles que iriam fazer parte da amostra. Em seguida, coletamos dados detalhados sobre esses projetos. Visando compreender a utilizac ¸˜ao da ferramenta de issue tracking, analisamos as caracter´ısticas de trˆes entidades: o projeto, o issue e o colaborador.', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Para cada projeto, foram coletadas a sua idade (medida em sema- nas desde a data de criac ¸˜ao do projeto at´e a data de in´ıcio da coleta, em maio de 2016), o l´ıder do projeto (usu´ario que registrou o pro- jeto na plataforma GitHub), a principal linguagem de programac ¸˜ao usada no projeto (conforme retornado pelo GitHub quando solicita- mos a linguagem em que o projeto foi desenvolvido), seu tamanho (em KBytes), seu n´umero de linhas de c´odigo, seu hist´orico de com- mits no sistema de controle de vers˜oes e um indicador que mostra se o projeto possui a funcionalidade de issue trackingativa. Capturamos tamb´em informac ¸˜oes sobre a popularidade dos pro- jetos. Um fork representa a “c´opia” de um projeto, aqui representada entre aspas porque ´e mantida uma relac ¸˜ao que permite submeter as alterac ¸˜oes realizadas em um fork para o projeto original. Assu- mimos que projetos com mais forks s˜ao mais populares e usamos o n´umero de forks como medida de popularidade do projeto. Para cada projeto da amostra, veri/f_icamos se ele´e um fork e capturamos o n´umero de forks que tiveram origem no projeto. Uma segunda medida de popularidade est ´a associada aos re- cursos de redes sociais oferecidos pela plataforma GitHub, que permitem que os usu´arios da plataforma demonstrem interesse por determinados projetos e por outros usu´arios. Do ponto de vista dos projetos, este interesse ´e demostrado “seguindo” as ac ¸˜oes que s˜ao realizadas nos projetos. Assim, os usu´arios se inscrevem nos pro- jetos e passam a ser noti/f_icados das ac ¸˜oes que os desenvolvedores realizam sobre estes. Como uma segunda medida de popularidade dos projetos, capturamos o n´umero de usu´arios da plataforma que “seguem” cada projeto da amostra. Tamb´em foram coletadas informac ¸˜oes sobre os issues registrados em cada projeto da amostra, incluindo o seustatus na data da coleta (aberto ou fechado), o usu´ario respons´avel pelo registro do issue, os r´otulos usados para classi/f_icar oissue, seu tempo de corre c ¸˜ao (medido em dias entre a data de registro e a data de encerramento, apenas para issues fechados) e seu modo de encerramento (por commit ou atrav´es da interface com o usu´ario da plataforma GitHub, apenas para issues fechados). Adicionalmente, coletamos informac ¸˜oes sobre os colaboradores, que s ˜ao os usu ´arios da plataforma GitHub que interagiram com algum projeto da amostra. Para cada colaborador, identi/f_icamos seu nome, seu login, seu e-mail e o n´umero de usu´arios da plataforma que “seguem” o colaborador. O n´umero de seguidores ´e tomado aqui como uma medida de popularidade do colaborador, de forma similar ao descrito para os projetos acima. Tamb´em classi/f_icamos os colaboradores como reporters, quando estes registram issues, e desenvolvedores, quando estes realizam commits no sistema de controle de vers˜oes. Importante notar que a classi/f_icac ¸˜ao ´e feita por projeto, ou seja, um determinado colaborador pode ser reporter em um projeto, desenvolvedor em outro ou mesmo desempenhar os dois pap´eis em um terceiro projeto. Ao /f_inal da coleta de dados, aplicamos uma curadoria sobre a amostra que se baseou em dois crit´erios de exclus˜ao. O primeiro, proposto no estudo original, sugere eliminar todos os projetos em que a ferramenta deissue trackingestiver desativada, visando retirar os casos em que os issues foram mantidos em ferramentas externas ao GitHub. Assim, foram retirados 97.286 projetos dos 220.010 cole- tados, restando 122.724 projetos na amostra. O segundo crit´erio de exclus˜ao foi aplicado para garantir que os projetos analisados ar- mazenavam o c´odigo-fonte de um so/f_tware. Assim, foram retirados 45.815 projetos em que n ˜ao foi identi/f_icada nenhuma linguagem de programac ¸˜ao, restando 76.909 projetos (35% do total de projetos inicialmente coletados).', '45.815 projetos em que n ˜ao foi identi/f_icada nenhuma linguagem de programac ¸˜ao, restando 76.909 projetos (35% do total de projetos inicialmente coletados). 3.2 /Q_uest ˜oes de pesquisa As quest˜oes de pesquisa a seguir s˜ao avaliadas em nossa an´alise. As quest˜oes de n´umero 1 a 6 foram propostas no estudo original [1] e sua an´alise ser´a repetida neste estudo. Na s´etima e oitava quest˜oes caracterizamos o uso do recurso de encerramento de issues por commits, que permite relacionar os m ´odulos de c ´odigo-fonte do projeto com os seus issues. RQ1. /Q_ual a proporc ¸˜ao de projetos que recebem registros de issues e que caracter´ısticas diferenciam estes projetos dos demais? RQ2. /Q_uantosissues, em m´edia, s˜ao registrados em projetos que utilizam issue tracking systems? RQ3. /Q_ual a popularidade da utilizac ¸˜ao de r´otulos para classi/f_icar issues? /Q_uais s˜ao os principais r´otulos utilizados? RQ4. /Q_uem cadastraissues? /Q_uantos destes colaboradores fazem parte da equipe de desenvolvimento? RQ5. ´E poss´ıvel identi/f_icar alguma relac ¸˜ao entre a utilizac ¸˜ao de issue tracking systemse a popularidade do projeto? RQ6. /Q_ue caracter´ısticas do projeto, dos colaboradores ou dos issues afetam o tempo de correc ¸˜ao dos issues? RQ7. /Q_uais s˜ao as caracter´ısticas dos colaboradores que registram issues? RQ8. /Q_uais s˜ao as caracter´ısticas dosissues que s˜ao encerrados por commit? 3.3 Di/f_iculdades na coleta de dados O GitHub restringe a quantidade de requisi c ¸˜oes feitas por um usu´ario a 5.000 por hora. Essa limita c ¸˜ao impediu que a coleta de dados fosse realizada de forma cont ´ınua, pois a coleta era inter- rompida sempre que o limite de requisic ¸˜oes era alcanc ¸ado. Essa foi a principal di/f_iculdade enfrentada durante a coleta de dados, pois tornou o processo mais lento devido ao tempo de espera quando o limite de requisic ¸˜oes era atingido. Outro obst´aculo foi a estrat´egia de coleta. O artigo original utili- zou os primeiros 100.000 projetos retornados pela API do GitHub, coletando os projetos mais antigos e executando a an´alise sobre eles. A primeira tentativa de coleta seguiu a mesma estrat´egia, coletando os primeiros 200.000 projetos retornados pela API. Por ´em, iden- ti/f_icamos que essa amostra n˜ao era representativa para a an´alise pretendida, visto que se concentrava em projetos antigos, enquanto a nossa intenc ¸˜ao era saber se projetos mais recentes mantinham o padr˜ao de uso de issue trackingdos projetos do artigo original. Por esse motivo, foi executada uma segunda coleta, gerando a amostra utilizada nesse estudo, com projetos cadastrados entre 2008 e 2016, garantindo que essa replicac ¸˜ao ter´a uma amostra com projetos mais recentes do que o estudo original. Outra ´ultima di/f_iculdade foi a inexistˆencia de padr˜oes de escrita nos campos de texto coletados dos commits e issues, onde foram identi/f_icados diversos caracteres de formatac ¸˜ao, tags de HTML e', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros pontuac ¸˜oes que atrapalhavam o processo de an´alise. Visando supe- rar essa di/f_iculdade, foram constru´ıdos mecanismos para tratamento pr´evio desses campos, retirando a “poluic ¸˜ao”. 3.4 T ´ecnicas de an´alise de dados Nas an´alises relatadas a seguir compararemos dois conjuntos de dados (por exemplo, o conjunto com as idades dos projetos que utilizam issue tracking systemscom o conjunto com as idades dos projetos que n˜ao utilizam este recurso). Neste contexto, interessa saber se existe diferenc ¸a entre estes conjuntos e se esta diferenc ¸a pode ser atribu´ıda a fatores aleat´orios ou ru´ıdo nos dados. Para tanto, utilizaremos o teste de Wilcoxon-Mann-Whitney, um teste n˜ao-param´etrico que veri/f_ica se existe diferenc ¸a estatisticamente signi/f_icativa entre as medianas de dois conjuntos de dados. Este teste exige que se estabelec ¸a um n´ıvel m´ınimo de con/f_ianc ¸a, que em nossas an´alises ser´a de 99% (ou seja, a diferenc ¸a entre as medianas dos valores ser´a considerada signi/f_icativa se p-value< 0, 01). No entanto, o teste estat´ıstico n˜ao ´e su/f_iciente quando temos um grande volume de dados, pois o volume tende a reduzir a varia- bilidade nos dados e a encontrar diferenc ¸as signi/f_icativas mesmo quando a diferen c ¸a nominal ´e muito pequena. Nestes cen ´arios, o teste estat´ıstico deve ser complementado com uma m´etrica de tamanho de efeito padronizado, que mede a frequ ˆencia com que um dado selecionado aleatoriamente de um conjunto de dados ser´a melhor que um dado selecionado aleatoriamente do outro conjunto segundo um crit´erio previamente de/f_inido (por exemplo, identi/f_icar o maior valor nominal). A m´etrica A12 de Vargha & Delaney serve a este prop´osito. Esta m´etrica varia de 0% a 100%. /Q_uando assume o valor de 50%, ela indica que os dois grupos tˆem chances iguais de produzir o melhor resultado. Valores acima de 50% indicam que o primeiro grupo tem chances maiores de apresentar o melhor resul- tado. Neste trabalho utilizaremos a m´etrica A12 para complementar o teste de Wilcoxon-Mann-Whitney, utilizando a tabela proposta por COHEN [ 3] para caracterizar tamanhos de efeito pequenos (A12 ≤60%), m´edios (A12 ≤75%) e grandes (A12 > 75%)1. Finalmente, algumas an´alises exigir˜ao o c´alculo da intensidade de dependˆencia entre dois conjuntos de dados. Nestes casos, uti- lizaremos o ´ındice de correlac ¸˜ao de Spearman ( ρ), uma medida n˜ao-param´etrica de depend ˆencia, que varia de -1 a +1. Valores pr´oximos de zero indicam que os conjuntos de dados n˜ao s˜ao rela- cionados, enquanto valores pr´oximos de +1 indicam que os valores do primeiro conjunto tendem a crescer junto com os valores do segundo e valores pr´oximos a -1 indicam que os valores do primeiro conjunto tendem a diminuir a medida que os valores do segundo crescem e vice-versa. Assim como na medida de tamanho de efeito, lanc ¸amos m˜ao de uma tabela proposta por COHEN [3] para carac- terizar relac ¸˜oes fracas (ρ ≥0.1), moderadas ( ρ ≥0.3) ou fortes (ρ ≥0.5) entre os dados analisados. 4 AN ´ALISE DE DADOS 4.1 Frequ ˆencia de projetos com issues A primeira quest ˜ao de pesquisa investiga a distribui c ¸˜ao do uso de issue tracking systemsnos projetos da amostra. Nas an ´alises abaixo utilizamos a amostra de 76.909 projetos que restaram ap´os 1Interpretado a partir dos limites da m´etrica d para tamanho de efeito. curadoria. Passamos ent ˜ao a relacionar o n ´umero de issues com caracter´ısticas do projeto, como sua idade, tamanho, tamanho de equipe e a popularidade do l´ıder. 4.1.1 Idade do projeto. A Figura 1 apresenta a distribuic ¸˜ao da idade dos projetos de acordo com o uso de issue tracking systems. Foi identi/f_icado que a m´edia de idade para projetos que possuem issues ´e de 94,6 semanas, com mediana de 71 semanas. Para os projetos que n˜ao possuem issues a m´edia ´e de 82,2 semanas, com', 'Foi identi/f_icado que a m´edia de idade para projetos que possuem issues ´e de 94,6 semanas, com mediana de 71 semanas. Para os projetos que n˜ao possuem issues a m´edia ´e de 82,2 semanas, com mediana de 62 semanas. Os testes estat ´ısticos indicam diferenc ¸a signi/f_icativa entre as idades dos projetos com e semissues, com pequeno tamanho de efeito pequeno (55%). Figura 1: Issues e a idade dos projetos 4.1.2 Tamanho do projeto. Projetos com issues registrados pos- suem uma m ´edia de 24.272,7 linhas de c ´odigo (LOC), com uma mediana de 957 LOC. Por sua vez, projetos sem issues possuem uma m´edia de 22.080,4 LOC e uma mediana de 515 LOC. Os testes estat´ısticos con/f_irmam que as duas distribuic ¸˜oes s˜ao signi/f_icativa- mente diferentes com tamanho de efeito pequeno (55%). 4.1.3 Tamanho da equipe. Identi/f_icamos uma m´edia de 12 desen- volvedores e uma mediana de trˆes desenvolvedores para projetos com issues registrados, enquanto que para projetos sem issues a m´edia ´e de 4,2 desenvolvedores e a mediana de apenas um desen- volvedor (67% dos projetos deste conjunto possuem apenas um desenvolvedor). Os testes estat´ısticos indicam diferenc ¸a signi/f_ica- tiva entre os dois conjuntos com tamanho de efeito grande (81%). 4.1.4 Popularidade do l ´ıder do projeto. A m´edia de seguidores de l´ıderes de projetos comissues ´e de 44,7 usu´arios, com uma medi- ana de apenas dois usu´arios. Para projetos sem issues, a m´edia de seguidores ´e 15 usu´arios, com mediana de um usu ´ario. Os testes estat´ısticos indicam diferenc ¸a signi/f_icativa entre os conjuntos de dados, com tamanho de efeito pequeno (54%). 4.1.5 Sum ´ario. Os t´opicos abordados nesta quest˜ao de pesquisa apresentam comportamento semelhante ao observado no estudo original. Assim, con/f_irmamos os resultados de BISSYANDE et al. que a/f_irmam que “projetos comissues reportados tendem a ser mais velhos, ter mais linhas de c´odigo, maior n´umero de desenvolvedores e l´ıderes mais populares que projetos semissues” [1].', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil 4.2 N ´umero de issues por projeto A segunda quest˜ao de pesquisa investiga a distribuic ¸˜ao do n´umero de issues por projeto. A partir desta quest˜ao de pesquisa descarta- mos os projetos que n ˜ao possuem issues. Assim, esta quest ˜ao foi respondida utilizando uma base de 226.340 issues reportados em 11.662 projetos. A Tabela 1 apresenta a distribuic ¸˜ao de issues por projeto. Observamos que 93,1% dos projetos possuem menos de 50 issues reportados, contra 86% apresentados no estudo original [1]. Tabela 1: Distribuic ¸˜ao do n ´umero de issues por projeto Nosso estudo Estudo original [1] # Issues # Proj % Total # Proj % Total 1-9 8.803 75,48% 11.602 57,89% 10-49 2.054 17,61% 5.526 27,57% 50-99 355 3,04% 1.411 7,04% 100-249 272 2,33% 976 4,87% 250-499 108 0,93% 290 1,44% 500-999 58 0,50% 163 0,81% 1000-4999 12 0,10% 69 0,34% 5000-9999 0 0,00% 2 0,01% 10000+ 0 0,00% 2 0,01% Investigamos tamb´em o n´umero de issues a cada 1.000 LOC (ou 1 kLOC) nos projetos. Observamos uma m´edia de 13,5 issues/kLOC, com uma mediana de 1,54 issues/kLOC. O estudo original reporta uma mediana de 6,32 issues/kLOC. A Figura 2 apresenta um scat- terplot em escala logar´ıtmica do n´umero de issues contra o n´umero de LOC do projeto. Observamos uma correla c ¸˜ao fraca entre as vari´aveis (ρ=0,199), ligeiramente inferior ao apresentado no estudo original (ρ=0,341). Figura 2: N ´umero de issues por tamanho de projeto, medido em LOC A Figura 3 apresenta a distribuic ¸˜ao de issues entre os projetos que tˆem como linguagem de programac ¸˜ao principal alguma das dez linguagens com maior n´umero de issues em nossa amostra. 194.081 issues (86%) est˜ao distribu´ıdos em projetos que usam uma destas linguagens (60.902 projetos, ou 79%). A maior m´edia foi encontrada para a linguagem C, com 5,27issues por projeto. No estudo original, a linguagem com maior m ´edia foi Ruby. A Tabela 2 apresenta a m´edia de issues por projeto para cada uma dessas linguagens. Como BISSYANDE et al. [1], observamos uma forte utilizac ¸˜ao de issue tracking systemsem projetos desenvolvidos para web. Em linha com o estudo original, conclu´ımos que a maioria dos projetos tˆem poucos ou nenhum issue: apenas 6,9% dos projetos da amostra possuem mais de 50 issues. Por /f_im, observamos uma relac ¸˜ao fraca entre o n´umero de LOC e o n´umero de issues, variando pouco entre as principais linguagens de programac ¸˜ao. Figura 3: N ´umero de issues em projetos de acordo com as linguagens de programac ¸˜ao mais utilizadas Tabela 2: N ´umero m´edio de issues por projeto de acordo com a sua linguagem de programac ¸˜ao principal Linguagem Projetos Issues Issues/Projeto C 2.998 15.807 5,27 C++ 3.565 18.536 5,20 C# 3.004 13.673 4,55 Java 12.111 43.211 3,57 Python 6.668 21.440 3,22 Java Script 14.023 39.358 2,81 Objective-C 2.053 5.647 2,75 PHP 5.147 14.112 2,74 HTML 4.960 11.214 2,26 Ruby 6.373 11.083 1,74 4.3 Uso de r ´otulos em issues Na terceira quest˜ao de pesquisa avaliamos a utilizac ¸˜ao de r´otulos para classi/f_icarissues. No issue tracking system oferecido pelo GitHub ´e poss´ıvel criar r´otulos espec´ı/f_icos para cada projeto ou utilizar os r´otulos disponibilizados pela plataforma. A plataforma n˜ao atribui signi/f_icado aos r´otulos: ao criar um novo r ´otulo, o cola- borador deve se assegurar de que seu nome seja su/f_icientemente descritivo para que um desenvolvedor ou reporter identi/f_ique o ob- jetivo do r´otulo ou documentos a parte devem especi/f_icar o objetivo de cada r´otulo usado no projeto. Foram encontrados 304.033 r ´otulos nos 226.340 issues identi/f_i- cados na nossa amostra, sendo 2.596 r´otulos distintos distribu´ıdos em 118.596 issues. V ´arios destes r´otulos possu´ıam signi/f_icado se- melhante e foram agrupados manualmente em 301 grupos. Os', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros trˆes r´otulos identi/f_icados como mais utilizados no estudo original (bug, feature e enhacement) tamb´em /f_iguram entre os dez grupos de r´otulos mais utilizados em nossa amostra. Por ´em, eles representam apenas 12% dos r´otulos identi/f_icados, contra 34% no estudo original. Os trˆes grupos mais utilizados em nossa amostra (auto-migrated, priority e defect) representam 72% do total de r ´otulos utilizados, tendo sido encontrados em 82.782 issues distintos, sendo que 60.874 issues possuem os trˆes r´otulos ao mesmo tempo. A m´edia de r ´otulos identi/f_icados porissue ´e de 2,6, com uma mediana de trˆes r´otulos e um m´aximo de 10 r´otulos em um issue. 52% dos issues da amostra possuem r´otulos, contra 30% no estudo original. 26% destes issues possuem apenas um r´otulo, 54% possuem dois r´otulos, 8% possuem trˆes r´otulos, 8% possuem quatro r´otulos, e 3% possuem cinco ou mais r´otulos. A recorrˆencia de issues com dois ou mais r´otulos indica que estes s˜ao utilizados com diversos /f_ins, como apontamento de tipo de problema, vers˜ao do so/f_tware, componente do so/f_tware, entre outros. 4.4 Per/f_il do colaborador que cadastra issues A quarta quest ˜ao de pesquisa discute se issue tracking systems s˜ao utilizados exclusivamente por reporters ou se desenvolvedo- res tamb´em registram issues. Identi/f_icamos 110.946 desenvolvedo- res nos projetos que possuem issues (581.856 no estudo original), com uma m´edia de 9,5 desenvolvedores por projeto e uma medi- ana de dois desenvolvedores. Consideramos como desenvolvedor todo usu´ario que contribuiu com ao menos um commit no projeto. Tamb´em foram identi/f_icados 44.522reporters (239.629 no estudo original), com uma m ´edia de 3,8 por projeto e uma mediana de apenas um. Consideramos como reporter todo usu´ario que apenas registra issues em um projeto. Observamos que em m´edia 38% dos desenvolvedores atuam como reporter em outros projetos, com uma mediana de 33% (31% no estudo original). Por sua vez, em m´edia 53% dosreporters participam de alguma atividade de desenvolvimento em algum projeto, com uma mediana de 50% (42% no estudo original). A Figura 4 mostra a distribuic ¸˜ao de desenvolvedores e reporters nos projetos. Como BISSYANDE et al. [ 1], veri/f_icamos que muitos desenvolvedores registram issues, enquanto grande parte dosreporters contribui para o c´odigo-fonte. Conclu´ımos ent˜ao que n˜ao existem grupos distintos de pessoas que apenas desenvolvem ou apenas registram issues, mas que diversos colaboradores executam os dois pap´eis. 4.5 Issues e a popularidade do projeto Para analisar a quinta quest˜ao de pesquisa utilizaremos o n´umero de forks e o seguidores para indicar a popularidade de um projeto e o interesse despertado na comunidade. Em seguida, analisaremos a relac ¸˜ao entre a popularidade do projeto e o registro de issues. 4.5.1 N ´umero de seguidores. A Figura 5 apresenta umsca/t_terplot da relac ¸˜ao entre o n´umero de issues e o n´umero de seguidores nos projetos que comp ˜oem a amostra. Observamos uma correla c ¸˜ao moderada (ρ=0,311) entre as vari´aveis, inferior ao encontrado no estudo original (ρ=0,628). Analisamos tamb ´em a relac ¸˜ao entre o n´umero de seguidores e o n´umero de reporters. Mais uma vez encon- tramos uma dependˆencia moderada entre estas vari´aveis (ρ=0,323), tamb´em inferior ao estudo original (ρ=0,789). Figura 4: Proporc ¸˜ao de desenvolvedores entre os issues que s˜ao reporters e vice-versa Figura 5: A relac ¸˜ao entre o n´umero de seguidores e n´umero de issues 4.5.2 N ´umero de forks. Observamos uma correlac ¸˜ao moderada entre o n´umero de issues e o n´umero de forks (ρ=0,426) nos projetos da amostra, inferior ao observado no estudo original ( ρ=0,669). Observamos tamb´em uma correlac ¸˜ao moderada entre o n ´umero de reporters e o n ´umero de forks (ρ=0,442), tamb ´em inferior ao reportado no estudo original (ρ=0,829).', 'Observamos tamb´em uma correlac ¸˜ao moderada entre o n ´umero de reporters e o n ´umero de forks (ρ=0,442), tamb ´em inferior ao reportado no estudo original (ρ=0,829). 4.5.3 Sum ´ario. Conclu´ımos que o n´umero de seguidores e forks possuem relac ¸˜ao moderada com a utilizac ¸˜ao de issue tracking sys- tems e com o n´umero de reporters em um projeto. Observamos que projetos com mais seguidores e forks tendem a ter mais issues e um maior n´umero de colaboradores reportando issues, con/f_irmando os resultados de BISSYANDE et al. [1], por´em com menor intensidade. 4.6 Tempo de correc ¸˜ao dos issues Na sexta quest˜ao de pesquisa investigamos aspectos relacionados com a velocidade com que issues s˜ao corrigidos.', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Figura 6: A relac ¸˜ao entre o n´umero de reporters e o tempo para correc ¸˜ao de issues 4.6.1 N ´umero de reporters. A Figura 6 apresenta um sca/t_terplot relacionando o n´umero de reporters e o tempo m´edio de correc ¸˜ao dos issues nos projetos. O gr´a/f_ico n˜ao apresenta relac ¸˜ao linear entre as duas vari´aveis, como no estudo original. Observamos correlac ¸˜ao quase nula entre o n ´umero de reporters e o tempo de corre c ¸˜ao (ρ=0,062), pr´oximo ao reportado no estudo original (ρ=0,161). 4.6.2 Uso de r ´otulos. Nesse ponto encerramos a replicac ¸˜ao do estudo original e passamos a analisar outras caracter ´ısticas que podem afetar o tempo de correc ¸˜ao dosissues, comec ¸ando pelo uso de r´otulos. O tempo m´edio de correc ¸˜ao de issues que possuem r´otulos ´e 22,52 dias, com mediana de zero dias, visto que 73% deles foram encerrados no mesmo dia em que foram registrados. Para issues que n˜ao possuem r´otulos, o tempo m´edio de correc ¸˜ao ´e 25,17 dias, com uma mediana de zero dias (62% dos issues foram encerrados em menos de um dia). Os testes estat ´ısticos indicam diferenc ¸a signi/f_icativa entre os dados com tamanho de efeito pequeno (56%). 4.6.3 Uso dos r ´otulos mais frequentes. Analisamos a rela c ¸˜ao entre o tempo de correc ¸˜ao de um issue e se ele possui algum r´otulos dos dez grupos mais utilizados. A m´edia do tempo de correc ¸˜ao para os issues que possuem algum r´otulo destes grupos´e de 17,3 dias, com mediana de zero dias (80% dosissues foram encerrados em menos de um dia). J´a para os issues que n˜ao possuem r´otulos ou usam r´otulos menos frequentes, a m´edia do tempo de correc ¸˜ao ´e de 28,8 dias, com mediana de zero dias (60% dos issues foram encerrados em menos de um dia). Os testes estat ´ısticos indicam diferenc ¸a signi/f_icativa entre os dados com tamanho de efeito pequeno (60%). 4.6.4 N ´umero de coment´arios. Investigamos a relac ¸˜ao entre o tempo de correc ¸˜ao de umissue e o n´umero de coment´arios recebidos. Encontramos correlac ¸˜ao quase nula ( ρ=0,065), indicando que o n´umero de coment´arios n˜ao in/f_luencia o tempo de correc ¸˜ao. 4.6.5 Sum ´ario. Conclu´ımos queissues com r´otulos tendem a ser corrigidos mais rapidamente e que issues que possuem r´otulos pre- sentes nos dez grupos mais utilizados tˆem essa tendˆencia acentuada. Al´em disso, veri/f_icamos que n˜ao h´a relac ¸˜ao entre a quantidade de coment´arios recebidos por um issue ou o n´umero de reporters em um projeto e o tempo m´edio de correc ¸˜ao dos issues. Figura 7: A relac ¸˜ao entre issues registrados e o tempo desde a criac ¸˜ao da conta do colaborador 4.7 Caracter ´ısticas dosreporters Na s´etima quest˜ao de pesquisa investigamos as caracter´ısticas dos colaboradores e a sua rela c ¸˜ao com a utiliza c ¸˜ao de issue tracking systems. Para tal, retiramos da amostra os colaboradores com contas canceladas ou privativas, restando 40.885 colaboradores dos quais 28.177 registraram pelo menos um issue nos projetos da amostra. 4.7.1 Tempo de cria c ¸˜ao da conta. Analisamos a relac ¸˜ao entre o n ´umero de issues registrados por um colaborador e o tempo, em dias, desde a cria c ¸˜ao da sua conta. A Figura 7 apresenta o sca/t_terplotdestes dados. Encontramos uma correlac ¸˜ao negativa e fraca entre as vari´aveis (ρ=-0,101), indicando que n˜ao existe uma relac ¸˜ao evidente entre o n ´umero de issues registrados e a idade da conta de um colaborador. Identi/f_icamos que a idade m´edia da conta dos colaboradores que registraramissues ´e de 1597,7 dias, com mediana de 1601 dias. Para os colaboradores que n˜ao registraram issues, a m´edia ´e de 1827,7 dias com mediana de 1836 dias. Os testes estat´ısticos indicam diferenc ¸as signi/f_icativas entre os dados, com tamanho de efeito pequeno (59%). 4.7.2 N ´umero de commits registrados. Avaliamos a relac ¸˜ao entre', 'estat´ısticos indicam diferenc ¸as signi/f_icativas entre os dados, com tamanho de efeito pequeno (59%). 4.7.2 N ´umero de commits registrados. Avaliamos a relac ¸˜ao entre o n´umero de issues registrados e o n´umero de commits realizados pelos colaboradores. Identi/f_icamos uma correlac ¸˜ao fraca e nega- tiva (ρ=-0,185) entre estas vari´aveis, indicando que existe uma leve tendˆencia de que reporters registrem mais issues do que desenvol- vedores. Os reporters realizaram em m ´edia 30,83 commits, com mediana de zero (56% dos reporters n˜ao registraram commits). J ´a para colaboradores que n ˜ao registraram issues a m´edia ´e de 99,6 commits, com mediana de cinco commits. Os testes estat ´ısticos indicam distribuic ¸˜oes signi/f_icativamente diferentes entre os grupos de colaboradores, com tamanho de efeito m´edio (74%). Conclu´ımos que colaboradores que registram issues tendem a realizar menos commits do que aqueles que n˜ao registram. 4.7.3 N ´umero de seguidores. Nesse t´opico avaliamos a relac ¸˜ao entre o n´umero de issues registrados e a popularidade de um cola- borador, medindo essa popularidade pelo n´umero de seguidores do colaborador em quest˜ao. Encontramos uma correlac ¸˜ao negativa e', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros quase nula (ρ=–0,063) na amostra, indicando que a popularidade n˜ao ´e um bom preditor do n´umero de issues registrados. Veri/f_icamos que a m´edia de seguidores para os colaboradores que registraramis- sues ´e de 144,43, com mediana de sete seguidores. Foi retirado dessa amostra um outlier que possu´ıa mais de 20 milh˜oes de seguidores; com ele, a m´edia seria de 1057,27 seguidores. Para colaboradores que n˜ao registraram issues, a m´edia ´e de 157,04, com mediana de 15 seguidores. Os testes estat´ısticos indicam diferenc ¸as signi/f_icativas entre os dados, com tamanho de efeito pequeno (59%). Portanto, colaboradores que registram issues tendem a possuir menos segui- dores do que aqueles que n˜ao registram issues. Supomos que esse comportamento pode estar ligado ao fato de que os desenvolvedores recebam mais atenc ¸˜ao da comunidade do que reporters. 4.7.4 Projetos p ´ublicos. Encerrando essa quest˜ao de pesquisa, avaliamos a relac ¸˜ao entre o n´umero de issues registrados por um co- laborador e o n´umero de projetos p´ublicos que ele possui. Mais uma vez, encontramos uma correlac ¸˜ao negativa e quase nula (ρ=-0,024) entre esses dados. A m ´edia de projetos p ´ublicos por colaborado- res que registraram issues ´e de 38,6, com mediana de 19 projetos. Para colaboradores que n˜ao registram issues, a m´edia ´e de 38,9 com mediana de 25. Os testes estat ´ısticos indicam distribuic ¸˜oes signi- /f_icativamente diferentes, com tamanho de efeito pequeno (55%). Conclu´ımos que os colaboradores que registramissues tendem a possuir um n´umero ligeiramente menor de projetos p´ublicos do que aqueles que n˜ao registraram issues. Esse comportamento pode estar ligado ao fato de que os l´ıderes de projetos geralmente participam como desenvolvedores, enquanto os reporters n˜ao precisam possuir projetos para registrar issues, realizando essa ac ¸˜ao em projetos de outros usu´arios da plataforma. 4.7.5 Sum ´ario. Conclu´ımos que as caracter´ısticas de umrepor- ter, como a idade da sua conta, o n´umero de seguidores e o n´umero de projetos p´ublicos tˆem pouca correlac ¸˜ao com o n´umero de issues registrados por ele. J´a o n´umero de commits realizados pelo colabo- rador tem uma correlac ¸˜ao fraca com o n´umero de issues registrados por ele: colaboradores que registramissues tendem a realizar menos commits que os demais colaboradores. 4.8 Issues encerrados por commit Na oitava quest˜ao de pesquisa investigamos as caracter´ısticas dos issues encerrados por commit em comparac ¸˜ao com issues encerra- dos atrav´es da interface com o usu ´ario do issue tracking system. Para isso, utilizamos os 159.118 issues marcados como fechados na amostra. Desses, 7.000 issues foram encerrados por commits. O n´umero m´edio de issues encerrados por commits para projetos com pelo menos um issue fechado ´e de 0,8 issue por projeto, indicando que em m´edia apenas 4% dos issues de um projeto s˜ao encerrados por commits. Para projetos com pelo menos um issue encerrado por commit, a m´edia de issues encerrados desta forma ´e de 6,5 issues, indicando que em m´edia 28% dos issues do projeto s˜ao encerrados por commit, com mediana de dois issues. Estes resultados apontam para uma concentrac ¸˜ao da pr´atica de encerramento de issues por commits em poucos projetos. Pesqui- sadores que busquem o desenvolvimento de modelos de predic ¸˜ao de m´odulos mais sujeitos a alterac ¸˜oes, explorando a relac ¸˜ao entre os issues e os m´odulos de c´odigo-fonte alterados para resolvˆe-los, devem concentrar seus esforc ¸os nestes poucos projetos que usam encerramentos de issues por commits. N ˜ao ´e recomendado o uso de uma ampla base de projetos, pois os issues encerrados por com- mits ser˜ao poucos na maior parte dos projetos componentes desta base, provendo assim pouco material para relacionar os issues e os m´odulos de c´odigo-fonte.', 'mits ser˜ao poucos na maior parte dos projetos componentes desta base, provendo assim pouco material para relacionar os issues e os m´odulos de c´odigo-fonte. 4.8.1 Linguagem de programa c ¸˜ao. As cinco linguagens que mais tiveram issues encerrados por commits s˜ao JavaScript (com 1.509 issues, 4% dos issues registrados em projetos baseados nessa lingua- gem), Java (com 1.377 issues, 3% dos issues registrados em projetos baseados nessa linguagem), C++ (com 778 issues, 4% dos issues em projetos com essa linguagem), Python (com 722issues, 3% dos issues em projetos com essa linguagem) e PHP (com 490 issues, 3% dos issues em projetos com essa linguagem). Todas estas linguagens est˜ao entre as dez que mais possuem issues registrados. 4.8.2 Tempo de corre c ¸˜ao. O tempo m´edio de correc ¸˜ao para issues encerrados por commits ´e de 27,61 dias, com mediana de dois dias. Para issues que n˜ao foram encerrados por commit, a m´edia ´e de 23,8 dias, com mediana de zero dias (70% foram encerrados no mesmo dia). Os testes estat´ısticos indicam uma diferenc ¸a signi/f_icativa entre os grupos, com tamanho de efeito m´edio (65%). 4.8.3 Utiliza c ¸˜ao de r´otulos. Dos 7.000 issues encerrados por com- mit, apenas 4.219 possuem r´otulos. O teste chi-quadrado, utilizado para saber se os dados de dois grupos s ˜ao estatisticamente dife- rentes, indica que o percentual de issues encerrados por commit ´e signi/f_icativamente maior entre osissues que possuem r´otulos e os issues que n˜ao possuem r´otulos, com tamanho de efeito pequeno (x2 = 278,1, p-value < 0,01, Cramer-V (tamanho de efeito) = 4,18%). Apenas 4% dos issues que n˜ao possuem r´otulos ou possuem r´otulos fora do grupo dos dez r´otulos mais utilizados foram encerrados por commits, enquanto 6% dos issues que possuem r´otulos entre os dez grupos mais utilizados foram encerrados desta forma. Utilizando o teste chi-quadrado veri/f_icamos que existe diferenc ¸a signi/f_icativa do porcentual de issues encerrados por commit entre os issues que possuem algum r´otulo entre os dez grupos mais utilizados e aqueles que possuem r ´otulos fora destes grupos, com tamanho de efeito pequeno (x2 = 114,34, p-value < 0,001, Cramer-V = 2,6%). Os cinco r´otulos mais utilizados em issues encerrados por com- mits pertencem aos grupos bug (1.768 issues encerrados por commit ou 19% dos issues registrados com r´otulos deste grupo); enhance- ment (1.392 issues encerrados por commit ou 6% dos issues com r´otulos deste grupo); feature (335 issues encerrados por commit ou 13% dos issues com r´otulos deste grupo); priority (211 issues encer- rados por commit ou 0,3% dos issues com r´otulos deste grupo); e user interface(92 issues encerrados por commit ou 20% dos issues com r´otulos deste grupo). Encontramos uma correlac ¸˜ao fraca ( ρ=0,158) entre o n ´umero de commits registrados por um colaborador e o n´umero de issues com r´otulos encerrados por commits. Veri/f_icamos que a m´edia de commits realizados por colaboradores que encerraram issues com r´otulos por commits ´e de 369,49 commits por projeto, com mediana de 53 commits. J ´a para aqueles que n ˜ao encerraram issues com r´otulos por commits a m´edia ´e de 45,98 commits, com mediana de', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil um commit. Os testes estat´ısticos identi/f_icaram distribuic ¸˜oes signi- /f_icativamente diferentes, com tamanho de efeito grande (82%). Ve- ri/f_icamos, conforme o esperado, que os colaboradores que utilizam a funcionalidade de encerramento de issues por commit executam mais commits do que os colaboradores que n˜ao a utilizam. 4.8.4 N ´umero de coment´arios por issue. Investigamos o n´umero de coment´arios recebidos por issues fechados por commit. A m´edia de coment´arios recebidos por issues encerrados por commits ´e de 1,7 coment´arios/issue, com mediana de zero (56% dos issues deste tipo n˜ao receberam coment´arios). Para issues que n˜ao foram encerrados por commits essa m´edia ´e de 2,4 coment´arios/issue, com mediana de um coment´ario. Os testes estat´ısticos indicam diferenc ¸as signi/f_i- cativas entre os grupos, com tamanho de efeito m´edio (62%). 4.8.5 N ´umero de commits. Encontramos uma correla c ¸˜ao pe- quena (ρ=0,211) entre o n´umero de issues encerrados por commit e o n´umero de commits registrados por um colaborador. A Figura 8 apresenta um sca/t_terplotos dados utilizados nesta an ´alise. A m´edia de commits registrados pelos colaboradores que encerraram ao menos um issue por commit ´e de 270,41, com mediana de 42 commits. Para aqueles que n ˜ao encerraram issues por commits a m´edia ´e de 44,39, com mediana de um commit. Os teste estat´ısticos identi/f_icaram diferenc ¸as signi/f_icativas entre os grupos, com tama- nho de efeito grande (82%). Conclu´ımos que o n´umero de commits realizados tem correlac ¸˜ao pequena com a quantidade deissues que o colaborador encerrou por commits. Veri/f_icamos tamb´em, conforme o esperado, que os desenvolvedores tendem a utilizar a funcionali- dade de encerramento de issues por commit com mais frequˆencia. Figura 8: A relac ¸˜ao entre o n´umero de issues encerrados por commit e o n´umero de commits realizados pelo colaborador 4.8.6 Issues externos encerrados por commit. Issues externos s˜ao issues recebidos por um projeto que foi criado a partir de um fork de outro projeto. Os issues externos s˜ao issues do projeto original, que podem ter sido resolvidos no fork. Identi/f_icamos 14.951issues externos encerrados por commit em nossa amostra de projetos. Issues externos n˜ao tiveram seus dados coletados para an´alise deta- lhada. Somando os issues presentes nos projetos coletados (7.000) aos externos (14.951), temos um total de 21.951 issues encerrados por commit. Identi/f_icamos uma correlac ¸˜ao fraca (ρ=0,155) entre o n´umero de issues externos encerrados por commits e o n´umero de commits realizados nos projetos. Veri/f_icamos que a m´edia de commits realizados por colaboradores que encerraram issues exter- nos por commits ´e de 521,08 commits por projeto, com mediana de 19 commits. J ´a para aqueles que n ˜ao encerraram issues externos atrav´es de commits a m´edia ´e de 39,07 commits, com mediana de um commit. Os testes estat ´ısticos identi/f_icam diferenc ¸a signi/f_icativa entre os grupos, com tamanho de efeito grande (77%). 4.8.7 Sum ´ario. Observamos que issues encerrados por commits tendem a consumir mais tempo para a sua correc ¸˜ao. Tamb´em ve- mos uma pequena predominˆancia de encerramento por commits para issues que possuem r´otulos. Os r ´otulos mais comuns indicam que a funcionalidade de encerramento de issues por commits ´e mais utilizada para issues referentes `a correc ¸˜ao de defeitos ou melhorias de c ´odigo, enquanto que issues em maior n´ıvel de abstrac ¸˜ao s ˜ao encerrados manualmente. Por /f_im, veri/f_icamos que o n´umero de coment´arios recebidos por issues encerrados por commit ´e ligeira- mente menor do que para os demais issues. Uma poss ´ıvel raz˜ao para essa diferenc ¸a´e a necessidade de acessar o issue tracking sys- tem quando se fecha um issue manualmente, o que pode motivar', 'mente menor do que para os demais issues. Uma poss ´ıvel raz˜ao para essa diferenc ¸a´e a necessidade de acessar o issue tracking sys- tem quando se fecha um issue manualmente, o que pode motivar os colaboradores a escreverem coment´arios, pois essa opc ¸˜ao /f_ica muito pr´oxima do comando de encerramento manual de issue. J´a nos issues encerrados por commit, o colaborador n˜ao tem a opc ¸˜ao de escrever coment´arios para o issue no momento do commit. 5 DISCUSS ˜AO E SUM ´ARIO DOS RESULTADOS Para as quest˜oes de pesquisa que diziam respeito `a replicac ¸˜ao do estudo de BISSYANDE et al. [1] (RQ1 a RQ6) foram encontrados resultados que corroboram aqueles reportados no estudo original, ainda que alguns destes resultados tenham variado em sua inten- sidade. Conclu´ımos que (i) os projetosopen-source do GitHub n˜ao recebem um grande n ´umero de issues, visto que apenas 15% da amostra possui issues registrados; (ii) n˜ao existem grupos bem de/f_i- nidos de desenvolvedores e reporters, pois grande parte daqueles que atuam em um papel tamb´em executa atividades relacionadas ao outro papel; (iii) que existe uma correla c ¸˜ao moderada entre o n´umero de seguidores eforks de um projeto e a quantidade derepor- ters envolvidos e issues registrados, evidenciando que o ambiente distribu´ıdo e colaborativo tem uma in/f_luˆencia positiva na utilizac ¸˜ao da ferramenta de issue tracking; e (iv) o n ´umero de reporters e o tempo de correc ¸˜ao dos issues possuem correlac ¸˜ao quase nula. O per/f_il de projetos que mais utilizamissue tracking systemsse manteve o mesmo daquele observado por BISSYANDE et al. [ 1], sendo eles projetos mais antigos, com maior base de c ´odigo, que possuem uma grande equipe de desenvolvimento e cujos l ´ıderes s˜ao populares. Con/f_irmamos que existe uma correlac ¸˜ao fraca entre o n´umero de issues e o n´umero de linhas de c´odigo de um projeto. A m´edia de issues por linha de c´odigo varia pouco entre as linguagens de programac ¸˜ao que servem de base para o projeto e o registro de issues ´e mais comum em projetos voltados para a Web. Tamb´em elencamos os r´otulos mais utilizados para classi/f_icac ¸˜ao de issues e veri/f_icamos a grande utilizac ¸˜ao de r´otulos como defect, relacionado `a correc ¸˜ao de erros, e auto-migrated, relacionado `a migrac ¸˜ao de c´odigo-fonte e issues vindo de outros sistemas de controle de vers˜ao e outros issue tracking systems.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros Na s´etima e oitava quest˜oes de pesquisa, o estudo buscou analisar novos pontos de vista referentes a utilizac ¸˜ao da ferramenta de issue tracking. Com a an´alise dessas quest˜oes, chegamos a algumas con- clus˜oes, sendo uma delas a tendˆencia de issues que possuem r´otulos serem encerrados mais rapidamente do que aqueles que n˜ao pos- suem. Identi/f_icamos tamb´em que os colaboradores que registram issues tendem a realizar um menor n´umero de commits no c´odigo- fonte, ainda que previamente n˜ao tenhamos encontrado diferenc ¸a signi/f_icativa entre os pap´eis de desenvolvedor e reporter. Por outro lado, n˜ao foram identi/f_icados resultados relevantes em relac ¸˜ao `as caracter´ısticas de tempo de criac ¸˜ao da conta do colaborador, seu n´umero de seguidores e seu n´umero de projetos p´ublicos. Conclu´ımos tamb´em que o n´umero de commits realizados por um colaborador tem relac ¸˜ao com o n´umero de issues que encerrou por commits. Assim, veri/f_icamos que os colaboradores que mais registram commits tendem a utilizar a funcionalidade de encerra- mento de issues atrav´es de commits com maior frequˆencia. J´a em relac ¸˜ao `as caracter´ısticas dosissues, veri/f_icamos que aqueles encer- rados por commits tendem a possuir um tempo de vida maior do que os encerrados atrav ´es da interface com o usu ´ario e recebem um n ´umero menor de coment ´arios. Tamb ´em veri/f_icamos que a funcionalidade de encerramento por commit ´e bastante utilizada para fechar issues que possuem r´otulos e que os principais r´otulos de issues encerrados desta forma possuem rela c ¸˜ao direta com o c´odigo-fonte, como bug, feature, enhancement e user interface. Por /f_im, identi/f_icamos uma concentrac ¸˜ao da pr´atica de encerra- mento de issues por commits em poucos projetos, onde a pr´atica ´e estabelecida e utilizada por grande parte dos desenvolvedores. Pes- quisas que busquem relacionar issues com m´odulos de c´odigo-fonte usando informac ¸˜oes da plataforma GitHub devem se concentrar nestes projetos, ao inv´es de tomar por base uma grande quantidade de projetos onde este recurso n˜ao ´e utilizado com frequˆencia. 5.1 Ameac ¸as `a validade Durante o planejamento e execuc ¸˜ao deste estudo identi/f_icamos al- guns pontos que devem ser citados como ameac ¸as `a validade dos seus resultados. A primeira ameac ¸a se refere ao n´umero de commits existentes em cada projeto analisado. Ao selecionarmos os projetos, n˜ao /f_iltramos a base dispon´ıvel pelo n´umero de commits. Sendo assim, em nossa amostra existem projetos com apenas um ou ne- nhum commit, quando possivelmente seria melhor para a an´alise que todos possu´ıssem muitoscommits, demonstrando maior ativi- dade nos projetos e maior probabilidade desses projetos possu´ırem issues corrigidos atrav´es de commits. Outra ameac ¸a est´a no m´etodo escolhido para identi/f_icar os cola- boradores distintos. Todos os registros de colaborador que possu´ıssem o mesmo nome e e-mail foram considerados como apenas um cola- borador em cada projeto. Pessoas que possuem mais de uma conta, cada qual com um e-mail distinto, foram contabilizadas duas vezes e suas contribuic ¸˜oes foram divididas entre as contas. Finalmente, n˜ao foi poss´ıvel realizar a replicac ¸˜ao exata do m´etodo utilizado por BISSYANDE et al. [ 1], pois os detalhes t ´ecnicos do estudo original n˜ao foram disponibilizados. Por exemplo, n˜ao sa- bemos qual ´e a intersecc ¸˜ao entre os projetos selecionados para as nossas an´alises e os projetos utilizados no artigo original. Replica- mos os conceitos, curadoria e quest˜oes de pesquisa apresentados no estudo original, por´em utilizando m´etodos constru´ıdos para esse estudo. Por falta de maiores detalhes, tivemos que aplicar algumas interpretac ¸˜oes do que foi feito no estudo original e n˜ao podemos garantir que essas interpretac ¸˜oes n˜ao in/f_luenciaram nos resultados', 'interpretac ¸˜oes do que foi feito no estudo original e n˜ao podemos garantir que essas interpretac ¸˜oes n˜ao in/f_luenciaram nos resultados encontrados na replicac ¸˜ao. 6 CONCLUS ˜AO Este artigo apresentou uma replicac ¸˜ao da pesquisa realizada por BISSYANDE et al. [1] para veri/f_icar se as conclus˜oes apresentadas pela pesquisa continuam v´alidas ap´os quatro anos da sua publicac ¸˜ao e uma an´alise das caracter´ısticas presentes emissues encerrados por commits. Como principais contribuic ¸˜oes do estudo aqui reportado, podemos citar: (i) a con/f_irmac ¸˜ao dos resultados do estudo replicado, adicionando evidˆencias que con/f_irmam as suas conclus˜oes; (ii) a realizac ¸˜ao de um estudo focado na utilizac ¸˜ao da funcionalidade de encerramento de issues por commit, que concluiu que este recurso ´e utilizado em projetos com muitos commits, que os principais issues encerrados desta forma s˜ao reportados por desenvolvedores, envolvem quest˜oes relacionadas ao c´odigo-fonte e seu tempo m´edio de correc ¸˜ao n˜ao ´e muito diferente dos demais issues. Identi/f_icamos como limitac ¸˜ao do presente trabalho o fato de n˜ao termos avaliado a relac ¸˜ao entre os m´odulos de c´odigo-fonte afetados por commits e outras caracter´ısticas dosissues, como por exemplo se determinados m´odulos est˜ao ligados ao mesmo desenvolvedor, aos mesmos r´otulos, entre outros. Assim, propomos como trabalho futuro estudos que investiguem a relac ¸˜ao dos issues, r´otulos e cola- boradores com os m´odulos de c´odigo-fonte afetados por commits que encerrem issues. Assim, poderemos estudar meios de identi/f_icar os colaboradores mais aptos a solucionar umissue relacionado a um m´odulo do so/f_tware ou encontrar os m´odulos que provavelmente precisar˜ao ser alterados para resolver umissue. Outras propostas de trabalho futuro incluem a realizac ¸˜ao de um estudo qualitativo sobre o uso deissue tracking systeme a replicac ¸˜ao desse estudo levando em considerac ¸˜ao projetos privativos, avaliando se as conclus˜oes apre- sentadas para projetos de c´odigo aberto s˜ao semelhantes quando analisadas para projetos que n˜ao sejam p´ublicos. REFERˆENCIAS [1] T.F. Bissyande, D. Lo, J. Lingxiao, L. Reveillere, and Y. Le Traon. 2013. Got Issues? Who Cares About It? A Large Scale Investigation of Issue Trackers from GitHub. In IEEE 24th Intl Symposium on So/f_tware Reliability Engineering. 188–197. [2] J. Cabot, J.L.C. Izquierdo, and V. Cosentino. 2015. Exploring the Use of Labels to Categorize Issues in Open-Source So/f_tware Projects. InSo/f_tware Analysis, Evolution and Reengineering (SANER’15). 550–554. [3] J. Cohen. 1992. A Power Primer. Psychological Bulletin(1992), 155–159. [4] L. Dabbish, C. Stuart, J. Tasy, and J. Herbsleb. 2012. Social Coding in GitHub: Transparency and Collaboration in an Open So/f_tware Repository. InACM 2012 Conference on Computer Supported Cooperative Work. 1277–1286. [5] E. Kalliamvakou, K. Blincoe, L. Singer, and D. Damian. 2014. /T_he Promises and Perils of Mining GitHub. In11th Working Conference on Minig So/f_tware Repositories (MSR’14). 92–101. [6] M.J. Lee, B. Ferwerda, J. Choi, J. Hahn, J. Moon, and J. Kim. 2013. GitHub developers use rockstars to overcome over/f_low of News. InHuman Factors in Computing System. 233. [7] Y. Weicheng, S. Beijun, and X. Ben. 2013. Mining GitHub: Why commit stops – Exploring the relationship between developer’s commit pa/t_tern and /f_ile version evolution. In Proc. of the Asia-Paci/f_ic So/f_tware Engineering Conference. 165–169. [8] J. Xavier, A. Macedo, and M. A. Maia. 2014. Understanding the popularity of reporters and assignees in the Github. In26th International Conference on So/f_tware Engineering and Knowledge Engineering. 484–489. [9] E. Yong. 2012. Bad Copy. Nature 495 (2012), 298–300.']","['ORIGINAL\tREFERENCE\tFOR\tTHE\tRESEARCH:\t Neto,\tC.,\tBarros,\tM.\t“A\tStructured\tSurvey\ton\tthe\tUsage\tof\tthe\tIssue\tTracking\tSystem\tprovided\tby\tthe\tGITHUB\tPlaLorm”,\tXI\t Simpósio\tBrasileiro\tde\tComponentes,\tArquiteturas\te\tReúso\tde\tSoRware,\tSBCARS\t2017,\tFortaleza,\tBrasil\t(in\tPortuguese)\t A\tStructured\tSurvey\ton\tthe\tUsage\tof\tthe\tIssue\t Tracking\tSystem\tprovided\tby\tthe\tGITHUB\tPla@orm\t \t \t \t \t ≈\t38.000.000\tPROJECTS\tON\tTHE\tPLATFORM\t \t 220.000\t SELECTED\tPROJECTS\t \t \t76.909\tANALYZED\tPROJECTS\t \t 11.662\tPROJECTS\tWITH\tISSUES\t \t 1,54\t ISSUES/KLOC\t 2\tor\t3\t LABELS/ISSUE\t 52%\t ISSUES\tW/\tLABELS\t 226.340\t ISSUES\t 304.033\t LABELS\t 2.596\t UNIQUE\tLABELS\t 7.000\t ISSUES\tCLOSED\t BY\tCOMMIT\t 159.118\t RESOLVED\tISSUES\t ENOUGH\tTALK\t…\t SHOW\tME\tTHE\tNUMBERS!\tStudies\taddressing\tissue\ttracking\tsystems\t(ITS)\tanalyze\thow\tthis\t tool\tcan\tsupport\tthe\tteam\tduring\tthe\tdevelopment\tof\ta\tsoRware\t project.\t We\t have\t replicated\t the\t analyses\t reported\t in\t a\t former\t study\taddressing\tthe\tITS\toﬀered\tby\tthe\tGitHub\tplaLorm.\t CONTEXT\t Researchers\t interested\t in\t replicaong\t experiments\t in\t SoRware\t Engineering\tand\tpracoooners\tinterested\ton\tthe\tGitHub\tplaLorm.\t PUBLIC\tOF\tINTEREST\t •\u202f Few\topen-source\tprojects\tuse\tITS\t(15%\tof\tour\tsample).\t •\u202f Even\ttaking\tonly\tthose\tprojects\twith\tat\tleast\tone\tissue,\t86%\tof\t our\tsample\thave\trecorded\tless\tthan\t50\tissues.\t •\u202f Larger\tprojects\t(both\tin\tLOC\tand\tnumber\tof\tdevelopers)\ttend\t to\t use\t ITS\t more\t extensively.\t The\t correlaoons\t in\t the\t original\t paper\twere\tstronger\tthan\twhat\twe\thave\tfound\tnow.\t •\u202f Projects\t led\t by\t popular\t developers\t tend\t to\t use\t ITS\t more\t extensively.\tCorrelaoons\twere\tstronger\tin\tthe\tformer\tpaper.\t •\u202f Labels\tare\tnot\tfrequently\tused\tand\tthe\tmost\tcommon\tlabels\t refer\tto\tautomated\tacoons,\tdefects,\tand\ttask\tprioriozaoon.\t •\u202f Issue\treporters\tare\ttypically\tdevelopers.\t •\u202f There\tis\ta\tmoderate\tcorrelaoon\tbetween\tthe\tnumber\tof\tforks\t created\tfrom\ta\tproject\tor\tthe\tnumber\tof\tdevelopers\tfollowing\t its\tleader\tand\tthe\tnumber\tof\tissues\treported\tfor\tthe\tproject.\t This\tcorrelaoon\twas\talso\tstronger\tin\tthe\tformer\tpaper.\t •\u202f There\t are\t only\t weak\t or\t moderate\t correlaoons\t between\t the\t number\tof\treporters\tin\ta\tproject,\tthe\tuse\tof\tlabels,\tthe\tuse\tof\t frequently-used\t labels\t or\t the\t number\t of\t comments\t and\t the\t average\tome\trequired\tto\tresolve\tan\tissue.\tThese\tcorrelaoons\t were\talso\tstronger\tin\tthe\tformer\tpaper.\t •\u202f Developers\t tend\t to\t mark\t issues\t as\t resolved\t by\t menooning\t them\ton\tcommits\tto\tthe\tversion\tcontrol\tsystem.\tThis\tallows\t some\ttraceability\tbetween\tthe\tchanges\tmade\tto\tthe\tcode\tand\t the\t issue\t they\t resolve.\t However,\t few\t issues\t are\t resolved\t through\tthese\tmeans\tand\tmost\tof\tthem\tare\trelated\tto\tbugs,\t new\t features,\t and\t feature\t enhancements\t –\t that\t is,\t they\t are\t closely\trelated\tto\tthe\tsource-code!\t FINDINGS\t Casimiro\tConde\tNeto\te\tMárcio\tBarros']","**Title: Understanding the Use of GitHub's Issue Tracking System**

**Introduction:**
This Evidence Briefing summarizes key findings from a study on the usage of the issue tracking system provided by the GitHub platform. The goal is to inform software engineering practitioners about the current state of issue tracking in open-source projects, how it has evolved since previous research, and its implications for software development practices.

**Main Findings:**
1. **Continued Validity of Previous Results:** The study replicates and extends earlier research conducted in 2013, confirming that many original findings remain valid. Specifically, only a small percentage of open-source projects (approximately 15%) actively utilize issue tracking systems.

2. **Project Characteristics:** Projects that implement issue tracking systems tend to be older, larger in codebase, and have more developers compared to those that do not. This suggests that established projects are more likely to adopt structured issue management practices.

3. **Issue Closure by Commits:** The analysis reveals that only about 4% of issues are resolved through commits, indicating that this feature is not widely used. Issues closed through commits are often linked to specific technical aspects of the project, suggesting a potential area for improvement in how developers relate issues to code changes.

4. **Label Usage:** The study found an increase in the use of labels for categorizing issues, with 52% of issues having labels compared to 30% in the original study. This indicates a growing recognition of the importance of categorization in managing issues effectively.

5. **Role of Contributors:** The research highlights that many contributors perform dual roles as both reporters and developers. Approximately 38% of developers also register issues, while 53% of reporters contribute to the codebase, indicating a collaborative environment where roles are not strictly defined.

6. **Impact of Popularity:** There is a moderate correlation between project popularity (measured by followers and forks) and the number of issues registered. Projects with higher community engagement tend to have more issues reported, reinforcing the idea that a collaborative atmosphere fosters better issue management.

7. **Speed of Issue Resolution:** Issues with labels are resolved more quickly than those without. The average resolution time for issues with labels is around 22.5 days, compared to 25.2 days for those without. This suggests that effective categorization may enhance responsiveness in addressing issues.

8. **Challenges with Commit Closure:** Issues closed via commits tend to take longer to resolve than those closed manually, indicating that while the commit closure feature exists, it may not be the most efficient method for issue resolution.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and researchers interested in understanding the dynamics of issue tracking systems within open-source software development.

**Where the findings come from?**
All findings in this briefing are drawn from a structured survey study conducted by Casimiro Conde Marco Neto and Márcio de O. Barros, presented at SBCARS 2017.

**What is included in this briefing?**
The briefing includes insights into the current state of issue tracking on GitHub, comparisons with previous research, and practical implications for improving issue management in software development.

**For additional information:**
- To access other evidence briefings on software engineering: [http://ease2017.bth.se/](http://ease2017.bth.se/)
- For additional information about the authors and their research: [Contact Casimiro Neto](mailto:casimiro.neto@uniriotec.br) or [Contact Márcio Barros](mailto:marcio.barros@uniriotec.br)

**Original Research Reference:**
Conde Marco Neto, C., & Barros, M. O. (2017). A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017. DOI: 10.1145/3132498.3134110"
"['On the Evaluation of Eﬀort Estimation Models Luigi Lavazza Universit`a degli Studi dell’Insubria Varese, Italy luigi.lavazza@uninsubria.it Sandro Morasca Universit`a degli Studi dell’Insubria Varese, Italy sandro.morasca@uninsubria.it ABSTRACT Background. Using accurate eﬀort estimation models can help so/f_t- ware companies plan, monitor, and control their development pro- cess and development costs. It is therefore important to de/f_ine sound accuracy indicators that allow practitioners and researchers to assess and rank diﬀerent eﬀort estimation models so that prac- titioners can select the most accurate, and therefore useful one. Several accuracy indicators exist, with diﬀerent advantages and disadvantages. Objective. We propose a general framework for building sound accuracy indicators for eﬀort estimation models. Method. /T_he accuracy indicators that comply with our proposal are built by means of a comparison between a reference eﬀort estimation model and the speci/f_ic model whose accuracy we would like to assess. Several existing indicators are built this way: we develop a framework so new indicators can be de/f_ined in a sound way. Results. From a theoretical point of view, we applied our ap- proach to accuracy indicators based on the square of the residuals and the absolute value of the residuals. We show that using a ran- dom model as a reference model, as done in some recent literature, sets too low a bar in terms of what may be acceptable. Instead, we use reference models that are built based on constant functions. From a practical point of view, we applied our approach to datasets containing measures of industrial so/f_tware development projects. With the proposed method we were able to derive indications both according to criteria already proposed in the literature and accord- ing to new criteria. Conclusions. Our method can be used to de/f_ine sound accuracy indicators for eﬀort estimation models. CCS CONCEPTS •So/f_tware and its engineering→ Risk management; KEYWORDS Eﬀort estimation models, Estimation accuracy, Model evaluation ACM Reference format: Luigi Lavazza and Sandro Morasca. 2017. On the Evaluation of Eﬀort Estimation Models. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017,10 pages. DOI: h/t_tp://dx.doi.org/10.1145/3084226.3084260 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. EASE’17, Karlskrona, Sweden © 2017 ACM. 978-1-4503-4804-1/17/06. . . $15.00 DOI: h/t_tp://dx.doi.org/10.1145/3084226.3084260 1 INTRODUCTION /T_he availability of accurate eﬀort estimation models can help so/f_t- ware companies plan, monitor, and control their processes, so accu- rate eﬀort estimation models may provide so/f_tware companies with competitive advantages over other companies. /T_hus, the accuracy of eﬀort estimation models needs to be carefully assessed before they can be used in practice. As a consequence, sound accuracy indicators need to be used when assessing the accuracy of a model and when comparing and ranking the accuracy of two or more competing models. A number of accuracy indicators have been proposed, including MMRE [2], the Mean Magnitude of Relative Errors, which for a while gained the status of almost being the de factopreferred accuracy indicator. Most of the accuracy indicators are based on the set of residuals (or estimation errors) given by the diﬀerences between the actual eﬀort values and the corresponding estimated values.', 'indicator. Most of the accuracy indicators are based on the set of residuals (or estimation errors) given by the diﬀerences between the actual eﬀort values and the corresponding estimated values. /T_hese indicators have advantages and disadvantages, which we discuss in Section 10. Shepperd and MacDonell [ 15] have recently proposed a new accuracy indicator, called “standardised accuracy measure” (SA) as a model accuracy indicator, along with a method to use it (see Section 3). Given a model mod whose accuracy we would like to assess, SA is based on the ratio of the Mean of Absolute Residuals MARmod of mod to the average MAR obtained by randomly generating a large number of estimation models (according to a random policy that we review in Section 3). /T_he idea is that the random policy should be used as a reference and mod, to be taken into account as a potentially useful model, should at least be more accurate than the random policy. Langdon et al. [7] basically re/f_ine the method proposal by Shep- perd and MacDonell in the computation of the accuracy of the random policy. Instead of generating a large number of random models, they propose a formula to compute what they call the “ex- act” value for the averageMAR obtained by randomly generating estimation models (see Section 3). Our paper builds on the proposals by Shepperd and MacDonell and by Langdon et al. by providing the following main research contributions. • We provide a generalized theoretical basis to de/f_ine accu- racy indicators, of which SA by Shepperd and MacDonell or the coeﬃcient of determination R2 used in ordinary least squares (OLS) linear regression, are special cases. /T_his also allows us to investigate the nature of the SA indicator and uncover the diﬀerences between the proposal by Shepperd and MacDonell and the proposal by Langdon et al. • We evaluate the adequacy of the random estimation policy to be taken as the reference, baseline model. We argue that the random estimation model may set too low a bar in terms of what may be an acceptable baseline, and that constant', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden L.Lavazza and S. Morasca estimation models may be used as reference ones. As a ma/t_ter of fact, constant estimation models use exactly the same information as random ones and they are computed in a very simple way. /T_hus, estimation models that use more information than random and constant models should behave be/t_ter than constant models and not only than random models. • We propose an alternative procedure to compare estima- tion models against the random model. /T_he proposed in- dicator is stricter than the one proposed by Shepperd and MacDonell, thus making the comparison with the random model more practically signi/f_icant. • We carry out an empirical investigation to evaluate the diﬀerent approaches in practice. /T_he remainder of this paper is organized as follows. Section 2 introduces the basic notation and terminology used throughout the paper. A thorough review of the proposals by Shepperd and Mac- Donell and by Langdon et al. is in Section 3. /T_he theoretical bases of these approaches along with a generalization are described in Section 4. /T_he approximation of a distribution used in the proposals by Shepperd and MacDonell and by Langdon et al. is in Section 5. We introduce in Section 6 a new approach and indicator for the assessment of eﬀort estimation models. We carried out an empirical study, described in Section 7, and whose results are summarized in Section 8. /T_he threats to validity are discussed in Section 9, and the analysis of the related work is in Section 10. /T_he conclusions and an outline for future work are in Section 11. 2 BASIC NOTATION AND TERMINOLOGY For easy reference, we here simply introduce and summarize the basic notation and terminology we use in the paper. By Y = {/y.alt1, . . . ,/y.altn }, we denote the multiset of eﬀort values observed in n projects. Y is technically a multiset and not a set because the same value may appear more than once. While appre- ciating this distinction, we call Y a dataset (and not a “datamulti- set”) because “dataset” is the term used in the literature. Likewise, ˆY = {ˆ/y.alt1,mod , . . . ,ˆ/y.altn,mod }denotes the set of corresponding eﬀort values estimated based on a model denoted as mod. Without any loss of generality, we suppose that Y is sorted in a nonmonotonically increasing way, so i < j ⇒/y.alti ≤/y.altj . However, ˆY may not necessarily be sorted in any way. In what follows, indices i, j, and k are used for projects, so they range between 1 and n. For short, we denote the integer range 1..n as I, and the integer range 1..n without a speci/f_ic value i as I(i)= I −{i}. Index t is used for iterations needed by some techniques to build a sampling distribution. T = 1..maxt denotes the integer range of values of t, for some given value of maxt . In the paper, we compare the accuracy of diﬀerent models, some of which are taken as “reference” ones. We denote a reference model as ref. Some reference models are “random” ones (see Section 3). We denote a random model as rnd. We use a few diﬀerent ways of assessing how accurate each esti- mate ˆ/y.alti,mod is to its corresponding actual value /y.alti . For generality, we denote by function f (/y.alti , ˆ/y.alti,mod )any such way of assessing the accuracy of the estimate of ˆ/y.alti,mod with respect to /y.alti . By avg[], we denote the averaging operation over a sample of data. We explicitly show the set of indices over which the aver- aging operation is taken. So, we write a/v.alt/afii10069.ital i ∈I [Y ]= ) i∈1. .n /y.alti n , for instance. By E[], we denote the expected value based on a prob- ability distribution. We need to stress that these two operations, which may appear closely related, are actually two diﬀerent ones. Operation avg[] is a sampling operation, so its result will in general be a random variable, i.e., a variable that takes a diﬀerent value depending on the sample on which avg[] is taken. Operation E[],', 'be a random variable, i.e., a variable that takes a diﬀerent value depending on the sample on which avg[] is taken. Operation E[], instead returns a value, which represents the expected value of the distribution to which E[] is applied. As such, it is not a random variable. In the paper, we investigate and elaborate on the approaches by Shepperd and MacDonell [15] and by Langdon, Dolado, Sarro, and Harman [7]. We abbreviate the former proposal as “SM” and the la/t_ter as “LDSH. ” 3 THE METHODS PROPOSED BY SHEPPERD & MACDONELL AND BY LANGDON ET AL. SM uses uniform random estimation, i.e., rnd, as the reference against which a given estimation model mod should be compared. To this end, SM introduces the “standardised accuracy measure” (which we denote by SASM ,mod ) as a model accuracy indicator SASM ,mod = 1 − MARmod a/v.alt/afii10069.ital t ∈T [MARrnd ] (1) where MARmod is the Mean Absolute Residual obtained on the model mod we would like to evaluate, i.e., MARmod = ) i ∈I |/y.alti −ˆ/y.alti,mod | n = a/v.alt/afii10069.ital i ∈I [|/y.alti −ˆ/y.alti,mod |] (2) and a/v.alt/afii10069.ital t ∈T [MARrnd ]is obtained by averaging the Mean Absolute Residuals obtained via the random models a speci/f_ied number of times (e.g., maxt = 1000). Speci/f_ically, according to SM, the ran- dom estimate ˆ/y.alti,rnd of a value /y.alti is any other value in Y , i.e., it is a value /y.altj ∈Y −{/y.alti }, selected according to a uniform random distribution, i.e., each value /y.altj (for any j ∈I(i)) has probability p(/y.altj )= 1 n−1 of being selected as the estimate for /y.alti . Clearly, there are several diﬀerent random models, corresponding to diﬀerent random selections for the estimates {ˆ/y.alt1,rnd , . . . ,ˆ/y.altn,rnd }, which result in diﬀerent values for MARrnd . /T_hus,MARrnd is not a /f_ixed value, but it is a random variable, and that is the reason why SM proposes to use a/v.alt/afii10069.ital t ∈T [MARrnd ]where the average is taken over a large number (i.e., maxt ) of diﬀerent random models, so as to average out the in/f_luence of the random models that can be used. Note that the upper limit to SASM ,mod is 1, which is obtained when mod is the “perfect” estimation model prf that perfectly estimates all /y.alti ’s, i.e., ∀i ∈I,/y.alti = ˆ/y.alti,pr f. However, there is no lower limit toSASM ,mod , which may very well take negative values. At any rate, when SASM ,mod > 0, then mod is more accurate than rnd and when SASM ,mod < 0, then mod is less accurate than rnd . LDSH is based on the same ideas as SM, but with two important diﬀerences. • First, as the random estimate of a value /y.alti , LDSH uses any value inY , including/y.alti itself, i.e., ˆ/y.alti,rnd ∈Y . Again, ˆ/y.alti,rnd is selected according to a uniform random distribution, so,', 'On the Evaluation of Eﬀort Estimation Models EASE’17, June 15-16, 2017, Karlskrona, Sweden each value /y.altj has probability p(/y.altj )= 1 n of being selected. Instead, when estimating/y.alti , SM does not take into account the i-th value itself as a possible estimate in the random model. /T_hus, SM and LDSH may actually serve two diﬀer- ent purposes. SM should be used to evaluate the average accuracy of an existing eﬀort estimation model, built based on a training set composed of n −1 datapoints, on a new project, which therefore plays the role of the test set. /T_his is what typically happens in real life when estimating the eﬀort for a new project. LDSH should be used to evaluate the accuracy of a model built with data from a training set on the training set itself. In a way, we evaluate the intrinsic accuracy of the model. • Second, instead of using a sample mean like a/v.alt/afii10069.ital t ∈T [MARrnd ], LDSH introduces what it calls the “ex- act” value ofa/v.alt/afii10069.ital t ∈T [MARrnd ], which is shown to equal 2 n2 n) i=1 j<i) j=1 |/y.alti −/y.altj | (3) /T_here is a subtle, but important diﬀerence between the very meaning of a/v.alt/afii10069.ital t ∈T [MARrnd ]in SM and the “exact” value for it computed in Formula (3) by LDSH. In SM, a/v.alt/afii10069.ital [MARrnd ]is the sample mean of a large number of actual values for MARrnd . In LDSH, what is presented as the “exact” value ofa/v.alt/afii10069.ital t ∈T [MARrnd ]is actually the ex- pected value E[MARrnd ]of MARrnd in the distribution of MARrnd . /T_hus, LDSH’s accuracy indicator is SALDSH ,mod = 1 − MARmod E[MARrnd ] (4) /T_he two proposals have a few things in common, though. • Both proposals use the random policy as the reference one. However, practical reference models should be sensible ones, i.e., models that practitioners may be willing to adopt. We believe that it is unrealistic that practitioners take any random value in Y −{/y.alti }or even in Y to estimate the eﬀort related to a project. For instance, suppose that Y contains projects whose eﬀort spans over a large interval, so it contains “small” and “large” projects. It is unlikely to think that a so/f_tware manager may use a “small” value as an estimate for what he or she may believe to be a “large” project or vice versa. So, rnd can only be considered a very theoretical reference model. In Section 4.2, we discuss other reference models that may be more likely to be adopted in practice. • Either version of the “standardised accuracy measure, ” i.e., SASM ,mod or SALDSH ,mod is used as the basic indicator of accuracy. • SM introduces an additional way for assessing the accu- racy of mod vs. rnd. (LDSH does not speci/f_ically address this further element of SM, therefore apparently accept- ing SM on this point.) Speci/f_ically, SM uses the sampling distribution of MARrnd (Figure 1 shows one such sam- pling distribution) to establish a threshold on the value of MARmod . A model mod is considered acceptable if MARmod < MARrnd ,5%, where MARrnd ,5% is the 5% quan- tile from the sampling distribution of MARrnd . For in- stance, in Figure 1, MARrnd ,5% is represented by the black dashed vertical line and MARmod by the red solid ver- tical line, so mod can be considered acceptable. /T_hus, MARrnd ,5% plays a similar role to the threshold corre- sponding to a statistical signi/f_icance thresholdα = 0.05 typically used in Empirical So/f_tware Engineering. 4 THEORETICAL BASES 4.1 A Common View of Average-based Accuracy Indicators SASM ,mod is a special case of a more general indicator of the form SASM ,f ,mod = 1 − a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,mod )] a/v.alt/afii10069.ital t ∈T [a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,ref )]] (5) with SM selection policy, i.e., ∀i ∈I, ˆ/y.alti,ref ∈Y −{/y.alti }, selected with uniform probability 1 n−1 . SALDSH ,mod is a special case of a more general indicator of the form SALDSH ,f ,mod = 1 − a/v.alt/afii10069.ital i ∈I', 'with uniform probability 1 n−1 . SALDSH ,mod is a special case of a more general indicator of the form SALDSH ,f ,mod = 1 − a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,mod )] E[a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,ref )]] (6) with LDSH selection policy, i.e., ∀i ∈I, ˆ/y.alti,ref ∈Y , selected with uniform probability 1 n . Note that the fractions of both Formulas (5) and (6) contain a single operator (avg) in the denominator and two operators (avg twice in Formula (5) and avg and E in Formula (6)). /T_he reason of this asymmetry between the numerator and the denominator is that the denominator is based on random reference models, while the numerator is not. In our study, we use Formula (6) with the SM selection policy because we are more interested in the assessment of the predictive accuracy of mod (so, we use the SM selection policy) in a way that is independent of the samples of values selected (so, we use Formula (6)). At any rate, results according to the LDSH selection policy could be obtained in much the same way as the results we show in this paper for the SM policy. In our analysis, we focus on the numerator and the denomi- nator of the fraction in Formula (6), which we call, respectively Numf ,mod and Denf ,ref . 4.2 Assessing the Accuracy of Constant Models Formulas (5) and (6) can be simpli/f_ied ifref is nonrandom. If this is the case, |T |= 1, so the denominator in the fraction of Formula (5) is a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,ref )]. /T_he denominator in the fraction of Formula (6) is the expected value of a distribution with only one value, so Denf ,ref = a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,ref )]. /T_hus, we have the same formula for both SM and LDSH, i.e., SAf ,mod = 1 − a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,mod )] a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,ref )] (7)', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden L.Lavazza and S. Morasca but with the two diﬀerent selection policies for ˆ/y.alti,ref , respectively. /T_his is the typical form of several other model accuracy indica- tors, like, for instance, the standard accuracy indicator R2 OLS used in Ordinary Least Squares (OLS ) linear regression: R2 OLS = 1 − )n i=1(/y.alti −ˆ/y.alti,mod )2 )n i=1(/y.alti −/y.alt)2 = 1 − )n i=1(/y.alti −ˆ/y.alti, mod )2 n)n i=1(/y.alti −/y.alt)2 n = 1 − a/v.alt/afii10069.ital i ∈I [(/y.alti −ˆ/y.alti,mod )2] a/v.alt/afii10069.ital i ∈I [(/y.alti −/y.alt)2] (8) We can note that the de/f_inition ofR2 OLS conforms to Formula (7), where f (x, z)= (x −z)2 and ˆ/y.alti,ref = /y.alt (the average value of y)1. In Formula (8), the denominator contains terms in whichˆ/y.alti,ref = /y.alt is not simply a given constant value, but it is the constant value that minimizes the denominator itself. In other words, once a speci/f_ic functionf (/y.alti , ˆ/y.alti,ref )is chosen (e.g., f (/y.alti , ˆ/y.alti,ref )= (/y.alti − ˆ/y.alti,ref )2), then we need to use ˆ/y.alti,ref that minimizes the denomina- tor (e.g., ˆ/y.alti,ref = /y.alt) because it is the best constant value that can be obtained based on the values in Y only. Whenever we build a model mod based on the values of one or more independent variables, so that ˆ/y.alti,mod is actually a function of these independent variables, we would like for mod to have be/t_ter average accuracy than the best average accuracy of any constant model. /T_he reason is thatmod uses the values of Y and the values of the independent variables, while the constant model /y.alt = ˆ/y.altonly uses the values in Y (which is the same information also used by the random model). /T_hus, we need to investigate which reference model should be used and whether the constant model makes be/t_ter use of the same information than the random model. Using a constant model is not necessarily more complex than using the random model. /T_hus, we now assess the accuracy of a constant model cst by using our approach ,i.e., we compute SALDSH ,f ,cst = 1 − a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,cst )] E[a/v.alt/afii10069.ital i ∈I [f (/y.alti , ˆ/y.alti,rnd )]] (9) with SM selection policy for rnd and for cst. In Formula (9), ˆ/y.alti,cst is the value that minimizes a/v.alt/afii10069.ital i ∈I(i) [f (/y.alti , ˆ/y.alti,cst )] (10) i.e., /y.alt = ˆ/y.alti,cst is the best constant model that can be de/f_ined based on the datapoints in Y −{/y.alti }. Numf ,cst can then be trivially computed. Its value will obviously depend on function f chosen. As for Denf ,rnd , we now show an additional property that holds for all possible functions f chosen. We /f_irst have Denf ,rnd = a/v.alt/afii10069.ital i ∈I [E[f (/y.alti , ˆ/y.alti,rnd )]] (11) thanks to a basic property of expected values, i.e., the expected value of a sum of random variables equals the sum of the expected 1Note that other ways of introducing model accuracy indicators are possible. For instance, the model accuracy indicator de/f_ined in [11] and generalized in [ 8] uses ratios of medians, and not of average values. values of the random variables (even when the random variables are not statistically independent). Given a speci/f_ici, we have Denf ,rnd = a/v.alt/afii10069.ital i ∈I \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 ) j ∈I(i) p(/y.altj )f (/y.alti ,/y.altj ) \uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb = a/v.alt/afii10069.ital i ∈I \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 ) j ∈I(i) f (/y.alti ,/y.altj ) n −1 \uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (12) Since the /y.altj ’s are selected with uniform random probability, we have p(/y.altj ) = 1 n−1 , as shown in the right-hand side of the last equality. /T_he average of thesen expected values is Denf ,rnd = 1 n(n −1) ) i ∈I ) j ∈I(i) f (/y.alti ,/y.altj ) (13) We now provide the values for the numerator and the denomina- tor for two speci/f_ic functionsf , namely, the square (Section 4.2.1) and the absolute value (Section 4.2.2). In these two sections, for lack of space, we provide the /f_inal results of our analysis without', 'and the absolute value (Section 4.2.2). In these two sections, for lack of space, we provide the /f_inal results of our analysis without showing all of the Mathematics that lead to them. 4.2.1 Squares of Residuals. We have Densq,rnd = 1 n(n −1) ) i ∈I ) j ∈I(i) (/y.alti −/y.altj )2 = 2 n −1 () i ∈I /y.alt2 i −n/y.alt2 ) (14) As for Numsq,cst , we /f_irst need to /f_ind the right value forˆ/y.alti,cst , i.e., the one that minimizes a/v.alt/afii10069.ital i ∈I(i) [(/y.alti , ˆ/y.alti,cst )2] (15) and it is well-known that the mean is the value that minimizes an average of squares like the one in Formula (15). /T_herefore, ˆ/y.alti,cst = /y.alti , where /y.alti is the average value of Y −{/y.alti }. Via mathematical manipulations, it can be shown that the nu- merator of Formula (9) is Numsq,cst = n (n −1)2 () i ∈I /y.alt2 i −n/y.alt2 ) (16) so, we have SALDSH ,sq,cst = 1 − n (n−1)2 () i ∈I /y.alt2 i −n/y.alt2 ) 2 n−1 () i ∈I /y.alt2 i −n/y.alt2 ) = n −2 2n −2 < 1 2 (17) It can be easily shown that n−2 2n−2 = 1 2 − 1 2(n−1)is a monotonically increasing function of n, for n ≥2 and it tends to value 1 2 from below as n tends to in/f_inity. /T_hus, thecst model has always be/t_ter accuracy than the rnd model. From a practical perspective, as a /f_irst approximation, we can take n−2 2n−2 ≈1 2 . For instance, when n = 20, we have n−2 2n−2 = 0.474. However, just because SALDSH ,sq,cst > 0 does not mean that we have solid evidence on the fact that cst is substantially more accurate than rnd. As recommended by SM, we also need to check whether Numsq,cst < MARrnd ,5%. To further appreciate how poorly rnd performs, let us now com- pute SALDSH ,sq,pr f by using the “perfect” estimation modelprf introduced in Section 3. We obviously have SALDSH ,sq,pr f = 1. Since SALDSH ,sq,cst ≈ 1 2 , the diﬀerence SALDSH ,sq,pr f − SALDSH ,sq,cst ≈ 1 2 . Since we would have SALDSH ,sq,rnd = 0,', 'On the Evaluation of Eﬀort Estimation Models EASE’17, June 15-16, 2017, Karlskrona, Sweden if instead of an average value in the numerator we had the same expression as in the denominator, we would obtain a diﬀerence SALDSH ,sq,cst −SALDSH ,sq,rnd ≈ 1 2 . In other words, using a random model instead of a constant model basically produces the same reduction in accuracy as using a quite coarse model like the constant model instead of the perfect model. /T_he above results show that the random estimation technique performs much worse than the constant estimation technique, so it is worth wondering whether the random estimation technique is a sensible reference comparison technique that can be used to assess the accuracy of other techniques, or if, instead, it sets too low a bar for comparison. 4.2.2 Absolute Residuals. We have Denab,rnd = 1 n(n −1) ) i ∈I ) j ∈I(i) |/y.alti −/y.altj |= 2 n(n −1) ) i ∈I i(/y.alti −¯/y.alti ) (18) In this case, we do not obtain a closed-formula result. We evalu- ate its value empirically, based on a sample of random estimates. As for Numab,cst , ˆ/y.alti,cst is equal to the median ˜/y.alti of the values in Y −{/y.alti }, since it is well-known that this median minimizes a/v.alt/afii10069.ital i ∈I(i) [|/y.alti , ˆ/y.alti,cst |] (19) /T_hus, we have Numab,cst = 1 n ) i ∈I |/y.alti −˜/y.alti |= \uf8f1\uf8f4\uf8f4 \uf8f2 \uf8f4\uf8f4\uf8f3 /y.alt(n 2 +1)−/y.alt(n 2 ) 2 + ¯/y.alt−¯/y.alt(n 2 ), if n is even /y.alt(n+1 2 +1)−/y.alt(n+1 2 ) 2 + ¯/y.alt−¯/y.alt(n+1 2 )+ 1 n ( ˜/y.alt−¯/y.alt(n+1 2 ) ) , if n is odd (20) where ¯/y.alt(n 2 ) = a/v.alt/afii10069.ital i ∈1.. n 2 [/y.alti ]for n even and ¯/y.alt(n+1 2 ) = a/v.alt/afii10069.ital i ∈1.. n+1 2 [/y.alti ]for n odd are the average values of the smaller values of /y.alti up to the median. (Recall that Y is nonmonotonically increasing.) For n odd, note that the last term 1 n ( ˜/y.alt−¯/y.alt(n+1 2 ) ) tends to 0 as n tends to in/f_inity, so it becomes irrelevant for large datasets and the values obtained for n odd tend to the values obtained for n even. For n even, the value of the numerator is given by the average /y.alt over the entire Y , minus the average /y.alt over the lower half of Y , plus half the diﬀerence between the two consecutive values /y.alt(n 2 +1) (the “high” median) and/y.alt(n 2 )(the “low” median). If this diﬀerence is small or even null (as it may very well be the case in several distributions), the value of the numerator basically only depends on ¯/y.alt and ¯/y.alt(n 2 ). 5 A NORMAL APPROXIMATION FOR THE DISTRIBUTION OFMARRN D SM uses a Monte Carlo approach to approximate the distribution of MARrnd , which can then be used to compute MARrnd ,5%. Here, we provide an analytical approximation for it (see Section 5.1) and show the results we have obtained empirically (see Section 5.2). 5.1 Analytical Results MARrnd , the sample mean of absolute residuals, is a random vari- able, obtained as MARrnd = 1 n ) i ∈I ARi , where ARi is the random variable denoting the absolute residual related to the i-th eﬀort estimation. Since MARrnd is a sample mean, we now investigate whether we can use the Central Limit /T_heorem (CLT) to approxi- mate its distribution. /T_he standard form of the CLT can be used for a set of independent, identically distributed random variables. /T_he values of the i-th absolute residual variableARi belong to a /f_inite, discrete set{ari, j }for j ∈I(i), where ari, j denotes |/y.alti −/y.altj |. /T_he random variablesARi ’s are statistically independent, since choosing a speci/f_ic residuals for thei-th eﬀort estimation has no in- /f_luence whatsoever on choosing a speci/f_ic residual for thej-th eﬀort estimation. However, the random variables ARi ’s have diﬀerent distributions, since the set of values {ari, j }for j ∈I(i)associated with ARi is in general diﬀerent from the set of values {ark, j }for j ∈I(k)associated with ARk . /T_hus, we cannot use the original formulation of the CLT. However, we can take advantage of a more general form of the', 'j ∈I(k)associated with ARk . /T_hus, we cannot use the original formulation of the CLT. However, we can take advantage of a more general form of the CLT, which can be used for the sample mean of independent, though not identically distributed random variables, provided that they satisfy the Lindeberg condition [3]. Suppose that the distribution of each ARi has expected value µi and variance σi . Let us denote by s2n = )n i=1 σ2 i the sum of all variances s2 n = n) i=1 σ2 i (21) /T_he Lindeberg condition states that, whenn tends to in/f_inity, the random variable 1 sn ) i ∈I (ARi −µi ) (22) tends to the standard normal distribution i.e., the normal distribu- tion with zero mean and unit variance, if the condition in Formula (23) holds for every /uni03F5 > 0 limn→∞ 1 s2n ) i ∈I E [ (ARi −µi )2 ·1{|ARi −µi |>εsn } ] = 0 (23) where 1{|ARi −µi |>εsn }is the indicator function, whose value is 1 if its argument (|ARi −µi |> εsn in our case) is true and 0 when the argument is false. In other words, in the expected value in Formula (23), we only take into account those residuals ARi whose distance from their means µi is greater than εsn. In our case, the expected value corresponding to ARi in Formula (23) is 1 n −1 ) j ∈I(i)∧|ari, j −µi |>εsn (ari, j −µi )2 (24) As a consequence of Formula (22), under the Lindeberg condition, MARrnd tends to be distributed as a normal random variable with expected value 1 n ) i ∈I µi and variance 1 n2 ) i ∈I σ2 i . Here, we are interested in /f_inding a sensible approximation to the exact distribution of MARrnd . /T_hus, we assume thatARi ≤ MAX , i.e., that the residuals have an upper bound. /T_his, in turn, implies that /y.alti has an upper bound too. /T_his assumption appears to be sensible, since the value of the actual eﬀort of a project is bounded by budget constraints. Even if this were not the case, we', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden L.Lavazza and S. Morasca can always choose to discard those projects whose eﬀort is larger than a speci/f_ied amount and evaluate a model for those projects whose eﬀort is not greater than that speci/f_ied amount. Given that that speci/f_ied amount can be /f_ixed arbitrarily, the assumption seems to be reasonable. As a consequence, |ari, j −µi |is limited too. As a second assumption, we assume thatsn tends to in/f_inity asn tends to in/f_inity. In practice, whenn tends to in/f_inity, any newσ2 i introduced in the computation of s2n provides a contribution to s2n that may even tend to zero, but that is still large enough to make sn tend to in/f_inity. We clearly do not deal with an in/f_inite number of projects, so we may very well assume that the contribution of σ2 i to s2n satis/f_ies this assumption. If both assumptions are satis/f_ied, there exists a value ofnmax such that |ARi −µi |> εsnmax for all of the residuals, so the sum in the Lindeberg condition is limited. Since the value of s2n tends to in/f_inity asn tends to in/f_inity, the Lindeberg condition is satis/f_ied. 5.2 Empirical Results Figure 1 shows the histogram of MARrnd from the dataset by De- sharnais [9]. Visually, the distribution looks normal, as also shown by the continuous normal curve superimposed on the histogram. Figure 1: Histogram of a sample ofMARrnd for Desharnais dataset. Figure 1 also shows the value of MARrnd ,5% = 3691, repre- sented by the vertical dashed line, and the value of Numab,cst = MARcst = 3201, represented by the vertical red line. /T_he /f_igure shows that the value of MARcst is much to the le/f_t of the value of MARrnd ,5%. To check whether the distribution is normal, we built the normal quantile-quantile (Q-Q) plot for a sample of MARrnd , shown in Figure 2. /T_he plot matches very closely the ideal straight line that represents a perfectly normal distribution. One could wonder if this result is due to the fact that it represents only a random sample of 1000 values of MARrnd . To address this doubt, we built the complete set of values of MARrnd for a small dataset (6 datapoints). /T_he normal QQ plot for these values of Figure 2: Normal Q-Q plot of a sample of random MAR’s for Desharnais dataset. MARrnd is shown in Figure 3. Again, the plot matches very closely the perfect normality line. Figure 3: Normal Q-Q plot of random MAR’s for a given ran- dom sample dataset. In conclusion, we can assume that the distribution of MARrnd can be well approximated by a normal distribution. At any rate, to be on the safe side, in our empirical study of Section 7, we used an algorithm to build a sampling distribution of MARrnd that is more accurate than the normal approximation based on the n values of µi and σ2 i available in our datasets, given the limited values for n of our datasets.', 'On the Evaluation of Eﬀort Estimation Models EASE’17, June 15-16, 2017, Karlskrona, Sweden 6 ESTIMATE COMPARISON BASED ON INDIVIDUAL ABSOLUTE RESIDUALS Given a dataset and two models mod1 and mod2, SM states that mod1 is more accurate thanmod2 if SAab,mod1 > SAab,mod2 , i.e., if MARmod1 < MARmod2 . However, this is not the only criterion that can be used to com- pare the performances ofmod1 and mod2. Let ⟨ˆ/y.alt1,mod 1, ..,ˆ/y.altn,mod 1⟩ and ⟨ˆ/y.alt1,mod 2, ..,ˆ/y.altn,mod 2⟩be the estimates provided by the two models. We may say that mod1 is more accurate than mod2 if and only if there are at least ⌈n+1 2 ⌉distinct values of i such that |/y.alti −ˆ/y.alti,mod 1|< |/y.alti −ˆ/y.alti,mod 2|, that is, if mod1 provides a higher number of smaller absolute residuals than mod2. As an example, suppose that {2, 5, 6, 11}is the set of observed values, and {11, 6, 5, 6}and {5, 2, 2, 6}the sets of estimates with mod1 and mod2, respectively. /T_he absolute residuals are{9, 1, 1, 5} for mod1 and {3, 3, 4, 5}for mod2. MARmod1 = 4 > MARmod2 = 3.75, thus SM concludes that mod2 is more accurate than mod1. However, we could observe that the absolute residuals withmod1 are smaller than those obtained with mod2 twice out of four times, the absolute residual with mod2 is smaller than the corresponding absolute residual with mod1 once out of four times, and the two absolute residuals are equal once out of four times. /T_herefore, we could conclude that mod1 is preferable to mod2. However, this criterion can be applied when both mod1 and mod2 provide only one estimate for each /y.alti . When mod2 is the random model, we have many estimates, each associated with its own probability. /T_hus, we consider a modelmod preferable to rnd if mod has a higher probability than rnd to return a lower absolute residual for the majority of estimates. We now show how we can compute the probability that mod returns a lower absolute residual than rnd. Take the i-th estimate ˆ/y.alti,mod obtained with mod, based on which we compute the absolute residual ari,mod = |/y.alti −ˆ/y.alti,mod |. With rnd, we have n −1 possible estimates, belonging to set Y −{/y.alti }, each of which is selected with probability 1 n−1 . Correspondingly, we have n −1 possible absolute residuals ari, j,rnd = |/y.alti −/y.altj |, ∀j ∈I(i), each of which is selected with probability 1 n−1 . /T_he probability of randomly selecting an absolute residual ari, j,rnd that is greater thanari,mod is the sum of the probabilities of selecting each of the random residuals that are greater than ari,mod , i.e., it is the number of the random residuals that are greater than ari,mod divided by (n −1). /T_hus, thei-th estimate ˆ/y.alti,mod has a probability pi of being more accurate than rnd (i.e., a probability of “success”) and a probability(1 −pi )of not being more accurate thanrnd (i.e., a probability of “failure”), i.e., the i-th estimate is associated with a Bernoulli probability distribution. We now need to compute the probability of having X successes over then estimates, one for each datapoint, i.e., the probability that mod is more accurate than rnd for X estimates. We can compute that probability for all values of X ∈0..n by using the convolution of the n probability distributions. We obtain a probability P(X) for each value of X ∈0..n. /T_his is the consequence of the fact that, in general, the probability distribution of the sum of a set of independent random variables is the convolution of their individual distributions [1]. In our case, each distribution is binary, so it provides the probability for either 0 or 1 successes. /T_he number of successes is therefore the sum of the successes over then estimates. /T_he convolution can be computed by an algorithm with O(n2) computational complexity. Now, we are interested in the probability thatmod provides be/t_ter results than rnd in the majority of estimates, which we measure via the IARA indicator that we propose, as follows: IARA = n) X =⌈n/2⌉', 'results than rnd in the majority of estimates, which we measure via the IARA indicator that we propose, as follows: IARA = n) X =⌈n/2⌉ P(X) (25) /T_he same train of thought can be used with the LDSH random selection policy. /T_he only diﬀerence is in the way the probability of success for each individual estimate is computed, because each ab- solute residual computed with rnd has probability 1 n to be selected and not 1 n−1 . It can be easily shown that IARA computed according to LDSH is always smaller than IARA computed according to SM. For evaluation purposes, when IARA is close to 1, model mod is de/f_initely be/t_ter than random estimation. WhenIARA is 0.5, model mod is essentially equivalent to random estimation, while a value below 0.5 indicates that mod is even worse than random estimation. A simple binomial test can be used to assess if IARA is signi/f_i- cantly greater than 1 2 . 7 EMPIRICAL STUDY /T_he indicators described above were tested using a few datasets: the Atkinson and Telecom datasets [14] and the Albrecht, Desharnais, Kitchenham, Kemerer and NASA93 datasets from the PROMISE repository [9]. Speci/f_ically, we here show two analyses that we carried out. /T_he results are reported in Table 1 and divided into a few sets of columns, as we now describe. 7.1 First Analysis /T_his analysis is based on the approach of Section 4.2.2. We /f_irst took into account the random model and computed the values of a/v.alt/afii10069.ital t ∈T [MARrnd ], of the standard deviation of MARrnd , and the value of the MARrnd ,5% threshold. /T_hese values are given in columns MAR, sd and 5% of section random in Table 1. /T_hen, we took into consideration thecst model where ˆ/y.alti,cst = ˜/y.alti , i.e., the median of the values in Y −{/y.alti }, as suggested in Section 4.2.2. We proceeded to compute MARcst , the percentage in the MARrnd distribution corresponding to MARcst , and the value of IARA. /T_hese values are given in columnsMAR, % rndand IARA of section cst model (AbsR)in Table 1. In addition, we built median regression linear models ( mdn) to correlate independent variables (e.g., adjusted function points for the Albrecht dataset, and KLoC for the NASA93 dataset) to actual eﬀort. /T_his is a type of models not explicitly mentioned in Section 4.2.2, but using a median regression model is the best choice when dealing with the minimization of MAR, in much the same way as medians are the best choice when dealing with constant models. For each dataset, we chose the speci/f_ic median regression model that was statistically signi/f_icant at the customaryα = 0.05 level. To check the statistical signi/f_icance of the linear model, we used a Wilcoxon’s sign rank test [17] by comparing the absolute residuals obtained with the constant model and the corresponding absolute residuals obtained with the median regression model, as we mentioned in Section 6. We report the same data as for constant models, i.e., MARmdn , the percentage in the MARrnd distribution', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden L.Lavazza and S. Morasca corresponding to MARmdn , and the value of IARA in columnsMAR, % rndand IARA of section mdn modelin Table 1. By looking at Table 1, the comparison of the performance of the constant model vs. the random model shows that MARcst < a/v.alt/afii10069.ital t ∈T [MARrnd ]in all cases. In addition, MARcst < MARrnd ,5% in 4 cases out of 7, MARcst ≈MARrnd ,5% for the Atkinson dataset, and MARcst > MARrnd ,5% in the remaining 2 cases. /T_hus, even though the evidence is not conclusive, it appears that the constant model may be used as a reference model that sets a higher bar than the random model, since not only is it characterized by a be/t_ter MAR, but it mostly satis/f_ies the 5% threshold set by SM for models that are substantially more accurate than the random model. In addition, the results about IARA are consistent with those about the percentile of MARcst in the MARrnd distribution. In fact, in 4 out of 7 cases (the same 4 cases in whichMARcst < MARrnd ,5%) the values of IARA are quite close or very close 2 to 1. In the re- maining 3 cases, we can spot a diﬀerence between the percentile of MARcst in the MARrnd distribution and IARA. Speci/f_ically, for the Atkinson dataset, we have MARcst ≲ MARrnd ,5%, but the value of IARA is 0.66. /T_his value is worse than the value ofIARA for the Kemerer dataset (i.e., 0.669), in which, however the percentile of MARcst (i.e., 9.6%) is almost double the percentile of the Atkinson dataset (i.e., 5.1%). /T_hese diﬀerences are due to the fact thatIARA only takes into account if an absolute residual of the given model is less than the absolute residual of the random model: to this end, 3 < 4 has the same weight as 3 < 25. On the contrary, values 4 and 25 provide a quite diﬀerent contribution to the MAR of the model. /T_herefore, the relationship betweenIARA and indexes based on MARrnd ,5% is not /f_ixed, as far as numerical values are concerned. /T_he median regression models perform clearly be/t_ter than both the random model and the constant model. /T_heMARmdn values are well below the MARrnd ,5%, and the values of IARA are very close to 1 or actually 1, with the exception of the result for the Atkinson dataset. Finally, we can notice that IARA provides stricter indications than the comparison with MARrnd ,5%. In fact, 1- IARA indicates the proportion of estimates that are worse than random estimates: we can notice that 1- IARA is always less than the percentage in the MARrnd distribution corresponding to MARmod . For instance, for the the cst model and the Atkinson and Kemerer datasets, 1- IARA indicates that about one third of the random estimates are be/t_ter than the estimates by thecst model; the criterion based on MARrnd ,5% is much more optimist, as it indicates that less than 10% of the random estimates are be/t_ter thancst (namely, 5.1% and 9.6% for the Atkinson and Kemerer datasets, respectively). 7.2 Second Analysis We use the approach on models that are commonly built when estimating eﬀort. In this case, for each project i in a dataset Y , ˆ/y.alti,cst = /y.alti , where /y.alti is the average value of Y −{/y.alti }. Like for the other approach, we proceeded to compute MARcst , the percentage in the MARrnd distribution corresponding to MARcst , and IARA. /T_hese values are in section ofcst model (SqR)of Table 1. 2in Table 1, 0.999 actually means ≥0.999 and 1 means that with the precision of the computational tools we used, no diﬀerence can be represented 1 and the probability that estimates that are worse than those of the model being evaluated. For each dataset, we then built Ordinary Least Squares linear models (using the same independent variable used for the mdn model). All the reported models are statistically signi/f_icant at the 0.05 level. We report the same data as for the constant model approach in section LR modelof Table 1.', 'model). All the reported models are statistically signi/f_icant at the 0.05 level. We report the same data as for the constant model approach in section LR modelof Table 1. Table 1 shows thatMARcst < a/v.alt/afii10069.ital t ∈T [MARrnd ]in all cases, like in the /f_irst analysis. It is also possible to observe that thecst models feature worse accuracy (with the one exception of the Telecom dataset) than the corresponding cst models based on the medians of absolute residuals described in Section 7.1. MARcst is less than MARrnd ,5% in only 3 cases out of 7, MARcst is close to MARrnd ,5% for two datasets (Atkinson and Telecom), and MARcst is de/f_initely greater than MARrnd ,5% in the remaining 2 cases. /T_he indications byIARA are consistent with those about the percentile of MARcst in the MARrnd distribution, as in Section 7.1. IARA also generally provides stricter indications thanMARrnd ,5%. In particular, for the Albrecht dataset there is not even any evidence that the estimates provided by the cst model are be/t_ter than 50% of the random estimates. /T_he LR model appears de/f_initely more accurate than thecst model, but slightly less accurate than the mdm model. 8 DISCUSSION Both the criterion of Shepperd and MacDonell and the IARA in- dicator assume random estimates as a baseline. Both methods appear eﬀective in evaluating the accuracy of estimation models. In general the two methods tend to provide consistent indications; however, IARA provides generally more severe evaluations. For instance, the linear model for Atkinson dataset is just below the acceptability threshold for Shepperd and MacDonell, while it is far from acceptable for IARA, assuming that—in analogy with the criteria by Shepperd and MacDonell—we set a threshold for IARA at 0.95. /T_hecst models appear generally more accurate than the random model, even if only the best 5% estimates are considered. As a consequence, one could consider adopting the cst model as the baseline against which comparing models. Moreover, computingcst estimates is extremely easy, while computing exactly MARrnd ,5% is fairly complex. 9 THREATS TO VALIDITY Internal validity. We used standard statistical methods to build the models used in our empirical analysis, for instance, by checking their statistical signi/f_icance. So, no major threats to internal validity can be envisioned. External validity. Our empirical study is based on a limited number of datasets, so there may be a threat to the external validity of our study. However, the seven datasets we used are of diﬀerent sizes and nature, which somewhat alleviates the threats to the generality of our results. In addition, our empirical study uses more datasets than the original proposal by Shepperd and MacDonell, which uses three datasets. Construct validity. /T_he indicators used in our study have been proposed as accuracy indicators. /T_he purpose of our paper is to propose re/f_ined accuracy indicators that formalize, generalize, and extend the existing ones and to check their performance. /T_hus,', 'On the Evaluation of Eﬀort Estimation Models EASE’17, June 15-16, 2017, Karlskrona, Sweden Table 1: Accuracy evaluation of the analyzed datasets. random cst model (AbsR) mdn model cst model (SqR) LR model Dataset MAR sd 5% MAR % rnd IARA MAR % rnd IARA MAR % rnd IARA MAR % rnd IARA Albrecht 25 4.98 17.2 16 2.6% 0.907 7 0.01% 0.999 20 12.50% fail 10 0.07% 0.999 Kemerer 225 57.6 129.8 149 9.6% 0.669 96 1.29% 0.951 165 14.90% 0.85 96 1.30% 0.97 Atkinson 281 43.9 208.9 209 5.1% 0.66 197 2.78% 0.854 214 6.30% 0.94 214 6.28% 0.799 Telecom 194 35.7 135.6 155 13.4% 0.643 76 0.05% 0.999 142 7.12% 0.83 77 0.05% 0.999 Desharnais 4350 400.1 3691.4 2913 0.02% 0.997 2088 <0.01% 1 3201 0.20% 0.91 2153 <0.01% 0.999 Kitchenham 1879 126.7 1670.6 1299 <0.01% 0.999 940 <0.01% 1 1391 0.005% 1 977 <0.01% 1 NASA93 850 105.4 676.2 544 0.19% 0.993 341 <0.01% 1 646 2.68% 0.99 347 <0.01% 1 a part of our paper is about mitigating the threats to construct validity of previous proposals. 10 RELATED WORK We thoroughly reviewed the proposals by Shepperd and MacDonell and by Langdon et al. in Section 3, so we here review a few other proposals of the literature. Traditionally, the Mean Magnitude of Relative Errors (MMRE) [2] has been used as the estimation accuracy indicator of choice MMRE = 1 n ) i ∈I |/y.alti −ˆ/y.alti,mod | /y.alti (26) /T_hus,MMRE takes the proportions of absolute error of the estima- tions of mod and computes their average. /T_he main idea behind MMRE is that the importance of the error depends on the size of the project itself. For instance, an absolute error |/y.alti −ˆ/y.alti,mod |= 10 PM (person-months) is almost irrelevant in a project i where, say, /y.alti = 1000 PM and ˆ/y.alti,mod = 990 PM. However, the same absolute error |/y.altj −ˆ/y.altj,mod |= 10 PM is much more serious in another project j where, say, /y.altj = 20 PM and ˆ/y.altj,mod = 10 PM. MMRE is able to capture this relative importance: in project i, the 10 PM error con- tributes 10 1000 = 0.01 to the sum in Formula (26); in project j, the same 10 PM error contributes 10 20 = 0.5. However, as several studies have shown [ 4, 6, 13], MMRE is /f_lawed, for a number of reasons. For instance,MMRE gives more importance to smaller project than to bigger ones. For instance, the contribution to the sum of an error |/y.altk −ˆ/y.altk,mod |= 400 PM obtained in project k with /y.altk = 1000 PM and ˆ/y.altk,mod = 600 PM is 400 1000 = 0.4, which is less than the 0.5 contribution of project j. However, there is li/t_tle doubt that, in practical terms, the 400 PM error in project k would be considered much more serious than the 10 PM error in project j. /T_hus,MMRE is a biased estimator of central tendency of the residuals of a prediction system. Also, MMRE does not diﬀerentiate between underestimation and overes- timation, while underestimation is generally much more serious then overestimation. In light of these and other problems, other indicators have been de/f_ined as variants ofMMRE. For instance [6], MMER = 1 n ) i ∈I |/y.alti −ˆ/y.alti,mod | ˆ/y.alti,mod (27) replaces the actual value /y.alti in the denominator of the term in the sum, with the estimated value ˆ/y.alti,mod . /T_his indicator alleviates the bias of MMRE towards small projects. /T_he Mean Balanced Relative Error (MBRE) indicator [10] com- bines MMRE and MMER, as follows MBRE = 1 n (( ( ) /y.alti > ˆ/y.alti, mod /y.alti −ˆ/y.alti,mod ˆ/y.alti,mod + ) /y.alti < ˆ/y.alti, mod /y.alti −ˆ/y.alti,mod /y.alti )) ) (28) where the /f_irst sum is over the underestimated projects, while the second sum is over the overestimated ones. /T_he Mean Inverted Balanced Relative Error (MIBRE) is also de/f_ined in [10], where the denominators in the terms in the /f_irst and the second sums are exchanged. However, Myrtveit and Stensrud [12] found that MMRE, MMER, MBRE, and MIBRE are invalid accuracy statistics for model selection, as they systematically select inferior models. Using composite', 'However, Myrtveit and Stensrud [12] found that MMRE, MMER, MBRE, and MIBRE are invalid accuracy statistics for model selection, as they systematically select inferior models. Using composite accuracy statistic, e.g. MMRE+MBRE+MIBRE, does not improve the validity of the results. All of the above indicators are based on the mean. However, other indicators are based on the median. For instance, MdMRE [5] is de/f_ined as the median of the set{|/y.alti −ˆ/y.alti, mod | /y.alti }. /T_he median may have some advantages with respect to the mean, since it is a much more robust operation. Another popular indicator is Pred (25)[16] which measures ac- curacy based on the number of projects in a dataset for which estimates are failing within 25% of the actual values, i.e., for which |/y.alti −ˆ/y.alti, mod | /y.alti ≤0.25. /T_he higherPred (25), the more projects have estimates close to the actual value, the more accurate the model. Finally, R2 of Ordinary Least Squares regression may be used as an indicator of the accuracy of a model, like other indicators based on the Least Median of Squares or the Least /Q_uantile of Squares, as de/f_ined in [8, 11]. 11 CONCLUSIONS AND FUTURE WORK In this paper, we have revisited, formalized, and generalized the proposals by Shepperd and MacDonell [15] and by Langdon et al. [7]. Speci/f_ically, (1) we have provided a generalized theoretical basis to de/f_ine accuracy indicators; (2) we have evaluated the adequacy of the random estimation policy to be taken as the reference, baseline model: constant models may instead be a be/t_ter alternative, to this end; (3) we have introduced an alternative way and an alternative indicator (IARA) to compare estimation models against the random model; (4) we have empirically evaluated the diﬀerent approaches in practice. Future work will include', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden L.Lavazza and S. Morasca • investigating the properties ofIARA, e.g., its computational complexity in comparison to SM and LDSH and how IARA values relate to eﬀect sizes; • investigating other operators for de/f_ining accuracy indi- cators, for instance based on the median, instead of the average; • applying the approaches to further datasets and comparing the results; • comparing our approach with the ones in the literature, in addition to SM and LDSH. 12 ACKNOWLEDGMENTS Parts of this work have been supported by the “Fondo di ricerca d’Ateneo” funded by the Universit`a degli Studi dell’Insubria. REFERENCES [1] Valentin V. Petrov (auth.). 1975. Sums of Independent Random Variables(1 ed.). Springer-Verlag Berlin Heidelberg. [2] S. D. Conte, H. E. Dunsmore, and V. Y. Shen. 1986. So/f_tware Engineering Metrics and Models. Benjamin-Cummings Publishing Co., Inc., Redwood City, CA, USA. [3] Robert B Ash; Catherine Doleﬃans-Dade. 2000. Probability and measure theory (2nd ed ed.). Harcourt/Academic Press. [4] Tron Foss, Erik Stensrud, Barbara Kitchenham, and Ingunn Myrtveit. 2003. A simulation study of the model evaluation criterion MMRE. IEEE Transactions on So/f_tware Engineering29, 11 (2003), 985–995. [5] Magne Jørgensen. 1995. Experience With the Accuracy of So/f_tware Maintenance Task Eﬀort Prediction Models. IEEE Trans. So/f_tware Eng.21, 8 (1995), 674–681. DOI:h/t_tp://dx.doi.org/10.1109/32.403791 [6] Barbara A Kitchenham, Lesley M Pickard, Stephen G. MacDonell, and Martin J. Shepperd. 2001. What accuracy statistics really measure.IEE Proceedings-So/f_tware 148, 3 (2001), 81–85. [7] William B Langdon, Javier Dolado, Federica Sarro, and Mark Harman. 2016. Exact mean absolute error of baseline predictor, MARP0. Information and So/f_tware Technology 73 (2016), 16–18. [8] Luigi Lavazza and Sandro Morasca. 2012. So/f_tware eﬀort estimation with a generalized robust linear regression technique. In Evaluation & Assessment in So/f_tware Engineering (EASE 2012), 16th International Conference on. IET, 206–215. [9] Krishna R. Pryor-D. Menzies, T. 2015. /T_he PROMISE Repository of Empirical So/f_tware Engineering Data. (2015). [10] Y. Miyazaki, M. Terakado, K. Ozaki, and H. Nozaki. 1994. Robust regression for developing so/f_tware estimation models.Journal of Systems and So/f_tware27, 1 (1994), 3–16. DOI:h/t_tp://dx.doi.org/10.1016/0164-1212(94)90110-4 [11] Sandro Morasca. 2009. Building statistically signi/f_icant robust regression mod- els in empirical so/f_tware engineering. InProceedings of the 5th International Conference on Predictor Models in So/f_tware Engineering. ACM, 19. [12] Ingunn Myrtveit and Erik Stensrud. 2012. Validity and reliability of evalua- tion procedures in comparative studies of eﬀort prediction models. Empiri- cal So/f_tware Engineering17, 1-2 (2012), 23–33. DOI:h/t_tp://dx.doi.org/10.1007/ s10664-011-9183-7 [13] Ingunn Myrtveit, Erik Stensrud, and Martin Shepperd. 2005. Reliability and validity in comparative studies of so/f_tware prediction models.IEEE Transactions on So/f_tware Engineering31, 5 (2005), 380–391. [14] Martin Shepperd and Michelle Cartwright. 2005. A replication of the use of regression towards the mean (R2M) as an adjustment to eﬀort estimation models. In So/f_tware Metrics, 2005. 11th IEEE International Symposium. IEEE, 10–pp. [15] Martin Shepperd and Steve MacDonell. 2012. Evaluating Prediction Systems in So/f_tware Project Estimation.Information and So/f_tware Technology54, 8 (2012), 820–827. [16] Martin J. Shepperd and Chris Scho/f_ield. 1997. Estimating So/f_tware Project Eﬀort Using Analogies. IEEE Trans. So/f_tware Eng.23, 11 (1997), 736–743. DOI: h/t_tp://dx.doi.org/10.1109/32.637387 [17] Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics bulletin 1, 6 (1945), 80–83.']","['Università degli Studi dell’Insubria  Dipartimento di Scienze Teoriche e Applicate    ACCURACY OF EFFORT ESTIMATION MODELS:   EVIDENCE OF BASELINE EFFECTIVENESS    This briefing reports scientific evidence on  the effectiveness of both new and  previously defined baseline methods for  evaluating the accuracy of effort  estimation methods.      FINDINGS    There is no doubt that it is very important  to software companies to estimate  software development effort accurately.  So, it is important to assess the accuracy of  effort estimation models .    We propose a new, general framework for  evaluating effort estimation models.    The general idea is to evaluate the  accuracy of an effort estimation model  against a reference model.  • If the accuracy of the model is greater  than the expected accuracy of the  reference model, the model is a  candidate for practical use;  •  Otherwise, the model is not sufficiently  accurate.    So, what is an adequate reference model ?    Previous work suggested the use of  random reference models. The random  reference model estimates the effort of a  new project by randomly picking the value  of the effort of a project in a pool of  previous project.    Clearly, if an effort estimation model is less  accurate than what can be expected of  random guessing, the model is not  accurate enough.    Also, we want the effort estimation model  to be not just better, but significantly  better  than the random reference model.  We may want to use an effort estimation  model that is better than random guessing  95% of the time.  We believe, however, that the bar should  be set higher. We can build better  reference models with the same  information used for random guessing. For  instance, even a constant model  where  effort is estimated as the simple mean of  the effort of previous projects would be a  more accurate estimation model than  random guessing . So, we propose that a  software manager should obtain the basic  estimate using the constant model, instead  of random guessing.    More accurate models can be built by  correlating some software characteristics  to effort. How much more accurate are  predictions based on models where these  characteristics are the independent  variables than predictions based on the  simple mean?    Our framework  • Is general  enough to be used with any  cost function ;  •  Uses as reference model the best  possible constant model  for the chosen  cost function;  •  Is based on the comparison of the  accuracies of the model and the  reference model ;  •  Checks whether the model is also  significantly better than the random  reference model .    We introduced a new indicator of model  accuracy. IARA (Individual Absolute  Residual Accuracy)  is the probability that  the effort estimation model provides  better results than the random model in  the majority of estimates it gives.    An extensive data analysis shows that  • The constant model sets a higher bar  than the random model;  •  The constant model is approximately  better than random guessing  approximately 95% of the time, but  much easier to compute;  •  IARA appears to be a good indicator of  the accuracy of estimation models.            Keywords    Effort estimation models, Estimation  accuracy, Model evaluation     Who is this briefing for?    Software engineering practitioners who  want to evaluate the accuracy of  estimates obtained via effort  estimation methods, based on scientific  evidence.        Where do the findings come from?    All findings of this briefing were  extracted from the empirical study  conducted by Lavazza and Morasca  (http://dl.acm.org/citation.cfm?id=308  4226&picked=prox).        What is included in this briefing?    The main findings of the original study.      What is not included in this briefing?    Detailed description of all the findings.      To access other evidence briefings  on software engineering:    http://ease2017.bth.se/', 'What is not included in this briefing?    Detailed description of all the findings.      To access other evidence briefings  on software engineering:    http://ease2017.bth.se/      For additional information about our  research, please contact    luigi.lavazza@uninsubria.it  sandro.morasca@uninsubria.it          ORIGINAL RESEARCH REFERENCE    Luigi Lavazza and Sandro Morasca On the Evaluation of Effort Estimation Models . 21 st  International Conference on Evaluation and Assessment in Software Engineering,  2017.']","**Title: Improving Accuracy in Software Effort Estimation Models**

**Introduction:**
This evidence briefing summarizes key findings from research on effort estimation models in software development. Accurate effort estimation is crucial for effective project planning and resource allocation. The study aims to establish sound accuracy indicators that enable practitioners to evaluate and compare different estimation models effectively.

**Main Findings:**
1. **Framework for Accuracy Indicators:** The authors propose a general framework for developing accuracy indicators for effort estimation models. This framework emphasizes the need for a robust reference model against which other models can be compared.

2. **Critique of Random Models:** The study critiques the common practice of using random estimation models as a baseline for accuracy comparison. It argues that random models set an unreasonably low standard for acceptable accuracy. Instead, constant estimation models, which are simpler yet more reliable, should be used as reference points.

3. **New Evaluation Methods:** The research introduces new methods for evaluating model accuracy, including the Individual Absolute Residuals Assessment (IARA) indicator. This indicator provides a more nuanced view of model performance by comparing the probability of a model producing lower residuals than random estimates.

4. **Empirical Validation:** The proposed methods were validated using various datasets from real-world software projects. Results indicated that constant models often outperformed random models, suggesting that they are a more appropriate baseline for comparison.

5. **Practical Implications:** Practitioners should consider adopting constant estimation models as a reference for assessing the accuracy of their effort estimation models. The findings highlight the importance of using robust indicators to ensure that estimation models provide meaningful and actionable insights for project planning.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and researchers interested in improving the accuracy of effort estimation in software development projects.

**Where the findings come from:**
All findings in this briefing are derived from the research conducted by Luigi Lavazza and Sandro Morasca, presented in their paper ""On the Evaluation of Effort Estimation Models,"" published in the Proceedings of EASE'17.

**What is included in this briefing?**
This briefing includes a summary of the proposed framework for accuracy indicators, critiques of existing models, new evaluation methods, empirical validation results, and practical implications for software practitioners.

**For more evidence briefings on software engineering:**
http://ease2017.bth.se/

**For additional information about the research group:**
Contact: luigi.lavazza@uninsubria.it; sandro.morasca@uninsubria.it

**Original Research Reference:**
Lavazza, L., & Morasca, S. (2017). On the Evaluation of Effort Estimation Models. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017. DOI: http://dx.doi.org/10.1145/3084226.3084260"
"[""Analysing Requirements Communication Using Use Case  Specification and User stories  Ana Carolina Oran, Elizamary Nascimento   Universidade Federal do Amazonas  (UFAM) Manaus – AM – Brazil  {ana.oran, elizamary.souza}  @icomp.ufam.edu.br  Gleison Santos  Universidade Federal do Estado do Rio  de Janeiro (UNIRIO)   Rio de Janeiro –  RJ – Brazil  gleison.santos@uniriotec.br  Tayana Conte  Universidade Federal do Amazonas  (UFAM) Manaus – AM – Brazil  tayana@icomp.ufam.edu.br  ABSTRACT  Effective requirements communication is essential in software  development project s due to the importance of understanding  the requirements throughout the software development cycle.  Software requirements can be specified in different formats, for  instance using free texts or more structured fo rms, such as use  cases and user stories  used in Behavior Driven Development  (BDD). We present a comparative analysis on the requirements  communication dynamics using use case specification and user  stories as the basis for mockups creation. We carried out an  exploratory empirical study involving 16 students. Th e study  comprised 3 steps: requirements specification, mockups  construction, and inspection to investigate whether the mockups  were in accordance with the specifications. Results show that  there is no  significant difference in using use case specification  or user stories  to communicate software requirements. Our  findings suggest that different specification formats can provide  similar results while communicating requirements, nonetheless  the human factor should not be neglected.1  CCS CONCEPTS  • Software notations and tools → System description  languages; Unified Modeling Language (UML); Specification  languages.  KEYWORDS  Requirements communication, requirements specification, use  case, Behavior Driven Development, experimental study.     ACM Reference format:  A. C. Oran, E. Nascimento, G. Santos, T. Conte. 2017. Analysing  Requirements Communication Using Use Case Specification and  User stories. In Proceedings of 31st Brazilian Symposium on  Software Engine ering, Fortaleza, Ceará, Brazil, September 2017  (XXXI SBES), 10 pages.   https://doi.org/10.1145/3131151.3131166                                                                    Publication rights licens ed to ACM. ACM acknowledges that this contribution was  authored or co -authored by an employee, contractor or affiliate of a national  government. As such, the Government retains a nonexclusive, royalty -free right to  publish or reproduce this article, or to allow others to do so, for Government  purposes only.  SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil   © 2017 Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM ISBN 978-1-4503-5326-7/17/09…$15.00   https://doi.org/10.1145/3131151.3131166  1 INTRODUÇÃO  A comunicação dos requisitos é essencial em todos os projetos de  software, uma vez que existe necessidade de entendimento das  informações durante todo o ciclo do processo de  desenvolvimento de software  [29]. A comunicação de requisitos  é iniciada com a elicitação de requisitos com o cliente e continua  ao longo de todo o projeto de desenvolvimento, envolvendo  diferentes papéis [33]. Na fase de elicitação de requis itos, os  clientes devem ser capazes de comunicar as suas necessidades  para os analistas de requisitos e os analistas precisam ser capazes  de transmitir essas necessidades para todos os demais membros  da equipe de desenvolvimento de forma clara e eficaz . Al-Rawas  e Easterbrook [1], Tu et al. [33] e Fernández et al. [11] afirmam  que tanto usuários finais quanto profissionais de software têm  dificuldades relacionadas à validaçã o e compreensão das  informações resultantes da especificação dos requisitos. Uma boa  comunicação não significa apenas emitir os requisitos para todo  o grupo, mas também é preciso garantir o entendimento destes  na sua recepção. Segundo Hoisl et al. [13], a forma como os"", 'comunicação não significa apenas emitir os requisitos para todo  o grupo, mas também é preciso garantir o entendimento destes  na sua recepção. Segundo Hoisl et al. [13], a forma como os  requisitos são descritos pode influenciar no esforço requerido  para que o receptor desta informação entenda o que deve ser  desenvolvido. É importante representar os requisitos de forma  que permita que  todos os stakeholders envolvidos no projeto  estabeleçam um entendimento comum sobre o funcionamento do  sistema para que o produto final  desenvolvimento pela equipe   esteja de acordo com as expectativas do cliente.   Há diferentes formas de se representar os requisitos de um  sistema, desde uso de textos livres a formas mais estruturadas.   Problemas com comunicação de requisitos podem surgir devido  ao modelo de especificação escolhida para o processo de  desenvolvimento do Sistema [15]. Uma forma muito difundida de  se descrever requisitos são os casos de uso, bastante utilizados  pela indústria  [3, 9, 22]. Casos de uso , além de descreverem os  requisitos, são utilizados também como meio de comunicação  entre os membros da equipe de projeto e outros stakeholders  [8]  e também como insumos em diferentes atividades não  diretamente relacionadas a um processo de Engenharia de  Requisitos, como atividades de planejamento, análise, projeto,  desenvolvimento, teste e manutenção do sistema [ 2]. Outra  forma que tem despertado interesse nas comunidades  acadêmicas e industriais é a user story usada em Behavior Driven  Development (BDD ou Desenvolvimento Dirigido a  Comportamento) [8]. BDD é uma metodologia ágil preocupada  em utilizar um vocabulário comum para entendimento de  214', 'requisitos por todos os stakeholders do projeto  [7]. S egundo  Silva [27], BDD impulsiona as equipes de desenvolvimento para  uma especificação de requisitos baseada em user stories [19] em  um formato abrangente de linguagem natural . A s user stories  consistem em descrições breves, na perspectiva do usuário final,  das funcionalidades desejadas [35] e englobam vários aspectos da  especificação de requisitos representados em linguagem natural ,  tais como: cenários alternativos, de exceção  e interação do ator  com o sistema.   Com o intuito de compara r a dinâmica da comunicação de  requisitos utilizando especifica ção de caso de uso e user story  como base para a criação de mockups (representações dos  aspectos da interface do usuário [18]), este artigo apresenta os  resultados de um estudo experimental exploratório onde foram  verificadas a emissão e recepção dos requisitos utilizando caso de  uso e user story, avaliando e comparando o grau de correção das  especificações e dos mockups criados. Para direcionar este  estudo, foi definida a seguinte questão de pesquisa : “O uso de  casos de uso ou user stories afeta a comunicação dos requisitos  para a construção de mockups de forma diferente? ”.  O restante deste artigo está organizado da seguinte forma: a  Seção 2 aborda conceitos sobre proces so de engenharia de  requisitos, caso de uso , user story utilizada no BDD e trabalhos  relacionados; a Seção 3 descreve o planejamento e execução do  estudo experimental; a Seção 4 apresenta a análise dos resultados  encontrados neste estudo ; a Seção 5  as discussões e ameaças à  validade do estudo . Por fim, a Seção 6 apresenta as conclusões ,  limitações e trabalhos futuros.  2 ESPECIFICAÇÃO DE REQUISITOS DE  SOFTWARE   O documento de especificação de requisitos é uma declaração  dos requisitos para os stakeholders. Este documento fornece base  para a análise e validação de requisitos junto aos stakeholders,  definição do que os projetistas têm que construir, bem como a  verificação da qualidade do produto final no momento da  entrega [5,19]. Através de um mapeamento sistemático  da  literatura sobre comunicação de requisitos de software,  identificou-se que existem diferentes formas para representar os  requisitos de software e a escolha por uma dessas formas  dependerá de fatores como, a experiência da equipe  e a  visibilidade destes requisitos [31]. Entre muitas formas de  representar requisitos, duas das mais utilizadas pela indústria  são: casos de uso [3, 17] e user stories, devido ao crescimento do  desenvolvimento ágil  [34]. A seguir é apresentada uma breve  descrição destas formas de representar os requisitos.   2.1 Caso de uso   Um dos artefatos uti lizados pelos engenheiros de software para   descrever e docume ntar requisitos do software é a especificação  de Caso de Uso (UC)  [31]. Originalmente proposto por Jacobson  et al.  [16] como uma forma dos profissionais de software  obterem uma melhor compreensão dos requisitos de um sistema .  Segundo Bezerra  [6] e Cockburn [9], caso de uso é a  especificação da sequência de interações  entre um sistema e um  ou mais atores (agentes ex ternos a esse sistema) de forma  sequencial, representando um relato de utilização de alguma  funcionalidade do sistema em questão, sem revelar a estrutura e  o comportamento internos desse sistema.   A estrutura amplamente utilizada pelos engenheiros de  requisitos em geral utiliza a seguinte estrutura  [24]: Nome:   descreve o nome do caso de uso, capturando a sua essência.  Descrição: uma descrição sucinta do caso de uso, descrevendo o  objetivo do caso de uso. Pré-condições (opcional): descrição de  condições que devem ser atendidas antes de iniciar o caso de uso.  Fluxo Principal: descreve os passos do caso de uso realizados em  situações normais, ou seja, sem considerar nenhum erro. Fluxos  Alternativos: descreve m formas al ternativas de realizar certos  passos do caso de uso. Fluxos de Exceção: descrevem formas de', 'situações normais, ou seja, sem considerar nenhum erro. Fluxos  Alternativos: descreve m formas al ternativas de realizar certos  passos do caso de uso. Fluxos de Exceção: descrevem formas de  recuperação de erros que podem ocorrer em certos passos do  caso de uso. Pós-condições (opcional): descrição de condições que  devem ser atendidas após a execução do caso de uso,  considerando que o fluxo de eventos normal seja realizado com  sucesso. Regras de Negócio:  são políticas, procedimentos ou  restrições que devem ser levadas em consideração durante a  execução do caso de uso.   Esta estrutura é voltada principalme nte para descrever o s  fluxos principal e alternativo de eventos. O uso dessa estrutura  facilita o entendimento da funcionalidade por utilizar uma  estrutura pré-definida [4] e permite aos engenheiros de software  flexibilidade na especificação dos requisitos  [31] ajudando assim  na comunicação de requisitos dentr o das equipes de  desenvolvimento.  2.2 User story usada em Behavior Driven  Development (BDD)  Behavior-Driven Development é uma metodologia  ágil que  auxilia grupos a construir e entregar software de maior valor e  qualidade de forma mais rápid a [28]. BDD tem como objetivo  promover o entendimento comum do domínio do negócio entre  os stakeholders do projeto, além de apoiar a automação de outras  atividades do ciclo de desenvolvimento e manter a  documentação atualizada [10, 12, 28].  A especificação dos requisitos em BDD é feita por todos os  stakeholders do projeto no formado de user stories (US). Onde  cada user story é instancia da com múltiplos cenários BDD, que  representam um comportamento esperado para uma  determinada situação.  Os cenários BDD possuem um template,  escrito em uma linguagem independente de domínio, de forma a  ser u tilizado em qualquer situação [7]. Segundo Silva  [27], as  user stories  utilizada em BDD são desenvolvidas sob uma  perspectiva de comportamento no ponto de vista do usuário. Este  método promove uma descrição de linguagem natural  semiestruturada, de forma não ambí gua, além de promover a  reutilização de comportamentos de negócios que podem ser  compartilhados para vários recursos no sistema.   Um cenário BDD é composto por uma parte inicial q ue  representa uma user story [7]: Como: descreve o papel/ator que  utilizará a  funcionalidade. Eu quero: descreve o objetivo, desejo  ou característica da funcionalidade. Para que: descreve o motivo,  justificativa ou benefício da funcionalidade.   215', '3  A segunda parte do cenário descreve o comportamento em si  e u tiliza-se palavras chaves: Dado que: descreve um contexto  inicial, descrito por  várias características,  são as pré -condições  para executar o cenário . Quando: descreve o que pode  eventualmente, ocorrer uma mudança no domínio,  os testes  (passos) para execução do cenário . Então: descreve o estado final  pretendido, o resultado esperado da execução dos passos.   Esta estrutura  utiliza uma linguagem ubíqua que permite  especificar o comportamento do sistema ao invés de detalhes  técnicos, o que possibilita a comunicação, colaboração e  compreensão dos requisitos do sistema por todos os envolvidos  no desenvolvimento do software [7, 8, 10, 12, 27, 28].  2.3 Trabalhos relacionados  Lauesen e Kuhail [17] compararam a técnica de descrição de  tarefas com casos de uso em projetos reais  e verificaram que  a  utilização de casos de uso torna o requisit o muito restritivo por  duas razõ es: 1. Força o analista a criar um diá logo muito cedo,  projetando assi m uma solução em vez de especificar uma  necessidade do cliente. 2. O template util izado para especificação  pode induzir o analista a criar regras, pré-condições e fluxos que  muitas vezes não refletem a necessidade do cliente e pode  prejudicar no desenvolvimento do s istema. Por outro lado, na  descrição de tarefas não existe o template para ser seguido  pelo  analista, a descrição é criada  ao long o do desenvolvimento e  descreve-se apenas a interação do   usuário com o sistema.   Tu et al.  [33] afirmam que o uso de documentos mais  transparentes, ou seja, com uma maior visibilidade de  informações para as partes interess adas, possa contribuir para  uma comunicação mais eficaz. Eles realizaram um  experimento  que mostrou evidências de que uma maior transparência dos  documentos de requisitos leva a uma comunicação mais eficaz.   Al-Rawas e Easterbrook [ 1] descrevem  um estudo s obre os  problemas de comunicação de requisitos em projetos de  desenvolvimento de software. Os resultados mostraram que as  questões organizacionais e sociais têm grande influência na  eficácia das atividades de comunicação. Além disso, apontam que  os usuário s finais têm dificuldades em entender e validar os  requisitos devido às notações utilizadas. Por outro lado, os  profissionais de software relatam que o uso de notações que são  legíveis para os seus clientes, muitas vezes, gera documentos  extensos e que apresentam ambiguidade.    Os trabalhos apresentados focaram n a comunicação de  requisitos entre cliente e analista . Estes não  tiveram como  objetivo verificar qual a melhor forma de representar os  requisitos para que todos da equipe tenham o entendimento do  que será desenvolvido.   3 ESTUDO EXPERIMENTAL  Com o prop ósito de comparar especificações  de casos de uso  e  user stories foi realizado um estudo  experimental para comparar  as abordagens em termos de  comunicação de requisitos entre os  membros da equipe de desen volvimento. Esta s eção descreve o  plano de estudo e sua execução. A versão completa do pacote  experimental encontra-se disponível no relatório técnico [23].  3.1 Planejamento do Estudo  Para orientar o projeto do estudo, a partir da questão de pesquisa  (O uso de casos de uso ou user stories afeta a comunicação dos  requisitos para a construção de mockups de forma diferente? ),  foi definido o objetivo da seguinte forma:  “Analisar as especificações de caso de uso e user story usadas  em BDD com a finalidade de compar á-las em relação a  comunicação de requisitos dentro de grupos de desenvolvimento,  do ponto de vista de pesquisadores em engenharia de software  no contexto de  especificação de requisitos e con strução de  mockups por estudantes de graduação e pós-graduação”.   3.1.1 Participantes. O estudo foi realizado em um ambiente  acadêmico, com 16 participantes, sendo 11 alunos de graduação e  5 alunos de pós-graduação do curso de Ciência da Computação e', '3.1.1 Participantes. O estudo foi realizado em um ambiente  acadêmico, com 16 participantes, sendo 11 alunos de graduação e  5 alunos de pós-graduação do curso de Ciência da Computação e  Sistema de Informação da Universidade Federal do Amazonas  (UFAM). Os alunos haviam cursado as disciplinas de Engenharia   de Software e Análise e Projeto de Sistemas e estavam cursando  a disciplina de Engenharia de Software Experimental.   Os alunos foram caracterizados como novatos,  pois tinham  apenas experiência acadêmica com a especificação de caso de uso  e user story. Somente um participante da pós -graduação já havia  trabalhado user story e com caso de uso na indústria.   3.1.2 Artefatos. Todos os participantes assinaram um  formulário de consentimento - Termo de Consentimento Livre e  Esclarecido (TCLE), em que concordaram em fornecer seus  resultados para análise. Os artefatos utilizados neste estudo  experimental foram: a) descri ção textual de quatro  cenários de  aplicações com níveis  de complexidade semelhantes ; b) modelo  de especificação de caso de uso e user story; c) artefatos para  criação da especificação; d) artefatos para criação dos mockups ;  e) artefatos para a inspeção dos  mockups; f) artefato para  descrição de e-mail de comunicação ; g)  questionários de  avaliação de especificação, avaliação de mockup e avaliação  geral. Os 4 cenários utilizados no estudo foram:  Cenário 1 : Um sistema de  restaurante para pessoas com  restrição alimentar onde é possível clientes procurarem  restaurantes que oferecem comidas diferenciadas e também  realizar o pedido sem sair de casa. As funcionalidades do sistema  são: cadastro de cliente, pesquisar restaurantes especializados em  algum tipo de restrição alimentar e realização de pedido online.  Cenário 2: Um sistema de planejamento de viagens para as  pessoas que criam seus roteiros de viagens. As funcionalidades  do sistema  são: cadastro de roteiros de viagens, cadastro de  informações de voos, reserva de hotéis e controle de dinheiro.  Cenário 3: Um sistema de acompanhamento de rotinas de  idosos para as famílias e para os idosos terem seus dados de  rotina disponível para consulta. As funcionalidades do sistema   são: cadastro de consultas médicas, cadastro e lembrete  de  remédios utilizados e registro de informações do idoso.   Cenário 4: Um sistema para facilitar a troca de livros entre  seus usuários. As funcionalidades do sistema  são: cadastro de  livros, disponibilizar livros para troca e gerenciar de troca.  3.1.3 Treinamento. Todos os participantes receberam  treinamento de 2 h30min em um mesmo ambiente sobre os dois  tipos de especificação. Durante o trei namento também foram  realizados exercícios práticos de especificação em casos de uso e  216', 'user stories.  3.1.4 Inspeção dos mockups. Para identificar os defeitos, foi  criado um formulário de inspeção baseado em Travassos et al.  [32] e no Método de Avaliação de Comunicabilidade (MAC) [25],  conforme apresentado na Tabela 1. Foram adotadas essas  nomeclaturas diferenciadas do MAC para d espertar o interesse  dos alunos na atividade de inspeção dos mockups.  Tabela 1: Classificação de defeitos entre especificação e  Mockups  Categoria Descrição Exemplo  Onde tá?  Informações que  foram descritas na  especificação e não  foram inseridas no  mockup  Na especificação tem a  descrição: “O cliente  seleciona opção Salvar”. No  mockup não tem o botão  Salvar   Não era  assim!   Informações que  foram descritas na  especificação e  inseridas no  mockup de outra  forma  Na especificação tem a  descrição: “O cliente  seleciona a opção Salvar”  No mockup o nome do  botão é Inserir   O que é isso? Informações que  foram inseridas no  mockup e que não  estavam descritas  na especificação  No mockup tem a opção  Salvar e Cancelar. Na  especificação só tem a  descrição da opção Salvar.   Falta de  dependência!  Informações que  foram descritas  como um cálculo ou  informação  derivada de outra  que no mockup não  obedeceram essa  dependência  No mockup tem que mostrar  um campo idade e preço da  entrada do cinema, ambos  dependentes da data de  nascimento. Ou seja, campo  idade e preço deve ser  calculado a partir da data de  nascimento e não inseridos.     3.1.5 Inspeção da especificação. Para avaliar as especificações  de casos de uso e user stories geradas pelos grupos, foi criado um  checklist de inspeção. A técnica de checklist foi escolhida por ser  uma das mais utilizadas para inspecionar casos de uso [ 4] e fo i  adaptada para inspecionar também as user stories . Com a  inspeção é possível reduzir esforços em relação ao retrabalho na  correção de defeitos em artefatos durante o desenvolvimento de  software [20]. Desse modo, f oi elaborado um checklist de  inspeção baseado em Travassos et al. [32] e Anda et al. [3] com o  objetivo de identificar defeitos na especificação de casos de uso e  user stories, como, omissão de regras de negócio, o missão de  fluxos alternativos e fluxo de exceções para caso de uso e  omissão de cenários para user stories . Travassos et al.  [32]  apresentam as classes de defeitos que comumente são  encontrados em artefatos de soft ware: Omissão (Informações  necessárias foram omitidas do caso de uso), Fato Incorreto  (Algumas informações no caso de uso contradizem a lista de  requisitos ou do conhecimento geral do domínio do sistema),  Inconsistência (As informações em uma parte do caso  de uso  estão inconsistentes com outras no caso de uso), Ambiguidade  (As informações no caso de uso são ambíguas, isto é, é possível  ao cliente, desenvolvedor ou testador interpretar as informações  de diferentes maneiras podendo não levar a uma implementaç ão  correta) e Informação Estranha (As informações fornecidas não  são necessárias para o caso de uso). A técnica de Anda et al. [3]  apresenta itens de verificação que auxiliam de forma mais  detalhada quais informações as especificações de caso de uso  devem conter para atender aos requisitos de qualidade de um  caso de uso, esses itens foram adaptados para a user story. Desta  forma os itens de verificação foram organizados seguindo a  taxonomia de Travassos et al. [32] como mostra a Tabela 2, itens  de verificação para especificação de caso de uso e itens de  verificação para especificação de user story. Os checklists foram  revisados por três pesquisadores antes de serem utilizados para a  inspeção das especificações.   Tabela 2: Exemplos dos Itens de Verificação para Caso de  Uso (UC) e User story (US)  Categoria  Código Itens de Verificação  Omissão UC_OMI _01 Faltou identificar/descrever o  nome do(s) ator(es)?  US_OMI _01 Faltou descrever a user story do  cenário?  Fato incorreto UC_FIN _01 O nome dado ao caso de uso', 'Omissão UC_OMI _01 Faltou identificar/descrever o  nome do(s) ator(es)?  US_OMI _01 Faltou descrever a user story do  cenário?  Fato incorreto UC_FIN _01 O nome dado ao caso de uso  expressa corretamente seu  objetivo?  US_ FIN_ 05 A descrição no campo “Dado que”  do cenário BDD está  incompleta/incorreta?  Inconsistência UC_INC_03 O sequenciamento nos fluxos  principal, alternativos e de  exceções estão coerentes?  US_INC_02  A descrição do cenário está  inconsistente com o  comportamento da funcionalidade  do sistema?  Ambiguidade UC_AMB_01 Os nomes de atores refletem seus  papéis? Ou estão ambíguos  (podem levar a dupla  interpretação)?  US_AMB_01 O nome do cenário permite  interpretações diferentes do seu  objetivo?  Informação  estranha  UC_ IES_02  Informações descritas nas regras  de negócio fazem parte do  contexto do caso  US_IES_01 Informações descritas nos  cenários não fazem parte do  contexto das funcionalidades do  sistema?    Foram criados graus de severidade para cada tipo de defeito.   Por exemplo, a severidade do tipo “Grave” foi utilizada para  classificar defeitos de omissão, ou seja, informações que não  foram descritas no caso de uso e na user story. Os defeitos de  severidade “Média” foram utilizados para classificar informações  que não foram descritos por completo ou que foram descritos de  217', '5  forma incorreta. Os defeitos de severidade “Baixa” identificaram  aqueles defeitos que não prejudicavam a compreensão e  entendimento do caso de uso e da user story.  3.2 Execução do Estudo  Este experimento foi executado em 3 etapas: 1) especificação dos  requisitos, 2) construção dos mockups e 3) inspeção para  investigar se os mockups estavam de acordo com o que  foi  especificado. Na etapa 1  os participantes foram divididos  aleatoriamente em quadro grupos (1 a 4)  com 4 pessoas cada  e  receberam os cenários para fazerem as especificações com user  story e caso de uso e modelos para serem seguidos . Neste ponto,  destaca-se que apesar de cada cenário conter várias  funcionalidades, cada grupo especificou apenas duas com grau  de complexidade equivalentes entre si.  A Tabela 3  apresenta a   divisão dos grupos, cenários e especificações durante o estudo.  Tabela 3: Distribuição dos grupos, cenários e especificações  Grupos Cenários Tipos de especificação  1 1 User story  2 2 User story  3 3 Caso de uso  4 4 Caso de uso    Na etapa 2 a principal atividade foi a construção dos mockups  baseados nas especificações  criadas. Destaca-se q ue cada grupo  recebeu uma especificação de caso de uso  e user story diferente  da que especificou na etapa 1, por exemplo, equipe 1 recebeu um  cenário e fez a especificação de requisitos  na etapa 1, na etapa 2  de construção de mockups, a equipe  1 recebeu as especificações  da equipe 2 e 3 para construir o mockup , conforme mostrado na  Tabela 4. Durante a construção dos mockups, os grupos podiam  tirar dúvidas com o grupo especificador através de bilhetes. Para  isto, foi utilizado uma folha de papel onde o grupo de construção  escrevia sua dúvida , o  moderador entregava -o para o grupo  especificador responder o questionamento e, por fim, o papel era  devolvido para a grupo de construção.  Esse procedimento foi  inserido no estudo com o intuit o de evitar a propagação de   defeitos inseridos na especificação para os mockups.  Tabela 4: Distribuição de construção dos mockups  Grupos Construção de Mockups dos cenários  1 3 e 2  2 4 e 1  3 1 e 4  4 2 e 3    Nesta etapa os grupos não fizeram o mockup das  especificações que tinham criado no anterior, havendo o cuidado  de garantir o rodízio da especificação. Por exemplo, quem  especificou com user story  construiu o primeiro mockup  utilizando uma especificação de caso de uso e depois construiu  um mockup com user story. Dessa forma, pro curou-se reduzir o  viés de aprendizado no tipo de especificação.  Na etapa 3 foi realizada a inspeção dos mockups pelos gru pos  que fizeram a especificação . Os grupos que criaram as  especificações receberam os mockups criados pel os outro s  grupos para identificar os defeitos  entre o que foi especificado  por eles e o que foi criado nos mockups pel os grupos de  construção. Dessa forma, o  grupo 1 verificou os defeitos dos  mockups criados pel os grupos 3 e 2. O grupo  2 verificou os  defeitos dos mockups criados pel os grupos 4 e 1. O grupo 3  verificou os defeitos dos mockups criados pelos grupos 1 e 4. E o  grupo 4 verificou os defeitos dos mockups dos grupos 2 e 3.  4 ANÁLISE DOS RESULTADOS  4.1 Resultados Quantitativos   Nesta seção, são apresentados  os resultados  quantitativos  referentes à análise, feita pelos pesquisadores, das especificações  de caso de uso e user stories criada pelos participantes , análise  dos defeitos cometidos ao criar os mockups a partir das  especificações e a relação entre as especificações e mocku ps  criados, visando à comunicação dos requisitos dentro dos grupos.  4.1.1 Análise da especificação . Para avaliar a corretude  das  especificações, criadas pelos participantes do estudo, os  pesquisadores realizaram inspeções nas especificações usando os  checklists de inspeção para auxiliar na verificação de defeitos da  especificação de casos de uso e user stories de forma guiada. Cada  item identificado como incorreto, impactou no grau de corretude', 'especificação de casos de uso e user stories de forma guiada. Cada  item identificado como incorreto, impactou no grau de corretude  da mesma. O processo de avaliação das especificações foi  realizado em duas etapas. Na primeira etapa dois , pesquisadores  avaliaram as es pecificações que os participantes elaboraram a  partir dos cenários entregues (duas especificações de caso s de  uso e duas especificações user stories). Na segunda etapa, três  pesquisadores revisaram por completo a  avaliação realizada e  retiraram as discrepâncias. A Tabela 5 apresenta uma parte dessa  avaliação.   A avaliação das especificações de caso de uso e user story  resultou em 45 defeitos, dos quais, 24 defeitos foram  identificados nas especificações de user stories dos grupos 1 e 2 e  21 defeitos foram identificados nas especificações de casos de uso  dos grupos 3 e 4. A Fig. 1  mostra o resultado quantitativo dos  diferentes tipos de defeitos identificados nas especificações.      Figura 1:  Números de defeitos encontrados nas  especificações de user stories e caso de uso.  218', 'Tabela 5: Classificação da avaliação das especificações  Cód. do  defeito  Descrição do defeito  encontrado  Nome do  UC  Elem. do  template  UC_AMB_04  Na pré-condição ficou  estranho à frase: “Estar  logado” – O usuário  deve estar logado,  como descrito no caso  de uso Cadastrar um  livro  Solicitar  livro  Pré- condição  UC_OMI_03  Faltou escrever o fluxo  de exceção livro já  cadastrado  Cadastrar  livro  Fluxo de  exceção  UC_INC_03  O fluxo de exceção 1  deveria retornar para o  passo 2 do fluxo  principal, já que já  atingiu o limite de  solicitações  Solicitar  livro  Fluxo de  exceção  UC_FIN_03  No passo 3 faltou  descrever o ator que  faz a ação  Cadastrar  remédio  Fluxo de  exceção    O maior número de defeitos encontrados nas especificações  de user stories  foram de Fato Incorreto (9), seguido por  Ambiguidade (6),  Omissão (5), Inconsistência (2) e Informação  estranha (2). Nas especificações de caso de uso os defeitos mais  encontrados foram Omissão (9), Ambiguidade (9) seguido por  Fato incorreto (2) e Inconsistência (1). Não foram encontrados  defeitos de Informação entranha. A Fig. 2  mostra o número de  defeitos encontrados nas especificações de user stories e casos de  uso distribuídas por grau de severidade. A especificação de user  story teve maior número de defeitos do tipo Médio (17), seguido  de Grave (5) e Baixo ( 2). Vale ressaltar que o tipo Grave na user  story foi a omissão de cenários nas especificações. Na  especificação do grupo 2 foram encontrados 8 defeitos, onde 4  eram omissões de cenários importantes para o desenvolvimento  da funcionalidade. A especificação  de caso de uso teve maior  número de defeitos do tipo Médio (10), seguido de Grave (9) e  Baixo (2). Os defeitos do tipo grave em caso de uso estão  relacionados a omissão de fluxo de exceção e regras de negócio.    Figura 2: Número de defeitos por severidade  4.1.2 Análise dos mockups.  A avaliação dos mockups em  relação à especificação resultou em 25 defeitos, dos quais, 11  foram identificados nos mockups construídos a partir de  especificação de user story  e 14 identificados nos mockups  construídos a partir de espec ificação de caso de uso. A Tabela 6  mostra o número dos diferentes tipos de defeitos identificados na  inspeção dos mockups.   Tabela 6: Defeitos encontrados nos mockups  Especificação Onde  tá?  Não era  assim!  Falta de  dependência!  O que é  isso?  User story 2 4 1 4  Caso de uso 4 4 4 2    Nos mockups criados a partir de especificação de user story, o  maior número de defeitos encontrados foram do tipo “Não era  assim!” (4) e “O que é isso?” (4), seguido de “Onda tá?” (2) e  “Falta de dependência” (1). Nos mockups c riados a partir de  especificação de caso de uso, o maior número de defeitos foram  do tipo “Onde tá?”, “Não era assim!” e “Falta de dependência”  com 4 defeitos cada, seguido de “O que é isso?”, com 2.  Além disso, foi realizada uma avaliação da completude do s  mockups criados pelos grupos em relação ao que foi  especificado. Para avaliar a completude dos mockups, foi  verificado se cada informação definida na especificação estava  representada de forma completa no mockup. Cada item  identificado como incompleto im pactou na completude do   mesmo, os resultados são apresentados na Fig. 3.       Figura 3: Corretude dos mockups criados  Percebe-se que não houve diferença significativa quanto à  completude dos mockups. A completude do s mockups criados  baseados em user stories pelos grupos 2 e 4 foi um pouco maior  do que utilizando caso de uso. Já os grupos 1 e 3 tiveram um  desempenho melhor utilizando o caso de uso como base para a  construção do mockup.   4.1.3 Análise de propagação de defeitos. Quanto a questão de  propagação de defeitos, ou seja, a quantidade de defeitos  inseridos nos mockups devido a defeitos inseridos previamente  nas especificações, destaca -se que enquanto os mockups criados  a partir de user stories apresentaram 36% de defeitos nesta', 'inseridos nos mockups devido a defeitos inseridos previamente  nas especificações, destaca -se que enquanto os mockups criados  a partir de user stories apresentaram 36% de defeitos nesta  situação, os mockups criados a partir de casos de uso  apresentaram 0% de defeitos nesta situação. Nesta seção, serão  apresentadas algumas respostas para esta questão.  219', 'Figura 4: Defeitos especificações x defeitos mockups  Defeitos inseridos na especificação de user stories: com o intuito  de prover uma visão geral sobre defeitos inseridos nos mockups  e defeitos inseridos na especificação, a Fig. 4  apresenta um  gráfico com a frequência de cada defeito, bem como, a relação  entre eles. A área do gráfico com fundo cinza destaca os defeitos  existentes nos mockups e especificações que não possuem  relação entre si.  Quanto aos defeitos que possuem relação com propagação de  defeitos, destaca -se que, dos 11 defeitos encontra dos nos  mockups, 4 (36%) foram gerados a partir de defeitos nas  especificações de user stories, os quais estão detalhados na Fig. 5.      Figura 5: Defeitos propagados para mockups   Quanto a propagação de defeitos dos mockups e  especificações, a área do gráf ico com fundo cinza da Fig. 4 ,  mostra que enquanto os mockups possuem 7 (74%) defeitos nesta  situação – “Onde está?” (1), “Não era assim” (4), “O que é isso?”  (2), as especificações possuem 20 (83%) defeitos nesta situação –  Omissão (5), Fato incorreto (8) , Inconsistência (2) e Ambiguidade  (5). Estes resultados indicam que a maioria dos defeitos não  possuem relação de propagação.  Devido à especificação do cenário 2 ter sido simplista,  possuindo um alto grau de omissões de user stories, os mockups  apresentaram poucos defeitos de construção, pois a simplicidade  da especificação deixava pouca margem para defeitos. Neste  caso, os grupos não inseriram nenhum defeito ao construir os  mockups.   Defeitos inseridos na especificação de Caso de Uso:  com o  intuito de pro ver uma visão geral sobre defeitos inseridos nos  mockups e defeitos inseridos na especificação, a Fig. 4 apresenta  um gráfico com a frequência de cada defeito.  O gráfico mostra,  também, que não houve relação de propagação de erro ao utilizar  especificação de caso de uso.   Nenhum defeito encontrado nos mockups foram gerados  pelos defeitos na especificação de caso de uso. Os defeitos  encontrados nos mockups criados pelos grupos foram  relacionados a: não inserções de informações que estavam  descritas, alteração de mensagem no mockup, inserção de botões  não especificados; não indicação da navegabilidade entre as telas  e apresentação dos campos de forma incorreta.  Foi observado que os defeitos dos mockups não estavam  relacionados aos defeitos inseridos na especificação dos cenários,  assim como mostra a Fig. 4 . Os defeitos com maior frequência  nas especificações foram: Omissão (9) e Ambiguidade (9).  Não houve propagação do defeito de Omissão inserido na  especificação, pois como não havia informação especificada, os   mockups construídos não consideraram tais informações,  portanto não houve como errar. Assim, os grupos não inseriram  nenhum defeito ao construir o mockup.   Quanto aos defeitos nas especificações relacionados a   Ambiguidade, destaca -se que uma das técnicas definidas na  Seção 3.2 Execução do estudo – utilização de bilhete de  comunicação para tirar dúvidas – quando utilizada evitou a  inserção dos defeitos do tipo “O que é isso?” (3) e “Não era  assim” (1) nos mockups. Esta situação indica a importância da  equipe esclarecer defeitos do tipo “Ambiguidade” com o emissor  da especificação e não tomar decisões por conta própria. Em  projetos reais pode-se substituir os bilhetes por e -mails, reuniões  ou videoconferências.   220', 'No contexto deste artigo, o termo emissor foi u tilizado para  representar o participante que especifica os requisitos dos  clientes no formato de caso de uso ou user story . E para o  participante que recebe a especificação para desenvolver sua  atividade foi utilizado o termo receptor [21].  4.2 Resultados Qualitativos  Nesta seção, são apresentados  os resultados  de uma análise  qualitativa das respostas dos participantes (P) aos questionários  aplicados ao longo da execução do estudo, com o objetivo de  avaliar a utilização das especificações de cas o de uso e  user story  para a comunicação de requisitos.  Os métodos qualitativos  segundo Seaman [26] apoiam uma melhor compreensão das  questões que necessitam de uma análise mais específica e  detalhada, permitindo ao pesquisador considerar o  comportamento humano e entender completamente o objeto  estudado. A análise qualitativa realizada neste trabalho baseia -se  em procedimentos d e codificação de Grounded Theory [30], mais  detalhes no relatório técnico [23].  Enquanto eram analisados os dados contidos no questionário,  foram criados códigos associados  com fragmentos de texto.  Outro pesquisador analisou os códigos  relacionados com as  citações em cada transcrição do questionário. Este pesquisador  verificou os códigos e categorias para validar o processo de  codificação e, portanto, mitigar o viés eventualmente causado  pela participação de um único pesquisador no processo de  codificação.   4.2.1 Análise dos resultados sobre especificação. Foram  identificadas as dificuldades encontradas pelos participantes  (P)  na utilização dos modelos de especificação:  - Dificuldade com o campo ‘quando’ da user story (ver citação  de P04 e P05 abaixo).  “(…) Um pouco de dificuldade para especificar as pré -condições  que algumas vezes têm semelhança com os passos do ""quando"" ” –  P04  “(…) esqueço as vezes a função do campo ""quando""” – P05   - Dificuldade com os fluxos do caso de uso  (ver citação de  P10 e P16 abaixo).   “(…) quando apareceu uma condição (se seria um fluxo  alternativo ou um fluxo de exceção)” – P10  “(…) nos momentos de especificar os fluxos alternativos (como  verificar o passo em que este será chamado)” – P16  Os participantes que apresentaram dificuldades na utilização  da estrutura de especificação das user stories e casos de uso   indicaram que pode haver um problema no modelo de  especificação ou a inexperiência nos modelos pode atrapalhar na  especificação dos requisitos.  Quanto à comunicação de requisitos a través de user stories,  um participante afirmou que user story é capaz de comunicar  requisitos para todos os integrantes da equipe de  desenvolvimento (P01). Os participantes P03 e P04 afirmaram  que depende do tamanho do projeto e equipe, como mostram as  citações abaixo:   “(…) cada cenário mostra as regras que devem ser  seguidas/inseridas no sistema” – P01  “(…) user story  é super eficaz para projetos pequenos de  desenvolvimento de software” – P03  “(…) depende muito do tamanho da equipe e da forma de  trabalho” – P04  No entanto, os participantes P02 e P05 afirmaram que user  story não é suficiente para comunicar requisitos, pelas seguintes  razões:  “Não, pois existem vários outros passos que não ficam  totalmente claros para seguir” – P02  “Não é tão bom para representar os fluxos de exceção” – P05  Quanto a comunicação de requisitos utilizando a  especificação de caso de uso somente um participante (P16)  discordou que caso s de uso são capazes de comunicar requisitos  para todos os integrantes da equipe de desenvolvimento, como:  “a especificação não detalha exatamente a consequência das  ações tomadas pelo usuário, por exemplo, além de agrupar muitas  informações em um único local ou que pode gerar confusão ao  seguir os fluxos” – P16  Ao verificar se os participantes que u tilizaram user story para  especificar utilizariam outro artefato complementar¸ foram  listados os seguintes artefatos que poderiam tornar a', 'Ao verificar se os participantes que u tilizaram user story para  especificar utilizariam outro artefato complementar¸ foram  listados os seguintes artefatos que poderiam tornar a  especificação mais completa: Protótipo (P01), Detalhes do  usuário (P02), Caso de uso (P03, P05 e P06) e Personas (P04, P07 e  P08). Os que utilizaram a especificação de caso de uso apontaram  os seguintes artefatos que poderiam tornar a especificação mais  completa: Diagrama de caso de uso (P11), Protótipos (P15) e User  stories (P09, P12 e P16).   4.2.2 Análise dos resultados sobre criação de mockups . Quanto  à utilização das especificações de caso de uso e user story para a  criação dos mockups foram encontradas três categorias de  problemas de comunicação: problema de especificação por parte  do emissor, p roblema no modelo de especificação e problema de  entendimento do modelo de especificação.  A Tabela 7 mostra  algumas citações referentes a estes problemas.  Todas as citações  relacionadas são mostradas em [23].  Tabela 7: Exemplos de códigos sobre problemas  Categorias Citações   Problema de  especificação  por parte do  emissor  “(…) alguns fluxos pareciam que levavam para  duas telas ao mesmo tempo” – P05 - UC  “Na construção do mockup percebemos que  algumas informações estavam faltando” – P10   - US  Problema no  modelo de  especificação  “(…) excesso de fluxos, algum elemento pode  passar batido, pois as informações ficam  espalhadas” – P16 - UC  “a especificação é falha em algumas  informações que são necessárias em cenários  adjacentes” – P09 - US  Problema de  entendimento  do modelo de  especificação    “(…) como tinha fluxo alternativos, foi  necessário buscar informações fora do fluxo  principal e isso dificulta a extração” - P11 - UC  “tive dificuldade na parte de ""dado que"" pois  fiquei em dúvida se os dados eram pré-condições  ou se eram para ser inseridos” - P05 - US    4.2.3 Análise dos resultados gerais.  Os resultados do  221', '3  questionário final, verificou -se na percepção dos participantes  qual das especificações, caso de uso ou user story, detalhava mais  as informações par a a construção dos mockups. Os argumentos  relacionados às preferências por user story foram:  “(…) user story mostra melhor as interações das ações” - P02  “(…) user story foi mais detalhista, pela possibilidade de criar  cenários separados” – P04  Os argumentos relacionados às preferências por caso de uso  foram:  “(…) a especificação com mais detalhes de informação para  construção de mockup é o caso de uso, porque as informações de  regra de negócio, por exemplo, deixam mais detalhada” – P10  “(…) casos de uso de talham melhor todos os fluxos, fornecendo  maiores detalhes das ações” – P16  Vale ressaltar que dos 8 participantes que especificaram com  user story somente 3 acham que caso de uso detalha mais as  informações para a construção dos mockups e dos 8  participantes que especificaram com caso de uso, somente um  acredita que user story detalha mais para construção do mockup.  5 DISCUSSÃO  Durante a análise d a totalidade de defeitos encontrados, a user  story totalizou 24 defeitos contra 21 de caso s de uso. Apesar da  especificação de caso de uso ter apresentado 9 omissões contra 5  da user story, os problemas gerados com as omissões da user story  foram mais impactantes, pois tratava-se de informações  essencias para a construção dos mockups.   Comparando o número de defeitos encontrados nos mockups  identificados na inspeção, foi observado o total de 11 defeitos nos  mockups criados util izando user story e 14 defeitos nos mockups  criados utilizando UC. Os mockups criados com caso de uso  tiveram mais defeitos do tipo “Onde tá? ”, “Não era assim! ” e  “Falta de dependência! ”. Esses defeitos podem ter sido  ocasionados por problema de entendimen to do modelo de  especificação.  Como mostrado na Tabela 7, os problemas de comunicação de  requisitos encontrados foram: 1) Problema de especificação por  parte do emissor, 2) Problema no modelo de especificação e 3)  Problema de entendimento do modelo de espe cificação. Estes  problemas podem ter sido ocasionados pela falta de experiência  dos participantes nestes tipos de especificação . Sugerindo assim  que o fator humano não deve ser negligenciado na dinâmica da  comunicação de requisitos dentro de equipes de  desenvolvimento.   Neste estudo, existiram algumas ameaças que podem afetar a  validade dos resultados e que foram mitigadas quando possível.  As principais ameaças foram:  (1) Efeitos de treinamento: os  participantes receberam treinamento equivalente nos dois ti pos  de especificação, incluindo atividades teóricas e exercícios  práticos em sala de aula; (2) Uso de cenários: foi minimizada  utilizando cenários escritos em linguagem natural, onde os  requisitos deste cenário estavam explícitos, de forma similar aos  exercícios realizados durante o treinamento ; (3)  Representatividade dos participantes: embora os sujeitos deste  estudo fossem estudantes, Höst [14] afirma que estudantes  podem representar uma população de profissionais  da indústria;  (4) Tamanho e h omogeneidade da amostra: devido ao número  limitado de participantes, também uma vez que todos os  participantes eram estudantes da mesma instituição, os  resultados deste estudo não podem ser considerados conclusivos.   6 CONCLUSÃO  Este artigo apresentou um estudo experimental exploratório, que  comparou a dinâmica da comunicação de requisitos e seus  resultados utilizando especificação de caso de uso e user stories  como base para a criação de mockups. A questão de pesquisa  investigada foi avaliar se a comunicação de requisitos para a  construção de mockups é afetada de forma diferente ao se usar  as especificações de casos de uso ou user stories.  De acordo com a análise realizada sobre os resultados  alcançados, foi identificad o que a especificação de caso de uso  gerou mais defeitos na parte da construção dos mockups e', 'De acordo com a análise realizada sobre os resultados  alcançados, foi identificad o que a especificação de caso de uso  gerou mais defeitos na parte da construção dos mockups e  menos defeitos na parte de especificação. Entretanto, destaca-se  que a quantidade e o impacto desses defeitos no resultado final  não são suficientes para determin ar qual das duas especificações  é melhor ou pior para a comunicação de requisitos entre equipes  de desenvolvimento de software.  Os resultados mostraram que  não há diferença significativa que sustente escolher por uma  forma de especificação em detrimento da  outra. Então, sob esta  ótica, as equipes de desenvolvimento de software que estiver em  com dúvida em qual das formas de especificação adotar, podem  optar tanto pela utilização de user story quanto caso de uso.  Percebe-se, também, que vários defeitos encont rados nos  mockups não foram causados por defeitos na especificação, mas  originaram-se por fatores relacionados  à proatividade do  receptor sem considerar o que estava especificado e sem  considerar a necessidade de validação por parte do emissor. Este  tipo d e proatividade deve ser desencorajado, pois pode gerar  mais problemas do que benefícios.  No entanto, foi observado que  a partir da análise geral dos  resultados, indícios que o fator humano foi uma das causas de  geração e propagação dos defeitos, além da pa rte técnica. Para  minimizar esses tipos de problemas, deve-se identificar quais os  defeitos possuem natureza essencialmente técnicas e quais  defeitos são advindos de fatores humanos. Os defeitos de  natureza técnicas estão relacionados com o artefato utiliz ado  para especificação dos requisitos e poderão ser evitados ou  eliminados através de treinamentos, definições de padrões  (nomenclaturas, escritas , templates ) e inspeção nas  especificações. Os defeitos de natureza humana  estão  relacionados com o profission al que utilizada o artefato em suas  atividades, podem estar relacionados à falta de conhecimento do  domínio do problema, falta de experiência, falta de compromisso  e deve-se procurar meios de minimizar esses tipos  de  interferência.   Neste estudo, existiram  algumas limitações que devem ser  consideradas. As principais limitações foram: (1)  utilização de  cenários com níve is de complexidade baixo s - foi adotado esse  nível de complexidade pois  o estudo foi realizado em ambiente  acadêmico e o tempo para execução das atividades foi limitado;  (2) aspectos avaliados - tratou-se a representação das  funcionalidades de interação  e não o aspecto de interferência  222', 'entre requisitos;  (3) avaliação da comunicação  - a comunicação  entre diferentes stakeholders não foi abordada, pois o foco foi  apenas na comunicação dentro da equipe de desenvolvimento.   Essas limitações são pontos a serem melhorados em estudos  futuros.  Como trabalhos futuros, pretende-se reaplicar este estudo  com novos alunos e na indústria para identificar outras causas de  problemas de comuni cação de requisitos . E, com o objetivo de  verificar como ocorre a comunicação de requisitos na fase de  implementação e testes, pretende -se entender o experimento  para outras fases do ciclo de vida do desenvolvimento de  software. Pretende-se, também, investigar quais fatores humanos  são capazes de influenciar a propagação de defeitos na  comunicação de requisitos em equip es de desenvolvimento de  software e como podemos minimizar esse tipo de problema .  Além disso , pretende-se realizar um levantamento sobre as  principais técnicas que utilizam linguagem d o contexto para  comparar o resultado com as técnicas deste trabalho.  AGRADECIMENTOS  Os autores agradecem o apoio financeiro do CNPq processo  423149/2016-4, CAPES processo 175956/201 3, da FAPEAM ,  processo 062.00578/2014, da  FAPERJ (projetos E- 26/010.000883/2016, E -211.174/2016), UNIRIO (PQ -UNIRIO  01/2016 e 01/2017) e também aos participantes do estudo.  REFERÊNCIAS  [1] A. Al-Rawasa and  S. Easterbrook. 1996. Communication problems in  requirements engineering: A field study, In  Proc. of Conf. on Professional on  Awareness in Software Engineering, London, 47–60.   [2] B. Anda, H. Dreiem, D. Sjøberg and M. Jørgensen. 2001. Estimating software  development effort based on use cases – experiences from industry. In UML  2001 - The Unified Modeling Language. Modeling Languages, Concepts, and  Tools. Springer, Berlin Heidelberg, v. 2185, 487–502.  [3] B. Anda, K. Hansen and G. Sand. 2009. An investigation of use case quality  in a large safety -critical software development project. In Information and  Software Technology , v . 51 , n. 12, 1699 –1711. DOI:  https://doi.org/10.1016/j.infsof.2009.04.005.  [4] B. Anda and D. Sjøberg. 2002. Towards an inspection technique for use case  models. In Proc. of the  14th Intl . Conf. on Software Engineering and  Knowledge Engine ering, ACM, NY, USA,  127–134. DOI:  http://dx.doi.org/10.1145/568760.568785.  [5] A. Belgamo and Luiz E. G. Martins. 2000. Estudo comparativo sobre as  técnicas de elicitação de requisitos do soft ware. In XX Congresso Brasileiro  da Sociedade Brasileira de Computação (SBC).  [6] E. Bezerra.  2007. Princípios de Análise e Projeto de Sistemas com UML,  Campus, 2ª edição.  [7] A. Ceverino e   Fernando P. Nascimento. 2016. Utilização da técnica de  desenvolvimento orientado por comportamento (BDD) no levantamento de  requisitos. Revista Interdisciplinar Científica Aplicada, v.10, n.3, 40-51, TRIII  2016. ISSN 1980-7031.  [8] D. Chelimsky , D. Astels, B. Helmkamp , D. North, Z. Dennis and A.   Hellesoy. (2010). Th e RSpec Book: Behaviour Driven Development with  Rspec. Cucumber, and Friends, Pragmatic Bookshelf.   [9] A. Cockburn. 2001. Writing Effective Use Cases, Vol. 1, Addison -Wesley,  Boston.  [10] E. Evans. 2003. ""Domain-Driven Design: Tacking Complexity In the H eart  of Software"". Boston, MA, USA: Addison-Wesley.  [11] Daniel M. Fernández , S. Wagner, M. Kalinowski , M. Felderer, P. Mafra, A.  Vetrò, T. Conte, et al.. 2016. Naming the pain in requirements engineering.  Empirical Software Engineering , 1 -41.DOI: http://dx.doi.org/10.1007/s10664- 016-9451-7.  [12] K. Gohil, N. Alapati and S. Joglekar. 2011. Towards behavior driven  operations (bdops). In: Intl. Conf. on Advances in Recent Technologies in  Communication and Computing, 262 -264.  [13] B. Hoisl, S. Sobernig and M.  Strembeck. 2014. Comparing Three Notations  for Defining Scenario -based Model Tests: A Controlled Experiment In 9th  Intl. Conf. on the Quality of Information and Communications Technology', 'for Defining Scenario -based Model Tests: A Controlled Experiment In 9th  Intl. Conf. on the Quality of Information and Communications Technology  (QUATIC 2014), 180-189. DOI: https://doi.org/10.1109/QUATIC.2014.62.  [14] M. Höst, B. Regnell and C. Wohlin . 2000.  Using Students as Subjects – A  Comparative Study of Students and Professionals in Lead -Time Impact  Assessment. In: Empirical Software Engineering. v. 5, n. 3, 201–214.  [15] I. Ibriwesh, Sin-Ban Ho, I. Chai and Chuie-Hong Tan. 2017. A Controlled  Experiment on Comparison of Data Perspectives for Software Requirements  Documentation. Arabian Journal for Science and Engineering, 1-15.  [16] I. Jacobson1987. Object-oriented development in na industrial e nvironment.  In Conf. on Object -Oriented Programming, Systems, Languages &  Applications, 183– 191.  [17] S. Lauesen and Mohammad A. Kuhail.  2012. Task descriptions versus use  cases. Requirements Engineering, v. 17, n. 1, 3 -18. DOI:  https://doi.org/10.1007/s00766-011-0140-1  [18] Esteban R. Luna, José I. Panach , J. Grigera, et al.  2010. Incorporating  usability requirements in a test/model -driven web engineering  approach.  In: Journal of Web Engineering, v. 9, n. 2, 132 - 156.  [19] M. Cohn. 2004. User stories  applied: For agile software development.  Addison-Wesley Professional.  [20] Rafael M. Mello, Eldânae N. Teixeira, M. Schots, Cláudia M. L. Werner and  Guilherme H. Travassos. 2014.  Verification of Software Product Line  Artefacts: A Checklist to Support Fe ature Model Inspections, Journal of  Univ. Comp. Science, v. 20, 720-745.  [21] V. Mikulovic and M. Heiss. 2006. How do I know what I have to do?: the  role of the inquiry culture in requirements communication for distributed  software development projects. I n Proc. of the 28th Intl. Conf. on Software  engineering, 921 - 925. DOI: https://doi.org/10.1145/1134285.1134453.  [22] P. Mohagheghi, B. Anda, R. Conradi. 2005. Effort estimation of use cases for  incremental large -scale software development. In 27th Intl.  Conf. on  Software Engineering , 303 –311. DOI:  https://doi.org/10.1109/ICSE.2005.1553573  [23] Ana C. Oran, E. Nascimento , G. Santos e  T. Conte. 2017. Relatório técnico:  Uma Análise sobre Comunicação de Requisitos Utilizando Especificação de  Caso de Uso e User story . TR -USES-2017-0010. Disponível em:  http://uses.icomp.ufam.edu.br/relatorios-tecnicos/.  [24] Keith T. Phalp, J. Vincent and K. Cox. 2007. Assessing the quality of use  case descriptions. In Software Quality Journal , v. 15, n. 1, 69 –97. DOI:  http://dx.doi.org/10.1007/s11219-006-9006-z.  [25] Raquel O. Prates, Clarisse S. de Souza and Simone D. Barbosa. 2000.  Methods and tools: a method for evaluating the communicability of user  interfaces interactions, v. 7, n. 1, 31-38.  [26] Carolyn B. Seaman. 1999. Qualitative methods in empirical studies of  software engineering. IEEE Transactions on software engineering, v. 25, n.  4, 557-572.   [27] Thiago R. Silva. 2016. Definition of a behavior -driven model for  requirements specification and testing of interact ive systems. In  Requirements Engineering Conf ., IEEE 24th Intl ., 444 -449. DOI:  https://doi.org/10.1109/RE.2016.12  [28] J. Smart. 2014. BDD in Action: Behavior -Driven Development for the  Whole Software Lifecycle. New York, USA: Manning Publications.  [29] K. Stapel , E. Knauss and K. Schneider. 2009. Using flow to improve  communication of requirements in globally distributed software projects. In  Requirements: Communication, Understanding and Softskills . Collaboration  and Intercultural Issues, 5 -14. IEEE. DO I:   http://dx.doi.org/10.1109/CIRCUS.2009.6  [30] A. Strauss and J. Corbin.   2014. Basics of Qualitative Research: Techniques  and Procedures for Developing Grounded Theory , in Thousand Oaks, CA,  SAGE publications.  [31] S. Tiwari and A.  Gupta. 2015. A syst ematic literature review of use case  specifications research. In Information and Software Technology, v. 67, 128 –', 'SAGE publications.  [31] S. Tiwari and A.  Gupta. 2015. A syst ematic literature review of use case  specifications research. In Information and Software Technology, v. 67, 128 – 158. DOI: https://doi.org/10.1016/j.infsof.2015.06.004.  [32] Guilherme H. Travassos, F. Shull,  M. Fredericks and V.  Basili. 1999.  Detecting de fects in object -oriented designs: using reading techniques to  increase software quality, In Proc. of XIV ACM SIGPLAN conf. on Object - oriented programming, systems, languages, and applications, v. 34, n. 10, 47 - 56. DOI: https://doi.org/10.1145/320384.320389.  [33] Yu-Cheng Tu, E. Tempero and C. Thomborson. 2016 . An experiment on the  impact of transparency on the effectiveness of requirements documents.  Empirical Software Engineering, v. 21, n. 3, 1035 -1066. DOI:   http://dx.doi.org/10.1007/s10664-015-9374-8.  [34] X. Wang, L. Zhao, Y. Wang and J. Sun.  2014. The Role of Requirements   Engineering Practices in Agile Development: An Empirical Study. In  Proc.  of the Asia Pacific Requirements Engineering Symposium, ser. CCIS. Springer,  v. 432, 195–209  [35] A. Zeaaraoui, Z. Bougroun, M. G. Belkasmi and T. Bouchentouf. 2013. User  stories template for object -oriented applications. In Innovative Computing  Technology (INTECH), 407–410.    223']","['Para quem é esse briefin? Profiisionasii de engenharsia de  ioftware que deieaam toomar  decsiiõei iobre o uio de caio de  uio ou user story para  eipecsifcaação de requsiisitooi com  baie em evsidêncsiai csientfcaia De oide vêm os resultados? Todai oi reiultoadoi deitoe brsiefng  foram extoraídai do eitoudo  expersimentoal exploratoórsio  conduzsido por Aa Ca Oran et ala O que está iicluído ieste  briefin? Oi prsincsipasii reiultoadoi do eitoudo  expersimentoala O que ião está iicluído ieste  briefin? Deicrsiaçõei detoalhadai iobre a  execuação do eitoudo e artoefatooi  utilsizadoia Para acessar outros briefins de  evidêicias sobre eineiharia de  software: http:////wwwalsiaaufcabr//ccbioft2017 //en//xxxsi-ibei// Para obter iiformações  adicioiais sobre USES: http:////uieiasicompaufamaeduabr//   ANALYSING REQUIREMENTS COMMUNICATION This briefin reports scieitfc evideice oi comparatve aialysis oi the requiremeits commuiicatoi  dyiamics  usiin  use  case specifcatoi aid  user stories  as the basis for mockups creatoin FINDINGS O  artigo  apreientoa  um  eitoudo  expersimentoal exploratoórsio,  comparando  a  dsinâmsica  da comunsicaação de requsiisitooi utilsizando eipecsifcaação de caio de uio e user story como baie para a crsiaação de mockupia A queitoão de peiqusiia sinveitigada fosi avalsiar ie a comunsicaação de requsiisitooi para a conitoruação de mockupi é afetoada de forma dsiferentoe ao ie uiar ai eipecsifcaaçõei de caioi de uio ou user storiesa Resultados  Quaittatvos :  ião  apreientoadoi  oi reiultoadoi quantitoativoi referentoei à análsiie dai eipecsifcaaçõei de caioi de uio e user stories crsiadai peloi  particsipantoei  e  a  análsiie  doi  defesitooi cometidoi  ao  crsiar  oi  mockupi  a  partir  dai eipecsifcaaçõeia Além dsiiio, sinveitigou-ie a relaação entore ai eipecsifcaaçõei e mockupi crsiadoi, vsiiando versifcar qual eipecsifcaação é capaz de paiiar melhor oi requsiisitooi para a equsipe de deienvolvsimentooa Análsiie  da  eipecsifcaação:  a  avalsiaação  dai eipecsifcaaçõei de caio de uio e user story  reiultoou em 45 defesitooi no tootoala Ai eipecsifcaaçõei no formatoo de user stories  apreientoaram 24 defesitooi contora 21 defesitooi dai eipecsifcaaçõei de caioi de uioa Apeiar dai  eipecsifcaaçõei  de  caioi  de  uio  toerem apreientoado 9 omsiiiõei contora 5 dai user stories, oi problemai geradoi com ai omsiiiõei nai user stories foram  masii  simpactoantoei,  posii  toratoava-ie  de sinformaaçõei  eiiencsiasii  para  a  conitoruação  doi mockup, como: deicrsiação de cenársioi, regrai de negócsio, nome de campoi e deicrsiação de meniagenia Análsiie doi mockupi: comparando o número de defesitooi encontoradoi na sinipeação doi mockupi, foram obiervadoi 11 defesitooi noi mockupi crsiadoi utilsizando  user  story e 14 defesitooi noi mockupi crsiadoi utilsizando caio de uioa Oi mockupi crsiadoi com caio de uio tiveram masii defesitooi do tipo “Onde toá?”, “Não era aiisim!” e “Faltoa de dependêncsia!”a Eiiei defesitooi podem toer isido ocaisionadoi por problema  de  entoendsimentoo  do  modelo  de eipecsifcaaçãoa Análsiie de propagaação de defesitooi: quantoo aoi defesitooi siniersidoi prevsiamentoe nai eipecsifcaaçõei de caioi de uio e user  stories e propagadoi para oi mockupi, deitoaca-ie que enquantoo oi mockupi crsiadoi a partir de user stories apreientoaram 36% de defesitooi neitoa isitouaação, oi mockupi crsiadoi a partir de caioi de uio apreientoaram 0% de defesitooi neitoa isitouaação,  ou  ieaa,  oi  defesitooi  siniersidoi  nai eipecsifcaaçõei  de  caioi  de  uio  não  foram propagadoi para oi mockupia Resultados  Qualitatvos :  ião  apreientoadoi  oi reiultoadoi de uma análsiie qualsitoativa dai reipoitoai doi particsipantoei aoi queitionársioi aplsicadoi ao longo da execuação do eitoudo, com o obaetivo de avalsiar a utilsizaação dai eipecsifcaaçõei de caio de uio e user story  para a comunsicaação de requsiisitooia Análsiie doi reiultoadoi iobre eipecsifcaação: ai', 'avalsiar a utilsizaação dai eipecsifcaaçõei de caio de uio e user story  para a comunsicaação de requsiisitooia Análsiie doi reiultoadoi iobre eipecsifcaação: ai dsifculdadei encontoradai na utilsizaação da eitorutoura de eipecsifcaação dai user stories  e caioi de uio foram decorrentoei à problemai noi  modeloi  de  eipecsifcaação  ou  a sinexpersiêncsia por partoe doi particsipantoeia Análsiie doi reiultoadoi iobre crsiaação de mockupi:  quantoo  à  utilsizaação  dai eipecsifcaaçõei de caio de uio e user  story para  a  crsiaação  doi  mockupi  foram encontoradai torêi catoegorsiai de problemai de comunsicaação: problema de eipecsifcaação por partoe do emsiiior (particsipantoe que eipecsifca oi requsiisitooi doi clsientoei no formatoo de caioi de uio ou user stories ), problema no modelo de eipecsifcaação e problema de entoendsimentoo do modelo de eipecsifcaaçãoa  Eitoei problemai podem toer isido ocaisionadoi pela faltoa de expersiêncsia doi particsipantoei neitoei tipoi de eipecsifcaaçãoa Iitoo iugere que o fatoor humano não deve ier neglsigencsiado na dsinâmsica da comunsicaação de requsiisitooi dentoro de equsipei de deienvolvsimentooa  Análsiie doi reiultoadoi gerasii: quantoo à análsiie  geral  fosi  sidentifcado  que  a eipecsifcaaçõei de caioi de uio geraram masii defesitooi na partoe da conitoruação doi mockupi e menoi defesitooi na partoe de eipecsifcaaçãoa Entoretoantoo, deitoaca-ie que a quantidade e o simpactoo deiiei defesitooi no reiultoado fnal não ião iufcsientoei para detoermsinar qual dai duai eipecsifcaaçõei é melhor ou psior para a comunsicaação de requsiisitooi entore equsipei de deienvolvsimentoo de ioftwarea Oi  reiultoadoi  moitoraram  que  não  há dsiferenaça isignsifcativa que iuitoentoe eicolher por  uma  forma  de  eipecsifcaação  em detorsimentoo da outoraa Entoão, iob eitoa ótica, ai equsipei de deienvolvsimentoo de ioftware que eitiverem com dúvsida em qual dai formai de eipecsifcaação adotoar, podem optoar toantoo pela utilsizaação de user story quantoo caio de uioa Percebe-ie, toambém, que vársioi defesitooi encontoradoi  noi  mockupi  não  foram cauiadoi por defesitooi na eipecsifcaação, mai orsigsinaram-ie por fatoorei relacsionadoi à proativsidade do receptoor (particsipantoe que recebe a eipecsifcaação para deienvolver iua ativsidade) iem conisiderar o que eitoava eipecsifcado noi cenársioi e iem conisiderar a neceiisidade de valsidaação  por  partoe  do emsiiiora Além dsiiio, foram obiervadoi sindícsioi que o fatoor humano fosi uma dai cauiai de geraação e propagaação doi defesitooi, além da partoe toécnsicaa  Para  msinsimsizar  eiiei  tipoi  de problemai, deve-ie sidentifcar quasii defesitooi poiiuem natoureza eiiencsialmentoe toécnsicai (relacsionadoi com o artoefatoo utilsizado para eipecsifcaação doi requsiisitooi) e quasii defesitooi ião  advsindoi  de  fatoorei  humanoi (relacsionadoi com o profiisional que utilsizada ORIGINAL RESEARCH REFERENCE Aa Ca Oran, Ea Naicsimentoo, Ga Santooi, Ta Contoea 2017a Analyising Requsirementoi Communsication  ising  ie Caie Specsifcation and User storiesa In Proceedsingi of 31ito Brazsilsian  Sympoisium on Software Engsineersing (XXXI SBES), Fortoaleza, Ceará, Brazsil, 10 pageia httpi:////dosiaorg//10a1145//3131151a3131166a']","**Title:** Comparing Use Case Specifications and User Stories for Effective Requirements Communication

**Introduction:**  
This evidence briefing summarizes the findings from a comparative analysis of requirements communication using Use Case Specifications and User Stories. The goal is to provide insights into how these two formats impact the creation of mockups and the overall communication of software requirements during development. Understanding these dynamics can help software engineering practitioners choose the most effective specification method for their projects.

**Core Findings:**  
1. **No Significant Difference in Effectiveness:** The study revealed that there is no significant difference in the effectiveness of Use Case Specifications and User Stories in communicating software requirements. Both formats yielded similar results when it came to the accuracy of the mockups created based on the specifications.

2. **Human Factors Matter:** Despite the comparable effectiveness of both specification formats, the study highlighted the importance of human factors in the communication process. Participants reported challenges related to understanding and interpreting the specifications, indicating that the effectiveness of either format can be influenced by the experience and engagement of the team members involved.

3. **Defect Analysis:** A total of 45 defects were identified in the specifications, with 24 in User Stories and 21 in Use Case Specifications. Most defects were related to omissions and ambiguities, which can significantly impact the clarity of requirements. The mockups constructed from User Stories had 11 defects, while those from Use Case Specifications had 14 defects, indicating that the choice of specification format may influence the types of errors that occur during mockup creation.

4. **Communication Challenges:** Participants noted difficulties in specifying requirements, particularly with the ""when"" field in User Stories and the flow of Use Cases. These challenges suggest that both formats have inherent complexities that can hinder effective communication if not properly understood.

5. **Recommendations for Practice:** Given the findings, teams can confidently choose either specification format based on their context and preferences. However, it is crucial to ensure that all team members are adequately trained and familiar with the chosen format to minimize misunderstandings and improve communication.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, project managers, and educators who are involved in requirements specification and communication within software development projects.

**Where the findings come from?**  
The findings are based on an exploratory empirical study conducted with 16 students from the Federal University of Amazonas (UFAM) and the Federal University of the State of Rio de Janeiro (UNIRIO), focusing on the dynamics of requirements communication using Use Cases and User Stories.

**What is included in this briefing?**  
This briefing includes an overview of the comparative analysis, key findings regarding the effectiveness of specification formats, insights into communication challenges, and practical recommendations for software development teams.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the authors and their research:**  
[http://uses.icomp.ufam.edu.br/relatorios-tecnicos/](http://uses.icomp.ufam.edu.br/relatorios-tecnicos/)

**Original Research Reference:**  
Oran, A. C., Nascimento, E., Santos, G., & Conte, T. (2017). Analysing Requirements Communication Using Use Case Specification and User stories. In Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES 2017), Fortaleza, Ceará, Brazil, September 2017. https://doi.org/10.1145/3131151.3131166"
"['Coding Dojo as a transforming practice in collaborative learning of programming: an experience report Peterson Luiz da R. Rodrigues Federal University of Pampa Av. Tiaraj´u, 810 Alegrete, RS 97546-550 petersonrodrigues@alunos. unipampa.edu.br Luiz Paulo Franz Federal University of Pampa Av. Tiaraj´u, 810 Alegrete, RS 97546-550 luizpaulofranz@alunos.unipampa. edu.br Jean Felipe P. Cheiran Federal University of Pampa Av. Tiaraj´u, 810 Alegrete, RS 97546-550 jeancheiran@unipampa.edu.br Jo˜ao Pablo S. da Silva Federal University of Pampa Av. Tiaraj´u, 810 Alegrete, RS 97546-550 joaosilva@unipampa.edu.br Andr´ea S. Bordin Federal University of Pampa Av. Tiaraj´u, 810 Alegrete, RS 97546-550 andreabordin@unipampa.edu.br ABSTRACT Learning computer programming is a challenging task as evidenced by high failure and dropout rates. Our work aims to overcome some diﬃculties of So/f_tware Engineering undergraduate students by col- laborative learning practices of computer programming. In this paper we report the experience of a project executed at Federal University of Pampa to improve the practice in programming us- ing Coding Dojos as a learning strategy. /T_he project lasted two years and had fourteen meetings. Every meeting had three dis- tinct moments: problem choosing, coding, and retrospective. We analyzed project results according to two perspectives. By Dojo Master perspective, we collected and summarized their perceptions by classifying what did it work and what did not work. In partici- pants’ perspective, we applied a survey to collect a/t_titudes, which were examined through statistics and content analysis. Intersection of results point to the following positive general /f_indings: collab- orative and joyful environment favors learning, adoption of new programming practices fosters participation, and participants feel more engaged in problem solving. On the other hand, results indi- cate some issues that shall be addressed: exposure of programming in front of classmates is a barrier, and subsequent Coding Dojos over the same programming language. CCS CONCEPTS •So/f_tware and its engineering→Agile so/f_tware development; Programming teams; KEYWORDS Coding Dojos, Collaborative Learning, Computer Programming, Test Driven Development ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or aﬃliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131180 ACM Reference format: Peterson Luiz da R. Rodrigues, Luiz Paulo Franz, Jean Felipe P. Cheiran, Jo˜ao Pablo S. da Silva, and Andr´ea S. Bordin. 2017. Coding Dojo as a transforming practice in collaborative learning of programming: an experience report. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017,10 pages. DOI: 10.1145/3131151.3131180 1 INTRODUCTION “Computer programming learning is one of the /f_irst and greatest challenges faced by computing and informatics students” [1] and this is re/f_lected in high failure rates and high dropout rates in pro- gramming courses [14]. /T_he So/f_tware Engineering undergraduate course at Universidade Federal do Pampa (UNIPAMPA) has nearly 25% of failure rate on Algorithms and Programming, Abstract Data Types, and Object-Oriented Programming classes [12]. /T_he causes that lead to these problems are many, including diﬃcult to create suitable mental models [19], lacking of logical-mathematical com- petences to solve problems [13], absence of annotating culture [21], lacking of collaborative work among students [1] and traditional teaching practices based only on intercalation: theoretical classes and practical laboratories [8]. /T_herefore, it’s important to apply new strategies to maximize', 'teaching practices based only on intercalation: theoretical classes and practical laboratories [8]. /T_herefore, it’s important to apply new strategies to maximize teaching and learning processes for students of So/f_tware Engi- neering undergraduate course. It’s also important to contribute to development of abilities and competences desired by organizations, e.g. autonomy in learning, collaborative work, and problem solving capability. Coding Dojos, as a traditional practice in so/f_tware industry [24], allow the introduction to an enhanced environment to pro- gramming learning that improves communication and cooperation among students. Since there is not necessarily a teacher or a pro- fessor role, this practice bene/f_its collaboration among pairs that exchange knowledge and reduce fear and resistance of teamwork. /T_he objective of this paper is to present a Coding Dojo experience report outside the classroom at UNIPAMPA./T_his study is related to a teaching project with duration of two years in compliance with academic calendar of UNIPAMPA. /T_his experience aimed to promote collaborative practices of programming to complement 348', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil P. Rodrigues et al. competences and abilities developed in So/f_tware Engineering un- dergraduate curriculum. We identi/f_ied diﬀerent Coding Dojo formats, we discussed needed adaptations to university environment, we ran Coding Dojo prac- tices, and, /f_inally, we compiled and analyzed (quantitatively and qualitatively) feelings about those practices from people involved. /T_his work is presented as an experience report and it is not presented as an empirical study. Nevertheless, we carry out this work based on the following guiding question: does Coding Dojo provide a collaborative environment for programming learning in academic environment? /T_his paper is organized as follows: at section 2 we show Coding Dojo practices and bene/f_its of collaborative work; at section 3 we describe detailed procedures on preparation, execution and conclusion of UNIPAMPA Coding Dojos; at section 4 we present collected data and analyze them; and at section 5 we present a snowballing process [27] to identify academic papers are related to application of Coding Dojos and how those works compare to our approach. 2 CODING DOJO Coding Dojo (and its precursor, Code Kata) has became popular as a practice for lonely training [25] and small groups in universities [4], but it quickly spread through companies [26] [24]. Currently, there are countless groups in Brazilian universities and companies to train programming in Dojos. Coding Dojos, while learning environments to programming, has been explored as a collaborative approach to problem solving. Since people involved in problem solving commonly have diﬀerent levels of ability and diﬀerent knowledges, this imbalance allows participants to collaborate and to exchange experiences so they can develop new competences. “Collaborative work starts from the principle that two or more individuals working together are able to arrive to a balanced situation in which ideas may be shared to group participants, generating new ideas and new knowledge” [1]. In some Coding Dojo formats, the process of rebalance is maxi- mized by the absence of a formal mentor1 or tutor (as a professor, a teacher or an instructor) that already knows the solution to the problem. /T_his situation requires autonomy and con/f_lict resolution abilities that favors learning by interaction and experience exchang- ing. Because learning o/f_ten demands to learn by another person experiences [15], dialogue leads to knowledge and it is always present in Coding Dojos. 2.1 Practices Coding Dojos are meetings organized around one or many program- ming challenges in which participants are encouraged to participate and share coding abilities [24]. /T_hey may be applied in diﬀerent formats that value cooperation, participation, courage, respect and simplicity [3] [6]: • Kata: this format requires a presenter (generally a mentor) with an already chosen and solved problem. Problem is shown to participants as well as an overview of solution. /T_he mentor starts to solve the problem and each step (from 1Mentor: someone with expertise on a particular domain or technology (framework, programming language, etc.) used in a Coding Dojo session. scratch to the end) must be explained to participants. Any- one can interrupt the presenter and ask for explanation at any time. • Randori: this format requires no previous problem and solution (o/f_ten the problem is suggested and chosen at the beginning of the Coding Dojo by participants). Two partici- pants are chosen as pilot and co-pilot of a computer to start solving the problem. /T_he pilot (coder) is responsible for coding and he is the only allowed to handle instruments (keyboard and mouse). /T_he co-pilot (assistant) directly helps pilot with code inspection and suggestions. /T_he re- maining participants (main group) pay a/t_tention to the pair programming and they can ask for explanation to piloting pair (some groups allow the main group to suggest code', 'maining participants (main group) pay a/t_tention to the pair programming and they can ask for explanation to piloting pair (some groups allow the main group to suggest code refactoring). Each programming cycle has a few minutes, so at the end of the clock (1) the pilot returns to the main group of participants, (2) the co-pilot assumes pilot seat, and (3) someone else comes from the main group to co- pilot seat. /T_he main group can also discuss features of the solution without in/f_luence the piloting pair. • Kake: this format is similar to Randori, but many pilot- ing pairs work simultaneously on diﬀerent computers to solve the same problem. Hence, the idleness of the main group through long periods of time (as happens in Ran- dori) is eliminated, since all participants are allocated on a computer. /T_he roles of pilot and co-pilot remain, but the programming cycle is slightly diﬀerent. On each round (1) every pilot in one pair becomes a co-pilot in another pair and (2) every co-pilot in one pair becomes a pilot in that pair. Cycles last a few minutes and participants are encouraged to join a diﬀerent pair each round. /T_he aforementioned Coding Dojo formats share two common features: Test Driven Development (TDD) and Baby Steps [6]. TDD is about to create “clean code that works”[ 2] and it follows the classical Red-Green-Refactor mantra 2. TDD leads the problem solving process by creating quick, frequent automated tests that increase con/f_idence in the code. Baby Steps ensure pilots to make small progresses in order to allow that all participants understand the solution and to guarantee that minimal solution is been created. Traditional Coding Dojo session has not a consensus of structure, but it o/f_ten has the following elements [3] [6]: • A few minutes to choose next meeting date; • A few minutes to make a retrospective of the previous session; • A few minutes to decide the problem for the current meet- ing; • Most of the session to code; • A few minutes break to review the current meeting progress; • Most of the session to code again; • A few minutes to make a retrospective of the current ses- sion. 2Red-Green-Refactor Mantra implies an order to programming: Red means to code a small test that doesn’t pass, Green means to code the minimal solution to make that test to pass, and Refactor means to eliminate redundant code while keeping previous tests passing. 349', 'Coding Dojo as a transforming practice in collaborative learning of programming. SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figure 1: Overview of adopted Coding Dojo process. 3 METHODOLOGY /T_he application of Coding Dojos followed a well-de/f_ined structure with steps and activities presented in Figure 1. In the next sections, we describe meetings planning, execution details and closing activities with instruments and strategies to result analysis. 3.1 Planning Firstly, we have made a preliminary study on literature on Coding Dojos to investigate their execution features. Based on this research, we de/f_ine activities to be accomplished in the project such as elic- itation of interested participants, Coding Dojo themes (potential problems and programming languages), organization process for meetings, meetings local and duration, data collection techniques, data recording media, and pilot event for process evaluation. We applied an interest survey to estimate the number of partici- pants in the project and their general pro/f_ile. /T_he survey was done by means of an electronic questionnaire and included objective questions like undergraduate course of the students, interest to learn how to program, programming knowledge level, preferred day of the week for meeting, and previous participation in Coding Dojos. We had got 63 responses in which we noticed a high percentage of interested people at Figure 2. Half of respondents also stated they had li/t_tle programming knowledge as summarized at Figure 3. Figure 2: Summarization of respondent general interest. Additionally, we noticed we had reached participants from out- side of UNIPAMPA. Even if our target audience was So/f_tware Engi- neering undergraduate students (35.7% of responses), we received answers from Computer Science undergraduate students (32.1%) from UNIPAMPA and System Analysis and Development technolog- ical undergraduate students (25%) from Farroupilha Federal Institute in Alegrete. Initial Coding Dojo themes were picked based on context of UNIPAMPA undergraduate courses at the moment of the project, i.e., focusing on programming languages used by courses available on campus. As result, C and Java programming languages were chosen to be used, since they are part of So/f_tware Engineering and Computer Science curricula. Moreover, themes could be iteratively discussed (introducing or removing elements) at the retrospective moment of each Dojo and adapted in every new meeting, because participants pro/f_ile could change during the project and new inter- ests or needs could arise. We also chose CodeBlocks and NetBeans Integrated Development Environment (IDE) for, respectively, C and Java development. We selected Coding Dojo Randori format as initial approach to drive meetings, because it would facilitate management for /f_irst Do- jos and it would allow intense interaction for participants. However, we also planned Kake format would be used since second meeting if participants number was above twenty in the same meeting (which did not happen), so we would ensure all participants to take the pilot seat at least once in a two hours Coding Dojo. Figure 3: Summarization of respondent programming knowledge. 350', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil P. Rodrigues et al. Coding Dojo organization process adopted in this project follows [8] guidelines. Brie/f_ly, we need to choose a problem to address and a programming language, to create test cases and to de/f_ine /f_irst pilots, to run coding cycles, and to make a retrospective session at the end. Figure 4 illustrates the detailed organization process to every meeting in our project. Our Dojo meeting structure is composed of the next phases: (1) to choose by voting one among many problems brought by partici- pants and by Dojo Master3 and to choose programming language for the current meeting, (2) to plan and write at the whiteboard a small collection of test cases (unity test coding was introduced at tenth meeting), (3) to pick two random people from the audience as pilot and co-pilot, (4) to set duration of each coding cycle based on number of participants (the more participants, the less time), (5) to set up the programming cycle, (6) to return to the /f_irst phase if the initial problem is solved and if there is enough time to solve another one, and, at the end of the meeting, (7) to drive a retrospec- tive and (8) to plan next Dojo meeting. Moreover, we encouraged participants in every meeting to bring new, interesting problems to be solved at the Coding Dojos, stimulating participation and enriching diversity. Commonly, Coding Dojos use Test Driven Development (TDD) agile practices (including test /f_irst, automated unity tests, “baby step” increments, and development based on Red-Green-Refactor mantra), though those practices could be an obstacle to novice programmers. /T_hus, we determined thatPlan and Write [and Code] Testsactivity before Code activity shouldn’t be carried out in initial meetings (being temporarily replaced by tests a/f_ter devel- opment), since novice programmers might have li/t_tle experience in used languages and TDD dynamics and might feel uncomfortable. A/f_ter nine Coding Dojo sessions and a TDD workshop, partici- pants (even newbies) showed openness and self-con/f_idence to learn new programming techniques, allowing the insertion of all TDD practices. We planned that each Coding Dojo meeting would last for two hours in informatics lab of Federal University of Pampa (UNI- PAMPA). /T_hese labs have favorable physical structure for Dojo meetings, providing island workstations (that contribute to the group communication), computers with Internet access (that al- low to quickly solve issues specially those related to programming language), data projector and whiteboard. We de/f_ined to use Moodle Learning Management System (LMS) provided by UNIPAMPA for event organization. /T_he virtual course “Collaborative Practices in Programming” has been created and all project artifacts (source codes, problems lists, schedule, photos, etc.) were stored there, except for a/t_tendance control that was made by using online spreadsheets and . Whiteboard was used at retrospectives moments to facilitate record and analysis of participants perceptions (related to Coding Dojo approach) and to stimulate participants to express learnings and new Dojo themes. Furthermore, we also used it as additional resource to support complementary explanation from Dojo Master when participants have seemed confused about a problem or a solution being developed. 3Dojo Master is an experienced facilitator who organizes and leads a Dojo meeting. In our project, two scholarship students performed this role and also performed observation and recording. A pilot event was planned and run before the beginning of main Dojo meetings in order to validate our process and identify im- provement points. /T_his /f_irst event consisted in two Coding Dojos ran during Academic Week 4 at UNIPAMPA in which we might publicize future meetings and foster participation. Dojo Master of our project also trained their abilities as facilitator by aiming be-', 'ran during Academic Week 4 at UNIPAMPA in which we might publicize future meetings and foster participation. Dojo Master of our project also trained their abilities as facilitator by aiming be- havioral aspects of participants like embarrassment, collaboration, and resistance to try something new. We were able to make some observations and assessments on that pilot: • One meeting had more than 18 participants, indicating we could have a large number of simultaneous participants during the project. • All participants were from So/f_tware Engineering or Com- puter Science undergraduate courses from UNIPAMPA. • Whiteboard was essential during Dojo meetings. • Dojo Master interfered sometimes in audience, because people were frequently discussing the solution without paying a/t_tention to pilot and co-pilot work. Conversations about external subjects were also suppressed, since they distracted the piloting pair. • Some participants tried to solve the problem by themselves in their own computer, violating the main goal of Cod- ing Dojos: collaborative learning. /T_his behavior was also suppressed. /T_his previous experience let us be/t_ter address and mitigate issues related to next Coding Dojo meetings. 3.2 Execution /T_he execution phase consisted in running planned activities such as participants quanti/f_ication, running Coding Dojos, and data col- lection and recording. In addition, we are going to discuss meetings advertising and evolution of organization process of Coding Do- jos with inclusion of TDD practices. /T_he quantity of meetings was planned to comply with academic calendar in UNIPAMPA academic year (eight months). Before every Coding Dojo meeting, Dojo Master advertised it by means of students e-mail lists, posters a/t_tached at UNIPAMPA panels, and face to face announcement in classrooms. /T_he number of enrolled participants in Moodle LMS was above the number of active participants and the number of interested that answer the survey. 74 students had enrolled in our Dojo course, but only 39 eﬀectively participated of at least one Coding Dojo meeting. We present an overview of participation on every Coding Dojo meeting in a course perspective at Figure 5 and general participants’ a/t_tendances over project lifetime at Figure 6. We ran fourteen Coding Dojo, and every meeting was orga- nized, advertised and executed by a Dojo Master. As a result of the turnover of participants, Dojo Master made a short, general presentation of the project (by describing goals and dynamics) at the beginning of the each meeting with newcomers. Positive and negative perceptions go/t_ten by Dojo Masters through intensive observation [7] are going to be listed ahead in subsection 4.2. We performed this observation technique to ensure a be/t_ter coverage of 4Academic Week is a yearly event organized by students and professors in UNIPAMPA that oﬀers talks, workshops and other academic activities. 351', 'Coding Dojo as a transforming practice in collaborative learning of programming. SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figure 4: Coding Dojo process. Adapted from Delgado et al [8]. Figure 5: Coding Dojo participation in a course perspective. information, and we wrote down talks, statements and impressions of participants at Dojo Diaries. /T_hese records included positive and negative features of meetings, so we could analyze noticeable learning improvements and opinions about Coding Dojo dynamics. /T_he /f_irst nine Coding Dojos followed the original planning: cod- ing without TDD practices. A/f_ter the ninth meeting and the TDD workshop described at subsection 3.1, we adopted planning, writing and codingunity test cases before code the solution. Whiteboard was extensively used to write test cases and to support technical discussions. During the project, participants noticed that problems addressed in Coding Dojos should be considered solved since the solution satis/f_ied all tests even when Dojo Master hadn’t orientated them so. /T_hose tests were wri/t_ten at the whiteboard, and they evolve as collective property of the group by aggregating new tests at any point of coding, testing or discussion phase. /T_herefore,Plan and write and code testsactivity (as seen at Figure 4) was totally incorporated into the Coding Dojos process. 3.3 Closing In the last Coding Dojo meeting, we obtained participants a/t_titudes about Coding Dojo dynamics and improvement suggestions to next editions of the project. We then discussed abilities and competences exercised during the project and delivered participants’ certi/f_ication that might be used as complementary activity at university. We elaborated an online a/t_titude questionnaire composed of (1) eight statements based on Likert Scale [17] and (2) three open questions to collect data for later content analysis [ 20]. Table 1 summarizes items of the questionnaire. We analyzed those data at section 4. Table 1: Statements and questions of attitude questionnaire applied to participants. (1) Likert Statements S1. I liked to work with my classmates to create a computer program. S2. My classmates supported me and give me nice tips while I was programming. S3. I liked to support and to teach new things to classmates while they were programming. S4. I felt more exposed and shy while programming with classmates watching me. S5. I liked to work with my classmates more than working alone. S6. I was able to focus on Coding Dojo exercises more than I usually do on class laboratory practices. S7. I learned more new things on Coding Dojos than I usually do on class laboratory practices. S8. I have more fun on Coding Dojo exercises than I usually do on class laboratory practices. (2) Open questions Q1. Tell us what you did enjoy the most on Coding Dojos. Q2. Tell us what you did enjoy the less on Coding Dojos. Q3. Do you have some suggestions or criticism related to Coding Dojos? 4 RESULTS AND DISCUSSION Results and project learning lessons were compiled and analyzed according to participants and Dojo Master viewpoint. From partici- pants viewpoint, the collected answers were analyzed by content analysis method [20]. From Dojo Master’s viewpoint, the percep- tions were collected by intensive and direct observation during the period of project execution. 352', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil P. Rodrigues et al. Figure 6: General participants’ attendances over project lifetime. 4.1 From Participants’ Viewpoint /T_he online questionnaire applied in the end of the project was answered by 24 undergraduate students: 16 from So/f_tware En- gineering course, 7 from Computer Science course, and 1 from Mechanical Engineering courses. We point out that the criteria to answer this questionnaire is having ever participated in at least one Coding Dojo, and it explains why number of respondents is superior to number of participants in the last meetings. /T_he nine statement answers can be viewed in Figure 7 /T_he answers analysis, by median and mode, revealed that, from the group of statements related to collaborating in programming (S1, S2, S3, S5), participants tend to agree with statements “I liked to work with my classmates to create a computer program” (S1) and “My classmates supported me and give me nice tips while I was programming” (S2), and strongly agree with the statement “I liked to support and to teach new things to classmates while they were programming” (S3). Even though the median is between agree and strongly agree for the statement “I liked to work with my classmates more than working alone” (S5), mode indicates a participants tendency to strongly agree. Regarding to the embarrassment feeling in Coding Dojos, partic- ipants tend to agree with the statement “I felt more exposed and shy while programming with classmates watching me” (S4). /T_he statement related to concentration, “I was able to focus on Coding Dojo exercises more than I usually do on class laboratory practices” (S6), showed wide dispersion of opinion. Even though the median is between neutral and agree, it is not possible to point a clear tendency of participants considering that data are bimodal and the most frequent are not adjacent. /T_he answers of the statement regarding to the learning of new subjects “I learned more new things on Coding Dojos than I usually do on class laboratory practices” (S7) mean a tendency in agree that Coding Dojos favor more the learning of new subjects when compared to the practices in laboratory. Lastly, in the statement “I have more fun on Coding Dojo ex- ercises than I usually do on class laboratory practices” (S8), the participants are inclined to strongly agree with the project joyful- ness bene/f_its since mode supports this tendency even when median is between agree and strongly agree. Content analysis [ 20] of the open questions started with the preparation of information stage, in which the research responses were exported to spreadsheets. In the content unitize stage, each of the responses was carefully read and analyzed, and the main response elements of each of the open questions were extracted and cataloged. /Q_uestion 9 generated 33 units, question 10 generated 10 units and question 11 generated 16 units. Each unit remained bound by the question of origin. In the classi/f_ication stage, the content units were grouped by similarity (for instance, “learning new programming languages” was grouped with “Contact with other languages”), forming ap- proximately 9 categories in question 1 (Q1), 6 categories in question 2 (Q2) and 11 categories in question 3 (Q3). /T_hese categories, in the description stage, were named with a common label that summa- rizes their contents. Tables 2, 3 and 4 present, for each open question, categories, number of units (N) and their frequency (%) inside each category. Table 2: Categories for question “Tell us what you enjoyed the most on Coding Dojos” (Q1). Category N % Working in a team/collaborative environment 10 33% Learning new programming languages 7 23% Pleasant and funny environment/experience 4 13% Applying programming knowledge 3 10% Improving interpersonal relationship 2 7% Encouraging fast thinking 4 3% Learning other topics related to programming 4 3% Pilot/co-pilot dynamics 4 3%', 'Applying programming knowledge 3 10% Improving interpersonal relationship 2 7% Encouraging fast thinking 4 3% Learning other topics related to programming 4 3% Pilot/co-pilot dynamics 4 3% Tutor’s knowledge generated safety and willingness to opine 4 3% /T_he isolated analysis of the most frequent categories clearly re- veals that many Coding Dojo positive features (for instance, collab- oration, new learning, funny experience) were realized by students who answered the questionnaire. In the question 2 (Q2) answers (Table 3) the category “Few people knew the language Python” came about because of a workshop oﬀered in the beginning of the meetings, as a way to a/t_tracted more 353', 'Coding Dojo as a transforming practice in collaborative learning of programming. SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figure 7: Answers to each statement of attitude questionnaire. Table 3: Categories for question “Tell us what you enjoyed the less on Coding Dojos” (Q2). Category N % Shyness/exposure to program in face of group 4 40% Many meetings on the same language, and few on another 2 20% Exercises demanded advanced knowledge in programming 1 10% Few people knew Python programming language 1 10% Professor solved most of the problem 1 10% Proposed problem was not solved the best way 1 10% people to Dojos meetings. As there was not much adhesion to the workshop we decided not to continue practicing this language. /T_he analysis of those categories indicates that shyness and con- sequent diﬃculty in programming in front of the group is the factor that causes the most discomfort for the participants, although ex- position is an inherent feature of the Dojos. Table 4: Categories for question “Do you have some sugges- tion or criticism related to Coding Dojos?” (Q3). Category N % More editions of the project 4 24% Programming language diversity 3 18% Less complex, more numerous activities 2 12% Audience should be able to opine 1 6% Complex problems (criticism) 1 6% Do not propose competitions so early 1 6% Having one more co-pilot 1 6% Less exposing dynamics should be be/t_ter 1 6% More time studying on a language (4 classes) 1 6% Participation of more professors 1 6% Reserve meetings to basic commands revision 1 6% It’s possible the word “classes” reported in the category “More time studying on a language (4 classes)” is, in fact, Coding Dojo meetings. Regarding to category “Do not propose competitions so early”, we point that competition is not a Dojo principle. Contrari- wise, the absence of competition and the stimulus for collaborative problem solving are true Coding Dojo foundations. /T_he comparison between the statements answers and open ques- tions answers tend to reinforce some aspects of the assessment. /T_he analysis of the responses of statements S1, S2, S3 and S5 and the most frequent category of the open question Q9 shows that the aspect related to programming collaboration was evaluated positively. /T_his reinforces the Coding Dojo main goal to promote the learning in a collaborative and inclusive environment. /T_he answer of S8 also corroborate the third most frequent cat- egory in the Q1, con/f_irming that the Coding Dojo experience is pleasant and funny. Regarding to S4, we realize that it is aligned with the most frequent category of Q2, highlighting the shyness issue in programming in front of people. 4.2 From Dojo Masters’ Viewpoint Perceptions of Dojo Master were compiled in two categories,things that didn’t workand things that did work, illustrating some learned lessons of ours. 4.2.1 Things that Didn’t Work.Many participants went away without to program:we noticed that Coding Dojos with too many participants are less productive in Randori format, since larger groups need more time in order to everyone participate. By using programming cycles with duration of /f_ive minutes, it would be necessary more than two hours of Coding Dojo to 25 participants code once. So, many participants in the /f_irst meeting (the most populous according to Figure 5) weren’t able to be pilots, resulting in a high rate of abandonment (most of them from outside of So/f_tware Engineering and Computer Science courses). /T_his could be solved by using Kake Dojo format. Larger groups also in/f_luenced more shy participants. Analysis of Table 2 and statement S4 in subsection 4.1 was con/f_irmed by observations. Shy participants did not participate of discussions in larger groups, and there was an occasion when three participants opted to stay exclusively as spectators. Even if the spectator role is not allowed in traditional Coding Dojos, it would be intrusive to force anyone to participate. 354', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil P. Rodrigues et al. Programming cycle duration:in many situations, 5-7 min- utes weren’t enough to allow the pilot to implement his ideas. Since pilots could spend much of their time discussing solution approaches with audience, they weren’t sometimes able to include a single line of code. Competitive participants: in some meetings, we needed to interrupt “Code” activity to clarify project goals and Coding Dojo goals, because we noticed unwelcome behavior from newcomers. New participants o/f_ten tried to solve the problem by themselves on their own computer and competed with classmates by presenting their solution as be/t_ter than the collective solution. /T_hose facts is a corroborating evidence of participants perception illustrated at Table 4 in the “Do not propose too early competition” category. We noticed those participants slowly had understood meetings dynamics and had commi/t_ted to Dojo format and goals. Resistance to TDD practices:participants presented resis- tance to write tests before coding and to code minimal solution to make a test to pass at meetings in which all TDD practiced had been included. Negligence of TDD steps is common for novices [2], and Dojo Master was able to /f_ix it slowly, iteration a/f_ter iteration. Pilots didn’t think aloud:another challenge is sometimes pilots didn’t say aloud what they were thinking or doing because of shyness. While remembrance issues (participants that simply forget about think aloud) might be quickly /f_ixed, shyness problems are harder to address, complicate understanding for the audience and negatively interfere in co-pilot collaboration. /T_his observation might be related to statement S4 of subsection 4.1 Workshop support:in the course of the project, we identify the need to support participants through workshops in technologies and techniques used across Coding Dojos. Before the inclusion of all TDD practices, we oﬀered workshops in so/f_tware testing and JUnit framework to level participants knowledges and abilities. In those workshops, we could be/t_ter identify misconceptions of participants related to so/f_tware testing and foresee troubles in TDD use. We could likewise to support novice learners to practice the planning of test scenarios. Even though it allowed us to introduce testing and JUnit concepts, not only those workshops had used traditional teaching approach, but they also goes against one main goal of the project: collaborative learning. Only two programming languages:even if new program- ming languages had not been identi/f_ied as interest themes, we noticed that use exclusively two programming languages (C and Java) might contributed to abandonment as indicated at Table 3 and Table 4. 4.2.2 Things that Did Work. Participants stabilization:large abandonment at /f_irst meetings had been expected, but a participa- tion increase starting from the eighth Coding Dojo (as illustrated at Figure 5) was surprising. By probing participants, we /f_igured out active participants had been stimulating friends and classmates and bringing them to experiment project meetings. Dojo Master interventions:Dojo Master interrupted Code phase in some speci/f_ic situations (e.g. when pilots were going to use an appropriate data structure to solve the problem but they didn’t know how to use it properly, as seen at Table 2). In nearly 15 minutes of technical discussion, Dojo Master presented how the structure works by showing examples disconnected from the addressed problem. In addition, we identi/f_ied importance in inter- rupt development to perform collective trace table at whiteboard. Tests were planned during the previoustechnical discussionand trace table was performed when someone wasn’t understanding the solution. Even pilots took those moments to explain what they had been implementing by using whiteboard as a communication medium, so everyone could contribute. /T_hose moments allowed to', 'the solution. Even pilots took those moments to explain what they had been implementing by using whiteboard as a communication medium, so everyone could contribute. /T_hose moments allowed to everyone to comprehend the logic model of the program as well as to contextualize ina/t_tentive participants. Dojo rules and goals internalization:we observed, among other things, that engagement in parallel conversations had de- creased progressively and that contributions to collective learning increased. We see it closely related to internalization of Coding Dojo rules and goals by participants, enforcing /f_indings at Table 2 and respective analysis of (S1), (S2), (S3) and (S5) statements at subsection 4.1. Dojo Master participation:Dojo Master not only organized and mediated meetings, but he also participated of Coding Dojo cycle as any other participant. /T_his posture made him get close to participants by turning meetings less formal. With this in mind, Dojo Master could probe interests, motivations and limitations of participants more easily from one Dojo to the other. Even if there is a category at Table 3 saying “Professor solved most of the problems”, most of participants didn’t see Dojo Master like a professor or a teacher. “Lazy” TDD adoption:/f_inally, progressive inclusion of TDD practices have been bene/f_icial to participants, since most of them didn’t even know fundamentals of unity testing. /T_he gradual adop- tion stimulated communication and collaboration, and it allowed natural acceptance of participants by ensuring time to feel comfort- able with new techniques. Consequently, we noticed that students have largely improved their abilities in so/f_tware testing a/f_ter this. 5 RELATED WORK We applied snowballing technique [27] to /f_ind studies that report the use of coding dojos to teach programming. Starting from an initial list of studies, we investigated all studies cited by them and all studies that cite them. /T_his way, we investigated 61 studies, which of 8 were considered relevant for our work. Following, we present a brief overview of each selected study. /T_hen, we analyze our /f_indings against the selected studies /f_indings. Sato et al. [24] report the use of coding dojos to develop agile skills, such as: Test Driven Development (TDD), refactoring, and pair programming. Bravo and Goldman [ 5] also propose coding dojos as learning method to teach agile practices. Luz et al. [ 18] present an investigation about the use of coding dojos to teach TDD practices. Heinonen et al. [16] apply coding dojos to provide a safe and playful environment to learn so/f_tware engineering concepts and practices. Rooksby et al. [ 23] discuss the learning theory behind coding dojos in order to propose a more appropriate theory for typical situations found in dojos meetings. Estcio et al. [ 9, 10] analyze coding dojo and pair programming impact in developing of juniors programmers/f_i programming skills. Estcio et al. [11] also analyze coding dojo participants/f_i a/t_titude in terms of learning dynamic, developers motivation, communication pa/t_terns. 355', 'Coding Dojo as a transforming practice in collaborative learning of programming. SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Table 5: Mapping of our /f_inding into selected studies. [24] [5] [18] [16] [23] [9] [10] [11] Coding dojos promote learning through collaborative work X X X X X X Coding dojos’ cycle time is short X X X X Diﬃculty to obtain consensus in large teams X X X Competitiveness disturbs the work in certain points X X Audience talking in bad times X Dojos dynamic do not match with participants’ learning process X Large teams may intimidate some participants X More people programming provide more con/f_idence X Participants feel uncomfortable when programming in front of colleagues X Participants have interest in to take part of new coding dojos X Playful environment provides a diﬀerent way to learn X Programming knowledge can be learned X Tasks are diﬃcult to solve X Team does not know what TDD step it are working X Worry-free learning environment X In this work, we present some /f_indings related to the use of coding dojos as a transformative practice to learn programming. Table 5 maps the studies that support our /f_indings. In our anal- ysis, we notice that coding dojos promotes learning through col- laborative work. /T_his /f_inding is supported by almost all studies [5, 10, 11, 18, 23, 24]. Another /f_inding supported by several studies [5, 10, 11, 18, 23, 24] is related to the coding dojos’ cycle time, which was considered short by participants. Diﬃculty to obtain consensus, especially in large teams, is supported by more the one study [9–11]. We realize that competitiveness disturbs the work in certain points. It also is supported by more the one study [ 16, 24]. Remaining /f_inding presented in Table 5 are supported by at least one study. Unlike cited studies, our work includes new /f_indings to anyone in academia who wants to use Coding Dojos as a learning practice: it’s a nice option to Dojo Master to participate of programming cycle as any other participant; pilots should be instigated to think aloud and to cooperate with audience; alternative Dojo formats (e.g. Kake) should be planned to deal with larger groups; TDD should be progressively introduced in groups with not much experience in so/f_tware testing, because it’s possible to notice participants dif- /f_iculties and to turn learning enjoyable; and new programming languages or technologies should every now and then be intro- duced to keep participants interest. 6 CONCLUSIONS /T_his paper had objective of reporting a two years project that aims to create a collaborative learning environment through Coding Dojos for UNIPAMPA students. Over the project time, we collected data from participants through observation and questionnaires. By analyzing those informations, we realize project achievements related to programming ability improvements and creation of alternative learning se/t_tings. /T_he collaborative scenario created by this experience is a remark- able bene/f_it supported by participants perceptions (subsection 4.1), Dojo Master observations (subsection 4.2, and literature review (Table 5). In our student interest survey, we noticed potential participants had had diﬀerent programming knowledge levels. /T_his is an ideal situation according to cognitive structures balancing theory [22], since Piaget theorized that grouping people with unbalanced abili- ties to ful/f_ill a task leads from an unbalanced condition to interac- tions and collaborative work to restore cognitive balance. Dojo Master full participation in programming cycles is positive to create a less formal scenario and increase participants security. Moreover, Dojo meetings are a suitable option in academia environ- ment for new programming techniques and technologies learning. Our adaptations of the Coding Dojo process [8] contributed to progressive evolve meetings organization and, specially, to improve participation and participants’ welfare. For instance, including', 'Our adaptations of the Coding Dojo process [8] contributed to progressive evolve meetings organization and, specially, to improve participation and participants’ welfare. For instance, including TDD practices a/f_ter the /f_irst half of the project period ensured participants feel safe and open enough to learn new techniques without resistance. On the other hand, exposition factor is an issue expressed by participants and con/f_irmed by Dojo Master that should be addressed. Data analysis also points that consecutive meetings on the same topic and programming language quickly become repetitive, and it discourages participation. We emphasize some limitations of our work such as winter and summer vacations that broke continuous application of Coding Dojos, lack of funding for scholarship in some months, and eﬀective realization of fourteen Coding Dojos (a small number of meetings to collect data and to form accurate conclusions). We also add our inability to longitudinally monitor academic performance and programming learning of participants due to extra-class feature of the project that causes participants turnover. We additionally point some future work to improve our results. It’s necessary to perform a deeper investigation of how the shyness is addressed in other projects. /T_hus, we are going to conduct a systematic literature review on dealing with shyness of Coding Dojo participants, and to proceed to experiments to evaluate eﬀectiveness of found approaches. Another issue is if and how exposure factor aﬀects Coding Dojo participants in so/f_tware industry, i.e., outside academic environ- ment. We are going to carry out a survey on industry and academia 356', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil P. Rodrigues et al. to study exposure issues and how to solve them. /T_his information could bring insights to mitigate the problems in our context. Besides, we need to search and to experiment new strategies to stimulate student and professor participation, being some pos- sibilities to approach new programming languages, to interpose another programming language between two subsequent Coding Dojos in certain language, and to identify and explore new themes. Finally, we strongly believe that Coding Dojo practice should be institutionalized in order to provide its bene/f_its to students all over the year. ACKNOWLEDGMENTS Authors would like to thank Federal University of Pampa (UNI- PAMPA) for support this work by the Pedagogic Development Program (PDP), editions 2014 and 2015. REFERENCES [1] Juan Manuel Ad´an-Coello, Wiris Sera/f_im de Menezes, Eust´aquio S ao Jos´e de Faria, and Carlos Miguel Tobar. 2008. Con/f_lito S ´ocio-cognitivo e Estilos de Aprendizagem na Formac ¸˜ao de Grupos para o Aprendizado Colaborativo de Programac ¸˜ao de Computadores. Revista Brasileira de Inform´atica na Educac ¸˜ao 16, 3 (2008), 1–12. [2] Kent Beck. 2003. Test Driven Development: By Example. Addison-Wesley Profes- sional, Boston. [3] M´arcio Bon/f_im. 2015. O que´e o Coding Dojo. (2015). Retrieved July 20, 2017 from h/t_tp://www.devmedia.com.br/o-que-e-o-coding-dojo/30517 [4] Laurent Bossavit and Emmanuel Gaillot. 2005. /T_he Coder’s Dojo – A Diﬀerent Way to Teach and Learn Programming. InInternational Conference on Extreme Programming and Agile Processes in So/f_tware Engineering. Springer, Berlin, 290– 291. DOI:h/t_tp://dx.doi.org/10.1007/1149905354 [5] M. Bravo and A. Goldman. 2010. Reinforcing the learning of Agile practices using Coding Dojos. Lecture Notes in Business Information Processing 48 LNBIP (2010), 379–380. DOI:h/t_tp://dx.doi.org/10.1007/978-3-642-13054-041 [6] CodingDojo.org. 2016. What is Coding Dojo? (2016). Retrieved July 20, 2017 from h/t_tp://codingdojo.org/WhatIsCodingDojo/ [7] Marina de Andrade Marconi and Eva Maria Lakatos. 2010. Fundamentos de Metodologia Cient´ı/f_ica. Atlas, S ˜ao Paulo. 310 pages. DOI:h/t_tp://dx.doi.org/10. 1590/S1517-97022003000100005 [8] Carla Delgado, Rodrigo de Toledo, and Vanessa Braganholo. 2012. Uso de Dojos no ensino superior de computac ¸˜ao. In Workshop de Educac ¸˜ao em Computac ¸˜ao (WEI 2012). SBC, Porto Alegre, 10. [9] B. A. Est ´acio, R. B. Oliveira, S. A. Marczak, M. C. Kalinowski, A. B. Garcia, R. A. Prikladnicki, and C. B. Lucena. 2015. Evaluating Collaborative Practices in Acquiring Programming Skills: Findings of a Controlled Experiment. In 29th Brazilian Symposium on So/f_tware Engineering. SBC, Belo Horizonte, 150–159. DOI:h/t_tp://dx.doi.org/10.1109/SBES.2015.24 [10] B. A. Est´acio, N. B. Valentim, L. B. Rivero, T. B. Conte, and R. A. Prikladnicki. 2015. Evaluating the use of pair programming and coding dojo in teaching mockups development: An empirical study. In Proceedings of the Annual Hawaii International Conference on System Sciences, Vol. 2015-March. IEEE, Kauai, 5084– 5093. DOI:h/t_tp://dx.doi.org/10.1109/HICSS.2015.602 [11] B. A. Est´acio, F. B. Zieris, L. B. Prechelt, and R. A. Prikladnicki. 2016. On the randori training dynamics. In 9th International Workshop on Cooperative and Human Aspects of So/f_tware Engineering. ACM, Austin, 44–47. DOI:h/t_tp://dx.doi. org/10.1145/2897586.2897603 [12] Luiz Paulo Franz, Jo ao Pablo S. da Silva, and Jean Felipe P. Cheiran. 2014. O uso de Coding Dojo no aprendizado colaborativo de programac ¸˜ao de computadores. Revista Novas Tecnologias na Educac ¸˜ao 12, 2 (2014), 9. [13] Anabela Gomes, Lilian Carmo, Emilia Bigo/t_te, and Antonio Mendes. 2006. Math- ematics and programming problem solving. In 3rd E-Learning Conference – Com- puter Science Education. [d.p.], Coimbra, 1–5. [14] Anabela Gomes, Fernanda Brito Correia, and Pedro Henriques Abreu. 2016.', 'puter Science Education. [d.p.], Coimbra, 1–5. [14] Anabela Gomes, Fernanda Brito Correia, and Pedro Henriques Abreu. 2016. Types of assessing student-programming knowledge. In Frontiers in Education Conference (FIE), 2016 IEEE. IEEE, Eire, 1–8. [15] Mark Guzdial and Karen Carroll. 2002. Exploring the lack of dialogue in computer- supported collaborative learning. In Proceedings of the Conference on Computer Support for Collaborative Learning: Foundations for a CSCL Community. ACM, Boulder, 418–424. [16] K. Heinonen, K. Hirvikoski, M. Luukkainen, and A. Vihavainen. 2013. Learning Agile so/f_tware engineering practices using coding dojo. InProceedings of the 2013 ACM SIGITE Annual Conference on Information Technology Education. ACM, New York, 97–102. DOI:h/t_tp://dx.doi.org/10.1145/2512276.2512306 [17] Rensis Likert. 1932. A technique for the measurement of a/t_titudes. InArchives of Psychology. APA, New York, 1–55. [18] R. B. D. A. Luz, A. G. S. S. B. Neto, and R. V. C. Noronha. 2013. Teaching TDD, the coding dojo style. In IEEE 13th International Conference on Advanced Learning Technologies. IEEE, Beijing, 371–375. DOI:h/t_tp://dx.doi.org/10.1109/ICALT.2013. 114 [19] Iain Milne and Glenn Rowe. 2002. Diﬃculties in Learning and Teaching Program- ming – Views of Students and Tutors. Education and Information Technologies 7, 1 (2002), 55–66. [20] Roque Moraes. 1999. An ´alise de Conte´udo. Revista Educac ¸˜ao 22, 37 (1999), 7–32. [21] Jan Erik Mostrm. 2011. A Study of Student Problems in Learning to Program. Ph.D. Dissertation. Umea University, Umea, Sweden. [22] Jean Piaget and Marion Merlone. 1976. A equilibra c ¸˜ao das estruturas cognitivas: problema central do desenvolvimento. Zahar, Rio de Janeiro. [23] J. A. Rooksby, J. B. Hunt, and X. C. Wang. 2014. /T_he theory and practice of randori coding dojos. Lecture Notes in Business Information Processing 179 LNBIP (2014), 251–259. DOI:h/t_tp://dx.doi.org/10.1007/978-3-319-06862-6 [24] D. A. Sato, H. B. Corbucci, and M. B. Bravo. 2008. Coding Dojo: An environment for learning and sharing Agile practices. In Agile 2008 Conference. IEEE, Toronto, 459–464. DOI:h/t_tp://dx.doi.org/10.1109/Agile.2008.11 [25] Dave /T_homas. 2016. CodeKata: How It Started. (2016). Retrieved July 20, 2017 from h/t_tp://codekata.com/kata/codekata-how-it-started/ [26] Bas Vodde and Lasse Koskela. 2007. Learning test-driven development by count- ing lines. IEEE So/f_tware24, 3 (2007), 74–79. [27] Jane Webster and Richard T. Watson. 2002. Analyzing the Past To Prepare for the Future : Writing a Review. MIS /Q_uarterly26, 2 (2002), 13–23. 357']","[""CODING DOJOS IN LEARNING OF PROGRAMMING This  briefin  reports  uidernraduate studeits'  perspectie  oi  the  use  of Codiin Dojos ii collaboratie leariiin of pronrammiin, aid it also reports what did work aid what did iot work by Dojo Master's perspectie. FINDINGS Results and project learning lessons were compiled and  analyzed  according  to  two  viewpoints (participants and Dojo Master.e Partcipaits Perspectie:  \uf0b7 The  study  reveals  that  participants generally  enjoyed  working  together  and receiving  tips  while  they  were programminge  Also,  participants especially  liked  to  support  or  to  teach each  other,  and  they  prefer  to  work together than alonee \uf0b7 Shyness must be addressed, since several participants  felt  embarrassed  while coding in front of the otherse \uf0b7 Participants  tended  to  agree  that  they learned  more  in  Coding  Dojos  than  in traditional lab classese They have strongly agreed that Coding Dojos are a fun option for programming learninge These two tables below present main categories extracted from participants' perspectivese Table 1: Categories for question “Tell us what you  enjoyed the most on Coding Dojos”e Catenory N % Working  in  a  team/collaborative environment 10 33 Learning new programing languages 7 23 Pleasant and funny environment […] 4 13 Applying programing knowledge 3 10 Improving interpersonal relationship 2 7 Encouraging fast thinking 4 3 Learning other programing  topics 4 3 Pilot/co-pilot dynamics 4 3 Tutor’s knowledge generated safety and willingness to opine 4 3  Table 2: Categories for question “Tell us what you  enjoyed the less on Coding Dojos”e Catenory N % Shyness[…]  to  program  in  face  of group 4 40 Many meetings on same language […] 2 20 […] demand of advanced knowledge 1 10 Few people knew python […] 1 10 Professor solved most of the problem 1 10 [ee]problem  wasn't  solved  the  best 1 10 way Dojo Master Perspectie  what did iot work):     \uf0b7 Too  many  participants  turned  Coding Dojo  in  Randori  format  less  productivee Also, shy participants did not participate of  discussions  and  several  people  went away without to codinge \uf0b7 Traditional  programming  cycle  duration, 5-7  minutes,  weren’t  enough  to  pilots implement their idease \uf0b7 Competition  isn’t  encouraged  in  Coding Dojose  In  some  meetings,  competitive behaviors have occurred and we had to interrupt  coding  cycle  to  clarify  Coding Dojo goalse \uf0b7 Participants  presented  an  initial resistance  to  write  tests  before  coding, but negligence of TDD steps is common for  novices  and,  meeting  afer  meeting, participants felt more confdent to accept TDD mantrae \uf0b7 Some participants didn’t say aloud what they  were  thinking  or  doing,  and  it hinders the understanding for otherse \uf0b7 Despite  being  a  traditional  teaching technique, we had to perform training for TDD practices  by workshops to  increase participants confdencee \uf0b7 Only  two  programming  languages  were adopted  on  Coding  Dojos  during  the entire projecte We noticed that it might contribute to participants abandonmente Dojo Master Perspectie  what did work):   \uf0b7 Abandonment at frst meetings had been expected, but we fgured out that active participants had been stimulating friends and  classmates  and  bringing  them  to Dojose \uf0b7 Dojo Master or pilots interrupted coding phase  in  some  situations  to  perform technical  discussions  with  participantse Whiteboard  was  an  important  resource and  it  was  collaboratively  used  for explanatione \uf0b7 We  observed  that  participants' engagement  and  contributions  had progressively increasede \uf0b7 Dojo  Master  not  only  organized  and mediated  meetings,  but  he  also participated of coding cyclee This attude made him more close to participants by turning meetings less formale \uf0b7 Keywords: Coding Dojos Collaborative Learning Computer Programming Test Driven Development Who is this briefin  or? Sofware  engineering  academics  and"", 'turning meetings less formale \uf0b7 Keywords: Coding Dojos Collaborative Learning Computer Programming Test Driven Development Who is this briefin  or? Sofware  engineering  academics  and practitioners  who  want  to  make decisions about learning of programing based on evidencese Where the fidiins come  rom? All  fndings  of  this  briefng  were extracted  from  the  experience  report conducted by Rodrigues et ale   What is iicluded ii this briefin? Main fndings of the original experience report based on participants and Dojo Master’s perspectivese What is iot iicluded ii this briefin? Detailed descriptions about the project, techniques  and  methods  as  well  as Coding Dojo practicese  ORIGINAL RESEARCH REFERENCE Peterson Luiz da Re Rodrigues, Luiz Paulo Franz, Jean Felipe Pe Cheiran, João Pablo Se da Silva, and Andréa Se Bordine Coding Dojo as a transforming practcc in collaboratic  lcarning of programming: an cxpcricncc rcporte Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017e DOI: 10e1145/3131151e3131180e']","**Title:** Enhancing Programming Skills Through Collaborative Coding Dojos

**Introduction:**
This briefing summarizes the findings from a two-year project at the Federal University of Pampa that utilized Coding Dojos as a collaborative learning strategy to improve programming skills among undergraduate students. The goal of this study was to address high failure and dropout rates in programming courses by fostering a supportive and engaging learning environment.

**Main Findings:**
The implementation of Coding Dojos revealed several key insights:

1. **Collaborative Learning Environment:** Participants reported that the collaborative and enjoyable atmosphere of Coding Dojos significantly enhanced their learning experience. Many students appreciated the opportunity to work together, share knowledge, and support one another during coding sessions.

2. **Engagement and Participation:** The project fostered greater engagement among students, with many feeling more motivated to solve problems collectively. The absence of a formal instructor allowed for peer-to-peer learning, which helped reduce anxiety and resistance to teamwork.

3. **Positive Attitudes Toward Learning:** Surveys indicated that students generally preferred working in groups over solitary programming tasks. They felt more supported and were more willing to assist their peers, contributing to a sense of community.

4. **Challenges Identified:** Despite the positive outcomes, some participants expressed discomfort with programming in front of their peers, which hindered their participation. Additionally, repetitive sessions focused on the same programming language led to decreased interest over time.

5. **Adoption of New Practices:** The gradual introduction of Test Driven Development (TDD) practices was well-received after initial resistance. As participants grew more comfortable, they began to embrace TDD, enhancing their coding skills and confidence.

6. **Need for Diversity in Topics:** The study highlighted the importance of varying programming languages and problems addressed in Coding Dojos to maintain participant interest and engagement.

**Who is this briefing for?**
This briefing is intended for educators, curriculum developers, and software engineering practitioners who are interested in innovative teaching strategies to enhance programming education. It is particularly relevant for those looking to implement collaborative learning environments in academic settings.

**Where the findings come from?**
The findings presented in this briefing are derived from an experience report on the implementation of Coding Dojos at the Federal University of Pampa, which included participant surveys and observational data collected over two years.

**What is included in this briefing?**
This briefing includes insights into the collaborative learning processes fostered by Coding Dojos, the benefits and challenges faced by participants, and recommendations for improving future sessions.

**To access other evidence briefings on software engineering:**
[Evidence Briefings](http://ease2017.bth.se/)

**For additional information about the project:**
Peterson Luiz da R. Rodrigues et al. (2017). ""Coding Dojo as a transforming practice in collaborative learning of programming: an experience report."" In Proceedings of SBES’17, Fortaleza, CE, Brazil. DOI: [10.1145/3131151.3131180](https://doi.org/10.1145/3131151.3131180)"
"[""Comparing Configuration Approaches for Dynamic Software  Product Lines    Gabriela Guedes1,2  1UNINFO / IFPB – Campus Cajazeiras  Cajazeiras/PB  CEP 58900-000  Brazil  ggs@cin.ufpe.br    Carla Silva2, Monique Soares2  2Informatics Center (CIn)/UFPE  Recife/PE  CEP 50740-560  Brazil  {ctlls, mcs4}@cin.ufpe.br   ABSTRACT  Dynamic Software Product Lines (DSPLs) are S oftware Product  Lines (SPLs) in which the configuration may occur at runtime.  DSPL approaches provide means for modeling variability as well  as configuring the  product according to its runtime context  and/or non -functional requirements (NFRs) satisfaction.  In this  paper, we present a Requirements Engineering (RE) approach for  DSPL, ConG4DaS (Contextual Goal models For Dynamic  Software product lines), which provides: (i) models for capturing  variability with goals, NFRs, contexts and the relationship  between them; and (ii) a configuration process that takes  contexts, NFRs and their priority and interactions into account.   We have used simulation based assessment to  compare  ConG4DaS with another approach, REFAS  (Requirements  Engineering For self-Adaptive Software systems), with respect to  the satisfaction level of the highest priority softgoal. For the  comparison, we modeled two DSPL examples and simulated  different scenarios where reconfiguration is necessary. Next, we  compared the configurations selected by the approaches with  respect to overall NFRs’ satisfaction. The results showed that  ConG4DaS, which uses utility function  in the configuration  process, selects configurations that better satisfy NFRs compared  to REFAS, which uses constraint programming.       CCS CONCEPTS  • Software and its engineering  ~ Requirements analysis   •  Software and its engineering ~ Software product lines  KEYWORDS  Dynamic Soft ware Product  Lines, Dynamic Variability, Goal  Models, Self-Adaptive Systems.    ACM Reference format:  Gabriela Guedes, C arla Silva, and M onique Soares . 2017.  Comparing Configuration Approaches for Dynamic Software  Product Lines. In Proceedings of 31st Brazilian Symposiu m on  Software Engineering, Fortaleza, CE, Brazil, September 2017  (SBES'17), 10 pages.  DOI: 10.1145/3131151.3131162  1 INTRODUCTION  Advances in technologies, platforms and devices have increased  the demand for pervasive, mobile and autonomic software. Users  require software systems that are capable of offer ing their  services even in changing conditions. According to  [13], the  need to cope with these challenges has led to the development of  many different approaches for adapting to changi ng needs,  including self-adapting, agent-based, autonomous, emergent, and  bio-inspired systems.   As a result of the interest in systems that are capable o f  adapting themselves, research ers have investigated the use of  Dynamic Software Product Lines (DSPL)  [12], which propose s  the use of Software Product Line (SPL) techniques  [5] for  developing this kind of systems. Given the success of SPLs,  DSPLs offer a promising strategy to deal with the design and  implementation of software changes that need to be handled at  runtime [2].  Dynamic variability management, as provided by DSPLs, has  multiple application areas, such as Software as a Service (SaaS),  self-adaptive systems (SAS), pervasive systems and context - aware mobile systems [4]. Hence, DSPLs may be applied for the  development of dynamic adaptive systems (DAS) in general, i.e.  systems capable of adapting themselves to runtime changes in  the execution environment or user requirements . DSPLs provide  a way of modeling DAS  where a dynamic adaptive system itself  is seen as a DSPL and every possible configuration of the DAS  could be seen as a product of the DSPL [ 3].  In general, DSPL approaches  use context information and/or  the level of satisfaction of non -functional requirements (NFRs)  for deciding when the DSPL should be reconfigured and the best"", ""In general, DSPL approaches  use context information and/or  the level of satisfaction of non -functional requirements (NFRs)  for deciding when the DSPL should be reconfigured and the best  configuration for the situation. For our research, we understand  context as the state of the envi ronment where the system is  operating.    According to our previous systematic mapping  [11], most  approaches do not model the interactions between contexts and  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee pr ovided that copies are not  made or distributed for profit or commercial advantage and that copies bear this  notice and the full citation on the first page. Copyrights for components of this  work owned by others than ACM must be honored. Abstracting with cr edit is  permitted. To copy otherwise, or republish, to post on servers or to redistribute  to lists, requires prior specific permission and/or a fee. Request permissions from  Permissions@acm.org.  SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil   © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503 -5326 -7/17/09...$15.00   https://doi.org/10.1145/3131151.3131162                 134"", 'NFRs and, therefore, do not take the se interactions  into account  when con figuring the DSPL. Some approaches model how the  priority or required satisfaction level of an NFR may vary  according to the context  [6, 8, 15, 18]. Other a pproaches model  how contexts affect the set of goals and NFRs to be analyzed [1].  But, in our systematic mapping, we found no approach capable  of modeling how contexts affect both the priority and the set of  NFRs to be analyzed [11].  Based on the findings of the mapping , we propose ,  ConG4DaS ( Contextual Goal models for Dynamic Software  product lines),  a Requirements Engineering (RE) approach for  DSPLs that captures the DSPL’s variability by using goals, NFRs   (modeled as softgoals) , contexts and the relationships among  them. In the configuration process, the available configurations  for the given context are ranked according to the priority of the  softgoals they satisfy, and the best ranked configuration is  selected.  The objective of this paper is to present ConG4DaS and, then,  a simulation based assessment of its configuration process.  In  this assessment we performed a comparison between the  configuration process of ConG4DaS and REFAS (Requirements  Engineering For self -Adaptive Software systems ) [15], another  approach that also takes contexts, NFRs and their relations into  account. Our intention was to find out whether the  configurations selected by these approaches differ in regard to  the overall satisfaction of the systems’ NFRs.  We modeled two  simple DSPL examples using both approaches, then, we  simulated different contexts and analyzed the configurations  selected by both approaches in each situation.   The remainder of this paper is organized as follows. Section 2  presents an overview of ConG4DaS. Section 3  presents some  related work . In Section 4 , we describe the comparison and its  results and, then, we conclude this paper in Section 5.    2 CONTEXTUAL GOAL MODELS FOR  DYNAMIC SOFTWARE PRODUCT LINES   Our proposal consists of a requirements engineering process for  developing Dynamic Software Product Lines, called ConG4D aS  (Contextual Goal models for Dynamic Software product lines ).  ConG4DaS has two processes: the Domain Engineering and the  Application Engineering.   When defining ConG4D aS, our goal was to provide an  approach that allows the DSPL to adapt itself to context, but  maximizing the NFRs’  (which are modeled as softgoals)   satisfaction as much as possible in the given context. To  accomplish this, ConG4DaS requirements models provide a way  to capture how the context affects softgoals priorities and a way  to model how an element’s contribution to a softgoal may  change with the context. Then, this information is used during  the configurati on process, so the variants are selected based on  how they contribute to the softgoals, giving preference to those  with higher priority.   In the next subsections, we briefly describe the activities of  each sub-process.  2.1 Domain Engineering   The DE process of ConG4DAS is based on the DE sub -process of   an existing approach [10], which is a RE approach for static SPLs  that allows the generation of feature models (FMs) and use case  scenarios from goal models. However, the goal model us ed is an  extension of the i* framework [ 19] that does not contain context  annotations. Therefore, it was not suitable for DSPLs, which  need to configure themselves according to context changes.   In ConG4DaS, not only we changed the variability model to  i*-orthogonal [ 14], we also defined the activities involved in the  generation of feature models (FMs) and use case scenarios from  i* models as optional, since these artefacts are not necessary for  configuring t he DSPL.  Since the focus of this paper is on the configuration process,  which corresponds to the Application Engineering  (AE) sub- process of ConG4DAS, we will not present in details the', 'Since the focus of this paper is on the configuration process,  which corresponds to the Application Engineering  (AE) sub- process of ConG4DAS, we will not present in details the  activities of the DE sub-process. This means that Guidelines 1 to  32, defined for the DE activities , are not described in this paper ,  since they were mostly inherited from our previous approach  [10], but they can be found in [ 9].  2.1.1 Specify Requirements in Goal Model  The purpose of this activity is to create the initial i*-orthogonal  model, that does not contain any context annotation or  cardinality in the variation points ( VPs). For creating the i*  model, the domain analyst may use any requirements elicitation  technique or  one of the specific approaches for elaborating i*  model, such as PRiM (Process Reengineering i* Method)  [7].  2.1.2 Specify Context Model  This activity concerns the construction of the context model and  relating it to the goal mode l. There are three guidelines  (Guidelines 1 to 3) in this activity to guide the creation of the  context model, which  uses the notation proposed in [ 1]. Then,  the goal model is annotated with contexts, which means that the  relationships and elements with contexts annotations are only  available in the given context.  Fig. 1 depicts the context model of the Mobile Game example.  According to the model, the Low Battery context is active if the  battery level is less  than 30%. Context Using Mobile Network  is  active if the mobile internet is on . The context Network Available  is active if the mobile device is connected on a Wi-Fi network or  if the mobile network is connected.  Finally, the Mobile Data  Ending context is a ctive is the device is using a mobile internet  connection and it has used more the 70% of the data plan.   After defining the contexts, the i* -orthogonal is annotated  with them. Any relationship that depends on a context is  annotated with it. In the Mobile G ame example (see Fig. 2), the  task Use Vibration Effects  is executed only if the battery level is  not low (¬C1), and the alternative task Use Multiplayer Mode  is  available only when there is Internet connection and the data  plan is not ending (C3 ∧ ¬C4). Also, this task’s contribution to  the softgoal Efficiency in Resource Usage  changes according to  context. If the device is using mobile internet connection (C2)  the contribution is of type hurt, if not, the contribution’s type is  some-.    In this activity, we also model the relationship between  contexts and NFRs’ priority. NFRs are represented as softgoals in  i*-orthogonal models. For each context, we specify the softgoals’  priority and also specify the priority for the default contex t  (when none of the modelled contexts is active). We use the  Priority4Context table to capture this information.   135', 'Figure 1: Context model of Mobile Game Example  The contexts’ order is used during the AE sub -process when  more than one context is active at the same time. In this case, the  softgoals’ priority used is the one related to the active context  with the highest order. The last context to be considered is the  Default. The softgoals’ priority and the contexts’ order are  defined by the domain engineer based on the DSPL’s  requirements. The Priority4Context table  for the running  example is presented in  Table 1.  Table 1: Priority4Context table of Mobile Game   Default C1 C2 C3 C4  High  Quality  Interaction  10 6 7 10 6  Efficiency  in  Resource  Usage  7 10 9 8 10  Context  Order - 2nd 3rd 4th 1st    2.1.3 Represent Variability  In this activity, we identify the variation points (VPs) in the i* - orthogonal model by finding the elements that probably  represent features and adding cardinality on them or  on their  decomposition relationships . This is achieved by following  Guidelines 4 to 8 . As we stated before, we will not present them  in this work since they are detailed in previou s work [10].  Fig. 2 shows an i*-orthogonal mode l for Mobile Game. In Fig.  2 it is possible to see that the actor Player depends on the actor  Mobile Game  to realize the goal dependenci es Play Game  and  Play with Other Players , as well as  the softgoal dependencies  High Quality Interaction  and Efficiency in Resource Usage . Those  softgoal dependencies represent the NFRs that the Mobile Game  system should satisfy.   Mobile Game  satisfies the Play Game  dependency by  providing the Play Game task, that is a mandatory task since it  has the cardinality [1..1]. This task is decomposed in the task  Set  Sound Effects and the goals Adjust Graphics Quality  and Provide  Gaming Environment . Adjust Graphics Q uality may be achieved  by executing one of the tasks Set to High Quality Graphics or Set  to Low  Quality Graphics. Set to High Quality  Graphics helps the  system to achieve the softgoal High Quality Interaction , but it  hurts the goal Efficiency in Resource U sage. The alternative Set to  Low Quality Graphics hurts the softgoal High Quality Interaction,  but it helps Efficiency in Resource Usage.       Figure 2: i*-orthogonal model for Mobile Game  Set Sound Effects  is decomposed in the task  Use Vibration  Effects and the goal Adjust Sound Quality . Use Vibration Effects is  an optional task – cardinality [0..1] – and has a positive  contribution to the softgoal High Quality Interaction, but it has a  negative contribution to Efficiency in Resourc e Usage .. Adjust  Sound Quality  can be satisfied by executing Set to High Qual ity  Sound or Set to Low Quality  Sound. The first alternative helps to  achieve High Quality Interaction  and hurts the satisfaction of  Efficiency in Resource Usage , while the second  alternative does  the exact contrary contributions.   The goal Provide Gaming Environment may be satisfied by the  tasks Use Single Player Mode  or Use Multiplayer Mode. Use Single  Player Mode contributes negatively to High Quality Interaction ,  while the alter native helps this softgoal, but its contribution to  Efficiency in Resource Usage varies according to context C2.   2.1.4 Create Optional Artefacts  This is an  optional sub -process that may be executed by the  domain analyst if he wants to improve the requirem ents  description of the DSPL by generating its feature model and use  case scenarios. The sub -process is composed by 4 activities: (1)  Create Feature Model; (2) Reorganize Feature Model; (3) Create  Use Case Scenarios; and (4) Improve Use Case Scenarios.   The first two activities are for generating  the FM from the i* - orthogonal model , while the other two are for generating the use  case scenarios. In this paper , we do not present these activities  136', 'since it is not focused in the optional artefacts.  But the  description of the guidelines from these activities can be found  in ConG4DaS website [9].  2.2 Application Engineering   The AE process guides the DSPL’s configuration when there is a  context change.  The first two activities are mandatory a nd  concern the DSPL’s configuration based on the i* -orthogonal  model. The last activity is optional, because it concerns the  configuration of the DSPL’s feature model.   2.2.1 Contextual Configuration  The first activity of the AE process has the objective of   discovering the possible configurations for current context  in  order to prioritize them in the next activity . There are f our  guidelines in this activity, AE-Guidelines 1 to 4, and we present  them as pseudo -code.   AE-Guideline 1 is for checking whether the new values of the  context variables change contexts’ activation. Listing 1 contains  the pseudo -code for AE-Guideline 1.  AE-Guideline 2 is executed when there is a context change.  Annotated relationships in the i*-orthogonal model are checked.  If the relationship depends on a context that is not active, it is  removed from the new configuration. Listing 2  contains the  pseudo-code for AE-Guideline 2.    Listing 1: AE-Guideline 1  aeGuideline1(){     read context variables;     check facts veracity;     if (facts have changed){        check context activation;   if (context has changed){      call guideline34;   }else {      print ""Context did not change.            No need to reconfigure"";   }     }else{        print ""Facts did not change. No need        to reconfigure"";     }  }    Listing 2: AE-Guideline 2  aeGuideline2(iStarModel){    for each dependency d in iStarModel{      if(d.contextExpression is false){        remove d from iStarModel;      }    }    for each meansEnd me in iStarModel{      if(me.contextExpression is false){        remove me from iStarModel;      }    }    for each taskDecomposition td in iStarModel{      if(td.contextExpression is false){        remove td from iStarModel;      }    }    for each contribution c in iStarModel{      if(c.contextExpression is false){        remove c from iStarModel;      }    }    return iStarModel;  }    In AE-Guideline 3, we analyze the variation points (VPs) in  the i* -orthogonal model returned by AE -Guideline 2. For each  VP, we perform the variant prioritization defined in the next  activity. Listing 3 contains the pseudo -code for AE-Guideline 3.  AE-Guideline 4 is for discovering the softgoals’ priority for  current context, taking the contexts’ order in consideration. The  softgoals’ priority will be used in the prioritization.  Listing 4   contains the pseudo -code for AE-Guideline 4.    Listing 3: AE-Guideline 3  aeGuideline3(iStarModel){    for each element e in iStarModel{      if(e is optional){        toRemove = prioritize(e);        if(toRemove is true){          remove e from iStarModel;        }      }    }    for each meansEndGroup meG in iStarModel{      chosenVariant = prioritizeVariants(meG);      for each variant v in meG{        if(v is not chosenVariant){          remove v from meG;        }      }    }    return iStarModel;  }    Listing 4: AE-Guideline 4  aeGuideline4(contextModel){    for each context c in contextModel{      if(c is active){        add c in activeContexts;      }    }    highC = highestOrderContext(activeContexts);    return highC.getSoftgoalsPriority();  }    2.2.2 Variant Prioritization  In this activity, those available variants are ranked according to  how they contribute to softgoals’ satisfaction, taking into  account the softgoals’ priority for c urrent context. The overall  contribution of a variant to the softgoals is calculated according  to the following function:     priority(𝑣) =  ∑ contPos(𝑣, 𝑠𝑔) sg∈v × priority(𝑠𝑔)  −  ∑ contNeg(𝑣, 𝑠𝑔) sg∈v × priority(𝑠𝑔)   where,  contPos(𝑣, 𝑠𝑔) =  ∑ numPosCont(𝑐, 𝑠𝑔) totalNumCont × typePosCont(𝑐) c∈tc   contNeg(𝑣, 𝑠𝑔) =  ∑ numNegCont(𝑐, 𝑠𝑔)', 'sg∈v × priority(𝑠𝑔)  −  ∑ contNeg(𝑣, 𝑠𝑔) sg∈v × priority(𝑠𝑔)   where,  contPos(𝑣, 𝑠𝑔) =  ∑ numPosCont(𝑐, 𝑠𝑔) totalNumCont × typePosCont(𝑐) c∈tc   contNeg(𝑣, 𝑠𝑔) =  ∑ numNegCont(𝑐, 𝑠𝑔) totalNumCont × typeNegCont(𝑐) c∈tc     where numPosCont(c,sg) represents the number of positive  contributions of the type c the configuration has to the softgoal  sg, while typePosCont(c) is the weight of the positive type of the  contribution c. Analogously, numNegCont(c,sg) represents the  number of negative contributions of the type c the configuration  has to the softgoal sg, while typeNegCont(c) is the weight of the  137', ""negative type of the contribution c. The variable totalNumCont  represent the total number of contributions to the softgoal in the  configuration.  The weight for each type of contribution is given in Table 2.  In i* -orthogonal, positive contributions are: Make, Help, Some+  and Unknown; the negative ones are: Break, Hurt and Some-.   This function takes  into account all positive and negative  contributions of a variant to all softgoals. However, the impact of  the contributions in the final value of the function depends on  the softgoals’ priority, i.e., weight of each softgoal is directly  proportional to i ts priority. After calculating the priority for each  configuration, the variant that should be selected is the one with  the highest value for the priority function.   Table 2: Weight of all contribution link types  Contribution Make or   Break  Help or  Hurt  Some+ or  Some-  Unknown  Weight 1,00 0,75 0,50 0,25    2.2.3 Feature Configuration  This is an optional activity; it is executed if the DSPL's feature  model was created in the Domain Engineering process .  Mandatory features are included and  the traceability table ,  created in the Create Feature Model activity, is used to select the  optional features that are related to the i* -orthogonal elements of  the configuration.    3 RELATED WORK  In this section, we present some related DSPL approaches th at  use both context and NFRs in to model and configure the DSPL’s  variability.   Ali, Dalpiaz and Giorgini [ 1] proposed a contextual goal for  modelling the variability of context -aware systems, including  DSPLs. The i* -orthogonal lan guage [14] was proposed based on  this contextual goal model. Although Ali, Dalpiaz and Giorgini’s  work allows the activation of goal model elements based on  current context, it does not model the relationship between  contexts and softgoals’ priority.   Esfahani, Elkhodary and Malek [ 6] proposed FUSION  (FeatUre-oriented SelfadaptatION), a framework for developing  self-adaptive systems that uses FM for capturing variability and  learning techniques for reasoning about adaptation.  Goals are  used to model functional and non -functional user objectives and  are related to a utility value. Contextual factors are properties  that may affect the goals’ utility. In FUSION, the set of goals is  fixed, the contextual factors o nly affect the level of satisfaction  of a goal, but they do not (de)activate goals, nor they determine  the priority of goals.   Greenwood et al. [ 8] proposed the DiVA (Dynamic  Variability in complex, adaptive systems) RE approach.  NFRs are  captured in the DiVA model in terms of properties that affect  their satisfaction. Properties are related context variables by  property rules. The impact of a variant in a property is modelled  as well. In DiVA, context changes trigger the adaptation.  Then,  the adaptation mechanism checks which properties should be  optimized for the current context. Variants are selected  according to their impact on the relevant properties. When there  is more than one possible configuration, they are scored  according to the properties.   Sawyer et al. [ 18] proposed a DSPL approach that combines  goal modelling and constraint programming  (CP). Goals are  refined until the operationalization level, where an  operationalization is a means for achieving a goal. Claims are  used to indicate the level of satisfaction an operationalization  provides to a softgoal. On the other hand, soft dependencies are  used to express the level of softgoal satisfaction for a particular  context variable value. The goal model is mapped to a constraint  program to let a solver determine the new configuration,  balancing the satisfaction of all softgoals .  Muñoz-Fernández et al. [15] propose a multi-view framework  for Requirements Engineering For self -Adaptive Software  systems (REFAS) . It comprises the REFAS modeling language  and the REFAS process. The REFAS language is based on Sawyer"", 'for Requirements Engineering For self -Adaptive Software  systems (REFAS) . It comprises the REFAS modeling language  and the REFAS process. The REFAS language is based on Sawyer  et al. approach [ 18] and has extended it to include new concepts  divided in five views: soft goals v iew, variability view, context  view, assets view, and soft goals satisficing view .  The soft goals view represents how the soft goals contribute  to each other satisficing level. Differently from the previous  approach, this one allows to represent how the sa tisficing level  of a soft goal affects its contribution to another soft goal.  The  variability view is for modeling goals and their composition until  the operationalization level.  The context view allows the  definition of concern levels and variables, these  concepts are  used to define which parts of the system and environment are  going to be monitored.  The assets view is new and it is used to  model system components, and their association with the  variability view. The soft goals satisficing view is for mode ling  claims and soft dependencies. In REFAS, they support  conditional expressions.    Table 3 summarizes the main modeling characteristics of  these DSPL approaches, comparing them to  ConG4DAS. The  abbreviations or si mplifications used in the table are the  following:  Goals: whether the set of system goals and NFRs may vary  according to the context;   Sat. Level:  whether the context may affect the required  satisfaction level of a NFR,  which may have been modelled as a  system property, softgoal or QoS (Quality of Service);   Priority: whether the context may affect the priority of a  NFR;  Contribution: whether the context affects how an element  contributes to a NFR;   As it can be seen in  Table 3, in Cong4D aS and in [1], the set  of active goals  may vary according to the context. In regard to  whether the context may affect the priority of a NFR , only  ConG4DaS and DiVA [ 8] allow that. In FUS ION [6], Sawyer’s et  al. approach [ 18], and REFAS [ 15] the required level of  satisfaction of a NFR may vary according to the context . In  ConG4DaS, [1], [18] and REFAS [ 15] the contribution of an  element (variant) may depend on a context.         138', 'Table 3: Summary of related work  Approach Goals Sat.  Level  Priority Contribu tion  Ali, Dalpiaz and  Giorgini [1] \uf0fc   \uf0fc  Esfahani,  Elkhodary and  Malek [6]  (FUSION)   \uf0fc    Greenwood et  al. [8] (DiVA)   \uf0fc \uf020 Sawyer et al.  [18]  \uf0fc  \uf0fc  Muñoz- Fernandez et al.  [15] (REFAS)  \uf020 \uf0fc \uf020 \uf0fc\uf020 ConG4DaS \uf0fc  \uf0fc \uf0fc\uf020   4 COMPARISON STUDY  In the previ ous section, we compared ConG4Da S to a set of  DSPL approaches, particularly those that use both context  information and NFRs to select a new configuration.  They were  compared in regard to some modeling ch aracteristics, therefore  we compared only how they model the dynamic variability and  not how this variability is configured.    In this section, the comparison will focus on the  configuration process, i.e., on how variability is configured . The  criterion use d was the overall satisfaction of the DSPL’s non - functional requirements , since that was the goal when proposing  ConG4DaS. Thus, our intention was to find out if the  configurations selected by ConG4DaS differ to the ones selected  by another approach with r espect to the softgoals satisfaction.   To select the other approach for the comparison study,  the  first criterion was that the approach should use both contexts  and NFRs and provide means to model how one affects the other.  We defined this criterion to ensure that we compare our  approach to on that has similar modeling capabilities and, thus,  we would not giv e an unfair advantage to ConG4Da S, by  comparing it to an approach that does not even consider NFRs,  for instance. Also, the approach should have tool support for the  configuration process, so that it would be possible to  automatically generate configurations.    Analyzing Table 3, it is possible to notice that the approach of  Ali, Dalpia z and Giorgini [ 1] and REFAS [ 15] are the ones that  are capable of modeling more characteristi cs related to co ntexts  and NFRs, besides ConG4Da S. Both of these approaches also  have tool support, according to their authors.  Although in A li,  Dalpiaz and Giorgini’s approach [ 1] it is possible to determine  whether a goal or softgoal should be considered in a given  context, it does not allow the softgoals’ priority to change  according to context. Therefore, in a conte xt that would require  another s oftgoal  to be prioritized, in detriment of the ones  previously set as high priority, the configuration process would  not prioritize the right softgoal.    We decided to choose REFAS [ 15] because it is possible to  model the required level of satisfaction of a softgoal according to  the context. Moreover, the authors also provide documentation  and tutorials on how to use their tool, VariaMos 1.  After selecting the second approach for the comparison, we  stated the goal of the comparison study as follows: analyze the  configuration process of ConG4DAS and REFAS for the  purpose of comparing the configurations generated by them  with respect to  the overall softgoals’ satisfaction, from the  point of view of the dom ain analyst , in the context of  two  DSPL examples.  Based on the goal definition, we proposed the  following research question for this assessment:   Do the configurations generated by ConG4DaS provide higher  satisfaction level to the top priority softgoals tha n those generated  by REFAS?  4.1 Metrics and Hypotheses   According to the goal definition, the configurations are  compared based on the satisfaction level of the softgoals.  However, the  criteria to determine softgoals’ satisfaction is are  not clear -cut, maki ng it a difficult task to objectively quantify  their satisfaction level.   Both ConG4DaS and REFAS are capable of modeling  contributions to a softgoal. In both approaches these  contributions may have negative or positive impact and the  number of negative and  positive contributions influences the  satisfaction level of a softgoal. Therefore, we decided to measure', 'contributions may have negative or positive impact and the  number of negative and  positive contributions influences the  satisfaction level of a softgoal. Therefore, we decided to measure  the softgoals’ satisfaction based on the number of positive and  negative contributions to them.   In ConG4DaS, we count as negative the contributions break,  hurt and some-, while the remaining are counted as positive. In  REFAS, we count the contributions complete denial (0)  and  partial denial (1)  as negative, the others are considered positive.  We also calculated the difference between these two totals  (positive - negative) to determine whether there were more  positive or negative contributions to the highest priority  softgoal.   Based on these metrics we defined the following null  hypotheses:   Null hypotheses (H0 1-3): The number of positive (1) and  negative (2) contributions to the highest priority softgoal and the  difference between them (3) in the configurations generated by  ConG4DAS is not different from the configurations generated by  REFAS.  4.2 DSPL Examples   We used two different and simple DSPL exampl es in the  comparison study. The next sub -sections present a brief  description of each example.   4.2.1 Mobile Game  The first DSPL example to be modeled in both approaches was  Mobile Game. This example was adapted from the one presented  in [16] and we also used it to illustrate ConG4D aS in Section 2.  In Mobile Game, the following variations are possible:  (1) the  quality of the game graphics may vary between low and high; (2)  the quality of the sound may also vary between low and high; (3)  the game may use vibration effects , and; (4) t he player may                                                                1 https://variamos.com/home/variamos/  139', 'choose between the single player mode and the multiplayer  mode, which depends on Internet connection.   First, we modeled it using ConG4D aS: t he context and i* - orthogonal models of Mobile Game a re depicted in Figs. 1 and 2,  respectively. Next, VariaMos tool was used to model Mobile  Game following REFAS modeling process and language.  For the  sake of space we do not present all views of REFAS , only th e  variability view is shown in the Discussion  section.  4.2.2 Smart Home  The second example was a Smart Home , adapted from the  example presented in [ 17]. In Smart Home , the following  variations are possible:  (1) t o refresh air insid e it, the Smart  Home may open the windows, if it is not raining, or turn on the  ventilator; (2) t he Smart Home should also provide meal  suggestions for its tenant. It may suggest a home cooked meal, if  there is enough food in stock, or suggest a restaurant  order ; (3)  the Smart Home controls the lights inside the house and it may  turn on the lights only in occupied rooms or, when there is no  one home, simulate occupancy by turning the lights on different  rooms; (4) the Smart Home also controls the food stock , warning  the tenant when it is low .  For the sake of space we do not present the images of Smart  Home models generated using ConG4DaS or REFAS . But these  models can be obtained in [ 9].  4.3 Tools Used  ConG4DaS does not have full too l support: only part of the  Domain Engineering process has tool support, since the tool was  developed for the pr evious approach in which ConG4Da S is  based on. The Application Engineering process is supported by a  rudimentary tool that implements its guidel ines but does not  have a graphic interface.   The tool uses XML files as input for the context and i* - orthogonal models and it reads the current values of the context  variables from a simple text file. It creates a XML for the new  configuration and prints in  the console the elements that have  been removed from the i* model to create the new  configuration. In the discussion section, we present the console  prints.   As stated before, the REFAS approach is supported by the  VariaMos tool, which has an integrated g raphic interface that  allows modeling all five views of the REFAS modeling language  and also the simulation of model configuration. The simulation is  done automatically, i.e., the tool generates all possible  configuration s and the user may navigate from on e to another.   We could not find a way to manually enter the specific  variable values we want to simulate in VariaMos. The user can  navigate through all possible configurations in the graphic  interface, as the tool generates them, or the user can choose to   generate a spreadsheet file with all possible configurations  discovered by the solver VariaMos uses.    In the spreadsheet generated by VariaMos, there are multiple  possible configurations for each scenario. So, w e decided to  randomly select one of the conf igurations for each scenario  for  the comparison . We filtered the configurations in the  spreadsheet by the context variable values to visualize only the  configurations for the scenario we were collecting the data.  Then, we used a random number generator to pick a line number  and used the configuration described in the line .  The files  used in both tools, as well as the simulated variable  values in this simulation based assessment are available at  ConG4DaS website [9].     4.4 Results  After modeling the examples, we simulated different values for  the context variables using both approaches in order to obtain  the configurations selected by them.  The simulated scenarios  correspond to all possible context combinations. For each  configuratio n scenario, we found the highest priority softgoal,  according to the Priority4Context table. Next, we counted the  number of positive and negative contributions for the softgoal in  the selected configuration of each approach.', 'according to the Priority4Context table. Next, we counted the  number of positive and negative contributions for the softgoal in  the selected configuration of each approach.   Since we apply both approaches  to each scenario and we  intend to compare the results obtained from ConG4DaS and  REFAS for the same scenario, the samples are dependent, as in a  paired comparison design . For the results analysis, we used the  Wilcoxon signed rank test, which does not requ ire normally  distributed samples . In the following sub -sections, we present  the results for each example.   4.4.1 Mobile Game  In Mobile Game, there are 4 contexts, thus, there should be 16  possible context combinations, where the first is when all  contexts are not active and the last is (the 16th) when all contexts  are active. However, C4 implies C2, and C2 implies C3.  Therefore, there are 8 contexts combinations that do not violate  the conditions above. The results for the Mobile Game scenarios  are shown in Table 4, where Pos is the number of positive  contributions to the highest priority softgoal in the selected  configuration, Neg is the number of negative contributions to the  same softgoal and Dif is the difference between them (Pos – Neg).   Table 4 - Results for Mobile Game  Scenario ConG4DaS REFAS  Pos Neg Dif Pos Neg Dif  1 3 1 2 1 2 -1  3 4 0 4 1 2 -1  7 4 4 0 2 3 -1  8 2 0 2 0 2 -2  9 2 0 2 2 1 1  11 2 1 1 1 1 0  15 2 1 1 1 1 0  16 2 0 2 2 0 2    Applying the Wilcoxon signed rank test  to compare Pos, we  have to eliminate the pairs corresponding to scenarios 9 and 16,  because the difference ( PosC - PosR) is zero. Then, we have W+ =  21 and W- = 0, which makes W = 0. The critical value of W for n   = 6 at p≤ 0. 05 is 0 . Therefore, we can reject the null hypothesis  H01 for the Mobile Game example. Moreover, since W + > W- ,  there is evidence that the number of positive contributions to the  highest priority softgoal in the configurations selected by  ConG4DaS is grea ter than in those selected by REFAS.    Applying the Wilcoxon signed rank test  to compare Neg, we  have to eliminate the pairs corresponding to scenarios 11, 15 and  140', '16, because the difference (Neg C - NegR) is zero. Hence, the  sample size n is 5, which is not big enough to return a critical  value for W at the level of significance p≤ 0.05. Therefore, the  null hypothesis H02 cannot be rejected, which means the number  of negative contributions is not significantly different between  both approaches, at least for the Mobile Game example.   Applying the Wilcoxon signed rank test  to compare  Dif, we  have to eliminate the pair corresponding to scenario 16, because  the difference ( DifC - DifR) is zero. Then, we have W + = 28 and  W- = 0, which makes W = 0. The critical value of W for n  = 7 at  p≤ 0.05 is 2. Therefore, we can reject the null hypoth esis H03 for  the Mobile Game example. Moreover, since W + > W- , there is  evidence that the difference between the number of positive and  negative contributions to the highest priority softgoal in the  configurations selected by ConG4DaS is greater than in t hose  selected by REFAS.  4.4.2 Smart Home  In Smart Home, there are 5 contexts, i.e. there are 32 possible  combinations and, since all contexts are independent, all 32  scenarios could be simulated.  The results for the Smart Home  example are presented in Table 5.  Applying the Wilcoxon signed rank test  to compare Pos, we  have to eliminate the 10 pairs corresponding to the scenarios  where the difference ( PosC - PosR) is zero. Thus, we have W + =  221 and W - = 32, which makes W = 32. The c ritical value of W  for n = 22  at p ≤ 0.05 is 65. Since W≤65, we can reject the null  hypothesis H01 for the Smart Home example. Moreover, since  W+ > W - , there is evidence that the number of positive  contributions to the highest priority softgoal in the  configurations selected by ConG4DaS is  greater than in those  selected by REFAS.  Applying the Wilcoxon signed rank test  to compare Neg, we  have to eliminate the 9 pairs where the difference (Neg C - NegR)  is zero. Thus, we have W + = 34 and W- = 242, which makes W =  34. The critical value of W fo r n = 23 at p ≤ 0.05 is 73. Since W ≤  73, we can reject the null hypothesis H02 for the Smart Home  example. Moreover, since W + < W- , there is evidence that the  number of negative contributions to the highest priority softgoal  in the configurations selected by REFAS is greater than in those  selected by ConG4DaS.   Applying the Wilcoxon signed rank test  to compare Dif, we  have to eliminate the 9 pairs where the difference ( DifC - DifR) is  zero. Then, we have W+ = 240 and W- = 36, which makes W = 36.  The critical value of W for n = 23 at p≤ 0.05 is 73. Therefore, we  can reject the null hypothesis H03 for the Mobile Game example.  Moreover, since W + > W- , there is evidence that the difference  between the number of positive and negative contributions to  the highest priority softgoal in t he configurations selected by  ConG4DaS is greater than in those selected by REFAS.   4.5 Discussion   The quantitative analysis of the results presented in the previous  section, has shown that ConG4DaS was able to selected  configurations that have more positiv e contributions to the  highest priority softgoal than REFAS, for the majority of the  scenarios in Mobile Game and Smart Home. ConG4DaS also  selected, in most scenarios of Mobile Game and Smart Home,  configurations where the difference between positive and  negative contributions to the highest priority softgoal is greater  than in the ones selected by REFAS.   Table 5 - Results for Smart Home  Scenario ConG4DaS REFAS  Pos Neg Dif Pos Neg Dif  1 1 0 1 1 0 1  2 2 0 2 0 2 -2  3 2 1 1 1 2 -1  4 2 1 1 1 2 -1  5 0 1 -1 0 1 -1  6 1 1 0 2 0 2  7 2 1 1 1 2 -1  8 2 1 1 0 3 -3  9 1 0 1 1 0 1  10 2 0 2 1 1 0  11 1 2 -1 0 3 -3  12 1 2 -1 1 2 -1  13 0 1 -1 1 0 1  14 1 1 0 2 0 2  15 1 2 -1 1 2 -1  16 1 2 -1 0 3 -3  17 1 0 1 1 1 0  18 2 0 2 1 1 0  19 3 0 3 1 2 -1  20 3 0 3 0 3 -3  21 0 1 -1 0 1 -1  22 1 1 0 2 0 2  23 3 0 3 1 2 -1  24 3 0 3 0 3 -3  25 1 0 1 0 1 -1  26 2 0 2 1 1 0  27 2 1 1 2 1 1', '17 1 0 1 1 1 0  18 2 0 2 1 1 0  19 3 0 3 1 2 -1  20 3 0 3 0 3 -3  21 0 1 -1 0 1 -1  22 1 1 0 2 0 2  23 3 0 3 1 2 -1  24 3 0 3 0 3 -3  25 1 0 1 0 1 -1  26 2 0 2 1 1 0  27 2 1 1 2 1 1  28 2 1 1 0 3 -3  29 0 1 -1 0 1 -1  30 1 1 0 0 2 -2  31 2 1 1 2 1 1  32 2 1 1 1 2 -1    In regard to the number o f negative contributions, the   difference between the configu rations selected by ConG4DaS  and REFAS was not significant in the Mobile Game example. But  in the Smart Home example the number of negative  contributions to the highest priority softgoal was small er in the  configurations selected by ConG4D aS, for the majority of the  scenarios.  Although these results indicate that ConG4DaS configuration  process (AE process) was capable to select configurations that  better satisfy the highest priority softgoal in a g iven context, we  cannot generalize them to most DSPLs. There are two main  reasons for that: (1) only two examples of DSPLs were used, so  they are not representative of all DSPL cases; (2) since we did not  have access to companies that use DSPLs, the exampl es used are  not from industrial case studies, but they come from other  studies in the DSPL area.   Aside from the results previously presented, in this section  we would like to discuss other differences between ConG4DaS  and REFAS that were discovered during this simulation based  assessment.  141', 'As it can be seen in Table 3, both ConG4Da S and REFAS are  capable of modeling how the contribution of an element from  the variability model to a softgoal may vary a ccording to the  context. ConG4Da S allows the priority of a softgoal to change  with the context, while in REFAS the required satisfaction (or  satisficing) level of a softgoal may vary according to the context.  Although these concepts are different, they may lead to similar  results.  The main modeling difference between the two approaches is  that in ConG4Da S it is possible to relate an element of the  variability model (th e i* -orthogonal model in ConG4Da S) to a  context. This means that the element is available only when the  related context is  active. To the best of our knowledge, there is  no straightforward solution in REFAS to set this kind of  relationship, therefore, the VariaMos tool may generate some  unwanted configurations.   That is the case of the selected configuration for scenario 9 of  Mobile Game (depicted in Fig. 3), where the Multiplayer Mode   operationalization is selected in a scenario that internet  connection is not available (not C3). In the Smart Home example,  there were scenarios (not presented in this pap er for space  issues) in which the operationalization Warn Tenant About Low  Stock was selected despite the fact that the food stock was not  below 15% (not C2).       Figure 3 - REFAS configuration for scenario 9 of Mobile  Game  In ConG 4DaS, it was possible to determine that Use  Multiplayer Mode  task could only be selected if there was  Internet connection and the mobile data plan was not ending ( C3  ∧ ¬C4). Thus, in Fig. 4 we can see that the Use Multiplayer Mode  task was removed, which means Use Single Player Mode  was  selected. Therefore , we can conclude that for similar cases, in  which it is necessary to create a relationship  between a  variability element and a context, ConG4DaS is more suitable for  modeling them and generating valid configurations.   Another difference between ConG4Da S and REFAS is in the  configuration strategy itself. ConG4Da S uses a utility function,  giving d ifferent weights to softgoals according to their priority  and different weights to contributions depending on their type.  Then, it chooses the configuration that has the highest value.  REFAS transforms the variability model into a constraint  satisfaction p roblem to let a solver find a configuration that fits  the constraints for the given context. However, during the  simulation we found that the solver returns any configuration  that is valid within the constraints, not necessarily an optimal  configuration. T hat also means that, for a given context, one  time it selects one configuration, but when the same context  occurs again it may return a different configuration, as long it  satisfies the constraints.       Figure 4 - ConG4DaS configuration for scenario 9 of  Mobile Game  For example, in  scenario 1 of  Mobile Game , none of the  contexts is active and the softgoal with the highest priority is  High Quality Interaction . But REFAS selected a configuration  where the Vibration operationalization is not present  (see Fig. 5),  even though the battery level was  high (not C1)  and it  contributes positively to the High Quality Interaction softgoal .      Figure 5 - REFAS configuration for scenario 1 of Mobile  Game  Another advantage of ConG4Da S is that negative  contributions are included in the function as a subtraction. Thus,  they have a negative impact on the softgoals satisfaction. In  REFAS, contributions have values from 0 to 4, thus,  contributions that completely or partially deny a softgoal do not  “subtract” from the satisfaction level. This leads to another  problem we found in REFAS configuration process: when one  142', 'operationalization contributes to the softgoal in the required  level for the current context,  the softgoal is considered to have  achieved the required level, even if another operationalization  denies it.  An example of how this affects the selected configurations is  also in the first scenario of Mobile Game, in which the High  Quality Interaction  softgoal should have  the highest priority in  ConG4DaS. In REFAS there is a soft dependency that requires  the highest satisfaction level for this softgoal for this context.   ConG4DaS selected variants that contribute more to the  prioritized softgoal in all va riation points, except for the goal  Provide Gaming Environment  where Use Single Player Mode  was  selected because the other variant required internet connection.  However, the same did not occur when using REFAS. As it can  be seen in  Fig. 5, it selected High Qual ity Sound, but for the  graphics it selected Low Quality  Graphics. The softgoal High  Quality Interaction  was considered  as if it had achieved the  required satisfaction level, since the operationalization High  Quality Sound contributes to it in the desired level and the  “partially denied” contribution given by the operationalization  Low Quality Graphics is not discounted.     5 CONCLUSIONS  In this paper, we pr esented ConG4DaS, a requirements  engineering approach for DSPL that uses contextual goal models  for capturing goals, contexts and NFRs (as softgoals) . ConG4DaS  models the relationship between contexts and the NFRs’ priority  and uses this information in the configuration process.  Contexts  are also used to annotate the  goal model , indicating the context  were a rel ationship is valid. This means that the set of available  elements (including goals and softgoals) varies according to the  context. Moreover, the contribution type of an element to a  softgoal may also  vary according to the  context.  We compared our approach to other DSPL approach es  regarding the ability to model some relationships among  contexts, NFRs and goals. Next, we presented a comparison  study of the configuration process with respect to the overall  softgoals satisfact ion for the selected con figurations. We  compared ConG4Da S to REFAS, a DSPL approach that is similar  to ConG4Da S but that uses constrain programming instead of  utility function in the configuration process.    The results of the comparison show that ConG4D aS  consistently selects configurations that maximize the satisfaction  of softgoals considering the contributions of all variation points.  On the other hand, REFAS may choose configurations that do  not maximize softgoals’ satisfaction for all VPs.   As future wo rk, we intend to create integrated tool support  for both processes of ConG4D aS, since only part of the DE sub - process has tool support  and the AE sub-process is supported by  a different tool . We also intend to evaluate  ConG4DaS  concerning the perceived eas e of use and perceived usefulness .  REFERENCES  [1] Raian Ali, Fabiano Dalpiaz, and Paolo Giorgini. 2013. Reasoning with  contextual requirements: Detecting inconsistency and conflicts. Information  Software Technology  55, 1 (January 2013), 35 -57. DOI  =  http://dx.doi.org/10.1016/j.infsof.2012.06.013   [2] Nelly Bencomo, Svein Hallsteinsen, and Eduardo Almeida. 2012. A View of  the Dynamic Software Product Line Landscape. Computer 45, 10 (October  2012), 36 -41. DOI=http://dx.doi.org/10.1109/MC.2012.292 .  [3] Nelly Bencomo, Pete Sawyer, Gordon Blair,  and Paul Grace. 2008.  Dynamically Adaptive Systems are Product Lines too: Using Model -Driven  Techniques to Capture Dynamic Variability of Adaptive Systems. In  Proceedings of the 2nd International Workshop on D ynamic Software Product  Lines (DSPL 2008), pp. 23 -32.  [4] Jan Bosch, and Rafael Capilla. 2012. Dynamic Variability in Software - Intensive Embedded System Families. Computer 45, 10 (October 2012), 28 -35.  DOI=http://dx.doi.org/10.1109/MC.2012.287', ""[4] Jan Bosch, and Rafael Capilla. 2012. Dynamic Variability in Software - Intensive Embedded System Families. Computer 45, 10 (October 2012), 28 -35.  DOI=http://dx.doi.org/10.1109/MC.2012.287    [5] Paul Clements, and Linda Northrop. 2001. Software Product Lines: Practices  and Patterns . Addison-Wesley Longman Publishing Co., Inc., Boston, MA,  USA.  [6] Naeem Esfahani, Ahmed Elkhodary, and Sam Malek. 2013. A learning-based  framework for engineering feature -oriented self -adaptive software  systems.IEEE Transactions on Software Engineering, 39, 11, pp. 1467 –1493.  DOI=http://dx.doi.org/10.1109/TSE.2013.37   [7] Gemma Grau, Xavier Franch, and Neil A. M. Maiden. 2008. PRiM: An i*-based  process reengineering method for information systems specification. Inf.  Softw. Technol. 50, 1 -2 (January 2008), 76 -100.  DOI=http://dx.doi.org/10.1016/j.infsof.2007.10.006   [8] Phil Greenwood, Ruzanna Chitchyan, Dhouha  Ayed, Vincent Girard-Reydet,  Franck Fleurey, Vegard Dehlen, and Arnor Solberg. 2011. Modelling service  requirements variability: The DiVA way. In Service Engineering: European  Research Results, pp. 55–84. DOI=http://dx.doi.org/10.1007/978-3-7091 -0415 - 6_3  [9] Gabriela Guedes. 2017. ConG4DaS website. Available at  https://sites.google.com/site/cong4das/  [10] Gabriela Guedes, Carla Silva, Jaelson Castro, Monique Soares, Diego  Dermeval, and Cleice Souza. 2012. GS2 SPL: Goals and Scenarios to Software  Product Lines. In Proceedings of the 24th International Conference on Software  Engineering & Knowledge Engineering (SEKE’2012), pp. 651 –656. Redwood  City, San Francisco Bay, USA: Knowledge Systems Institute Graduate School.  [11] Gabriela Guedes, Carla Silva, Monique Soares, and Jaelson Castro. 2015.  Variability Management in Dynamic Software Product Lines: A Systematic  Mapping. In Proceedings of the 2015 IX Brazilian Symposium on Components,  Architectures and Reuse Software  (SBCARS '15). IEEE Computer Society,  Washington, DC, USA, 90-99. DOI=http://dx.doi.org/10.1109/SBCARS.2015.20  [12] Svein Hallsteinsen, Mike Hinchey, Sooyong Park, and Klaus Schmid. 2008.  Dynamic Software Product Lines. Computer 41, 4 (April 2008), 93 -95.  DOI=http://dx.doi.org/10.1109/MC.2008.123   [13] Mike Hinchey, Sooyong Park, and Klaus Schmid. 2012. Building Dynamic  Software Product Lines. Computer 45, 10 (October 2012), 22 -26.  DOI=http://dx.doi.org/10.1109/MC.2012.332   [14] Carlos Lima. 2011.  E-SPL – a approach for requirements phase in domain  engineering and application engineering with goal models  (in Portuguese: “E- SPL -  uma abordagem para a fase de requisitos na engenharia de domínio e  na engenharia de aplicação com modelos de objetivos”). Master’s thesis.   Informatics Center (CIn), UFPE, Brazil.  [15] Juan C. Muñoz-Fernández., Gabriel Tamura, Raúl Mazo, and Camille Salinesi.  2015. Towards a Requirements Specification Multi-View Framework for Self- Adaptive Systems. In CLEI electronic journal, Volume 18, Number 2, Paper 5.  DOI=http://dx.doi.org/10.19153/cleiej.18.2.5   [16] Gustavo G. Pascual, Roberto E. Lopez-Herrejon, Mónica Pinto, Lidia Fuentes,  and Alexander Egyed. 2015. Applying multiobjective evolutionary algorithms  to dynamic software product lines for reconfiguring mobile applications.  Journal of  Systems and  Software 103, C (May 2015), 392 -411.  DOI=http://dx.doi.org/10.1016/j.jss.2014.12.041   [17] João Pimentel, Márcia Lucena, Jaelson Castro, Carla Silva, Emanuel Santos,  and Fernanda Alencar. 2012. Deriving software architectural models from  requirements models for adaptive systems: the STREAM-A approach. Requir.  Eng. 17, 4 (November 2012), 259-281. DOI=http://dx.doi.org/10.1007/s00766 - 011-0126 -z  [18] Peter Sawyer, Raul Mazo, Daniel Diaz, Camille Salinesi, and Daniel Hughes.  2012. Using Constraint Programming to Manage Configurations in Self - Adaptive Systems. Computer 45, 10 (October 2012), 56 -63.  DOI=http://dx.doi.org/10.1109/MC.2012.286"", '2012. Using Constraint Programming to Manage Configurations in Self - Adaptive Systems. Computer 45, 10 (October 2012), 56 -63.  DOI=http://dx.doi.org/10.1109/MC.2012.286   [19] Eric Yu, Paolo Giorgini, Neil Maiden, John Mylopoulos, and Stephen Fickas.  2011.  Modeling Strategic Relationships for Process Reengineering. In Social  Modeling for Requirements Engineering. 1st ed., MIT Press, 2011, pp.11 -152   143']","['COMPARISON OF TWO CONFIGURATION PROCESSES FOR DSPL This  briefin  reports  the  results  of  a comparisoi  betweei  two  coifnuraaoi processes for Dyiamic Sofware Product Liies,  based  oi  the  simulaaoi  of  two DSPL examples. FINDINGS The  configuraon  puocess  of  ConG4DrS (Contextgrl  Gorl  models  Fou  Dynrmic Softwrue puodgct lines)d wrs comprued to the  configuraon  puocess  of  REFAS (Reqgiuements  Eniineeuini  Fou  self- Adrpave Softwrue systems)d.   The  configuraons  obtrined  fuom  both rppuorches weue comprued with uespect to  the  srasfrcaon  of  the  top  puiouity softiorls  (which  crn  be  gsed  to  model non-fgncaonrl  ueqgiuements  in  iorl models)d. Difeuent  context  scenruios  in  two  DSPLi exrmples  –  Mobile  Grme  rnd  Smrut Home – weue simglrted. The metuics gsed weue: \uf0b7 Pos: ngmbeu of posiave contuibgaons to the top puiouity softiorll \uf0b7 Nen: ngmbeu of neirave contuibgaons to the top puiouity softiorll \uf0b7 Dif: the  difeuence  between  the ngmbeu  of  posiave  rnd  neirave contuibgaons  to  the  top  puiouity softiorl (PLos - Nei)d. The  strasacrl  test  wrs  the  Wilcoxon siined urnk test. In  both  exrmples  (Mobile  Grme  rnd Smrut Home)d, ConG4DrS’  Pos cognt wrs hinher thrn  REFAS’,  i.e.,  ConG4DrS selected  moue  vruirnts  with  posiave contuibgaons to the top puiouity softiorl. In the Smrut Home exrmple, ConG4DrS’ Nen cognt  wrs  lower thrn  REFAS’,  i.e., ConG4DrS  selected  feweu  vruirnts  with neirave contuibgaons to the top puiouity softiorl.  Howeveu,  the  ngll  hypothesis wrs  not  uejected  fou  the  Mobile  Grme exrmple.  In  both  exrmples  (Mobile  Grme  rnd Smrut  Home)d,  ConG4DrS’  Dif cognt  wrs hinher thrn  REFAS’,  i.e.,  ConG4DrS selected  configuraons  in  which  the difeuence between posiave rnd neirave contuibgaons to the top puiouity softiorl wrs  hiiheu  the  in  the  ones  selected  by REFAS. Moueoveu, in ConG4DrS, it is possible to uelrte vruirnts rnd vruiraon points (iorls, trsks rnd uesoguces)d to r context, bgt thrt is not tuge fou REFAS. Addiaonrlly, fou rssessini the srasfrcaon of  r  softiorl,  ConG4DrS  trkes  into rccognt  rll  contuibgaons  in  the configuraon.  In  REFAS,  when  one opeuraonrlizraon  contuibgtes  to  the softiorl  in  the  ueqgiued  level  fou  the cguuent context, the softiorl is consideued to hrve rchieved the ueqgiued srasfrcaon level,  even  if  rnotheu  opeuraonrlizraon denies it. The limitraons of these fndinis rue: \uf0b7 They rue brsed on r compruison thrt gsed  only  two  simple  exrmples.  No indgstuirl crse wrs gsedl \uf0b7 The  ueserucheus  hrve  moue expeuience with ConG4DrS thrn with REFAS. Keywords: Dynrmic Softwrue PLuodgct iines Dynrmic Vruirbility Gorl Models Self-Adrpave Systems Who is this briefin  or? Softwrue eniineeuini purcaaoneus who wrnt to mrke decisions rbogt  vruirbility configuraon of Dynrmic  Softwrue PLuodgct iines brsed on  scienafc evidence. Where the fidiins come  rom? All fndinis of this buiefni weue  exturcted fuom the simglraon brsed  compruison condgcted by Ggedes et rl. What is iicluded ii this briefin? The mrin fndinis of the simglraon  brsed compruison rnd buief  infoumraon rbogt the context of the  fndinis. What is iot iicluded ii this briefin? The descuipaon of the DSPLi rppuorches rnd exrmples gsed in the compruison. For additoial ii ormatoi about  LER – Laboratório de Eineiharia de  Requisitos http://www.cin.gfpe.bu/~lleu/ ORIGINAL RESEARCH REFERENCE Grbuielr Ggedes, Crulr Silvr, rnd Moniqge Sorues. Comparing Confggraaon Approaches for Dynamic Softare Prodgct Lines . Burzilirn Symposigm on Softwrue Eniineeuini, 2017 (SBES’17)d.']","**Title: Enhancing Dynamic Software Product Line Configuration with Contextual Goal Models**

**Introduction:**
This briefing presents key findings from a study comparing two approaches for configuring Dynamic Software Product Lines (DSPLs). The goal is to highlight the advantages of the ConG4DaS (Contextual Goal models for Dynamic Software Product Lines) approach over REFAS (Requirements Engineering For self-Adaptive Software systems) in maximizing the satisfaction of non-functional requirements (NFRs) during runtime configuration.

**Core Findings:**
1. **Contextual Modeling**: ConG4DaS effectively captures the relationship between contexts, NFRs, and goals, allowing for dynamic adaptation based on runtime conditions. This contrasts with REFAS, which does not model how contexts affect the priority of NFRs. 

2. **Configuration Process**: The configuration process in ConG4DaS utilizes a utility function that prioritizes configurations based on their contributions to NFRs. In contrast, REFAS employs constraint programming, which may yield valid but not optimal configurations.

3. **Simulation Results**: The study simulated various scenarios for two DSPL examples—Mobile Game and Smart Home. Results indicated that ConG4DaS consistently selected configurations that provided a higher number of positive contributions to the highest priority softgoal compared to REFAS. For instance, in the Mobile Game example, configurations chosen by ConG4DaS showed a significant increase in positive contributions, while the negative contributions were not significantly different.

4. **Practical Implications**: The findings suggest that practitioners seeking to implement DSPLs should consider using ConG4DaS for its enhanced ability to adapt configurations based on varying contexts, leading to better satisfaction of NFRs. This is particularly relevant in domains where user requirements and environmental conditions frequently change.

5. **Limitations and Future Work**: While the results are promising, they are based on two simplified examples and may not generalize to all DSPL applications. Future research should involve more complex scenarios and real-world applications to validate these findings further.

**Who is this briefing for?**
This briefing is intended for software engineers, system architects, and requirements engineers involved in the development of adaptive software systems and those interested in dynamic software product lines.

**Where do the findings come from?**
The findings are derived from a comparative analysis of the ConG4DaS and REFAS approaches, as detailed in the paper ""Comparing Configuration Approaches for Dynamic Software Product Lines"" by Gabriela Guedes, Carla Silva, and Monique Soares, presented at the 31st Brazilian Symposium on Software Engineering (SBES 2017).

**What is included in this briefing?**
The briefing includes a summary of the core findings from the study, highlighting the advantages of ConG4DaS in terms of contextual adaptability and NFR satisfaction in dynamic configurations.

**To access the original research article:**
Guedes, G., Silva, C., & Soares, M. (2017). Comparing Configuration Approaches for Dynamic Software Product Lines. In Proceedings of 31st Brazilian Symposium on Software Engineering (SBES'17). DOI: [10.1145/3131151.3131162](https://doi.org/10.1145/3131151.3131162)"
"['A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments Waldemar Ferreira Federal University of Pernambuco - CIn Av. Prof. Moraes Rego, 1235 Recife 50670-901 wpfn@cin.ufpe.br Maria Teresa Baldassarre University of Bari - DIB Piazza Umberto I, 1 Bari, Italy 70121 mariateresa.baldassarre@uniba.it Sergio Soares∗ Federal University of Pernambuco - CIn Av. Prof. Moraes Rego, 1235 Recife 50670-901 scbs@cin.ufpe.br Bruno Cartaxo Federal Institute of Pernambuco Av. Prof. LuÃŋs Freire, 500 Recife, Brazil bfsc@cin.ufpe.br Giuseppe Visaggio University of Bari - DIB Piazza Umberto I, 1 Bari, Italy 70121 visaggio@uniba.it ABSTRACT Context: Through the years researchers have proposed several ap- proaches to foster the growth and quality of experiments in Soft- ware Engineering. Among these approaches, there are some initia- tives that rely on tool support for specifying controlled experiments. Goal: This paper reports results from a study, which aims to or- ganize, analyze and outline the specifications of each initiative through a comparative analysis. Method: Specifications of each ini- tiative have been compared through a comparative analysis, carried out according to eight criteria: (i) standard empirical concepts, (ii) goals and targets, (iii) involved variables, (iv) subject description, (v) design of experiment, (vi) tasks and activities, (vii) instruments and measurements, and (viii) the threats to research validity. Re- sults: The results show that, among the tools currently existing and used in literature, the eSEE (Experimental Software Engineering Environment) is a complete model. However, it is also the most complex. In the other hand, the most flexible one is Experiment DSL. Conclusion: Based on our results, the currently existing solutions have strengths and weaknesses that should address efforts to make improvements in this area. In principal, our general suggestion is to place emphasis on methodological quality âĂŞ more than on method quantity. CCS CONCEPTS • Software and its engineering → Specification languages ; KEYWORDS Experiment Plan; Experiment Specification; Experimentation ∗Also affiliated with SENAI Innovation Institute for ICT. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. EASE’17, June 15-16 2017, Karlskrona, Sweden © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4804-1/17/06. . . $15.00 https://doi.org/10.1145/3084226.3084258 ACM Reference format: Waldemar Ferreira, Maria Teresa Baldassarre, Sergio Soares, Bruno Car- taxo, and Giuseppe Visaggio. 1997. A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments. InProceedings of 21st In- ternational Conference on Evaluation and Assessment in Software Engineering, Karlskrona, Sweden, June 15-16 2017 (EASE’17), 10 pages. https://doi.org/10.1145/3084226.3084258 1 INTRODUCTION Experiments are an essential method for gathering empirical data in Software Engineering (SE) [22]. However, this empirical method is not simple to carry out. Researchers need to plan, conduct, and analyze the experiment correctly and meticulously; otherwise, its results may not be reliable. Over the last years, the SE community has proposed different approaches to aid the accomplishment of experiments. They include guidelines, tools, infrastructure, and many others [5]. In our work, we only focus on the model-driven approaches that support the scoping and planning phases of an experiment. The scoping phase', 'guidelines, tools, infrastructure, and many others [5]. In our work, we only focus on the model-driven approaches that support the scoping and planning phases of an experiment. The scoping phase is the stage when the objectives and goals of the experiment are defined [35]. While the planning phase is where the foundation for the experiment is laid, such as variables, design, environment, and others [35]. One of the most important documents in any experiment is the experiment protocol [35]. This document serves as a guideline for establishing information as what is expected from the experiment, what it consists of, how it is going to be executed, the data to be collected, and so on. In this context, some authors argue that such information can be (partly or wholly) specified in a model. So that, this model can document the experiment for replication or meta-analysis. Besides, some authors argue that a model-driven approach model-driven may support the conduction of controlled experiments in SE [16]. The definition of what is a model may be broad. In this work, we adopt the definition proposed by Pidd et al. [29] “A model is an external and explicit representation of a part of reality as seen by people who wish to use that model to understand, change, manage, and control that part of reality”. In our context, the “part of reality”', 'EASE’17, June 15-16 2017, Karlskrona, Sweden W. Ferreira et al. is the scoping and planning phases of an experiment. We identified four model-driven approaches that support such phases: ESEML [7], ExpDSL [17], Exper Ontology [18], and eSEE [32]. The first two solutions are based on Domain Specific Languages (DSL) [15]. More- over, the last two are based on ontologies [6]. The approaches mentioned above have brought significant ben- efits to supporting SE experiments. However, in our experience in performing SE experiments, we believe that they have some limita- tions and potential for future improvements. In this scenario, this paper analyzes these approaches with the intention of evaluating, comparing them and making some considerations about how their models represent the scoping and planning phases. We adopted eight criteria as comparison parameters: (i) standard empirical con- cepts, (ii) goals and targets, (iii) involved variables, (iv) subject description, (v) design of experiment, (vi) tasks and activities, (vii) instruments and measurements, and (viii) the threats to research validity. In this work, the main principle when evaluating these criteria is how a model in each approach adheres to the guidelines to conduct an individual experiment. Moreover, , depending on the perspective, such criteria may have different interpretations. This work targets the perspective of the planners of an individual experiment. However, our results are relevant to replication, family of experiment, and meta-studies. We summarize the goal of this work in the following research question: ""How do the model-driven approach to support individ- ual experiments in SE adhere to the guidelines?"". Moreover, the adherence is evaluated according to the eighth criteria cited before. The rest of the paper is composed as follows. Section 2 presents the related work. Section 3.2 provides an overview of the approaches to specify SE experiments. Section 4 presents the criteria used to analyses and compare the approaches. Section 5 presents an ana- lyzes using the criteria previously defined. Section 6 discusses the results from our analysis. Section 7 concludes and offers suggestions for further research. 2 RELATED WORK Despite the growing need to run controlled experiments in software engineering, their development is still very complex. Furthermore, controlled studies need to be replicated because a single controlled experiment may be insufficient and their results are limited con- cerning conclusions’ generalization [5]. Conduction and replication of large-scale experimental SE studies are even more complex. One factor that contributes to that is the lack tools supporting the ex- periment process phases. Concerned about the methodology of SE experiments, many re- searchers published procedures and guidelines aimed at improving the rigor of conducting and reporting such experiments [8, 20, 21, 35]. Afterwards, some other researchers put some effort to identify and compile such relevant publications [20, 27]. However, to the best of our knowledge, no effort has been directed towards cata- loging and analysing the model-driven tools regarding scoping and planning SE experiments. Borges et al. [5] surveyed all the papers published in eight well- known venues in empirical SE. Besides classifying the articles ac- cording to the empirical method, that work also identified the used support mechanisms. According to the authors, a mechanism is any artifice used to support the execution of the empirical method (such as tools, guidelines, and processes). However, the authors only offer a catalog of mechanisms; they do not present any comparison. Freire et al. [16] performed a systematic review of the existing lit- erature to identify the automated support tools for SE experiments. This study identified and analyzed seven support tools of which only five provided support to the scoping and planning phases.', 'erature to identify the automated support tools for SE experiments. This study identified and analyzed seven support tools of which only five provided support to the scoping and planning phases. However, the authors do not present any comparison of the models to specify these phases. Modeling experiments and their evaluations are standard in other areas. For instance, the Simulation Experiment Markup Language (SED-ML) was developed to exchange models the biological model- ing domain [34]. However, it is not true for models to specify SE experiments. 3 BACKGROUND 3.1 Model-Driven Environment A model is an abstraction of a system often used to replace a system under study [25, 26]. In general, a model represents a partial and simplified view of a system, so, the creation of multiple models is usually necessary to better represent and understand the system under study. Modeling is a well-known technique adopted by Engi- neering fields as well as other areas such as Physics, Mathematics, Biology, Economy, Politics and Philosophy [13]. However, in this research, we focus on models in the context of Experimentation in SE and Information Systems fields. That means that our models are thus language-based in nature and tend to describe or prescribe an experiment as opposed, for example, to models in Mathematics which are understood as an interpretation of a theory [9]. In the last decades, approaches have been proposed to support the design and the development of experiments in SE. Some of them were defined in the context of methodological approaches, fundamentally with the purpose of facilitating and sharing a com- mon and coherent vision of the experiment to be carried out and, consequently, of easing the communication among researchers [24]. Moreover, during this last decade, a new trend of approaches has emerged considering models not just as documentation artifacts, but as central artifacts in the software engineering process [23]. In addition to the benefits referred above, it also allows the creation or automatic execution of experiment execution based on those models. These proposals usually have been classified generically as Model-Driven Engineering (MDE) but also by related names such as model-based engineering (MBE), model-driven development (MDD), model-driven software development (MDSD). Regardless of the adopted term and the particular application, all of them share common concepts and terms. Rodrigues da Silva [ 11] reported a survey of the state-of-the-art regarding MDE. 3.2 An Overview of Experiment Specification Models As mentioned in Section 1, some model-driven approaches emerged to support SE experiment. Regarding the supporting of the scop- ing and planning phases in SE experiments, we identified four approaches that provide support to these phases (Table 1).', 'A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments EASE’17, June 15-16 2017, Karlskrona, Sweden Table 1: Models for scoping and planning phases of SE ex- periment Reference Year Paradigm ExpDSL [17] 2013 DSL ESEML [7] 2012 DSL Exper Ontology [18] 2008 Ontology eSEE [32] 2004 Ontology • ExpDSL (Experiment Domain-Specific Language) also com- prises a DSL and tool. However, besides supporting the specification of the experimental procedure, the tool only supports monitoring the execution of an experiment. The model for specifying experiments in ExpDSL has four views (process, metric, experimental plan, questionnaire). The process view is responsible for defining the activities, ar- tifacts, and roles. The metric view describes the metrics collected during the experiment execution. The experimen- tal plan view describes information such as the factors or the statistical design. Finally, the questionnaire view rep- resents all surveys to collect quantitative and qualitative data from participants. • ESEML (Empirical Software Engineering Modeling Lan- guage) is a DSL and tool that supports the specification of experimental plans in SE. The authors describe the DSL following a model also called ESEML. According to the authors, the language enables a researcher to represent all relevant information while the tool allows an automated generation of the experimental plan in PDF. • ExperOntology (Experiment Ontology) is an ontology whose concepts were created to accommodate the representation of SE experiments. It aims to facilitate the reviewing and understanding of experimental lab packages. This ontol- ogy is composed of two levels of detail. The first refers to the general concepts of controlled experiments (similar to the experimental plan view of ExpDSL). The second level only focuses on the laboratory package. Related to ESEML and ExpDSL, there is a tool based on this ontology that supports the execution of SE experiments [31]. • eSEE (Experimental Software Engineering Environment) is an infrastructure capable of instantiating SE environments to manage knowledge about the definition, planning, exe- cution, and packaging of SE experiments. There are two key components to specify experiments in this infrastructure: the glossary and ontologies. The first aims at establish- ing a common terminology in Experimental SE area. The ontologies represent the formalization of the knowledge expressed in the glossary’s list of terms. This work focuses on the ontologies. Freire et al. [16] cited other four tools that support the scoping and planning phases (SESE, FIRE, Ginger2, and Mechanical Turk). However, these tools do not follow a MDE approach. 4 COMPARISON CRITERIA Proper criteria are needed to interpret and scrutinize the analysis of the models presented in Section 3.2. We choose eight criteria that are relevant and complementary for addressing the analysis purposed in this paper. We extracted them from Wohlin et al. [35]. The first criterion is general to the specification of any empirical study. The Other two criteria concern the scoping phase: Goals and Involved Variables. The remaining five criteria concern the planning phase: Subject Description, Design of Experiment, Task, Instrument, and Validity Evaluation. In the following paragraphs, we detail each criterion. Criterion 1: Basic Concepts (Generic Criteria) . Empirical studies share some common features in their description and re- search methodology. This information is an excellent means for introducing the study, no matter what its field and its theme [12]. There are some different definitions for most common concepts in empirical research. In this study, we adopted the APA Manual [2] as reference. The Chapter 2 of this manual describes a set of standards relating to minimal information to any research. The most basic elements are: (i) title, (ii) authorship information (such as name,', 'relating to minimal information to any research. The most basic elements are: (i) title, (ii) authorship information (such as name, institute affiliation, department, and contact), (iii) abstract. Another standard to empirical studies was proposed by Jedlitschka at el. [20]. This guideline includes also keywords and reference information. Criterion 2: Goal (Scoping Criteria) . Primarily, the goals de- fine the scope of an experiment. The purpose of its definition is to ensure that important aspects of an experiment are identified before planning or executing it. According to Wohlin et al. [ 35], and Juristo et al. [ 21], the most crucial element is the scoping phase. However, this criterion is not only restricted to goal spec- ification. It also includes the research questions and hypothesis. For this part of the analysis, we adopted as reference the GQM (Goal/Question/Metric) [4] and PICO (Population, Intervention, Control, Outcome) frameworks [ 33]. The Goal/Question/Metric paradigm is a mechanism for deïňĄning and evaluating a set of op- erational goals, using measurement. In this paradigm, the goals are defined in an operational, traceable way by refining them into a set of quantifiable questions that are used to extract the appropriate in- formation. Therefore, the models have to make a clear relationship among these concepts. In some researches, the goals are described as results of the description of Population, Intervention, Control, and Outcomes. Criterion 3: Involved Variables (Scoping Criteria) . When conducting an experiment, we explore the outcome varying the input variables to a process [35]. Essentially, there are two kinds of variables in an experiment, the independent and dependent vari- ables. The independent variables are those variables that we can control and change. The independent variable is designed to cause an effect that is measured in the dependent variable. The specifica- tion of a variable includes choosing the measurement scales and the range. Typically, the changes in the dependent variables are due to systematic changes in the independent variable, rather than to variations in any uncontrolled extraneous variables (moderator- mediator variables) [3]. Often there is only one dependent variable, and it should, therefore, be derived directly from the hypothesis. A variable is mostly not directly measurable, and we have to measure it via an indirect measure. The description of the variables is the heart of any experiment. All the models have to allow the specifi- cation of experiment’s variables. However, there is a vast range of', 'EASE’17, June 15-16 2017, Karlskrona, Sweden W. Ferreira et al. possibilities to specify the experiment’s variables [35]. Therefore, we compare the specification regarding the data specified by them. Criterion 4: Subject Description (Planning Criteria) . Typi- cally in SE experiments, humans are the subjects applying different treatments to objects. It implies several limitations to the control of the experiment [21, 35]. Firstly, humans have different skills and abilities, which in itself may be an independent variable. Secondly, people learn over time, which means that if one subject applies two methods, the order of application of the methods may matter, and also the same object cannot be used for both occasions. Thirdly, human-oriented experiments are impacted by all sorts of influences and threats, due to the subject’s ability to guess what the experi- menter expects, their motivation for doing the tasks, and others. Hence it is critical for the outcome of the experiment to know how subjects are selected and treated. We have to consider when analyz- ing the models to describe the subject’s characteristics. And similar to involved variables, there is a vast range of possibilities to specify this information [35]. Criterion 5: Design of Experiment (Planning Criteria) . A key factor of any experiment is the Design of Experiment (DoE). The statistical analysis is applied depending on the chosen design, and the measurement scales. The designs range from a simple experiment with a single factor to more complex experiments with many factors. We adopted as references to analyze the models in Montgomery [27], and Juristo [21]. The minimal set of DoE that have to be supported are: Completely Randomized, Randomized Complete Block; Factorial Design 2x2; and Factorial Design 2k. However, other DoE also can be supported (such as, the Taguchi method [28]). Criterion 6: Task (Planning Criteria) . In general, an experi- ment task elicits a change in a human response while interacting with a computer [35]. More precisely, a task is any activity in an experiment that is part of the evaluation. The qualities of a good task are represented and discriminated [21, 36]. Representative ac- tivities mean that the sample performs most closely to reality. It improves external validity (but it also may compromise internal va- lidity). Besides, a task has to discriminate among the test conditions. It increases the likelihood of a statistically significant outcome (i.e., the sought-after “change” occurs). Most experiment tasks are performance-based or skill-based (e.g., inserting an equation, pro- gramming a destination location, or others). However, sometimes the task is knowledge based (e.g., “Use an Internet search interface to find the birth date of Albert Einstein. ”). In this case, participants become contaminated (in a sense) after the first run of a task, since they have acquired the knowledge. In this context, such information has to be taken into considerations when designing an experiment. Therefore, it has to be clearly defined in a models that specify SE experiments. Criterion 7: Instrument (Planning Criteria) . There are three types of instruments in SE experiments, namely objects, guidelines, and measurement instruments [35]. Experiment objects may be, for example, specification or code documents. Guidelines are needed to guide the participants in the experiment. Guidelines include, for instance, process descriptions and checklists. Finally, the mea- surement instruments are any artifices used in the experiment to collect data. All the types of instruments must to be specified by the models. Criterion 8: Validity Evaluation (Planning Criteria) . A fun- damental question concerning results from an experiment is how valid the results are. It is important to consider the question of validity already in the planning phase to plan for an adequate va- lidity of the experiment results. Adequate validity refers to the', 'valid the results are. It is important to consider the question of validity already in the planning phase to plan for an adequate va- lidity of the experiment results. Adequate validity refers to the fact that the results are valid for the population of interest. First of all, the results should be accurate for the population from which the sample is drawn. Secondly, it may be of interest to generalize the results to a broader population. The results are said to have adequate validity if they are valid for the population to which we would like to generalize. There are four types of threats to validity: Internal, external, conclusion, and construct [21, 35]. Therefore, a model for an experiment has to specify all the threats involved in the experiment execution. Besides, it has to determine the actions taken to mitigate each risk. 5 COMPARATIVE ANALYSIS In this section, the models presented in Section 3.2 are analyzed using the criteria defined in Section 4. Each criterion is analyzed sep- arately, namely Basic Concepts in Section 5.1, Goals in Section 5.2, Involved Variables in Section 5.3, Subject Description in Section 5.4, Design of Experiment (DoE) in Section 5.5, Task in Section 5.6, Instruments in Section 5.7, and Validity Evaluation in Section 5.8. 5.1 Basic Elements There are many options to satisfy the criteria presented in Sec- tion 3.2. Table 2 summarizes the accuracy of each model in repre- senting the basic elements presented in Section 4. Table 2: Basic Element Comparison. Basic Elements ExpDSL title, abstract, authors, and keywords ESEML - Exper Ontology author eSEE title, abstract, authors, keywords, references, and others In ESEML, there is an absence of such entities to determine the key elements. Possibly this solution focuses mainly on the experi- ment concepts. In the other hand, the ExpDSL has an entire view only to describe the experimental plan. In this view, a researcher specifies some of the elements presented in Section 4 (Table 2). In fact, this DSL allows the specification of authors name. However, it does not include other information like the author’s affiliation and institution. The ExperOntology has two entities to describe the people involved in the experiment (the Designer and the Replicator). The Designer creates the original experiment, while the Replicators have their profiles associated with the replicated experiment. Only the eSEE allows the specification of all the concept pre- sented in Section 4. The entity Document of Experimental Study in the Ontology of the Experiment Package can specify all the basic elements. Besides, this ontology specifies some meta-information', 'A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments EASE’17, June 15-16 2017, Karlskrona, Sweden about the experiment protocol: comments, date of last change, date of creation, idiom, file name, links title, and version. 5.2 Goals The scoping phase determines the foundation of the experiment. Table 3 summarizes how each model specifies the experiment goals. Table 3: Goal Specification Comparison. Goal Elements Frameworks ExpDSL Goal, Question, and Hypothesis GQM ESEML Goal, Question, and Hypothesis GQM Exper Ontology Goal and Hypothesis - eSEE Goal, Question, and Hypothesis GQM and PICO As presented in Table 3, the ExpDSL allows the specification of all elements when scoping the experiment goal. In theExperimental Plan View , there are three sub-packages: Goal, Research Question, and Research Hypothesis . Regarding the Goal package, there are two entities to specify it, the Simple Goal or Structured Goal . The first specifies it in a paragraph in natural language. Moreover, the Structured Goal defines the goal following the GQM template. The entities Research Question and Research Hypothesis are similar to the Simple Goal. They define their information in a paragraph in natural language. Besides, both Research Question and Research Hypothesis are associated to a goal. In ESEML, the goal specification follows the GQM paradigm. The goals are refined into a set of quantifiable questions so that the goals become operational and traceable. Moreover, the questions define a particular set of metrics and data to collect and provide a framework for their interpretation. Besides, ESEML has an entity hypothesis to specify each hypothesis in natural language. However, there is no relationship between hypotheses and a question, goal or other elements (such as variables or instruments). The ExperOntology has a sub-Ontology (Lab Packages Ontology) only to specify the experiment scope. Besides, Two classes repre- sent the hypothesis in an experiment: the Initial Hypothesis and the Formalized Hypothesis . The Initial Hypothesis is a preliminary hypothesis proposed while the experiment scope is being explored. It comprises the objects of study in agreement with thePurpose and Context. After, an Initial Hypothesis generates a Formalized Hypoth- esis. This last entity includes the Null Hypothesis and Alternative Hypothesis. Unfortunately, the ExperOntology does not provide an explicit support to define the questions and goals involved in the experiment. However, the entity Purpose can be seen as the goal. Finally, the eSEE has two sub-ontologies for defining these goals, questions, and hypothesis. The sub-ontology of Scientific Research specifies the goals, which each goal follows the GQM template. However, the eSEE does not make a clear relationship between goals, questions, and metrics (as recommended by the GQM par- adigm). Besides, this onlotogy specify all the elements required to specify the research goal as recommended by PICO. The sub- ontology of Controlled Study defines both the hypothesis (both the null and alternative) and the research questions. Besides, the ontology connects each hypothesis to its goals and questions. 5.3 Involved Variables An experiment can prevent the analysis from masking the effects of experimental manipulations upon the objects under study. There- fore, a precise description of such variables is very important. All the selected models specify the involved variables. However, they differ in the way how they specify it. In the experimental plan view, the ExpDSL proposes three enti- ties for defining the experiment variables. TheDepVariabledescribes each dependent variable. Each DepVariable includes at least one re- search question or search hypothesis. This association is important to allow data traceability. The Parameter is an entity used to char- acterize the context of the experiment, which means that the trial results will be specific to the conditions defined by these parame-', 'acterize the context of the experiment, which means that the trial results will be specific to the conditions defined by these parame- ters. An experiment has a set of Parameter where each parameter has a fixed value. Both Parameter and DepVariable has a scaleType and range. The last entity is the Factor. This element describes the variables that are manipulated in the experiment, which influence the dependent variables. ESEML has five entities to describe the variables involved in the experiment. The entities Response Variables outline the outputs of the experiments. TheFactor describes the features that intentionally vary during executions of the experiment. Moreover, the Treatment identifies one particular value of the Factor. The entity Parameter specifies the fixed characteristics at a given value in the experiment, so they do not vary throughout the experiment execution process. Finally, the last class is the Experimental Object . It represents the object that “suffers” the performance of the experiment. All these entities have to be in the natural language. In ExperOntology, the Ontology for Lab Package describes the involved variables. Two entities represent the dependent and inde- pendent variables with homonym names. Both dependent and inde- pendent variables have an association with a Formalized Hypothesis (Section 5.2). This ontology identifies the factor by associating an Independent Variable to the Experimental Design (Section 5.5). Be- sides, an Independent Variable can be linked to Experiment Object . This association identifies such controlled variables as the source code or tools involved in the experiment. Finally, a Dependent Vari- able can have an association with the analysis specification. This relationship specifies how the observed data shall be analyzed. The Sub-Ontology of Quantitative Method defines all the selected variables in eSEE. As the previously presented models, the eSEE represents the dependent and independent variable with homonym names. This ontology also defines the Block Variables and the Not- controlled variables . Besides, it defines how to manipulate all these variables and their scales. Moreover, the results of each dependent variable include its corresponding hypothesis. 5.4 Subject Description The selection of participants is important when conducting an experiment. It represents the sample from a population. The results of an experiment can only be generalized to the population if the selection is representative of the desired population. Therefore, this information has to be in the experiment specification. Moreover,', 'EASE’17, June 15-16 2017, Karlskrona, Sweden W. Ferreira et al. as said in Section 4, each model has a proper manner to specify the subject characteristics. Therefore, similar to involved variables (Section 5.3), we are not targeting a specific characteristic. The ESEML has an entity to specify the participants of the experi- ment, the Subject. This object determines the individuals who apply, the techniques, and methods in the experimental unit. However, it not clear which characteristic of the participant are present in ESEML. Besides, this entity is not associated with any other entity in the model. Unfortunately, the ExpDSL does not allow to specify the sample of an experiment. However, the tool support for the experiment based on ExpDSL, a researcher can register each participant of the experiment [1]. In ExperOntology, the entities Subject and Profile are responsible for specifying the subjects involved in the experiment. The Profile records each important characteristic of subject background. Cap- turing the subject background aims at identifying possible influence on results. Moreover, the Profile has a link to the class Question- naire that specifies the instrument to collect this data about the subject (details in Section 5.4). Furthermore, the Subject has an association with three other classes: Experimental Plan (details in Section 5.5), Execution Plan (details in Section 5.6), and Results. The eSEE adopts a different model for specifying the subjects involved in the SE experiments. Two sub-ontologies specify them: Sub-ontology of Participants, roles, and responsibilities and the sub- ontology of the target population of the study. The first ontology defines the experiment executors (professional researcher, student, and professional developer), visitor, and a software engineer (or a researcher). This ontology also specifies the roles for each person involved in the experiment (such as average participant, repetitor, or a practitioner). The former determines the sampling of a population. The sub-ontology of the target population of the study follows the sampling strategies presented by Wohlin et al. [35]. 5.5 Design of Experiment (DoE) To draw meaningful conclusions from an experiment, we apply statistical analysis methods on the collected data to interpret the results. Moreover, it must be carefully planned and designed to get the most out of the experiment. It is also important to know which statistical analyses we can apply depending on the chosen design, and the used measurement scales. Therefore, the models to specify SE experiments have to specify the DoE. Said that, Table 4 summarizes how each model specifies the experiment DoE. The first column presents the supported standard designs. The second column shows if the entities to specify the DoE are associated to other entities in the model (factors, variables, question, and so on.). The entity Design in ESEML defines the experiment design. This class defines the following Standard Design Types: Latin square, one or two-sample comparison, and others. In the model, it is not clear how the entity Design has an association with the other objects in the model. In ExpDSL, the element DoE is responsible for defining the statis- tical design of the experiment. Currently, the model supports three statistical models: Completely Randomized Design, Randomized Complete Block Design, or Latin Square. When a researcher needs Table 4: DoE Specification Comparison. Design of Experiments Associated to Other Entities ExpDSL Completely Randomized Design, Randomized Complete Block Design, Latin Square, and Others Yes ESEML One-Sample Comparison, Two-Sample Comparison, Latin Square, and Others No Exper Ontology General Description Yes eSEE Completely Randomized Design; Randomized Complete Block Design; Factorial Design 2x2; and Factorial Design 2k Yes to set any other DoE, he/she should select the entity “Other”. This entity has a text field to describe the details of the DoE.', 'Design; Factorial Design 2x2; and Factorial Design 2k Yes to set any other DoE, he/she should select the entity “Other”. This entity has a text field to describe the details of the DoE. The ExperOntology has the entity Experimental Design to spec- ify the DoE. It is built combining experiment objects, independent variables, and subjects, in agreement with the hypothesis under investigation. The authors do not make it clear if this ontology supports the Standard Design Types. In eSEE, The sub-ontology Design Process is responsible for describing the Doe. In principle, this sub-ontology supports four Standard Design Types: Completely Randomized Design; Random- ized Complete Block Design; Factorial Design 2x2; and Factorial Design 2k. Besides, this sub-ontology can define the basic principles (Balanced, Blocking, or Randomized) depending on the population size and the number of participants. 5.6 Task The description of the tasks is key action in the experiment de- sign. For instance, in an experiment that involves coding, the tasks can be maintenance, testing, debugging, and others. Besides, the description of the action to be performed in the task, it is funda- mental describe the required artifacts (Section 5.7), the variables to observe (Section 5.3), and the task order. Table 5 summarizes the task representation in each model. Table 5: Task Specification Comparison. Association Artifacts Variables Task Order ExpDSL Yes Yes Sequential ESEML No No Not Clear Exper Ontology Yes Yes Sequential eSEE Yes Yes Sequential In ESEML, the entity Design describes this information. Accord- ing to the authors, the DoE, task description, and task order are closely related. Therefore, the class Design defines all this informa- tion in natural language. Another limitation in ESEML is that a task description is not associated with any other element in the model.', 'A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments EASE’17, June 15-16 2017, Karlskrona, Sweden The ExpDSL provides extensive support to specify the tasks of an experiment. The entity Task has an association with other three entities: Artifact, Field, and Questionnaire. The first defines the artifacts required to execute each task. The entities Field and Questionnaire are somehow similar; they demand explicitly some information provided by the participant (details in Section 5.4). Besides, there is no entity for specifying the task order. The asso- ciation that a Task has with another Task defines the task order. It specifies the next Task to be executed by the participant. The experiment ends when there are no more tasks to perform. The ExperOntology defines the relevant information about the task execution in the entity Execution Plan. An Execution Plan has an association with the Planned Task and the Performed Tasks. These objects specify the status whether a task was accomplished or not. Regarding the task order, the sequential association between the Execution Plan and the Tasks specifies it. Finally, the Experi- ment Object defines the artifacts involved in each task. It can be a Technology (Technique, method, or tool) or an Artifact (details in Section 5.7). In eSEE, the Sub-ontology of the controlled study is responsible for defining the tasks of the experiment. Regarding the task order, the eSEE adopts a model similar to ExpDSL, each task knows the next tasks. The eSEE has a complete description of all the artifacts involved in the experiment (details in Section 5.7). However, it is not clear how each artifact can be associated with a particular task. 5.7 Instruments There are three types instruments in an experiment, namely objects, guidelines, and measurement instruments. The objects (or experi- ment objects) may be, for example, specification or code documents. Guidelines are needed to guide the participants in the experiment. Guidelines include, for instance, process descriptions and checklists. The measurement instruments are any artifice used in an exper- iment to collect data from the subjects. Table 6 summarizes the representation in each model for each type of instrument. Table 6: Instrument Specification Comparison. Objects Guidelines Measurement Instruments ExpDSL Partially No No ESEML Yes Partially Partially Exper Ontology Partilly Yes Yes eSEE Yes Yes Yes The entity Object specifies a set of instruments in ESEML. As said before, the Object defines the experimental unit. However, the concept of experimental unit sometimes does not represent an instrument. For instance, the experimental unit in a SE experiment can then be the software project as a whole. Regarding the other types of instruments, the ESEML is not clear about whether it can or cannot specify the guidelines and measurement instruments. In ExpDSL, the entity Artifact is responsible for determining the instruments. According to the authors, an artifact is one of many kinds of tangible by products produced during the development of software. There is not a clear distinction between an object or a guideline since an artifact can be either a piece of source code as a PDF with the task description. Besides, considering that each task has a description, we can interpret it as the guideline for performing the task. Similar to ExpDSL, the ExperOntolotogy also define the instru- ments through an entity called Artifacts. The artifact can be a Document, Questionnaire, Form, or Tool. The Document can de- scribe the guidelines. Furthermore, the Forms or Questionnaires can describe some measurement instruments. Moreover, the generic Artifacts can specify the other artifacts (such as source code or unit tests). The eSEE has a whole sub-ontology to describe the instruments of the experiments. In this sub-ontology, the entity Document stores relevant information to conduct the experiment such as lists, ques-', 'The eSEE has a whole sub-ontology to describe the instruments of the experiments. In this sub-ontology, the entity Document stores relevant information to conduct the experiment such as lists, ques- tionnaires, and observation notes. This class can represent both measurement instruments and guidelines. Another entity in this sub-ontology is the Software Artifacts. This class represents any software document such as user’s documents, requirements docu- ments, analysis documents, design documents, system documen- tation, business domain model, software process model, and data test case. Finally, the code document represents the program text document, function, and interface. 5.8 Validity Evaluation A fundamental question concerning results from an experiment is how valid the results are. It is important to consider the question of validity already in the planning phase to plan for an adequate valid- ity of the experiment results. As said in Section 4, we analyzed the models according to the specification for the four threats: construct, conclusion, internal, and external validity. Table 7 summarizes how each model specifies the experiment validity. Table 7: Validity Specification Comparison. Type of Treats Mitigation Actions ExpDSL All No ESEML internal, and external Yes Exper Ontology All No eSEE All No In ESEML, the entity Validity specifies the risks in an experiment. Moreover, this entity covers only two types of validity, internal and external valitiy. However, the authors are not clear about whether there is a way for specifying the actions to mitigate these threats. Similar to ESEML, ExpDSL has also only one entity to determine the threats, the Threat to Validity. Each TheatToValidity has an identifier, a description, type of threat (Conclusion, Internal, Con- struct, External), and optionally a control action (in the control action attribute). It can be described in natural language which activity was performed to mitigate the threat. ExperOntology defines one super Type to specify a general threat (the Threat to Validity) and four subtypes to determine the particu- lar type of the validity. According to the authors, the conclusion validity refers to the relationship between the treatment and out- come; internal validity refers to the points that assure there is', 'EASE’17, June 15-16 2017, Karlskrona, Sweden W. Ferreira et al. a causal relationship between the factors and the outcome; the construct validity concerns with the relation between theory and observation; and the external validity concerns with generalization. As ESEML, the ExperOntology is not clear about how to document the actions to mitigate the threats. Finally, the eSEE defines the threat to validity in the Sub-ontology of Experiment Planning. The entity experimental plan has four attributes, one for each type of threat. However, similar to ESEML and ExperOntology, the authors do not mention how to specify the actions to mitigate each risk. 6 DISCUSSION In this section, we discuss the results of our analysis. The purpose is to identify some improvements for each model. Besides, we also suggest in which scenarios each model is most suitable. Table 8 summarizes our analysis. Basic Concepts. Different models focus on the various aspects of the basics concepts. Firstly, we have to raise the question whether it is more profitable to cover more and to be more extensive, or cover less and to be more precise. In our analysis, we realized that âĂŹcompletenessâĂŹ is an element that must be associated both with vertical (i.e., the level of detail) and horizontal (i.e., core elements) dimensions. None of the models evaluated were both extensive and precise. There is always a trade-off in that sense. With that being said, the eSEE is the most adequate to specify an experiment since it is the most extensive. However, it is too complex. Completely specifying an experiment in eSEE is a challenging work, and even with a tool, it is error-prone. Besides, it has few spots to provide flexibility in the specification, For instance; ExpDSL allows a researcher to specify the abstract as structured abstract or simple abstract. Goals. As recommended by Wohlin [35], almost all models pro- vide support to specify the scoping phase. Only ExperOntogy that does not provide complete support for this phase. Besides, similar to the Basic Concepts, the most extensive model is the eSEE. However, the specification of an experiment is not a straightforward process. As recommended by Wohlin et al. [ 35], it is an incremental and iterative process. With that being said, only the ExperOntology takes care of documenting this process with, for instance, the ini- tial Hypothesis and Final Hypothesis. All other models focus only on the last version of these entities. Regarding flexibility, ExpDSL allows a researcher to specify the scope following two methods, a simple specification or based on the GQM template. Involved Variables. All models well describe this concept. We expected it from the models since the variable description is a fun- damental concept in any experiment. However, it is clear that there is no consensus about a terminology for experiments in SE. For instance, all models agree with the term Dependent Variable. How- ever, the ESEML and ExpDSL use the terms Parameter and the ExperOntology use the terms independent variable for the same concept 1. The ExperOntology and eSEE specifies clearly the dis- tinction between the factors and parameters. Regarding the ways to specify this information, in ESEML and ExperOntology these concepts are described in natural language. The ExpDSL allows the description of the scales and ranges for these concepts. However, 1According to Wohlin et al. [35], factor and parameters are independent variables this DSL makes some confusion between what is the variable de- scription and its measurement description2. Only the eSEE explains this distinction. The variables and the measurements are specified separately. However, the eSEE also allows an association between these two concepts. Subject Description. Similar to Variables, the subject descrip- tion is a core concept in any experiment. Surprisingly, the ExpDSL does not provide support to specify it. Both ESEML and ExperOntol-', 'Subject Description. Similar to Variables, the subject descrip- tion is a core concept in any experiment. Surprisingly, the ExpDSL does not provide support to specify it. Both ESEML and ExperOntol- ogy support a limited specification of the sample of the experiment. Only the eSEE provides a complete framework to specify almost all relevant characteristics of the sample of the experiment. Design of Experiment (DoE) . All the models provide excel- lent support to specify the Standard Design Types. However, only the eSEE supports design principles depending on the population sampling. Besides, only the ExpDSL is flexible so that it supports uncommon DoEs. For instance, the ExpDSL can specify a DoE following the Taguchi methods [28]. Tasks. The ESEML is not clear about the task execution. This model defines the tasks, execution, and the DoE altogether (Sec- tion 5.5). The ExpDSL and eSEE specify the sequence of tasks by each task referencing its next tasks. In ExperOntology, the Execu- tion plans have an ordered association to identify the task order. Unfortunately, all these models fall in the same limitation, only spec- ify sequential execution. Wohlin et al. [35] argue that a random task order is desirable since it can avoid the learning effect. An example of experiments that did not follow a sequential implementation of tasks [30]. Validity Evaluation. All the models can specify all types of threats to validity. Some models specify it by one entity represent one type of threat (ExperOntology and eSEE). Other models specify it as an attribute in the generic threat. Moreover, only the ExpDSL can specify the actions to mitigate the threats. The previous paragraphs analysis each approaches regarding each criterion separately. Below, we present a more general sum- mary of each approach: • ESEML. This model is the most simple way of specify- ing the experiment. Any experiment is specified with few entities. Models in ESEML are used to generate an initial version of the experiment protocol. However, it has to be en- riched with more precise information. However, complete models may be addressed in this model by model exten- sions [7]. However, it is not a simple task, since the model is bound to its language. Therefore, any extension has to be followed by an extension to the bound language. We recommend this approach for simple or initial experiments (proof of concept) performed by experienced researcher; • ExpDSL. This approach is more expressible them the ESML. The authors focused on a more practical perspective of the experiment realization. So that, the authors also provide a tool that supports the automation of some experiment management tasks [16]. Regarding the extensibility, it falls in the same limitation as the ESEML, any extension in the model has to be mapped in changes in the bound language. 2Wohlin et al. [35] discuss this distinction in Chapter 7.', 'A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments EASE’17, June 15-16 2017, Karlskrona, Sweden Table 8: Comparative analysis summary. Basic Concepts Goals Variables Subjects DoE Task Instrument Validity eSEE ✓ ✓ ✓ ✓ ✓ ∼ ✓ ✓ Exper Ontology ∼ ✓ ✓ ✓ ✓ ∼ ∼ ✓ ESEML ✗ ✓ ✓ ✓ ∼ ∼ ∼ ✓ ExpDSL ∼ ✓ ✓ ✗ ✓ ∼ ∼ ✓ ✓: It specifies all relevant characteristics ∼: It specifies some relevant characteristics ✗: It does not specify the relevant characteristics This approach is recommended to researcher that can ex- ploit the automation and management of the experiment tasks provided by the tool support; • ExpOntology. This ontology provides a full or partial sup- port to all criteria evaluated in this work. Unlike the previ- ous approaches, this model can be easily extended, since there is no bound language. So that, any information not specified in the model may be specified in the experiment protocol, model extension, or both. We recommend the ExperOntology when the researcher wants traceability in some design decisions for the experiment. This model has some entities that specify historical data important to specify historical information. For instance, this model has entities like Initial Hypothesis and Final Hypothesis , and planned DoE and executed DoE; • eSEE. It is the most complete and expressive model. About all the criteria are fulfilled by this model (only random task execution cannot be specified). Moreover, similar to ExpOntology, any extension of this model no need updates in any bound language. However, any extension of this model is not a simple task, since a change in one class may provoke a cascade change in many other entities. We recommend such model for an inexperienced researcher, or for the researcher that wants a precise specification of the experiment. Following an MDE approach, this last model is relevant since transformation M2M can be used to generate models in any other approach. 7 CONCLUSIONS This paper has presented an analytical study of the currently avail- able models for specifying experiments in SE. In this regard, we have introduced a set of perspectives based on fundamental ele- ments of main guidelines of the SE experiment. The selected criteria were (i) basic concepts, (ii) goals, (iii) involved variables, (iv) sub- ject description, (v) Design of Experiment (DoE), (vi) tasks, (vii) instruments, and (viii) the threats to research validity. We have identified several common deficiencies in the models especially in the description of tasks and instruments. Moreover, we identified the limitations and benefits of each model. In sum- mary, we can say that the most complete and expressive model is the eSEE. On the other hand, the most simple approach is the ESEML. Besides, the ExpDSL focus on the most a most practical perspective of the experiment execution. Finally, we recommend the ExperOntology when the researchers want some tractability in theirs design decisions. As future work, we plan to extend the comparison to other phases in the experiment execution, such as data analysis and experiment packaging. Besides, we are going to include other tools that do not follow an MDE approach or approaches specific to SE sub-domain as languages [19], testing [10], and coding experiments [14]. ACKNOWLEDGMENTS This work was supported by CNPQ, grants 141066/2013-0 and 205613/2014-4. Moreover, it is partially supported by INES, grants CNPq/465614/2014-0, FACEPE/APQ/0388-1.03/14. Sergio Soares is partially supported by CNPq, grant 304499/2016-1. This work took place through a partnership between Cin/UFPE and Uniba. REFERENCES [1] Fellipe Araújo Aleixo, Marília Aranha Freire, Wanderson Câmara dos Santos, and Uirá Kulesza. 2010. A Model-driven Approach to Managing and Customizing Software Process Variabilities.. In ICEIS (3) . 92–100. [2] American Psychological Association and others. 1994. Publication manual of the American psychological association . American Psychological Association', '[2] American Psychological Association and others. 1994. Publication manual of the American psychological association . American Psychological Association Washington. [3] Reuben M Baron and David A Kenny. 1986. The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of personality and social psychology 51, 6 (1986), 1173. [4] Victor R Basili and H Dieter Rombach. 1988. The TAME project: Towards improvement-oriented software environments. Software Engineering, IEEE Trans- actions on 14, 6 (1988), 758–773. [5] Alex Borges, Waldemar Ferreira, Emanoel Barreiros, Adauto Almeida, Liliane Fonseca, Eudis Teixeira, Diogo Silva, Aline Alencar, and Sergio Soares. 2015. Support mechanisms to conduct empirical studies in software engineering: a systematic mapping study. In Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering . ACM, 22. [6] Coral Calero, Francisco Ruiz, and Mario Piattini. 2006. Ontologies for software engineering and software technology . Springer Science & Business Media. [7] Bruno Cartaxo, Italo Costa, Dhiego Abrantes, Andre Santos, Sergio Soares, and Vinicius Garcia. 2012. Eseml: empirical software engineering modeling language. In Proceedings of the 2012 workshop on Domain-specific modeling . ACM, 55–60. [8] Jeffrey C Carver, Natalia Juristo Juzgado, Maria Teresa Baldassarre, and Sira Vegas Hernández. 2014. Replications of software engineering experiments. Em- pirical Software Engineering 19, 2 (2014), 267–276. [9] Chen Chung Chang and H Jerome Keisler. 1990. Model theory . Vol. 73. Elsevier. [10] Iaron da Costa Araújo, Wesley Oliveira da Silva, José B de Sousa Nunes, and Francisco Oliveira Neto. 2016. ARRESTT: A framework to create reproducible experiments to evaluate software testing techniques. In Proceedings of the 1st Brazilian Symposium on Systematic and Automated Software Testing . ACM, 1. [11] Alberto Rodrigues da Silva. 2015. Model-driven engineering: A survey supported by the unified conceptual model. Computer Languages, Systems & Structures 43 (2015), 139–155. [12] Arthur Donovan and Rachel Laudan. 2012. Scrutinizing science: Empirical studies of scientific change . Vol. 193. Springer Science & Business Media. [13] Jean-Marie Favre. 2005. Megamodeling and etymology-a story of words: From MED to MDE via MODEL in five milleniums. InIn Dagstuhl Seminar on Transfor- mation Techniques in Software Engineering, number 05161 in DROPS 04101. IFBI .', 'EASE’17, June 15-16 2017, Karlskrona, Sweden W. Ferreira et al. Citeseer. [14] Waldemar Ferreira. 2014. Together we are stronger: facilitating the conduction of distributed human-oriented experiments. In Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering . ACM, 56. [15] Martin Fowler. 2010. Domain-specific languages . Pearson Education. [16] Marília Freire, D Alencar, Edmilson Campos, Tainá Medeiros, Uirá Kulesza, Eduardo Aranha, and Sérgio Soares. 2013. Automated Support for Controlled Experiments in Software Engineering: A Systematic Review. SEKE, Boston/USA (2013). [17] Marília Aranha Freire, Paola RG Accioly, Gustavo Sizílio, Edmilson Campos Neto, Uirá Kulesza, Eduardo Aranha, and Paulo Borba. 2013. A Model-Driven Approach to Specifying and Monitoring Controlled Experiments in Software Engineering.. In PROFES. Springer, 65–79. [18] Rogério Eduardo Garcia, Erika Nina Höhn, Ellen Francine Barbosa, and José Car- los Maldonado. 2008. An Ontology for Controlled Experiments on Software Engineering.. In SEKE. 685–690. [19] Florian Häser, Michael Felderer, and Ruth Breu. 2016. An integrated tool en- vironment for experimentation in domain specific language engineering. In Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering . ACM, 20. [20] Andreas Jedlitschka, Marcus Ciolkowski, and Dietmar Pfahl. 2008. Reporting experiments in software engineering. In Guide to advanced empirical software engineering. Springer, 201–228. [21] Natalia Juristo and Ana M Moreno. 2013. Basics of software engineering experi- mentation. Springer Science & Business Media. [22] Barbara Kitchenham, Dag IK Sjøberg, Tore Dybå, O Pearl Brereton, David Budgen, Martin Höst, and Per Runeson. 2013. Trends in the Quality of Human-Centric Software Engineering Experiments–A Quasi-Experiment. IEEE Transactions on Software Engineering 39, 7 (2013), 1002–1017. [23] Anneke G Kleppe, Jos B Warmer, and Wim Bast. 2003. MDA explained: the model driven architecture: practice and promise . Addison-Wesley Professional. [24] John Krogstie. 2012. Model-based development and evolution of information systems: A Quality Approach . Springer Science & Business Media. [25] Thomas Kühne. 2006. Matters of (meta-) modeling. Software and Systems Modeling 5, 4 (2006), 369–385. [26] Jochen Ludewig. 2003. Models in software engineering–an introduction.Software and Systems Modeling 2, 1 (2003), 5–14. [27] Douglas C Montgomery. 2008. Design and analysis of experiments . John Wiley & Sons. [28] Glen Stuart Peace. 1993. Taguchi methods: a hands-on approach . Addison Wesley Publishing Company. [29] Michael Pidd. 1997. Tools for thinkingâĂŤModelling in management science. Journal of the Operational Research Society 48, 11 (1997), 1150–1150. [30] José AM Santos, Manoel G de Mendonça, and Carlos VA Silva. 2013. An ex- ploratory study to investigate the impact of conceptualization in god class de- tection. In Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering . ACM, 48–59. [31] Lilian Passos Scatalon, Rogério Eduardo Garcia, and Ronaldo Celso Messias Cor- reia. 2011. Packaging Controlled Experiments Using an Evolutionary Approach Based on Ontology (S).. In SEKE. 408–413. [32] Guilherme Horta Travassos, Paulo Sérgio Medeiros dos Santos, PGM Neto, and Jorge Biolchini. 2008. An environment to support large scale experimentation in software engineering. In Engineering of Complex Computer Systems, 2008. ICECCS 2008. 13th IEEE International Conference on . IEEE, 193–202. [33] James S Trefil. 2001. Encyclopedia of science and technology . Taylor & Francis. [34] Dagmar Waltemath, Richard Adams, Frank T Bergmann, Michael Hucka, Fedor Kolpakov, Andrew K Miller, Ion I Moraru, David Nickerson, Sven Sahle, Jacky L Snoep, and others. 2011. Reproducible computational biology experiments with SED-ML-the simulation experiment description markup language. BMC systems', 'Snoep, and others. 2011. Reproducible computational biology experiments with SED-ML-the simulation experiment description markup language. BMC systems biology 5, 1 (2011), 1. [35] Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and Anders Wesslén. 2012. Experimentation in software engineering . Springer Science & Business Media. [36] Ilze Zigurs and Bonnie K Buckland. 1998. A theory of task/technology fit and group support systems effectiveness. MIS quarterly (1998), 313–334.']","['COMPARING MODEL-DRIVEN APPROACHES FOR SOFTWARE ENGINEERING EXPERIMENTS This  briefin  reports  scieitfc  evideice oi the adoptoi of MDE approaches  to support  the  executoi  of  codiin experimeits. FINDINGS The main MDE approaches to support the executon of experiments,  namey:\x19 \uf0b7 ExpDSL-  Experiment  Domain-Specifc Language; \uf0b7 ESEML - Empiricay Softare Engineering Modeying Language) \uf0b7 ExperOntoyog: - Experiment Ontoyog:; \uf0b7 eSEE  -  Experimentay  Softare Engineering Environment Possibye  criteria  to  compare  approaches to  support  experiment  pyanning  and executon are\x19 \uf0b7 Standard empiricay concepts \uf0b7 Goays and targets \uf0b7 Invoyved variabyes \uf0b7 Subject descripton \uf0b7 Design of experiment \uf0b7 Tasks and actvites \uf0b7 Instruments and measurements \uf0b7 Threats to research vayidit:.   Fig 1. Standard Concept Comparison Fig 2. Goay Specifcaton Comparison Fig 3. Design of Experiment Comparison Fig 4. Artfact Comparison Fig 5. Measurement Comparison Fig 6. Threat Comparison Who is this briefin  or? Softare engineering practtoners tho tant to make decisions about  experimnetaton based on scientfc  evidence. Where the fidiins come  rom? Ayy fndings of this briefng tere  extracted from the comparatve stud:  conducted b: Ferreira Waydemar et ay.   To access other evideice briefins  oi software einiieeriin: http\x19////ease201..bth.se For additoial ii ormatoi about  ESE-INES: https\x19////sites.googye.com//site//eseportay']","**Title: Enhancing Experimentation in Software Engineering through Model-Driven Approaches**

**Introduction:**
This evidence briefing summarizes findings from a comparative study on model-driven approaches for scoping and planning experiments in Software Engineering (SE). The goal is to inform researchers and practitioners about the strengths and weaknesses of existing tools that support the experimentation process, thereby improving the methodological quality of SE experiments.

**Main Findings:**
The study evaluated four model-driven approaches: ESEML (Empirical Software Engineering Modeling Language), ExpDSL (Experiment Domain-Specific Language), Exper Ontology, and eSEE (Experimental Software Engineering Environment). The analysis was based on eight criteria: standard empirical concepts, goals, involved variables, subject description, design of experiment (DoE), tasks, instruments, and threats to research validity.

1. **ESEML**: 
   - **Strengths**: Provides a simple framework for specifying experiments with basic elements like title, authors, and abstract.
   - **Weaknesses**: Lacks comprehensive support for all aspects of experimentation; limited flexibility in extensions.

2. **ExpDSL**: 
   - **Strengths**: Offers a practical perspective with good expressiveness and automation capabilities for experiment management.
   - **Weaknesses**: Does not support subject description, which is critical for generalizing results.

3. **Exper Ontology**: 
   - **Strengths**: Flexible and easily extendable, with a focus on traceability of design decisions through initial and final hypotheses.
   - **Weaknesses**: Limited support for specifying goals and tasks, which may hinder clarity in planning.

4. **eSEE**: 
   - **Strengths**: The most comprehensive and expressive model, fulfilling all criteria except random task execution.
   - **Weaknesses**: Complexity in implementation can lead to challenges in use, making it less accessible for inexperienced researchers.

**Practical Implications:**
- **Methodological Quality**: The findings suggest that while existing models have various strengths, there is a need for improvements in methodological quality over mere quantity of methods. Practitioners should focus on the completeness and clarity of experimental designs.
- **Tool Selection**: Researchers should choose tools based on their specific needs—e.g., eSEE for comprehensive experiments and ExpDSL for practical applications with automation.
- **Future Research**: There is an opportunity to enhance these models by integrating their strengths and addressing their weaknesses, particularly in task and instrument specification.

**Who is this briefing for?**
This briefing is intended for software engineering researchers and practitioners who are involved in planning and conducting experiments and are looking for tools to enhance the quality and rigor of their experimental methodologies.

**Where the findings come from?**
The findings presented in this briefing are derived from a comparative analysis of existing model-driven approaches for scoping and planning experiments in Software Engineering, as reported by Ferreira et al. (2017).

**What is included in this briefing?**
This briefing includes a summary of the strengths and weaknesses of four model-driven approaches, insights on methodological quality, and practical implications for researchers and practitioners in Software Engineering.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**ORIGINAL RESEARCH REFERENCE:**
Waldemar Ferreira, Maria Teresa Baldassarre, Sergio Soares, Bruno Cartaxo, and Giuseppe Visaggio. 2017. A Comparative Study of Model-Driven Approaches For Scoping and Planning Experiments. In Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering, Karlskrona, Sweden, June 15-16, 2017 (EASE’17), 10 pages. https://doi.org/10.1145/3084226.3084258"
"['Empirical evaluations on the cost-effectiveness of state-based testing: An industrial case study Nina Elisabeth Holta,⇑, Lionel C. Briandb, Richard Torkarc,d a Simula Research Laboratory, Martin Linges vei 17, 1325 Lysaker, Norway b SnT Centre, University of Luxembourg, Luxembourg c Chalmers and the University of Gothenburg, Gothenburg, Sweden d Blekinge Institute of Technology, Karlskrona, Sweden article info Article history: Received 28 February 2013 Received in revised form 16 February 2014 Accepted 25 February 2014 Available online 5 March 2014 Keywords: State-based testing UML Cost effectiveness Automated testing Empirical evaluation Industrial case study abstract Context: Test models describe the expected behavior of the software under test and provide the basis for test case and oracle generation. When test models are expressed as UML state machines, this is typically referred to as state-based testing (SBT). Despite the importance of being systematic while testing, all test- ing activities are limited by resource constraints. Thus, reducing the cost of testing while ensuring sufﬁ- cient fault detection is a common goal in software development. No rigorous industrial case studies of SBT have yet been published. Objective: In this paper, we evaluate the cost-effectiveness of SBT on actual control software by studying the combined inﬂuence of four testing aspects: coverage criterion, test oracle, test model and unspeciﬁed behavior (sneak paths). Method: An industrial case study was used to investigate the cost-effectiveness of SBT. To enable the evaluation of SBT techniques, a model-based testing tool was conﬁgured and used to automatically gen- erate test suites. The test suites were evaluated using 26 real faults collected in a ﬁeld study. Results: Results show that the more detailed and rigorous the test model and oracle, the higher the fault- detection ability of SBT. A less precise oracle achieved 67% fault detection, but the overall cost reduction of 13% was not enough to make the loss an acceptable trade-off. Removing details from the test model signiﬁcantly reduced the cost by 85%. Interestingly, only a 24–37% reduction in fault detection was observed. Testing for sneak paths killed the remaining eleven mutants that could not be killed by the con- formance test strategies. Conclusions:Each of the studied testing aspects inﬂuences cost-effectiveness and must be carefully con- sidered in context when selecting strategies. Regardless of these choices, sneak-path testing is a neces- sary step in SBT since sneak paths are common while also undetectable by conformance testing. /C2112014 Elsevier B.V. All rights reserved. 1. Introduction In practice, software testing is often conducted as a manual,ad hoc task, instead of as an automated and systematic procedure. As a result, testing is likely to be incomplete and costly when trying to achieve an adequate level of dependability. Since thorough soft- ware testing is an expensive task, reducing the cost of testing while ensuring sufﬁcient fault-detection effectiveness, is a common goal in the industry. In order for companies to make the right trade-offs when deciding how to test their software, it is essential that they understand how various test strategies compare in terms of cost effectiveness. One approach to software testing derives test cases from a behavior model of the system under test (SUT) and is referred to as model-based testing (MBT)[1]. MBT is not a new research area in software engineering[2] but empirical evidence about its cost- effectiveness, especially in industrial contexts, is very limited. However, in recent years, the level of interest regarding MBT in both industry and academia has been increasing. This is visible from the many reported academic studies[1,3–7] and industrial projects [8–11] on model-based testing. This suggests that there might be an increasing awareness of the beneﬁts provided by MBT [1].', 'from the many reported academic studies[1,3–7] and industrial projects [8–11] on model-based testing. This suggests that there might be an increasing awareness of the beneﬁts provided by MBT [1]. As one of several possible input models for MBT, state machines are widely used to specify the behavior of the most critical and complex components that exhibit state-driven behavior [12]. Many object-oriented methodologies recommend modeling such http://dx.doi.org/10.1016/j.infsof.2014.02.011 0950-5849//C2112014 Elsevier B.V. All rights reserved. ⇑ Corresponding author. Tel.: +47 95087972. E-mail addresses:neh@accedo.no, ninaeho@simula.no (N.E. Holt). Information and Software Technology 56 (2014) 890–910 Contents lists available atScienceDirect Information and Software Technology journal homepag e: www.elsevier.c om/locate/infsof', 'components with state models for the purpose of test automation [13], which is referred to as state-based testing (SBT). This is in part due to the fact that the speciﬁcation of a software product can be used as a guide for designing functional tests[14]. As stated by Off- utt and Abdurazik [14], formal speciﬁcations represent a signiﬁ- cant beneﬁt for testing because they precisely describe what functions the software is supposed to provide. In particular, such speciﬁcations enable the automatic genera- tion of test cases and oracles using MBT tools. Tool support for MBT has improved in recent years, but most of the tools speciﬁ- cally target an application context and cannot be easily adapted to others. A number of tools have been developed to support MBT, e.g., [8,11,15–19]. However, all of them have at least one of the following drawbacks. Well-established standards for modeling the SUT are not supported, thus making it difﬁcult to integrate MBT with the rest of the development process, which in turn makes the adaptation and use of MBT more costly. Often, tools cannot be eas- ily customized to different needs and contexts. For instance, a tes- ter may want to experiment with different test strategies to help target speciﬁc kinds of faults. Moreover, practical constraints can evolve, such as the test-script language a company works with, and such changes are often not easy to accommodate. However, regardless of the applied tool, there are several chal- lenges related to investigating fault-detection effectiveness, for example the possibly limited number of faults that are present in the SUT[12]. Previous work has addressed this problem by seeding artiﬁcial faults into correct versions of the SUT using mutation operators [20,21]. Although the results from various studies[22– 24] have suggested that faults seeded using mutation operators may, under certain conditions, be representative of realistic faults, it is still necessary to study fault-detection effectiveness in more realistic settings, using real faults, if we wish to increase the exter- nal validity of the results. A few studies have partially used natu- rally occurring faults [25]. However, these constituted only a minor percentage of the total number of faults. The research pre- sented in this paper was motivated by a lack of empirical evidence regarding the state-based testing of industrial software with real- istic faults and, thus, complements studies conducted in artiﬁcial settings. Another key challenge in software testing is how to deﬁne and automate the test oracle, to determine whether or not the system is behaving as intended during a test execution. Very few studies empirically evaluate test oracles in state-based contexts. This area deserves more research, since the cost and fault-detection ability of different oracles may vary substantially[13]. Yet another interesting area of SBT, which has hitherto been gi- ven little attention, is the possible beneﬁt of increasing cost effec- tiveness by removing details from the test model. Several studies, e.g.[26,27], focus on lowering the cost of testing by reducing the size of the test suites, while preserving their original coverage. There are conﬂicting results on how this reduction inﬂuences the fault-detection ability of the test suites, in particular with respect to how rigorous the test criteria are. Heimdahl and George[26] ex- pressed concern about using this technique on structural coverage, due to the possible loss in fault-detection. Whereas such test- reduction techniques are based on removing tests in a test suite that do not affect the achieved level of coverage, this paper instead focuses on abstracting the test model itself. This means that not only will the cost of testing potentially be reduced due to a lower number of test cases that need to be generated and maintained, but savings would also result from a less detailed test model that re- quires less modeling and maintenance effort.', 'number of test cases that need to be generated and maintained, but savings would also result from a less detailed test model that re- quires less modeling and maintenance effort. The need for credible empirical results regarding the cost effec- tiveness of SBT leads to the central purpose of this paper: To empir- ically evaluate the cost effectiveness of SBT within the context of a safety–critical system by conﬁguring and applying an extensible tool for automating the test procedure according to various strate- gies and oracles. More speciﬁcally, we will investigate the effect of coverage criteria, test oracle, test model, and sneak-path testing on cost effectiveness. Details regarding the requirements, design and development of the tool can be found in[28]. We used the embedded case study method[29] to evaluate these four aspects of SBT and their inﬂuence on cost effectiveness. The case study was conducted within the context of a research pro- ject at ABB, where a safety-monitoring component in a control sys- tem was developed using UML state machines [30] and implemented according to the extended state-design pattern[31]. In order to evaluate SBT techniques, an MBT tool, the TRansfor- mation-based tool for Uml-baSed Testing (TRUST), was conﬁgured and used to automatically generate the studied test suites. The generated test suites were evaluated using 26 real faults collected in a global ﬁeld study. The ﬁeld study included 11 developers from three ABB departments who solved a maintenance task, split in six sub tasks, for the safety-monitoring component under study. Man- ual code inspections were used to collect actual faults. Results from the case study indicate that the evaluation of cov- erage criteria regarding fault-detection are consistent with the re- sults obtained using artiﬁcial faults in existing research, thus increasing the external validity of those results. We used two ora- cles of different precision levels: (1) the state-invariant oracle checks the state invariant of the resulting state; (2) the state-poin- ter oracle only checks that the array holding a pointer to the cur- rent state is as expected. Applying the most rigorous oracle, i.e., the combination of the state invariant and state pointer oracle, ap- pears to be worthwhile given the signiﬁcant increase in fault- detection effectiveness as compared to applying the state-pointer oracle alone. Interestingly, removing a rather substantial part of state-machine details resulted in a signiﬁcant cost reduction (85% across all six criteria for both oracles), at the expense of an average reduction in fault-detection of 24–37% (depending on the applied oracle). Such results warrant further research into this type of test-suite reduction strategy. The application of sneak-path testing, i.e., testing triggers that should not result in any transition, also highlights the importance of this type of testing: All the mu- tants that remained undetected after applying conformance testing (i.e., checking that the system under test reacts according to the speciﬁed behavior) were killed by the sneak-path tests. In terms of beneﬁts, the comparison of the cost of modeling with the number of generated test cases has shown that using TRUST should yield signiﬁcant cost savings when applying stan- dard state-machine coverage criteria. In other words, the cost of manually writing the same test cases is likely to be larger than the cost of modeling the SUT and generating the test cases. Such beneﬁts will tend to increase with subsequent versions of the SUT. The ﬁndings from this case study are relevant for both industry and academia. Research on software testing that is to be adopted in industrial settings must provide evidence that it is cost-effective for the industry. For this, case studies are essential, in that they give the opportunity to test concepts in a real environment. In addition, the effort required by keeping state machine models', 'for the industry. For this, case studies are essential, in that they give the opportunity to test concepts in a real environment. In addition, the effort required by keeping state machine models updated may be easier for practitioners to accept when there is evidence for the cost effectiveness of SBT. Finally, the directions for future work should be useful as guidance for further research on SBT. The remainder of this paper is structured as follows. Section2 provides background information about SBT, test oracles, test mod- els, and MBT tools. The research methodology is described in Sec- tion 3, whereas the execution of the study is described in Section4. Section 5 reports on the results which are analyzed in Section6, and further discussed in Section7. Threats to validity are addressed in Section8. Finally, Section9 concludes the paper and suggests fu- ture work. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 891', '2. Background 2.1. Concepts State-based testing (SBT) derives test cases from state machines that model the expected system behavior. The system under test (SUT) is tested with respect to how it reacts to different events and sequences of events. SBT thus validates whether the transi- tions that are ﬁred and the states that are reached are compliant with what is expected, given the events that are received. States are normally deﬁned by their invariant, a condition that must al- ways be true when the SUT is in that state. Previous work on SBT has used coverage criteria that were de- ﬁned to cover ﬁnite state machines. However, as an extension to a ﬁnite state machine, those criteria also apply to UML state ma- chines if concurrency and hierarchy are removed[12]. The descrip- tions of the coverage criteria used in the following paragraph are based on deﬁnitions given by Binder[32], pp. 259–266 and Offutt et al.[33]. All transitioncoverage (AT) is obtained if every speciﬁed transi- tion in a state machine is exercised at least once[32]. The order of the exercised transitions is not important. Applying this criterion ensures that all states, events and actions are exercised, and is con- sidered as being the minimum amount of coverage that one should achieve when testing software[32]. All round-trip coverage (RTP) demands that all paths in a state machine that begin and end with the same state be covered. To cover all such paths, a test tree (con- sisting of nodes and edges corresponding to states and transitions in a state machine) is constructed by a breadth- or depth-ﬁrst tra- versal of the state machine. The test tree that corresponds to the RTP strategy is called a transition tree. A node in the transition tree is a terminal node if the node already exists anywhere in the tree that has been constructed so far or is a ﬁnal state in the state ma- chine. Now, by traversing all paths in the transition tree, we cover all round-trip paths and all simple paths (the paths in the state ma- chine that begin with the initial state and end with the ﬁnal state). According to Binder[32], p. 248, this technique will ﬁnd incorrect or missing transitions, incorrect or missing transition outputs and actions, missing states, and will detect some of the corrupt states. A weaker form of the RTP exercises only one of the disjuncts in guard conditions. All transition-pairs coverage (ATP) is given by a test suite that contains tests covering all pairs of adjacent transitions. For each pair of adjacent transitions from stateSi to Sj and from state Sj to Sk in the state machine, the test suite contains a test that traverses the pair of transitions in sequence. A test suite that achieves full predicate coverage (FP) ensures that each clause in each predicate on guarded transitions is tested independently [14]. ThePaths of length nare all possible sequences of transitions of lengthn from the initial state. The above-mentioned test strategies are known asconformance testing, which seek to compare explicitly modeled behavior to ac- tual software execution. However, it is also important to test whether or not the software correctly handles unspeciﬁed behav- ior, that Binder refers to assneak paths [32]. State machines are usually incompletely speciﬁed and this is normally interpreted as events for which the system should not react, i.e., changing states or performing actions. Sneak-path testing sends every unspeciﬁed event in all states. In other words, its aim is to verify the absence of unintentional sneak paths in the software being tested as they may have catastrophic consequences in safety–critical systems. One approach to evaluate the cost effectiveness of SBT strate- gies ismutation analysis. It is carried out by seeding automatically generated faults into ‘‘correct’’ versions of the SUT; one fault is seeded in each mutant version to avoid interaction effects between faults [12]. Mutants are identiﬁed via static analysis of the source', 'seeded in each mutant version to avoid interaction effects between faults [12]. Mutants are identiﬁed via static analysis of the source code by a mutation system, such as in[34]. When a test suite de- tects the seeded fault, we say the test suite has ‘‘killed’’ the mutant. The number of mutants killed by a speciﬁc test suite divided by the number of total mutants, referred to as the mutation score, is used to assess the test suite’s fault-detection ability. Some mutants may be functionally equivalent to the correct version of the SUT. These are called equivalent mutants and should not be included in the pool of mutants used for analysis. 2.2. Related work Some of the existing MBT research has evaluated state-based coverage criteria within the context of UML state machines. The most studied state-based coverage criteria are FP, ATP, RTP, and AT. The FP criterion tends to obtain higher or similar mutation score as ATP, although at a higher cost. With this in mind, the AT, RTP, and ATP coverage criteria were selected for use in this paper. AT has been found not to provide an adequate level of fault detection [12]. With the exception of results reported in [35], where only 54% of mutants were killed, ATP has shown to be a rather strong coverage criterion as compared to AT and RTP, although at a higher cost. RTP was shown to be more cost-effective than AT and ATP[12]. Another study[13] found that RTP testing is not likely to be sufﬁcient in most situations as signiﬁcant numbers of faults remained undetected (from 10% to 34% on average) across subject classes. This is especially true when using the weaker form of RTP. In[36], RTP was compared to random testing. The study concluded that the RTP strategy is reasonably effective at detecting faults; 88% of the faults were detected, as compared to 69% for ran- dom testing. Moreover, their results showed that RTP left certain types of faults undetected, and like in[13], it was suggested that by augmenting RTP with category-partition (CP) testing, the fault-detection can be enhanced, although at an increase in cost that must be taken into account. Several other studies also focus on combining test strategies; in[37,38], RTP was combined with white-box testing, resulting in signiﬁcantly better fault-detection. A study by Briand et al.[39] was conducted that aimed at inves- tigating how data-ﬂow information could be used to improve the cost effectiveness of RTP when more than one transition tree could be generated. The results showed that data ﬂow information was useful for the selection of the most cost effective transition tree. The RTP criterion was further studied by Briand et al.[40] with a focus on how to improve the criterion’s fault-detection effective- ness. They investigated how data ﬂow analysis on OCL guard con- ditions and operation contracts could be used to ‘‘further reﬁne the selection of a cost-effective test suite among alternative, adequate test suites for a given state machine criterion’’[40]. The results suggested that data ﬂow information in a transition tree could be used to select the tree with the highest fault-detection ability. The majority of the results found in existing SBT research are based on studies where mutation operators have been applied to small, non-industrial programs, like the well-known Cruise Control system example [41], p. 595. With the exception of two studies where only a small percentage of the seeded faults were real [14,25], artiﬁcial faults were overwhelmingly used in the reported testing strategy evaluations[12,13,36,42,43]. The few studies that partially use realistic faults, e.g.,[14,25], often fail to mention how those faults were collected. As stated by Andrews et al.[24],a problem when evaluating testing strategies is that real programs with realistic faults are rarely available. As a consequence, little is known about how such structured test approaches compare when detecting realistic faults. When also considering the setting', 'with realistic faults are rarely available. As a consequence, little is known about how such structured test approaches compare when detecting realistic faults. When also considering the setting of the experiment, only two studies [43,44] were executed in industrial settings. Of these two studies, the ﬁrst study[43] did 892 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'not report the nature of the seeded faults, whereas the second study [44] reported the use of mutation operators. The two studies [14,25] that actually did report the use of realistic faults were con- ducted in laboratory settings, and in those cases only 4/20 and 4/21 of the faults were real. Very few studies have compared oracles within the context of SBT; in fact, the study of Briand et al.[13] appears to be unique in its empirical comparison of oracles within this particular con- text. Although not being the sole focus of their study, the results revealed statistically signiﬁcant differences between the two ora- cle strategies. They found that the precise oracle had signiﬁcantly stronger fault-detection abilities than the state-invariant oracle but led to increased cost: Since less attributes are checked, the state-invariant oracle required less resources during both develop- ment and execution. Fault-detection went from 50% when only checking the state invariant to 72% when also checking contract assertions. Reducing the test-suite size by abstracting the test model is yet another area of related work where there is a lack of research with- in the context of SBT. Several studies, in other contexts, focus on lowering the cost of testing by reducing the test suites while pre- serving the original coverage, but at the expense of fault-detection ability. Heimdahl and George[26] found that the size of the spec- iﬁcation-based test suites can be dramatically reduced but that, as a result, the fault detection of the reduced test suites is adversely affected. Wong et al.[27] investigated the effect on fault-detection of reducing the size of a test suite while keeping block and all-uses coverage constant. They found that the reduction in effectiveness was not signiﬁcant, even for the most difﬁcult faults, which sug- gests that the minimization of test suites can reduce the cost of testing at a slightly reduced level of fault-detection effectiveness. Similar to the work in[26], we investigated the fault-detection effectiveness of reduced test suites, but based on a different idea. Whereas test-reduction techniques are based on removing tests that do not contribute to an increase in coverage for the test suite, this paper focuses instead on abstracting the test model itself. This means that not only are the number of test cases in the test suites reduced, but the detail level in the test model is reduced as well, thus reducing its construction and maintenance cost. The ﬁnal aspect of SBT is sneak-path testing. Mouchawrab et al. [38] conducted a series of controlled experiments for the purpose of investigating the impact of RTP on cost and fault detection when compared to structural testing. The study was a replication of the experiment in[13] where one of the ﬁndings was that not testing sneak-path transitions resulted in many faults not being detected. Hence, in the replication experiments they extended the testing strategy by complementing the RTP criterion with sneak paths, as recommended by Binder[32]. The results showed that sneak- path testing clearly improved fault detection. The collected data thus strongly suggests complementing RTP with sneak-path test- ing. No other empirical study evaluates the testing of sneak paths and there are no such studies in realistic industrial contexts. Part of the study reported here, regarding sneak-path testing, has been previously published[45]. To conclude, though some reported research has evaluated state-based coverage criteria, the focus has mostly been directed towards fault-detection effectiveness based on artiﬁcial seeded faults (e.g., with mutation operators) and was not concerned with the cost of such testing. In addition, the majority of the results are based on studies involving small academic programs. The few studies that partially use realistic faults[14,25] tend not to de- scribe how those faults were collected. Our study complements', 'based on studies involving small academic programs. The few studies that partially use realistic faults[14,25] tend not to de- scribe how those faults were collected. Our study complements and extends existing research on the cost effectiveness of SBT by: /C15using an industrial safety–critical control system as subject, /C15using real faults (collected from an industrial ﬁeld study), /C15comparing six state-based coverage criteria, /C15comparing two test oracles at different levels of precision, /C15studying the impact of removing details from the test model, and /C15applying sneak-path testing. 3. Case study design This section describes how our empirical evaluations on the cost effectiveness of state-based testing (SBT) were carried out. Studying multiple aspects within the same context makes this a single embedded case study[29]. 3.1. Research objectives The main purpose of this study was to investigate the inﬂuence of four aspects of cost effectiveness in SBT: six state-based cover- age criteria, two different oracles, two test models at different lev- els of detail, and sneak path-testing. The following research questions were addressed: /C15RQ1: What is the cost and effectiveness of the following state- based coverage criteria: all transitions (AT), all round-trip paths (RTP), all transition pairs (ATP), paths of length 2 (LN2), paths of length 3 (LN3) and paths of length 4 (LN4)? /C15RQ2: How does varying the oracle affect cost and effectiveness? /C15RQ3: What is the inﬂuence of the test model abstraction level on cost and effectiveness? /C15RQ4: What is the impact of sneak-path testing on cost and effectiveness? 3.2. Case and subject selection The case study was conducted within the context of a software process improvement project at ABB. ABB is a global company that operates in approximately 100 countries with 135,000 employees. Its primary business focus is automation technologies, for which the company develops software and hardware. The current project was initiated to investigate which UML dia- grams may be beneﬁcial to ABB, especially during the process of going from the speciﬁcation to the implementation phase, and for improving testing procedures. It provided a unique opportunity to assess the usage of precise, state machine-driven UML modeling and to evaluate state-based testing in a realistic safety–critical development environment. In order to satisfy safety standards (e.g., EN 954 and IEC 61508) and enhance the safety–critical behavior of their industrial ma- chines, ABB developed a new version of a safety–critical system for supervising industrial machines: the safety board. The safety board can safeguard up to six robots by itself and can be intercon- nected to many more via a programmable logic controller (PLC). It was implemented on a hardware redundant computer in order to achieve the required safety integrity level (SIL). The focus of this study is a part of this system, called the Application Logic Control- ler (ALC). The main function of this module is to supervise the sta- tus of all safety-related components interacting with the machine, and to initiate a stop of the machine in a safe way when one of these components requests a stop or if a fault is detected. It should also interface with an optional safety bus to enable a remote stop and a reliable exchange of system status information. The system is conﬁgurable with respect to which safety buses the system can interact with. ThisALC subsystem was chosen for the study as it exhibits a complex state-based behavior that can be modeled as a UML state machine. Complemented by constraints specifying state invariants, the state machine is the main source in the pro- cess of deriving automated test oracles. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 893', '3.3. Data collection procedures To measure the cost and effectiveness of SBT, four surrogate measures were used, inspired by the study of Briand et al.[13]. Cost is measured in terms of: /C15Test-suite preparation time. The time spent on generating the test tree, generating the test suite, and building the test suite. /C15Test-suite execution time. The time spent on executing the test suite, as measured by completion time minus the time where inputs from external devices are simulated. /C15Test-suite size. The number of test cases in a test suite. Effectiveness is measured by: /C15Mutation score. The number of non-equivalent mutants killed divided by the total number of non-equivalent mutants. /C15Timing data was collected by running the experiment on a Win- dows 7 machine with an Intel(R) Core(TM)2 Duo CPU P9400 @ 2.4 GHz processor, and with 2.4 GB memory. Note that time is measured in seconds. A Technical Requirements Speciﬁcation, developed by the busi- ness unit in cooperation with ABB scientists, was the starting point for developing a common understanding of the system. The mod- eling was a cooperative activity between the researchers and ABB. Each modeling activity was closely monitored. The control system was implemented as a joint effort between ABB and Simula researchers. The testing, however, was exclusively performed by the ﬁrst author of this paper. 4. Case study execution Motivated by the lack of extensible and conﬁgurable model- based testing (MBT) tools, Ali et al.[28] proposed an MBT tool, the TRansformation-based tool for UML-baSed Testing (TRUST), to be used in conjunction with other tools. The software architecture and implementation strategy of TRUST facilitated its customization for different contexts by supporting conﬁgurable and extensible features such as input models, test models, coverage criteria, test data generation strategies, and test script languages. The remainder of this section describes how data collection was conducted. The main activities include the preparation of test mod- els (Section 4.1), collecting fault data for creating mutants (Sec- tion 4.2), extending and conﬁguring the testing tool TRUST (Section 4.3), and the generation and execution of test suites (Section 4.4). 4.1. The preparation of test models The original model was tested using TRUST and the round-trip path (RTP) criterion. However, due to limitations in TRUST, some adjustments had to be made to the original model before it could be used as an input for TRUST. Since the ﬂattening component does not support transitions that cross state borders, these transitions had to be re-modeled to and from the super-state edges. This also required adding initial states, entry points, and exit points in sev- eral of the super states. The state-machine ﬂattener does not sup- port multiple events on a single transition. Such transitions were thus transformed to one transition per event. Note that the applied changes did not affect the functionality as originally modeled. The implemented ﬂattening algorithm is a stepwise process that allows the user to modify the UML model at several points during the transformation towards the ﬂattened version. The ﬁrst step in the ﬂattening process is to search all nested levels for submachine states and transform these into a set of simple com- posite states. Next, all simple composite states with one region are transformed to a set of simple states or orthogonal states. If there are orthogonal states present in the model, these may now be transformed to simple composite states. Finally, the simple composite state(s) created in the previous step are transformed to a set of simple states. The result is a state machine consisting of an initial state, simple state(s) and possibly a ﬁnal state. The ﬂat- tening follows a set of transformation rules implemented in Ker- meta [46]. The key aspects in these rules address (1) how to combine concurrent states, and (2) how to redirect transitions.', 'tening follows a set of transformation rules implemented in Ker- meta [46]. The key aspects in these rules address (1) how to combine concurrent states, and (2) how to redirect transitions. Interested readers may consult[47] for more detailed information about the ﬂattening algorithm and its corresponding transforma- tions and implementation. After executing the ﬂattening transformation and removing unreachable state combinations due to conﬂicting state invariants, the ﬂattened state machine consisted of 56 states and 391 transi- tions, mostly guarded. TRUST was executed with the conﬁguration values presented in Table 1 and the ﬂattened state machine de- scribed inTable 2as an input. In this case, TRUST was conﬁgured for the RTP criterion, applied on a test tree, which conforms to the test tree metamodel presented in[28]. The test suite was then executed on what is considered to be a ‘‘correct’’ version of the code, i.e., one that does not cause the test suites to detect failures. Results were analyzed in order to remove actual infeasible test cases and to resolve infeasible transitions caused by unsatisﬁed guard conditions due to externally controlled variables. The latter issue was handled by providing an environ- ment which enables transitions to be ﬁred, and then re-generating the concrete test cases. An example of such an externally con- trolled variable is theenable device, which is managed by the oper- ator and used for starting and stopping the movement of industrial machines. Furthermore, even when a system is carefully speciﬁed and implemented, there is always the risk of introducing discrepancies between the speciﬁcation and the implementation. Minor incon- sistencies between the speciﬁcation and the original version of the SUT were found during RTP testing – these inconsistencies, however, had to be resolved in order to run the tests without errors before moving on with the study. To enable the evaluation of SBT strategies, a ﬁeld study was planned for the purpose of collecting real fault data to be used in the generation of realistic mutants. For this reason, the original version of the system was modiﬁed to include a fourth operation mode: the ExtraSlow mode. Figs. 1 and 2illustrate the change at a high level. The UML state machine consisted of one orthogonal state with two regions. Enclosed in the ﬁrst region are two simple states and two simple-composite states. The simple-composite states contain two and three simple states. The second region encloses one simple state and four simple-composite states that again con- sist of, respectively, two, two, two, and three simple states. This adds up to one orthogonal state, 17 simple states, 6 simple-com- posite states, and a maximum hierarchy level of 2. The unﬂattened state machine contained 61 transitions. Having both concurrent Table 1 Conﬁguration parameters of TRUST. Parameter Value Input model UML2.0 state machine Test model Test tree for round trip paths Coverage criterion All round-trip paths Oracle State invariant + state pointer Test scripting language C++ Test data generation technique Random data generation OCL evaluator EOS 894 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'and hierarchical states, the state machine had to be ﬂattened before being used as an input for test case generation. For this pur- pose, the state-machine ﬂattening component of TRUST was used. On the outset, the ﬂattened model consisted of 82 states and 508 transitions, of which 193 were guarded. However, as addressed above, the ﬂattened model contained infeasible state combinations and transitions, as the current version of the state-machine ﬂat- tener does not automatically remove these. The user can specify preferences in the provided Kermeta[46] transformation. The out- come of the transformation is a model where these combinations are excluded. After removing infeasible transitions, due to both incompatible state invariants (12 state combinations, 145 transi- tions) and guards that cannot evaluate to true (14 transitions), the state machine included 68 simple states (excluding initial and ﬁnal state) and 349 transitions, of which 107 were guarded. The characteristics of the unﬂattened and ﬂattened UML state ma- chines are summarized inTable 3. To observe the effect of having a less precise test model on the cost effectiveness of the testing strategies, the complete test model was modiﬁed. By removing one level in the state hierarchy, the model was abstracted to generate a less precise but correct test model. The contents of every simple composite state were Table 2 A features summary of the hierarchical scale of state machines – the original version of the SUT. State-machine feature Unﬂattened Flattened Maximum level of hierarchy 2 – Number of submachines 0 – Number of simple-composite states 5 – Number of simple states 14 56 Number of orthogonal states 1 – Number of transitions 53 391 Fig. 1. The Mode region in the SUT prior to change task. Fig. 2. The Mode region in the SUT after implementation of the change task. Table 3 A features summary of the hierarchical scale of state machines – the modiﬁed version of the SUT. State-machine feature Unﬂattened Flattened Maximum level of hierarchy 2 – Number of submachines 0 – Number of simple-composite states 6 – Number of simple states 17 68 Number of orthogonal states 1 – Number of transitions (guarded) 61 (17) 349 (107) N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 895', 'removed. Note that, from now on, this model is referred to as the abstract model. Raising the abstraction level prompted questions regarding how to set the inclusion criterion for transitions connected to those modiﬁed super-states. In the end, only transitions that were sourced or targeted in the edge of the super-state were kept (see Fig. 3). This means that the transitions that were targeted in entry points or sourced in exit points contained in the super-states were deleted. This is due to the fact that those transitions do not capture a common behavior to all sub-states of that super-state; such behavior is only common to those sub-states that have incoming transitions to a particular entry point. The same regards outgoing transitions from exit points. Consequently, parts of the super-state behavior are overlooked. Transitions sourced in the edge of a super state, on the other hand, concern all sub states. In order to potentially capture more of the SUT’s behavior, the transitions sourced in exit points belonging to the super-state could also be kept. This means that more of the possible behavior is tested, although with an increased number of infeasible test cases due to unsatisﬁed guard conditions. This is particularly true when the guard contains state variables that are speciﬁc to the re- moved sub-states. Although not impossible, it is difﬁcult to know upfront whether or not the guard can be satisﬁed. It would be applicable if an analysis of the required path was implemented in order to satisfy the guard. The outcome of applying the previously described abstraction approach was a UML state machine consisting of an initial state, a ﬁnal state, two transitions, and one orthogonal state with two re- gions. The ﬁrst region contained one initial state, ﬁve simple states and 14 transitions, whereas the second region contained one initial state, one ﬁnal state, four simple states and 15 transitions (of which seven were guarded). Due to the concurrent state, this model also had to be ﬂattened. The ﬂattened version resulted in one initial state, one ﬁnal state, 25 simple states, and 123 transitions (of which 35 were guarded). After removing four incompatible states and 37 infeasible transi- tions from the ﬂattened version, the abstract model was left with one initial state, 21 simple states, one ﬁnal state and 86 feasible transitions, of which 28 were guarded. The characteristics of the unﬂattened and ﬂattened UML state machines are summarized in Table 4. The SUT was implemented according to the extended state-de- sign pattern [31]. The extended state-design pattern provides a template on how to implement a UML state machine in such a way as to improve traceability from the model to the implementa- tion and facilitate change. It extends the original state pattern by specifying how to implement state and transition actions. The ex- tended state-design pattern localizes state-speciﬁc behavior in an individual class for each state and, hence, puts all the behavior for that state into one class. The pattern allows a system to change its behavior when its internal state changes. This is accomplished by letting the system change the class representing the state of the object.Fig. 4shows an abstracted version of the class diagram of the SUT. It consists of a context classApplicationLogicCon- troller (ALC) that represents the system controlling the ma- chine. The ALC is assigned all responsibilities for handling the state behavior. TheALC is always in two concurrent states. In the Mode region, theALC can be inIdentifyingMode, Auto, Manual, or ManualFullSpeed (ManualFS); in the DriveEnable region, the ALC can be inInit, Enabled, Disabled or Halt. The context class represents the system and its interface with the external environment. TheALC will act differently depending on what state it is in. Two abstract classes,Mode and DriveEnable, represent the states of theALC, and declares interfaces common to all classes', 'environment. TheALC will act differently depending on what state it is in. Two abstract classes,Mode and DriveEnable, represent the states of theALC, and declares interfaces common to all classes that represent different operational states. The subclasses of the abstract classes implement state-speciﬁc behavior, which means that they provide operation implementations that are speciﬁc to every state (and possibly empty) of theALC. Experiences and les- sons learned from applying the extended state design pattern can be found in[48]. The resulting C++ code for the SUT consisted of 26 classes and 3372 LOC (1227 h, 2145 cpp) (without blank lines). 4.2. Collecting fault data for creating mutants For the purpose of evaluating the cost effectiveness of SBT and to increase the external validity of the results, actual fault data was collected in a global ﬁeld study to generate mutated versions of the SUT. The ﬁeld study was conducted at ABB’s ofﬁces in Västerås, Baden, and Shanghai. Having varied UML and domain knowledge, 11 ABB engineers were asked to implement a change task to the modiﬁed version of the SUT. The change task was sug- gested by ABB as a realistic modiﬁcation. They were instructed to modify both the model and code. Participants were instructed to work strictly individually. One researcher supervised the sessions. The maintenance task itself was initially suggested at a high le- vel by ABB; however, it was deﬁned and split into six sub tasks by the researchers, and ﬁnally approved by the company. The mainte- nance task consisted of adding an extra gear or mode,ExtraSlow- Speed, in which industrial machines may be operated. The subjects attended an introductory session where the extended state-design pattern was explained. They were also provided with both textual and graphical documentation, in addition to a manual on how to apply the design pattern. A version of the model and code, representing the SUT after the implementation of the maintenance task, was developed by one re- searcher and tested with each of the coverage criteria within the focus of this study in order to verify its correctness. The faults col- lected from the ﬁeld study, described below, were inserted into this correctly modiﬁed version of the SUT to generate mutants. Manual code inspections of the ten1 solutions produced in the ﬁeld study were used to collect actual faults. In total, 26 faults were detected during the code inspections. This is a low number of faults Fig. 3. Removing details from the test model. Table 4 A features summary of the hierarchical scale of state machines – the abstract version of the SUT. State-machine feature Unﬂattened Flattened Maximum level of hierarchy – – Number of submachines – – Number of simple-composite states – – Number of simple states 25 21 Number of orthogonal states 1 – Number of transitions (guarded) 123 (35) 86 (28) 1 The ﬁeld experiment originally had 11 participants. However, as one of the participants made no modiﬁcations to the code, that particular solution was considered as irrelevant for this study. 896 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'compared to artiﬁcial mutation testing. However, real faults are rarely used in the reported testing strategy evaluations. As a conse- quence, little is known about how such structured test approaches compare in detecting real faults. In this paper, however, the compar- ison is exclusively based on real faults. These faults were introduced in both the model and code or only the latter. Note that because the objective was to compare the fault-detection abilities of the testing criteria, it was crucial that the seeded faults did not cause compila- tion errors. This means that only logical faults that could not be de- tected by the compiler could be selected. The extracted fault data were used to create 26 faulty versions (mutants) of the code by seed- ing one fault per program. The faults in the source code reﬂected the following modeling errors: /C15Missing transitions(Fig. 5): An expected guarded transition trig- gered by a completion event fromState A to State B was missing from the model. One of the participants only accounted for the transition that was explicitly triggered by the e1() event. The participant did not consider the transition that would ﬁre ifg1 was already false. /C15Additional transitions (Fig. 6): Another participant erroneously added transitions fromState Cnot only, as speciﬁed, toState A, but also toState Band State D, and vice versa. /C15Guards that were not correctly updated (Fig. 7): A participant added a clause in the guard on the transition fromState Gto State H so that a transition would be ﬁred only if the mode is in State C or the speed is extraSlow in the concurrent region. /C15Guards that were modiﬁed which should not have been changed (Fig. 8): In one of the erroneous versions, the event handling operation e2()’s guard was modiﬁed so that the state machine would transition fromState Ito State Jonly if the concur- rent region was inState C. Fig. 4. Class diagram of the ALC. Fig. 5. Missing guarded transition. Fig. 6. Additional transitions to and fromState C. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 897', '/C15Erroneous on-entry behavior (Fig. 9): The on-entry behavior of State K was introduced with an error; the sub states,State L and State M, were updated with the same values for state variables. The only common value, however, should have been variable x. Variable y should have been given different values in the two sub states. /C15Incorrect state invariants: State variable x was missing from State C’s state invariant. /C15Missing on-entry behavior:OnEntry() was missing forState C (super-state). Among these faults, eleven were sneak paths. To detect sneak paths, the model should reﬂect the preferred behavior of the sys- tem when exposed to unexpected events. At this stage, our model had no support for the handling of such unexpected behavior. Hence, as sneak paths could not be caught by any conformance test suite generated from the model, only 15 out of 26 mutant pro- grams were detectable by the conformance test suites (AT, RTP, ATP, LN2, LN3, and LN4). 4.3. Extending and conﬁguring the testing tool TRUST In order to use the model-based tool TRUST, it had to be ex- tended and conﬁgured to meet the requirements and conditions of the investigation of this study. TRUST was extended to support the relevant coverage criteria and to support two oracles. More- over, the concrete test-case generator was extended for producing C++ code, in addition to providing a test environment that facili- tates the selection of values for externally controlled variables in guard conditions. Finally, TRUST was extended with support for sneak-path testing. 4.4. Generation and execution of test suites TRUST was used to automatically generate executable test suites by model transformations. The prepared test model, previ- ously introduced as the modiﬁed version of the SUT, was used as an input model for TRUST. As the state-based criteria are deﬁned for ﬁnite state machines, a prerequisite for generating the test suites was to use an input state machine without concurrency and hierarchy. The ﬁrst step in TRUST was thus to ﬂatten[32] the test model, i.e., remove hierarchy and concurrency from the model, as previously described. The ﬂattened state machine was then transformed into test trees by a set of ATL[49] transformation rules. To create the abstract test cases, in the shape of a test tree, each of the algorithms for obtaining test suites satisfying the cov- erage criteria under study were applied to the ﬂattened state machine. To illustrate the scope of the criteria, the following sequences of transitions are generated from the example model shown in Fig. 10: /C15AT: {(t1, t2, t6); (t1, t2, t3, t4); (t1, t2, t3, t5)} /C15RTP: {(t1, t2, t6); (t1, t2, t3[x = 2], t4); (t1, t2, t3[x = 2], t5); (t1, t2, t3[y = 0])} /C15RTP weak:{(t1, t2, t6); (t1, t2, t3, t4); (t1, t2, t3, t5)} /C15ATP: {(t1, t2, t6, t2); (t1, t2, t3, t4); (t1, t2, t3, t5)} /C15LN2: {(t1,t2, t6); (t1, t2, t3)} /C15LN3: {(t1, t2, t3, t4); (t1, t2, t6, t2); (t1, t2, t3, t5)} /C15LN4: {(t1, t2, t3, t4); (t1, t2, t3, t5, t2); (t1, t2, t6, t2, t6); (t1, t2, t6, t2, t3)} The generated test trees were input to the next transformation, which generated executable test cases. In this transformation, Fig. 7. Incorrect guard on transition fromState Gto State H. Fig. 8. Incorrect guard on transition fromState Ito State J. 898 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'TRUST created concrete test cases using MOFScript2 [50], which took the ﬂattened state machine in addition to the generated test tree as an input. The MOFScript transformation traverses the test tree (e.g., the transition tree) to obtain the abstract test cases and transforms them to concrete (executable) test cases, which are writ- ten in a test-scripting language (in our study, we used C++). Each path in the tree produces one test case. That is, an abstract test case consists of a sequence of nodes and edges. Nodes are mapped from states in the state machine and states are deﬁned by state invariants, which are OCL constraints serving as test oracles. An edge contains all the information related to the trigger including event (e.g., an operation call or a signal reception), a guard, and an effect from the state machine’s transitions. A separate C++ ﬁle was created for each test case. A main C++ ﬁle was generated where each of the test cases were invoked. Each test case was invoked on a new instance of the SUT. The test suites were ﬁrst executed on what was considered a ‘‘correct’’ version of the code; that is, one that does not cause the test suites to detect failures. The results were then analyzed in or- der to remove actual infeasible test cases and to resolve infeasible transitions caused by unsatisﬁed guard conditions due to exter- nally controlled variables. The latter issue was handled by provid- ing an environment which enables the transitions to be ﬁred, and then re-generating the concrete test cases. The environment is con- trolled by the test-data values. It is possible to generate several concrete test cases from an abstract test case by using different test-data values. There are many possible approaches for genera- tion of test data[51,52], which are applicable in different situa- tions. TRUST was extended to support intelligent test-data generation; more precisely to provide test data that satisfy guard conditions. Automated test-data generation has shown good results for identifying dynamic test data (e.g.,[53]). In this study, however, the dynamic test-data generation was hard coded due to limited time resources. When the test suites executed success- fully on the ‘‘correct’’ version, they were then run on the mutant versions of the SUT. A batch ﬁle was created for each test strategy to automate the build, execution, and time data collection when executed on the correct and mutated programs. The version of the SUT to be exe- cuted was copied into a Visual Studio project folder. The project was then rebuilt and executed. One result text ﬁle was created for each test strategy. The result ﬁle contains the results of the cor- rect version and the 26 mutants. Note that the conformance test suites were only run on the mutants that were not based on sneak paths. The rationale for this decision is simply that it is impossible to detect sneak paths with conformance test suites. In summary, the following generation and execution steps were automated: 1. Flatten input state machine. 2. Select test adequacy criterion. 3. Construct abstract test cases in the form of a test tree. The algo- rithms used to traverse the state machine were described previously. 4. Select oracle. 5. Traverse the tree and select test data to generate concrete test cases. One test suite is generated per tree. 6. Build and execute the test suite on the correct version. 7. Ensure that the test suite is executed with no errors. 8. Then build again and execute the test suite on each of the mutant programs. 9. Analyze test results provided by the selected test oracle on all mutants. For certain coverage criteria, like AT, RTP, and ATP, the genera- tion of test trees from state machines is not deterministic, since several test trees could possibly satisfy the criterion. The structure of the tree depends on the sequence of the selected outgoing tran- sitions when traversing the state machine as illustrated byFigs. 11', 'of the tree depends on the sequence of the selected outgoing tran- sitions when traversing the state machine as illustrated byFigs. 11 and 12. There are two types of dissimilarities: symmetric and semantic. Only the latter type has impact on the test results. The Fig. 9. Erroneous on-entry behavior that set state variables. Fig. 10. Example state machine. 2 A transformation language that generates text from models by following model- to-text transformation rules.  Fig. 11. Example state machine. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 899', 'trees inFig. 12were generated by following the round trip path cri- terion using breadth ﬁrst traversal. Even though the four trees dif- fer symmetrically, Tree 1 is semantically equal to Tree 3 and Tree 2 is semantically equal to Tree 4. This implies that for example Tree 1 differs semantically from both Tree 2 and Tree 4. Thus, due to pos- sible differences in fault-detection power, the results of executing different test suites that fulﬁll the same test criterion may differ. Such random variations in the results were accounted for by repeating the experiment 30 times. Traditionally, in many ﬁelds of science,n = 30 has been a common rule of thumb to enable sta- tistical testing[54]. We also complement our tests for signiﬁcance with measures for effect size. Thus, 30 different trees were created using a random selection of outgoing transitions from states to generate distinct test suites. The test suites were obtained by tra- versing each of the 30 test trees and covering all paths. By selecting without a replacement from the population of all possible trees that achieves each of the criteria, only trees distinct from already selected trees were added to the selection. 5. Results Tables 5–8report the obtained results for each combination of coverage criteria, test oracle, and test model. The tables present descriptive statistics for each of the surrogate measures on cost and effectiveness: test-suite size (Table 5), test-suite preparation time (Table 6), test-suite execution time (Table 7), and mutation score (Table 8). Note that the state invariant oracle applied to- gether with the state pointer oracle is referred to as oracle O1; the state pointer oracle applied in isolation is referred to as oracle O2. The size of the sneak-path test suite is equal to the number of states in the SUT (68 simple states in the complete model and 21 simple states in the abstract model).Tables 9 and 10show the time spent on preparing and executing a selection of test cases from the sneak-path test suite. We only measured preparation and execu- tion time for the minimum number of sneak-path test cases that were required in order to kill all mutants.Table 11shows realistic estimates for the entire sneak-path test suites. All eleven sneak path faults were detected by the sneak path test suite generated from the complete model. Due to an infeasible test case, the execu- tion of the sneak-path test suite generated from the abstract model detected 10 out of 11 sneak paths. 6. Analysis The analysis is divided into four parts: Sections6.1–6.4 address each of the four research questions. For each research question, we present statistical tests on the data for AT, RTP, and ATP; and an analysis of cost and effectiveness. We use thebA12 statistic by Var- gha–Delaney [55] to evaluate the effect size. Its range from 0 to 1 is divided in three categories: Small, medium, and large. Values ofbA12 such that 0:36 < bA12 6 0:44 or 0:56 6 bA12 < 0:64 indicate small ef- fect size. Values of bA12 such that 0 :29 < bA12 6 0:36 or 0:64 6 bA12 < 0:71 indicate medium effect size. Values ofbA12 such that 06 bA12 6 0:29 or 0:71 P bA12 6 1 indicate large effect size. A value of 0.5 indicates no difference between the populations. 6.1. RQ 1: What is the cost and effectiveness of the state-based coverage criteria all transitions (AT), all round-trip paths (RTP), all transition pairs (ATP), paths of length 2 (LN2), paths of length 3 (LN3) and paths of length 4 (LN4)? In this section, we compare the cost effectiveness of six state- based coverage criteria: all transitions (AT), all round-trip paths (RTP), all transition pairs (ATP), paths of length 2 (LN2), paths of length 3 (LN3), and paths of length 4 (LN4). The state-invariant ora- cle was applied together with the state-pointer oracle. Also note that test cases were generated from a complete test model. 6.1.1. Statistical tests Tables 12–14 show the results from the paired Wilcoxon', 'that test cases were generated from a complete test model. 6.1.1. Statistical tests Tables 12–14 show the results from the paired Wilcoxon signed-rank tests that were performed to test for statistically sig- niﬁcant differences in the 30 replications of AT, RTP, and ATP. The purpose of the tests was to reject the following null hypotheses: H0-cost: There are no signiﬁcant differences in cost when applying the testing strategies AT, RTP, and ATP on the complete model when combined with oracle O1. H0-eff: There are no signiﬁcant differences in effectiveness when applying the testing strategies AT, RTP, and ATP on the complete model when combined with oracle O1. The statistical tests executed on preparation and execution time resulted in signiﬁcantly different results – ATP spent signiﬁcantly more time on both preparing and executing the test suites than AT and RTP. Effect sizes of the differences in preparation and exe- cution time are also provided.Tables 12 and 13present large effect sizes for all tests regarding costs; indicating practically signiﬁcant differences in cost among test strategies. No signiﬁcant differences were found in terms of mutation score though. Both RTP and ATP killed all mutants. AT killed all mutants in 29 out of 30 test suites – the ﬁnal test suite killed 14 out of 15 mutants. The effect size of 0.5 shows a 50% probability of either strategy performing better. Fig. 12. Possible trees generated fromFig. 11. 900 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'In short, ATP costs more than AT, which again costs more than RTP. Regardless of these differences, however, the three strategies yielded a similar mutation score. Thus, RTP can be considered to be the most cost-effective strategy when using the complete model and the combination of the state-invariant and state-pointer oracles. Based on the obtained results, the null hypothesis was rejected for the surrogate measures for cost. However, no evidence was found to reject the null hypothesis with respect to effectiveness (mutation score). 6.1.2. Cost effectiveness analysis This section focuses on the relationship between cost and effec- tiveness. Each of the surrogate measures of cost and effectiveness has been depicted inFig. 13: Dark colors indicate high values; light colors represent low values. We observed that ﬁve of the criteria Table 5 Test-suite sizes. Coverage criterion Test model Oracle Test-suite size total Test-suite size feasible Infeasible test cases (%) Diff. between O1 and O2 (%) Diff. between abstract and complete (%) (O1) Diff. between abstract and complete (%) (O2) AT Complete O1 166 166 0 0 /C089.8 /C080.1 O2 166 166 0 Abstract O1 33 17 48.5 /C048.5 O2 33 33 0 RTP Complete O1 299 299 0.0 0 /C077.9 /C070.2 O2 299 299 0.0 Abstract O1 89 66 25.8 /C025.8 O2 89 89 0 ATP Complete O1 1425 1425 0 0 /C086.5 /C079.2 O2 1425 1425 0 Abstract O1 301 192 36.2 /C035.1 O2 301 296 1.7 LN2 Complete O1 27 27 0 0 /C014.8 /C07.4 O2 27 27 0 Abstract O1 25 23 8.0 /C08.0 O2 25 25 0 LN3 Complete O1 143 143 0 0 /C029.4 /C014.0 O2 143 143 0 Abstract O1 123 101 17.9 /C017.9 O2 123 123 0 LN4 Complete O1 764 764 0 0 /C045.7 /C023.6 O2 764 764 0 Abstract O1 585 415 29.1 /C028.9 O2 585 584 0.2 Table 6 Descriptive statistics – preparation time. Coverage criterion Oracle Model Min (s) Q1 (s) Mean (s) Median (s) Q3 (s) Max (s) St. dev. N Diff. between abstract and complete (%) Diff. between O2 and O1 (%) (abstract) Diff. between O2 and O1 (%) (complete) AT O1 Abstract 173 180 222 192 210 972 143 30 /C094.5 /C03.8 /C07.0 Complete 2506 2763 3995 3013 3665 15,939 3264 30 O2 Abstract 168 175 213 185 202 965 143 30 /C094.3 Complete 2249 2474 3715 2766 3394 15,663 3280 30 RTP O1 Abstract 182 184 204 193 210 309 30 30 /C061.6 /C04.7 0.2 Complete 484 512 531 525 545 607 28 30 O2 Abstract 179 184 194 189 200 250 17 30 /C063.5 Complete 476 492 533 523 559 675 51 30 ATP O1 Abstract 1089 1105 1115 1111 1123 1150 14 30 /C096.1 2.3 /C00.6 Complete 28,377 28,576 28,819 28,797 29,028 29,398 273 30 O2 Abstract 1088 1097 1141 1116 1161 1368 64 30 /C096.0 Complete 28,305 28,409 28,641 28,504 28,787 29,548 345 30 LN2 O1 Abstract 83 83 83 83 83 83 – 1 /C034.1 1.2 0.0 Complete 126 126 126 126 126 126 – 1 O2 Abstract 84 84 84 84 84 84 – 1 /C033.3 Complete 126 126 126 126 126 126 – 1 LN3 O1 Abstract 315 315 315 315 315 315 – 1 /C038.1 /C01.9 1.4 Complete 509 509 509 509 509 509 – 1 O2 Abstract 309 309 309 309 309 309 – 1 /C040.1 Complete 516 516 516 516 516 516 – 1 LN4 O1 Abstract 3943 3943 3943 3943 3943 3943 – 1 /C025.5 1.3 3.8 Complete 5295 5295 5295 5295 5295 5295 – 1 O2 Abstract 3993 3993 3993 3993 3993 3993 – 1 /C027.3 Complete 5494 5494 5494 5494 5494 5494 – 1 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 901', 'provided a good mutation score – LN2 was the weakest. Looking at the cost for each strategy, ATP stands out with its high values for all cost measures. RTP and LN3 performed quite similarly. AT had the second highest cost, but a similar effectiveness to that of ATP, RTP, and LN4. Despite killing all mutants, ATP is clearly the most expensive criteria to prepare. On the other extreme, LN2 has the lowest costs, but the fault-detection ability is correspondingly poor. There are signiﬁcant differences regarding costs, e.g., preparation time versus execution time, between AT and RTP. Preparing the AT test suites Table 7 Descriptive statistics – execution time. Coverage criterion Oracle Model Min (s) Q1 (s) Mean (s) Median (s) Q3 (s) Max (s) St. dev. N Diff. between abstract and complete (%) Diff. between O2 and O1 (%) (abstract) Diff. between O2 and O1 (%) (complete) AT O1 Abstract 29 42 64 57 86 129 27 30 /C097.4 /C073.8 /C083.1 Complete 1765 2108 2455 2377 2785 3617 469 30 O2 Abstract 9 12 17 16 19 37 7 30 /C095.9 Complete 293 358 415 417 473 571 71 30 RTP O1 Abstract 52 62 72 74 76 100 11 30 /C085.2 /C069.4 /C080.5 Complete 341 460 489 504 524 607 54 30 O2 Abstract 14 19 22 21 23 47 7 30 /C076.9 Complete 80 88 95 97 101 119 9 30 ATP O1 Abstract 215 342 395 398 448 528 75 30 /C088.2 /C070.6 /C083.0 Complete 2607 2972 3341 3381 3669 3978 429 30 O2 Abstract 65 100 116 117 127 177 22 30 /C079.6 Complete 429 493 569 547 654 773 101 30 LN2 O1 Abstract 16 16 16 16 16 16 – 1 /C011.1 /C068.8 /C072.2 Complete 18 18 18 18 18 18 – 1 O2 Abstract 5 5 5 5 5 5 – 1 0.0 Complete 5 5 5 5 5 5 – 1 LN3 O1 Abstract 85 85 85 85 85 85 – 1 /C037.5 /C068.2 /C079.4 Complete 136 136 136 136 136 136 – 1 O2 Abstract 27 27 27 27 27 27 – 1 /C03.6 Complete 28 28 28 28 28 28 – 1 LN4 O1 Abstract 667 667 667 667 667 667 – 1 /C021.5 /C072.6 /C079.6 Complete 850 850 850 850 850 850 – 1 O2 Abstract 183 183 183 183 183 183 – 1 5.8 Complete 173 173 173 173 173 173 – 1 Table 8 Descriptive statistics – mutation score. Coverage criterion Oracle Model Min Q1 Mean Median Q3 Max St. dev. N Diff. between abstract and complete (%) Diff. between O2 and O1 (%) (abstract) Diff. between O2 and O1 (%) (complete) AT O1 Abstract 0.000 0.200 0.262 0.200 0.200 0.800 0.168 30 /C073.7 /C071.3 /C020.2 Complete 0.900 1.000 0.997 1.000 1.000 1.000 0.018 30 O2 Abstract 0.000 0.000 0.075 0.000 0.050 0.530 0.151 30 /C090.5 Complete 0.730 0.800 0.795 0.800 0.800 0.800 0.018 30 RTP O1 Abstract 0.870 0.870 0.870 0.870 0.870 0.870 0.000 30 /C013.0 /C031.0 /C020.0 Complete 1.000 1.000 1.000 1.000 1.000 1.000 0.000 30 O2 Abstract 0.600 0.600 0.600 0.600 0.600 0.600 0.000 30 /C025.0 Complete 0.800 0.800 0.800 0.800 0.800 0.800 0.000 30 ATP O1 Abstract 0.867 0.867 0.867 0.867 0.867 0.867 0.000 30 /C013.3 /C027.2 /C026.9 Complete 1.000 1.000 1.000 1.000 1.000 1.000 0.000 30 O2 Abstract 0.600 0.600 0.631 0.600 0.667 0.667 0.034 30 /C013.6 Complete 0.600 0.730 0.731 0.730 0.730 0.800 0.035 30 LN2 O1 Abstract 0.267 0.267 0.267 0.267 0.267 0.267 – 1 /C020.0 /C0100.0 /C060.0 Complete 0.333 0.333 0.333 0.333 0.333 0.333 – 1 O2 Abstract 0.000 0.000 0.000 0.000 0.000 0.000 – 1 /C0100.0 Complete 0.133 0.133 0.133 0.133 0.133 0.133 – 1 LN3 O1 Abstract 0.867 0.867 0.867 0.867 0.867 0.867 – 1 /C07.1 /C030.8 /C021.4 Complete 0.933 0.933 0.933 0.933 0.933 0.933 – 1 O2 Abstract 0.600 0.600 0.600 0.600 0.600 0.600 – 1 /C018.2 Complete 0.733 0.733 0.733 0.733 0.733 0.733 – 1 LN4 O1 Abstract 0.867 0.867 0.867 0.867 0.867 0.867 – 1 /C013.3 /C030.8 /C020.0 Complete 1.000 1.000 1.000 1.000 1.000 1.000 – 1 O2 Abstract 0.600 0.600 0.600 0.600 0.600 0.600 – 1 /C025.0 Complete 0.800 0.800 0.800 0.800 0.800 0.800 – 1 902 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'took slightly more time than RTP, but less time than LN4. Looking at the execution time,Fig. 13 indicates that AT uses more time compared to both RTP and LN4. Finding 1:The results for cost and effectiveness indicate that LN2 may be too weak as a testing strategy. The remaining testing strategies killed all mutants (except from sneak paths), but with varying costs. ATP was the most expensive criterion and RTP the least expensive. In our case, RTP is therefore the most cost-effective strategy, closely followed by LN3. 6.2. RQ2: How does varying the oracle affect cost and effectiveness? This section extends the comparison of state-based coverage criteria by investigating the inﬂuence of varying the test oracle on cost effectiveness. It is important to remember the difference between the two oracles O1 and O2: the former checks the state invariant in addition to the pointer to the current system state, whereas the latter only checks the pointer to the current system state. Test cases were generated from the complete test model. Section 6.2.1 reports on the results of applying the statistical tests. Section 6.2.2 presents an analysis of the cost and effectiveness. 6.2.1. Statistical tests The paired Wilcoxon signed-rank test was executed on the col- lected data when applying two different oracles on the complete model. The purpose of the tests was to reject the following null hypotheses: H0-cost: There are no signiﬁcant differences in cost when applying two different oracles for each of the AT, RTP, and ATP criteria on the complete model. H0-eff: There are no signiﬁcant differences in effectiveness when applying two different oracles for each of the AT, RTP, and ATP criteria on the complete model. Varying the oracle shows that there were signiﬁcant differences in the preparation time for AT (medium effect size) and ATP (large effect size); O1 required more preparation time than O2. For RTP, on the other hand, no signiﬁcant differences between the two ora- cles were found. The execution time was signiﬁcantly higher when applying oracle O1. Moreover, the effect size was large. This pays off, however, in that O1 consistently achieves a signiﬁcantly higher mutation score. In this case, the effect size is also large. As we can see from the results of applying the statistical tests (Tables 15–17), the null hypotheses were rejected for all strategies regarding effectiveness, but not for cost. Note, however, that the only null hypothesis regarding cost that could not be rejected was related to RTP’s preparation time. For AT and ATP there are sig- niﬁcant differences in cost and effectiveness when applying two different oracles on the complete model. 6.2.2. Cost effectiveness analysis This section investigates the relationship between cost and effectiveness for SBT when varying the oracle.Fig. 14 graphically illustrates the relationship between cost and effectiveness by dis- playing the surrogate measures for cost3, preparation and execution time, and the surrogate measure for effectiveness, mutation score. Table 9 Sneak-path test cases – preparation time and execution time – abstract test model. Test case executed on mutant Oracle Time prepare test case (s) Diff. O2 by O1 (%) Time execute test case (s) Diff. between O2 and O1 (%) M18 O1 19 /C010.5 4 /C050.0 O2 17 2 M23 O1 19 /C05.3 4 /C050.0 O2 18 2 M24 O1 19 /C010.5 4 /C050.0 O2 17 2 M19 O1 21 /C014.3 4 /C050.0 O2 18 2 M25 O1 19 /C010.5 3 /C033.3 O2 17 2 M16 O1 19 /C010.5 5 /C060.0 O2 17 2 M17 O1 20 /C010.0 4 /C050.0 O2 18 2 M20 O1 20 /C015.0 4 /C050.0 O2 17 2 M21 O1 18 /C05.6 4 /C050.0 O2 17 2 M22 O1 19 /C010.5 5 /C060.0 O2 17 2 M26 O1 20 /C010.0 5 /C060.0 O2 18 2 Table 10 Sneak path test cases – preparation time and execution time – complete test model. Executed on mutant Oracle Time prepare test case (s) Diff. between O2 and O1 (%) Time execute test case (s) Diff. between O2 and O1 (%) M23 O1 25 0.0 7 /C071.4 O2 25 2 M18 O1 26 /C011.5 4 /C050.0 O2 23 2 M24 O1 26 0.0 4 /C050.0 O2 26 2', 'test case (s) Diff. between O2 and O1 (%) Time execute test case (s) Diff. between O2 and O1 (%) M23 O1 25 0.0 7 /C071.4 O2 25 2 M18 O1 26 /C011.5 4 /C050.0 O2 23 2 M24 O1 26 0.0 4 /C050.0 O2 26 2 M19 O1 25 /C012.0 6 /C066.7 O2 22 2 M25 O1 25 /C012.0 4 /C050.0 O2 22 2 M16 O1 28 /C025.0 3 /C033.3 O2 21 2 M17 O1 25 /C016.0 4 /C050.0 O2 21 2 M20 O1 23 /C08.7 5 /C060.0 O2 21 2 M21 O1 34 0.0 6 /C066.7 O2 34 2 M22 O1 34 0.0 4 /C050.0 O2 34 2 M26 O1 34 0.0 4 /C050.0 O2 34 2 Table 11 Estimated preparation and execution times for sneak-path test suites. Test model Oracle Preparation time (estimate in seconds) Execution time (estimate in seconds) Complete test model Oracle O1 1885 315 Oracle O2 1750 136 Abstract test model Oracle O1 407 88 Oracle O2 365 42 3 Recall that test-suite sizes were not inﬂuenced by varying the oracle when generating tests from the complete test model; hence, this surrogate measure is not addressed in this section. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 903', 'Obviously, an ideal situation would have been low values for cost, but high mutation scores. However, this is not the case for all com- binations of coverage criteria and oracles. In particular, we can ob- serve fromFig. 14that ATP combined with both oracles O1 and O2 is distant from the desired area. The results suggest that the muta- tion score is negatively affected for all coverage criteria when using the oracle O2. One assumption to make is that a stronger coverage criterion should have less need for a strong oracle as compared to weaker coverage criteria having less code exercised. As we can see from Fig. 14, a strong oracle combined with weaker coverage criterion clearly improves fault-detection ability. Even though the two oracles achieved rather similar levels of cost in terms of preparation time (Fig. 14), results show that using oracle O1 resulted in an overall higher cost when compared to ora- cle O2. The highest impact was seen for LN2, followed by LN3. Lar- ger differences were seen for cost when focusing on execution time (Fig. 15); oracle O2 required less time than oracle O1. Finding 2:The combination of coverage criterion and oracle signiﬁcantly impacts the cost (except from preparation time for RTP) and effectiveness. Minor differences in preparation time were observed when applying oracle O2. Execution time, on the other hand, was signiﬁcantly lower when applying oracle O2 for all six strategies. The signiﬁcant amount of cost savings when using O2, however, had an overall negative impact on effectiveness. 6.3. RQ 3: What is the inﬂuence of the test model abstraction level on cost and effectiveness? In Sections6.1 and 6.2, we noted how six coverage criteria com- pare when applied to a complete test model combined with two oracles of different strengths. This section deals with the test mod- el itself, especially with respect to the level of detail, and seeks to provide answers as to how a less detailed test model in input to the test case generation would affect cost effectiveness. The results are collected by generating test suites and running tests with a model where the contents of every composite state, in addition to the belonging transitions, were removed (abstract model). 6.3.1. Statistical tests Due to the independence between test paths for each of the 30 replications of AT, RTP, and ATP in the complete versus the abstract test model, these cannot be considered to be pairs in statistical tests. Hence, the independent samples Wilcoxon signed-rank test was used for the statistical testing of differences. The purpose of the tests was to reject the following null hypotheses: Table 12 Paired Wilcoxon signed-rank test – preparation time. H0 Oracle Model Measure p-Value bA12 Effect size Result Sign. diff. (CI) AT = RTP O1 Complete Prep. time 1.86e /C009 1 Large AT > RTP Yes AT = ATP O1 Complete Prep. time 1.86e /C009 1 Large AT < ATP Yes RTP = ATP O1 Complete Prep. time 1.86e /C009 0 Large RTP < ATP Yes Table 13 Paired Wilcoxon signed-rank test – execution time. H0 Oracle Model Measure p-Value bA12 Effect size Result Sign. diff. (CI) AT = RTP O1 Complete Exec. time 1.82e /C006 1 Large AT > RTP Yes AT = ATP O1 Complete Exec. time 4.50e /C006 0.086 Large AT < ATP Yes RTP = ATP O1 Complete Exec. time 1.86e /C009 0 Large RTP < ATP Yes Table 14 Paired Wilcoxon signed-rank test – mutation score. H0 Oracle Model Measure p-Value bA12 Effect size Result Sign. diff. (CI) AT = RTP O1 Complete Mut. score NA 0.5 No effect AT = RTP No AT = ATP O1 Complete Mut. score NA 0.5 No effect AT = ATP No RTP = ATP O1 Complete Mut. score NA 0.5 No effect RTP = ATP No Fig. 13. Preparation and execution time, test-suite size and mutation score. 904 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'H0-cost: There are no signiﬁcant differences in cost for the strategies AT, RTP, and ATP when varying the level of details in the test models. H0-eff: There are no signiﬁcant differences in effectiveness for the strategies AT, RTP, and ATP when varying the level of details in the test models. Tables 18–21provide results from the statistical tests. Overall, when considering the three strategies combined with each of O1 and O2, signiﬁcant differences were found for all preparation times, execution times and mutation scores. The low p-values show that the null hypotheses can be rejected, and we can con- clude that there are signiﬁcant differences in both cost and effec- tiveness for the strategies AT, RTP, and ATP when varying the level of detail in the test model. Preparation times, execution times, and mutation scores are signiﬁcantly higher when using the complete test model. To summarize, test suites generated from the complete model required both higher preparation and execution time (large effect sizes were found for both measures). On the other hand, the com- plete models achieved higher mutation scores. The results showed that ATP applied to a complete model is an expensive strategy. The high fault-detection effectiveness may come at too high a cost. Combining ATP with the abstract model resulted in a signiﬁcant cost reduction. The effectiveness was also reduced. On the other extreme, LN2 had the lowest cost but also the lowest effectiveness; at least for the complete model. Results for the abstract model combined with oracle O1 showed similar results to what was found for AT applied to the abstract model. The level of detail in the model had an enormous impact on the cost and effectiveness for AT; all mutants were killed by AT when using the complete model, although at a large increase in cost. RTP and LN4, when applied to the complete model and oracle O1, obtained as good mutation scores as ATP and AT. Of these, RTP had the lowest costs. Overall, the abstract model performs better with O1, the most pre- cise oracle. AT, RTP, ATP, and LN4 all provided the highest mutation scores when generated from a complete model used with oracle O1. Of these, RTP had the lowest costs. Using the state-pointer oracle, and still based on the complete test model, the effectiveness was slightly reduced: AT, RTP, and LN4 killed 80% of the mutants. Regarding the abstract test model, we saw that LN3, LN4, RTP, and ATP killed 87% of the mutants – interestingly, the cost of ATP was dramatically reduced as compared to the test suites generated from the complete model. 6.3.2. Cost effectiveness analysis This section discusses how the removal of details from the test model affects cost effectiveness.Fig. 16displays the cost4-effective- ness of the 24 combinations of coverage criteria, test oracles, and test models. Each combination of oracle and test model is represented by a unique color: sky blue represents oracle O1 and the complete test model; pale blue represents oracle O2 and the complete test model; yellow represents oracle O1 and the abstract test model; and ﬁnally, red represents oracle O2 and the abstract test model. It is worth examining the bottom of the ﬁgure on the left hand side. For LN2, we can, as predicted, observe that by reducing the le- vel of details in the test model, both for oracles O1 and O2, the mutation score is lowered even further. Signiﬁcant differences on both cost and effectiveness were observed for AT when increasing the abstraction level. None of the combinations of LN2 can be rec- Table 15 Paired Wilcoxon signed-rank test comparing oracles O1 and O2 – preparation time. H0 Strategy Measure p-Value bA12 Effect size 95% CI forbA12 Result Sign. diff. (CI) O1 = O2 AT Prep. time 1.82e /C006 0.67 Medium [0.521,0.794] O1 > O2 Yes O1 = O2 RTP Prep. time 1 – – [NA,NA] O1 = O2 No O1 = O2 ATP Prep. time 0.033 0.72 Large [0.573,0.835] O1 > O2 Yes Table 16', 'O1 = O2 AT Prep. time 1.82e /C006 0.67 Medium [0.521,0.794] O1 > O2 Yes O1 = O2 RTP Prep. time 1 – – [NA,NA] O1 = O2 No O1 = O2 ATP Prep. time 0.033 0.72 Large [0.573,0.835] O1 > O2 Yes Table 16 Paired Wilcoxon signed-rank test comparing oracles O1 and O2 – execution time. H0 Strategy Measure p-Value bA12 Effect size Result Sign. diff. (CI) O1 = O2 AT Exec. time 1.82e /C006 1 Large O1 > O2 Yes O1 = O2 RTP Exec. time 1.82e /C006 1 Large O1 > O2 Yes O1 = O2 ATP Exec. time 1.86e /C009 1 Large O1 > O2 Yes Table 17 Paired Wilcoxon signed-rank test comparing oracles O1 and O2 – mutation score. H0 Strategy Model Measure p-Value bA12 Effect size Result Sign. diff. (CI) O1 = O2 AT Complete Mut. score 1.08e /C007 1 Large O1 > O2 Yes O1 = O2 RPT Complete Mut. score 4.61e /C008 1 Large O1 > O2 Yes O1 = O2 ATP Complete Mut. score 2.76e /C007 1 Large O1 > O2 Yes Table 18 Non-paired Wilcoxon signed-rank test comparing abstract and complete model – preparation time. H0 Coverage criterion Oracle Measure p-Value bA12 Effect size Result Sign. diff. (CI) A = C AT, RTP, ATP O1 Prep. time <2.2e /C016 0.115 Large A < C Yes A = C AT, RTP, ATP O2 Prep. time <2.2e /C016 0.115 Large A < C Yes 4 Please note that time is shown as the sum total of the preparation time and execution time. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 905', 'ommended; neither the combinations of AT and the abstract test model. The inﬂuence of the abstract test model on ATP shows a major reduction in cost, while retaining an overall high mutation score when using oracle O1. Looking at the results for LN4, we see that there are signiﬁcant differences in cost, both for test-suite size and time. The reduction in mutation score is also present for both combinations of test model and oracle (i.e. complete test model and oracle O1 versus abstract test model and oracle O2). Interest- ingly, however, the abstract model in combination with the better oracle (O1) actually performs better with respect to fault-detection when compared to the complete model combined with oracle O2. The latter also applies to LN2, LN3, RTP, and ATP. The results for RTP when generating test suites from the com- plete model using the oracle O1 show a similar fault-detection le- vel when compared to LN3 (complete model, oracle O1); AT (complete model, oracle O1); LN4 (complete model, oracle O1); and ATP (complete model combined with oracle O1). The costs, however, differ. Notice that the cost of RTP is lower than that for AT, LN4, and ATP (complete model). Finding 3:Reducing the level of detail in the test model signiﬁcantly inﬂuenced cost effectiveness. The results show that both the costs and fault-detection ability are signiﬁcantly lower for test suites generated from the abstract model, as compared to the complete model. However, when using the state-invariant oracle in addition to the state-pointer oracle, and either of the RTP, LN3, LN4, or ATP strategies, a comparable cost effectiveness could be obtained from the abstract test model as compared to test suites generated from the complete test model. 6.4. RQ4: What is the impact of sneak-path testing on cost and effectiveness? Sections 6.1, 6.2, 6.3 deal with conformance testing aimed at detecting deviations from speciﬁed system behavior when ex- pected events were invoked on the SUT. We will now focus on a different type of testing, sneak-path testing. For each state in the SUT, all possible events that are not speciﬁed for the particular state are invoked. This technique is intended to catch faults that introduce undesired, additional behavior, in terms of extra transi- tions and actions. Complementing conformance testing with sneak-path testing at an additional cost in preparation and execution time resulted in 11 mutants being killed – those 11 mutants were not killed by any of the six state-based coverage criteria. The execution of the sneak- path test suite on the abstract model killed 10 out of 11 sneak- paths. This was, however, due to an infeasible test case. Being equal to the number of states in the SUT, the cost of sneak-path test suites are rather inexpensive when compared to the state-based coverage criteria investigated in this study. This demonstrates that these test strategies are complementary in order to catch different types of faults. We are aware of the lack of formal statistical tests in this sec- tion. However, as the evidence found in the collected data is clear-cut and does not leave much place for uncertainty, we con- sider this as no threat to the drawn conclusions. Finding 4:The results indicate quite strongly that sneak-path testing is a necessary step in state-based testing (SBT) due to the following observations: (1) The proportion of sneak paths in the collected fault data was high (42%), and (2) the presence of sneak paths is undetectable by conformance testing. 7. Validity threats This section discusses what the authors consider to be threats to validity. A threat to internal validity is the fact that just one researcher was involved in the preparation and execution of the test cases. This was due to the lack of resources and a general lack of state- based testing (SBT) experience in the company. It is also worth noting that the purpose of the case study was to demonstrate the', 'This was due to the lack of resources and a general lack of state- based testing (SBT) experience in the company. It is also worth noting that the purpose of the case study was to demonstrate the possible beneﬁts ABB could gain by applying SBT in the future. The main purpose was not to study the interaction between the tester and the technology; the focus was directed at the actual achievements that could be obtained with respect to the cost and effectiveness via SBT. The industry needs help in introducing new techniques, and this is one pragmatic approach in order to demon- strate the possible advantages of this particular technique. Another threat to internal validity could be related to fault seeding: both the generation of test cases and the insertion of faults to develop mutants were conducted by a researcher. The question that must be raised is: How can we ensure that the fault seeding was impartial and unbiased? The researcher could poten- tially inﬂuence the implementation of the test suites so as to be better at detecting certain types of faults. Ideally, the generation of test cases and fault seeding should be conducted by different Fig. 14. Oracle O1 versus oracle O2 – mutation score versus preparation time (diamond = O1; square = O2). Fig. 15. Oracle O1 versus oracle O2 – mutation score versus execution time (diamond = O1; square = O2). 906 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'people. Nevertheless, since the test suites were automatically generated, following known algorithms, this is not considered to be a threat. As a result, the implementations of the algorithms do not suffer a great risk of being manipulated in favor of detecting speciﬁc faults. Furthermore, the seeded faults were actual errors introduced by engineers from ABB. Hence, we believe that, in this case, the researcher had no impact on how the faults were seeded. One detected risk in terms of internal validity was the possible randomness in results for the RTP coverage criterion. This issue was handled by generating 30 random test trees, thus replicating the experiment for these criterion 30 times. Even though the experiment was repeated 30 times, the num- ber of seeded faults was low compared to mutation testing using artiﬁcial mutation operators. This may be a threat to the conclu- sion validity as the statistical power was low due to the small sam- ple size. It is important to note, however, that this is a ﬁrst study that should be complemented by additional studies with focus on increasing the sample size using mutation operators. Neverthe- less, this study is unique for its use of actual industrial faults which positively affect the external validity and many key differences turned out to be statistically signiﬁcant. External validity covers to what extent it is possible to general- ize the ﬁndings, and to what extent the ﬁndings are of interest to other people outside the investigated case[56]. The main strength of this study is, in fact, its external validity. Two factors in particu- lar increase the external validity, namely the industrial context and the use of actual faults when evaluating test strategies. The system that is the focus of this paper is highly representative of control systems with state-based behavior, thus improving the external validity. It is important to provide detailed context descriptions as we have done in this paper, such as system characteristics, development and testing procedures, so that others can relate the results to their own context. Moreover, in contrast to the majority of existing studies, which apply artiﬁcial faults, the faults used in this study are realistic faults collected in a ﬁeld study con- ducted at ABB. In spite of these two factors, however, there are sev- eral issues that should be discussed. First, let us consider the SUT. An obvious threat to the external validity of this study, which reduces its potential for contributing with general results, is the fact that only one system was used in the evaluation. It is a highly appropriate and relevant case, as it was developed in an industrial context, and represents a real sys- tem, and is of real-world importance. The SUT is a typical example of control systems: It is a device that controls the movement of machines in industrial production by supervising inputs from a number of sensors. The characteristics of the selected system are expected to be similar for many control systems, which may in- crease the possibility of these results being generally valid for these types of systems. In previous studies, where artiﬁcial mutation operators were applied to the evaluation of test strategies, external validity was considered uncertain. The use of actual faults when generating mu- tant programs is not common practice in testing research. In this study, only 26 mutants were applied, but the seeded faults were real and manually extracted from a ﬁeld study as described in Sec- tion 3. But again, it is not certain that the results apply to other organizations because the types of faults that are introduced into a system depend on the engineers working on the system. Keeping the preparation and execution time in mind, the feasibility of the study would be threatened by a dramatic increase in the number of mutants. To avoid the masking of faults, only one fault was Table 19', 'study would be threatened by a dramatic increase in the number of mutants. To avoid the masking of faults, only one fault was Table 19 Non-paired Wilcoxon signed-rank test comparing abstract (A) and complete (C) model – execution time. H0 Oracle Measure p-Value bA12 Effect size Result Sign. diff. (CI) A = C O1 Exec. time <2.2e /C016 0.017 Large A < C Yes A = C O2 Exec. time <2.2e /C016 0.091 Large A < C Yes Table 20 Non-paired Wilcoxon signed-rank test comparing abstract (A) and complete (C) model – mutation score. H0 Oracle Measure p-Value bA12 Effect size Result Sign. diff. (CI) A = C O1 Mut. score <2.2e /C016 0 Large A < C Yes A = C O2 Mut. score <2.2e /C016 0.005 Large A < C Yes Table 21 Non-paired Wilcoxon signed-rank test comparing abstract (A) and complete (C) model – all measures. H0 Measure p-Value bA12 Effect size 95% CI forbA12 Result Sign. diff. (CI) A = C Prep. time <2.2e /C016 0.115 Large [0.086,0.152] A < C Yes A = C Exec. time <2.2e /C016 0.075 Large [0.054,0.104] A < C Yes A = C Mut. score 1.027e /C010 0.170 Large [0.132,0.217] A < C Yes Fig. 16. Abstract versus complete test model – mutation score, test-suite size and time. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 907', 'seeded per mutant program. Thus, like other studies, e.g.[38], this study only evaluates the detection of single faults. Complex fault patterns and interactions have not been accounted for. As with any empirical studies, however, this study should be replicated for other types of faults and other control systems in order to make the results more convincing. Reliability concerns to what extent the data and the analysis are dependent on the speciﬁc researchers[56] meaning that unclear descriptions of data collection procedures may give different re- sults in subsequent replications of the study. This is addressed by providing a detailed description of the study design and analysis. 8. Discussion In this study, we have seen that sneak-path testing and the choice of coverage criterion, test oracle, and test model have a large impact on cost effectiveness. By looking at the reported results on fault-detection ability, we see that ATP killed more mutants (100%) in this study when compared to results reported in[14,25,35]. The latter reported a mutation score as low as 54%. The relationship between the fault-detection ability of AT versus ATP presented in[25], however, seems to be rather consistent with the results in this study. In spite of the differences between the study of Briand et al.[12] and this study, our results support the ﬁndings of Briand et al.; with the exception of the ﬁndings showing that AT did not provide an ade- quate level of fault detection. In this study, by using a complete test model and oracle O1 (checking state invariant and state pointer), we saw that AT detected as many mutants as ATP. However, the study of Briand et al. included many more mutants, albeit artiﬁcial. Our results showed that AT obtained a very low mutation score when applied to the abstract test model. Empirical studies on RTP have, on the other hand, shown that, in terms of cost and effectiveness, this particular criterion is a com- promise between the weak AT and the more expensive ATP criteria [12]. Our results support these ﬁndings. Note, however, that also for RTP, the reported results on fault detection in existing studies are highly variable (22–90%). We also found that LN2 is not recom- mended as a testing strategy due to its very low fault-detection ability, both when applied to the complete and abstract test mod- els in our case study. LN3, on the other hand, obtained results com- parable to RTP. Although limited, existing research has suggested that the ora- cle used when testing has a large inﬂuence on fault-detection abil- ity [13,57–59]. Overall, our study empirically supports these ﬁndings. For all strategies, the state-invariant oracle obtained sig- niﬁcantly higher mutation scores than those that were obtained by the state-pointer oracle. AT, RTP, ATP and LN4 all provided the highest mutation scores when generated from the complete model used with the state-invariant oracle. Of these, RTP had the lowest costs. Using the state-pointer oracle, still based on the complete test model, the effectiveness was reduced: AT, RTP and LN4 killed 80% of mutants. Only the study of Briand et al.[13] is more pre- cisely comparable as they also studied one of the coverage criteria in the focus of this study, namely RTP. Results for RTP indicated a 25% increase in fault-detection when using the strongest oracle. The study of Briand et al. differs from this study in the following manners: (1) The two oracles that were compared are not exactly the same. Both studies involve the state-invariant oracle. Never- theless, Briand et al. compared the state-invariant oracle to a fully precise oracle capturing exact expected results; in this study, an oracle weaker than the state-invariant oracle was used as compar- ison. (2) Furthermore, in contrast to this study, the study of Briand et al. did not provide collected data on preparation time. (3) Briand et al. used students to perform the testing; in this study, the testing', 'et al. did not provide collected data on preparation time. (3) Briand et al. used students to perform the testing; in this study, the testing was carried out by one of the researchers. (4) The test suites were automatically generated in this study, but manually generated in the study of Briand et al. Finally, (5) the SUT was signiﬁcantly lar- ger in this study. Although Briand et al.[13] found great variations in results, both studies suggest signiﬁcant improvements in fault- detection when using a stronger oracle —yet at a signiﬁcantly high- er cost. For RTP, this applied to both preparation and execution time. Reducing the test-suite size by removing details from the test model is yet another area of related work where no studies have been carried out within the context of SBT. Nevertheless, the desire of reducing test-suite sizes has received a signiﬁcant amount of attention. There is a trade-off between a sufﬁciently detailed level in the test model and the cost effectiveness of the resulting test cases. The reduced costs of generating tests may, on the other hand, increase the number of undetected faults. In our case, removing contents from super-states resulted in signiﬁcantly smaller test suites, reducing the costs, yet retaining its fault-detection ability at a reasonable level. As described earlier, however, a large part of the generated test cases were infeasible due to guard conditions that could not be satisﬁed. The reason for this was that sub-state speciﬁc values could not be controlled in the same way as when those sub-states were included in the test model. To increase the number of feasible tests, ulterior work is necessary with respect to test-data selection. Although our results on using abstract test models as an input for test generation proved to have acceptable fault-detection effectiveness combined with certain coverage criteria and the state-invariant oracle, we must take into account the omitted details and be aware of those parts that cannot be tested based on the model[60]. We found that the mutants that were seeded in the removed sub-states were not detected. An overall trend in the results for the mutation score was that the abstract test models obtained lower mutation scores than those that were achieved by the complete test models. Inter- estingly, we observed that by removing details from the test model and using a stronger criterion, the results revealed that a compara- ble cost effectiveness was obtained when compared to test suites generated from the complete test model. Although generated based on a different idea, our results thus support the ﬁndings of Wong et al. [27] that test-set minimization can greatly reduce the evaluation costs, and thus the cost of testing, with limited loss in fault-detection effectiveness. (This is, however, dependent on the choice of coverage criteria and oracle.) The sneak-path test suite detected the eleven remaining mu- tants that were not killed by any of the conformance test suites. This demonstrates that sneak-path testing is complementary to coverage criteria in order to catch different types of faults. Our re- sults support the recommendation of Binder[32] and the conclu- sions drawn in the study of Mouchawrab et al. [38]: Testing sneak paths is an essential component of SBT in practice. The addi- tional cost is justiﬁed by a signiﬁcant increase in fault-detection effectiveness, especially in a safety–critical context where robust- ness to unexpected events is often crucial. This study contributes to existing research with its realistic con- text both in terms of the SUT and by using real faults when evalu- ating the testing strategies. The level of details in the test model is most often insufﬁciently speciﬁed; we have provided as detailed information as possible regarding the test models. The large major- ity of existing studies on cost effectiveness related to SBT, with the exception of [44], utilize small, non-industrial or example cases', 'ity of existing studies on cost effectiveness related to SBT, with the exception of [44], utilize small, non-industrial or example cases that are signiﬁcantly smaller or less complex than the SUT in our study. For example, recall that the number of tests generated for ATP in this study was 1,425 as compared to 34 in[14,25]. In partic- ular, recall the frequent use of the Cruise Control case in existing research. The number of seeded mutants is in most cases higher than in this study, but then again, the seeded faults are primarily 908 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910', 'artiﬁcial in existing studies. Nevertheless, the latter difference is interesting given the lack of studies on artiﬁcial versus real faults. Our contributions also include comparing two different oracles. The applied oracle is most often not speciﬁed in existing research; the lack of oracle information makes it difﬁcult to provide mean- ingful comparisons of the results. Only Briand et al.[13] speciﬁ- cally addressed and, in fact, compared the type of oracles used. Existing research rarely reports the cost of SBT; in particular not when comparing several strategies. The focus has mostly been di- rected towards fault-detection effectiveness. The studies that re- port the cost of test strategies, primarily report the test-suite size. Our results contribute by also reporting the cost in terms of preparation and execution times. The measuring strategy pre- sented in this paper is but one way to measure cost. 9. Conclusions In this paper, we reported on an industrial case study that eval- uated the cost effectiveness of state-based testing (SBT), i.e., mod- el-based testing using state models, by studying the inﬂuence of four testing aspects: coverage criteria, test oracles, test models and sneak paths. Although SBT is not a new area of research, this paper was motivated by a lack of realism in studies evaluating SBT. Moreover, existing research has primarily focused on separate aspects of SBT, e.g., test oracles, test strategies. In this paper, how- ever, we consider several aspects of SBT that may inﬂuence its cost effectiveness, and most importantly, evaluate these aspects to- gether. Our overall goal was to achieve maximum realism and understand the interplay between selected test strategies, oracles, and modeling precision. The comparison is exclusively based on real faults. In this study, we evaluated six SBT coverage criteria: all transi- tions (AT), all round-trip paths (RTP), all transition pairs (ATP), paths of length 2 (LN2), paths of length 3 (LN3), and paths of length 4 (LN4). The results of studying the coverage criteria as applied to a complete test model and the state-invariant test oracle, indicate that LN2 may be too weak a testing strategy. The remaining testing strategies yield test suites that kill the seeded faults (except for sneak paths). When also considering the costs, signiﬁcant differ- ences were observed. ATP was the most expensive criterion. RTP appeared to be the most cost-effective strategy (closely followed by LN3). By using the less rigorous state-pointer oracle, only minor dif- ferences in preparation time were observed as compared to using the state-invariant oracle. Execution time, on the other hand, was signiﬁcantly lower when applying the former oracle for all six strategies. However, effectiveness was negatively impacted. Yet, 80% of the mutants (except sneak paths) were killed by AT, RTP, and LN4. In addition, reducing the level of detail in the test model re- sulted both in lower costs and effectiveness as compared to the complete model. As we can see fromFig. 16, however, the results for LN3, RTP, LN4 and ATP show that the obtained mutation score exceeds 80% even when using the less detailed test model. Complementing conformance testing with sneak-path testing resulted in killing all the remaining mutants, though at an addi- tional but reasonable cost in preparation and execution times. This demonstrates that these test strategies complement each other in order to catch different types of faults. Thus, the results indicate quite strongly that sneak-path testing is a necessary step in state-based testing (SBT) due to the following observations: (1) The proportion of sneak paths in the collected fault data was high (42%), and (2) the presence of sneak paths is undetectable by con- formance testing. Each of the testing aspects, coverage criteria, test oracles, and test models inﬂuence cost effectiveness, and constitute a tradeoff between increasing fault-detection effectiveness and reducing', 'Each of the testing aspects, coverage criteria, test oracles, and test models inﬂuence cost effectiveness, and constitute a tradeoff between increasing fault-detection effectiveness and reducing costs. These aspects must be carefully considered when selecting a strategy. However, regardless of these choices, sneak-path testing is a necessary step in SBT as the presence of sneak paths is unde- tectable by conformance testing. The results in this study regarding cost and effectiveness of SBT should provide useful guidance for industry on how to select appropriate testing strategies by accounting for their relative cost and the criticality of the SUT. Acknowledgements Nina E. Holt was funded by Simula Research Laboratory and The Research Council of Norway through the industry project EVISOFT (EVidence based Improvement of SOFTware engineering). Lionel Briand has been supported by the National Research Fund, Luxem- bourg (FNR/P10/03). Richard Torkar was partly funded by The Knowledge Foundation (KKS) through the project 20130085 Test- ing of Critical System Characteristics (TOCSYC). References [1] M. Utting, B. Legeard, Practical Model-Based Testing: A Tools Approach, Morgan-Kaufmann, 2006. [2] T. Chow, Testing software design modeled by ﬁnite-state machines, IEEE Trans. Softw. Eng. 4 (3) (1978) 178–187. [3] L. Briand, Y. Labiche, A UML-based approach to system testing, in: Proceedings of the 4th International Conference on The Uniﬁed Modeling Language, Modeling Languages, Concepts, and Tools, 2001. [4] I. El-Far, J. Whittaker, Model-based software testing, in: J.J. Marciniak (Ed.), Encyclopedia of Software Engineering, 2002. [5] S. Ali, L. Briand, M. Rehman, H. Asghar, M. Iqbal, A. Nadeem, A state-based approach to integration testing based on UML models, Inf. Softw. Technol. 49 (2007) 1087–1106. [6] A. Dias Neto, R. Subramanyan, M. Vieira, G. Travassos, A survey on model- based testing approaches: a systematic review, in Proceedings of the 1st ACM International Workshop on Empirical Assessment of Software Engineering Languages and Technologies, Atlanta, Georgia, 2007. [7] D. Drusinsky, Modeling and Veriﬁcation using UML Statecharts: A Working Guide to Reactive System Design, Runtime Monitoring and Execution-Based Model Checking, ﬁrst ed., Newnes, 2006. [8] M. Vieira, X. Song, G. Matos, S. Storck, R. Tanikella, B. Hasling, Applying model- based testing to healthcare products: preliminary experiences, in: Proceedings of the 30th International Conference on Software Engineering, Leipzig, Germany, 2008. [9] D-MINT, Deployment of Model-Based Technologies to Industrial Testing. <http://www.elvior.com/motes/d-mint> (accessed 10.13). [10] J. Feldstein, Model-Based Testing using IBM Rational Functional Tester, Developer Works, IBM, 2005. [11] Y. Gurevich, W. Schulte, N. Tillmann, M. Veanes, Model-Based Testing with SpecExplorer, Microsoft Research, 2009. [12] L. Briand, Y. Labiche, Y. Wang, Using simulation to empirically investigate test coverage criteria based on statechart, in: Proceedings of the 26th International Conference on Software Engineering, 2004. [13] L. Briand, M. Di Penta, Y. Labiche, Assessing and improving state-based class testing: A series of experiments, IEEE Trans. Softw. Eng. 30 (11) (2004) 770– 793. [14] J. Offutt, A. Abdurazik, Generating tests from UML speciﬁcations, in: Proceedings of the 2nd International Conference on the Uniﬁed Modeling, Language, 1999. [15] MOTES. <http://www.elvior.com/motes/generator. (accessed 10.13). [16] A. Hartman, K. Nagin, The AGEDIS tools for model based testing, in: International Symposium on Software Testing and Analysis (ISSTA ’04), 2004. [17] Working with TestConductor and Automatic Test Generation (ATG), IBM. <http://publib.boulder.ibm.com/infocenter/rhaphlp/v7r6/index.jsp?topic=%2 Fcom.ibm.rhp.integ.testingtools.doc%2Ftopics%2Frhp_r_dm_vendor_doc_testing. html> (accessed 07.12). [18] T. Santen, D. Seifert, TEAGER – test automation for UML state machines, in:', 'Fcom.ibm.rhp.integ.testingtools.doc%2Ftopics%2Frhp_r_dm_vendor_doc_testing. html> (accessed 07.12). [18] T. Santen, D. Seifert, TEAGER – test automation for UML state machines, in: Proceeding of Software Engineering, Leipzig, 2006. [19] Conformiq Tool Suite. <http://www.conformiq.com/> (accessed 10.13). [20] P. Black, V. Okun, Y. Yesha, Mutation operators for speciﬁcations, in: ASE’2000, 15th Automated Software Engineering Conference, Grenoble, France, 2000. [21] A. Offutt, A. Lee, G. Rothermel, R. Untch, C. Zapf, An experimental determination of sufﬁcient mutation operators, ACM Trans. Softw. Eng. Methodol. 5 (2) (1996) 99–118. [22] J. Andrews, L. Briand, Y. Labiche, Is mutation an appropriate tool for testing experiments? in: ICSE ’05: Proceedings of the 27th International Conference on Software Engineering, 2005. [23] M. Thévenod-Fosse, P. Daran, Software error analysis: a real case study involving real faults and mutations, in: Proceedings of the 1996 ACM SIGSOFT International Symposium on Software Testing and Analysis, 1996. N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910 909', '[24] J. Andrews, L. Briand, Y. Labiche, A. Namin, Using mutation analysis for assessing and comparing testing coverage criteria, IEEE Trans. Softw. Eng. 32 (8) (2006) 608–624. [25] J. Offutt, S. Liu, A. Abdurazik, P. Ammann, Generating test data from state- based speciﬁcations, Softw. Test. Verif. Reliab. 13 (1) (2003) 25–53. [26] M. Heimdahl, D. George, Test-suite reduction for model based tests: effects on test quality and implications for testing, in: 19th IEEE International Conference on Automated Software Engineering (ASE’04), Linz, 2004. [27] W. Wong, J. Horgan, S. London, A. Mathur, Effect of test set minimization of fault detection effectiveness, in: International Conference on Software Engineering, Proceedings of the 17th International Conference on Software Engineering, 1995. [28] S. Ali, H. Hemmati, N. Holt, E. Arisholm, L. Briand, Model Transformations as a Strategy to Automate Model-Based Testing: A Tool and Industrial Case Studies, Simula Research Laboratory, 2010. [29] R. Yin, Case Study Research Design and Methods, fourth ed., SAGE Publications, 2009. [30] T. Pender, The UML Bible, Wiley, 2003. [31] E. Gamma, R. Helm, R. Johnson, J. Vlissides, Design Patterns – Elements of Reusable Object-Oriented Software, Addison-Wesley, 1995. [32] R. Binder, Testing Object-Oriented Systems, Addison-Wesley, 2000. [33] A. Offutt, Y. Xiong, S. Liu, Criteria for generating speciﬁcation-based tests, in: Proceedings of the Fifth IEEE International Conference on Engineering of Complex Computer Systems (ICECCS ’99), 1999. [34] Y.-S. Ma, J. Offutt, Y. Kwon, MuJava: an automated class mutation system, Softw. Test. Verif. Reliab. 15 (2) (2005) 97–133. [35] A. Paradkar, Plannable test selection criteria for FSMs extracted from operational speciﬁcations, in: Proceedings of the International Symposium On Software Reliability Eng. ‘2004, 2004. [36] G. Antoniol, L. Briand, M. Di Penta, Y. Labiche, A case study using the round-trip strategy for state-based class testing, in: Proceedings of the 13th International Symposium on Software, Reliability Engineering (ISSRE’02), 2002. [37] S. Mouchawrab, L. Briand, Y. Labiche, Assessing, comparing, and combining statechart-based testing and structural testing: an experiment, in: First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007), 2007. [38] S. Mouchawrab, L. Briand, Y. Labiche, M. Di Penta, Assessing, comparing, and combining state machine-based testing and structural testing: a series of experiments, IEEE Trans. Softw. Eng. 37 (2) (2011) 161–187. [39] L. Briand, Y. Labiche, Q. Lin, Improving statechart testing criteria using data ﬂow information, in: Proceedings of the 16th IEEE International Symposium on Software, Reliability Engineering (ISSRE’05), 2005. [40] L. Briand, Y. Labiche, Q. Lin, Improving the coverage criteria of UML state machines using data ﬂow analysis, Softw. Test. Verif. Reliab. 20 (3) (2010) 177–207. [41] H. Gomaa, Designing Concurrent, Distributed, and Real-Time Applications with UML, Addison-Wesley, 2000. [42] A. Abdurazik, P. Ammann, W. Ding, J. Offutt, Evaluation of three speciﬁcation- based testing criteria, in: Proceedings of the 6th IEEE International Conference on Complex Computer Systems, 2000. [43] K. Bogdanov, M. Holcombe, Statechart testing method for aircraft control systems, Softw. Test. Verif. Reliab. 11 (1) (2001) 39–54. [44] P. Chevalley, P. Thévenod-Fosse, An empirical evaluation of statistical testing designed from UML state diagrams: the ﬂight guidance system case study, in: Proceedings of the 12th International Symposium on Software, Reliability Engineering (ISSRE’01), 2001. [45] N. Holt, R. Torkar, L. Briand, K. Hansen, State-based testing: industrial evaluation of the cost-effectiveness of round-trip path and sneak-path strategies, in: IEEE International Symposium on Software Reliability Engineering, 2012. [46] Kermeta – Breathe Life into Your Metamodels. <http://www.kermeta.org/> (accessed 10.13).', 'strategies, in: IEEE International Symposium on Software Reliability Engineering, 2012. [46] Kermeta – Breathe Life into Your Metamodels. <http://www.kermeta.org/> (accessed 10.13). [47] N. Holt, E. Arisholm, L. Briand, Technical Report 2009-06: An Eclipse Plug-in for the Flattening of Concurrency and Hierarchy in UML State Machines, Simula Research Laboratory, Lysaker, 2009. [48] N.E. Holt, B. Anda, K. Asskildt, L.C. Briand, J. Endresen, S. Frøystein, Experiences with precise state modeling in an industrial safety critical system, in: Critical Systems Development Using Modeling Languages, CSDUML’06, Genova, 2006. [49] ATLAS Transformation Language (ATL). < http://www.eclipse.org/atl/> (accessed 10.13). [50] MOFScript Model Transformation Tool. < http://marketplace.eclipse.org/ content/mofscript-model-transformation-tool#.UktD3tJHKPw> (accessed 10.13). [51] R. DeMillo, A. Offutt, Constraint-based automatic test data generation, IEEE Trans. Softw. Eng. 17 (9) (1991) 900–910. [52] P. McMinn, Search-based software test data generation: a survey, Softw. Test. Verif. Reliab. 14 (2) (2004) 105–156. [53] M. Alshraideh, L. Bottaci, Search-based software test data generation for string data using program-speciﬁc search operators, Softw. Test. Verif. Reliab. 16 (3) (2006) 175–203. [54] R. Wilcox, Fundamentals of Modern Statistical Methods: Substantially Improving Power and Accuracy, Springer Verlag, 2001. [55] A. Vargha, H. Delaney, A critique and improvement of the CL common language effect size statistics of McGraw and Wong, J. Educ. Behav. Stat. 25 (2) (2000) 101–132. [56] P. Runeson, M. Höst, Guidelines for conducting and reporting case study research in software engineering, Empiric. Softw. Eng. 14 (2) (2009) 131–164. [57] Q. Xie, A. Memon, Designing and comparing automated test oracles for GUI- based software applications, ACM Trans. Softw. Eng. Methodol. 16 (1) (2007). [58] M. Staats, M. Whalen, M. Heimdahl, Better testing through oracle selection, in: Proceeding of the 33rd International Conference on Software Engineering, 2011. [59] M. Staats, M. Whalen, M. Heimdahl, Programs, tests, and oracles: the foundations of testing revisited, in: Proceeding of the 33rd International Conference on Software Engineering, 2011. [60] M. Utting, A. Pretschner, B. Legeard, A Taxonomy of Model-Based Testing, Working Paper Series, University of Waikato, Department of Computer Science, No. 04/2006, 2006. 910 N.E. Holt et al. / Information and Software Technology 56 (2014) 890–910']","['COST-EFFECTIVENESS EVALUATION OF TEST CASE GENERATION VIA MODEL CHECKING This  briefin  reports  scieitfc  evideice oi the cost-efectveiess of Specifcctoi Pctteris  Syster  (SPSr  pctteris/pctteri scopes to neiercte test ccses vic Model Checkiin withii the HiMoST rethod. FINDINGS → The empirical evaluatin repirted here aims at assessing the cist and efectveness if test suites generated via the filliwing SPS paterns/patern scipes:  Absence/Glibal  ( ABS),  Respinse Chain/Glibal ( REC), Precedence Chain (P prec. S, T)/Glibal  (PC1),  Precedence  Chain  (S,  T  prec. P)/Glibal (PC2). → The empirical evaluatin cinsiders cist as the nirmalized average number if test steps per test case,  and  efectveness  as  three  measures:  the average  value  if  the  pripirtin  if  transitins civered in the Statecharts, the average value if civered  instructins  and  the  average  value civered branches if the cide.    → ABS presented the best cist filliwed by REC and in the last pisitin is PC1.  → Surprisingly, PC2 generated ni test case fir all samples. In fact, it did generate ciunterexamples (test cases) fir all Cimputatin Tree Ligic (CTL) pripertes  fir  all  case  studies  but they were  all discarded,  because  these  ciunterexamples  had inly 1 state (the inital state) and thus less than a defned threshild. → ABS created a signifcant number if test cases which  were  discarded  while  REC  shiwed  a pripirtinally  huge  number  if  repeated  test cases. These fndings suggest that, in accirdance with  the  HiMiST  methid,  these  paterns  can cintribute, albeit marginally, ti increasing the cist if the testng pricess as a while. → Stll under the cist perspectve, PC1 presented mire satsfed ( true) CTL firmulas. Thus, it is nit necessary  ti  perfirm  any  pricessing/verifcatin if  repeated  ir  discarded  test  cases  fir  such firmulas that were satsfed. →  Cinsidering  the  three  measures  if efectveness, PC1 was the best silutin filliwed by REC, and ABS was the wirst. This is precisely the  inverse  irder  if  the  cist  analysis.  In  sime samples,  PC1  even  achieved  100%  if  civered transitins if the Statechart midel.  → Hiwever, even PC1 did nit nit present a very high average civerage if instructins (75%)  and especially  branch  civerage  (49%)  if  the  siurce cide. This is explained by the Statechart midel. In general, such midels have sime nirmal behaviir addressed and a few exceptin handling situatins. Hence, ti get higher civerage it is required mire detailed situatins in the Statechart midels.  →  When  cist  and  efectveness  are  assessed tigether, PC1 was  beter in all situatins. When cist  and  efectveness  cintribute  equally,  the Euclidean distance frim the iptmum piint, op, ti ABS is 24.62% greater than the distance frim op ti PC1. And frim  op ti REC, the distance is 25.12% greater  than  frim  op ti  PC1.  There  is  ni  cist- efectveness diference when cimparing ABS with REC with a slight advantage fir ABS. →  If  the  efectveness  cintributin  is  diubled, again PC1 was the best iptin where the Euclidean distances frim  op ti ABS and ti REC are 63.5% and 48.55% greater than the distance frim  op ti PC1, respectvely. Hiwever, here REC perfirmed beter than ABS where the distance frim op ti the later  is  10.07%  greater  than  frim  op ti  the firmer. →  The  iverall  cinclusiin  if  this  empirical evaluatin  (quasiexperiment)  is  that  the Precedence Chain (P prec. S, T) patern with Glibal scipe (PC1) generates the best test suite regarding cist-efectveness.  The  Respinse  Chain  patern with Glibal scipe (REC) is beter than the Absence patern  with  Glibal  scipe  ( ABS)  inly  if efectveness  has  a  priirity  iver  cist.  If  they cintribute equally, there is ni diference between REC and ABS. Keywords: Software Testing Model Checking Quasiexperiment Who is this briefng  or? Siftware engineering practtiners whi want ti make decisiins abiut cist- efectveness if test suites generated', 'Keywords: Software Testing Model Checking Quasiexperiment Who is this briefng  or? Siftware engineering practtiners whi want ti make decisiins abiut cist- efectveness if test suites generated  via Midel Checking based in scientfc  evidence. Where the fndings come  rom? All fndings if this briefng were  extracted frim the Midel Checking test case generatin methid called HiMiST  presented in Santiago Júnior and Silva.   What is included in this briefng? The main fndings if the cist- efectveness empirical evaluatin  presented in Santiago Júnior and Silva.  To access other evidence briefngs  on software engineering: htp://www.lia.ufc.br/ccbsift2017/ Original Reference Santagi Júniir, Valdivini Alexandre de; Silva, Fo. E. C. . Forim Statecharts inti Midel Checking: A Hierarchy-based Translatin and Specifcatin Paterns Pripertes ti Generate Test Cases.  In: 2nd Brazilian Sympisium in Systematc and Autimated Siftware Testng (SAST 2017), 2017, Foirtaleza, CE, Brazil. Priceedings if the 2nd Brazilian Sympisium in Systematc and  Autimated Siftware Testng, 2017. DOI: htps://dii.irg/10.1145/3128473.3128475.']","**Title:** Evaluating the Cost-Effectiveness of State-Based Testing in Industrial Software

**Introduction:**
This Evidence Briefing summarizes key findings from an empirical study on the cost-effectiveness of state-based testing (SBT) applied to industrial control software. The goal is to provide insights into how various testing strategies, including coverage criteria, test oracles, and model abstraction, influence both the cost and effectiveness of testing efforts in a real-world context.

**Main Findings:**
1. **Coverage Criteria Impact:** The study evaluated six coverage criteria: All Transitions (AT), All Round-Trip Paths (RTP), All Transition Pairs (ATP), and paths of varying lengths (LN2, LN3, LN4). Among these, RTP was found to be the most cost-effective strategy, closely followed by LN3. ATP, while effective, was the most expensive, indicating a trade-off between cost and fault detection.

2. **Oracle Precision Matters:** Using a more precise oracle (state-invariant plus state pointer) significantly improved fault detection compared to a less precise oracle (state pointer alone). However, this increased precision came at a higher cost in terms of preparation and execution time.

3. **Model Abstraction Benefits:** Reducing the detail in the test model led to substantial cost savings (up to 85%) while still maintaining an acceptable level of fault detection (24-37% reduction in effectiveness). This suggests that a balance can be achieved between model complexity and testing efficiency.

4. **Sneak-Path Testing Necessity:** The study highlighted the importance of sneak-path testing, which detected faults that conformance testing could not. All mutants that remained undetected after conformance testing were killed by sneak-path tests, demonstrating its necessity in safety-critical systems.

5. **Trade-offs to Consider:** Each aspect of testing—coverage criteria, oracle precision, model detail, and sneak-path testing—affects cost-effectiveness. Practitioners must carefully consider these factors based on the specific context and criticality of the software system being tested.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, including project managers, quality assurance professionals, and software testers, who are involved in the testing of safety-critical systems and are seeking to optimize their testing strategies.

**Where the findings come from?**
The findings are derived from an industrial case study conducted by Nina Elisabeth Holt, Lionel C. Briand, and Richard Torkar, which evaluated the cost-effectiveness of SBT using real faults from a control software developed at ABB.

**What is included in this briefing?**
This briefing includes a summary of the study's main findings related to SBT strategies, their cost-effectiveness, and the implications for testing practices in industrial settings.

To access other evidence briefings on software engineering:  
[http://ease2017.bth.se/](http://ease2017.bth.se/) 

For additional information about the research group:  
[Simula Research Laboratory](http://www.simula.no)

**Original Research Reference:**
Holt, N. E., Briand, L. C., & Torkar, R. (2014). Empirical evaluations on the cost-effectiveness of state-based testing: An industrial case study. *Information and Software Technology*, 56(8), 890-910. [https://doi.org/10.1016/j.infsof.2014.02.011](https://doi.org/10.1016/j.infsof.2014.02.011)"
"['On the Benefits/Limitations of Agile So/f_tware Development: An Interview Study with Brazilian Companies Fernando Kamei*, Gustavo Pinto†, Bruno Cartaxo‡∓, Alexandre Vasconcelos‡ *IFAL, Brazil †UFPA, Brazil ‡UFPE, Brazil ∓IFPE, Brazil fernando.kenji@ifal.edu.br,gpinto@ufpa.br,{bfsc,amlv}@cin.ufpe.br ABSTRACT Context: For more than 15 years, Agile So/f_tware Development (ASD) has been used to improve so/f_tware development, process, and qual- ity. However, there are scenarios where the eﬀectiveness of these methods and practices has not been rigorously evaluated. Objective: Understand the bene/f_its and limitations related to these methods and practices in a particular context: two so/f_tware compa- nies based on Pernambuco’s Technology Park, Brazil. Method: In this paper, we conducted 22 semi-structured interviews to understand the bene/f_its and limitations of ASD in an industrial context. /T_he data were extracted using open coding and analyzed through qualitative techniques. Results: Our preliminary analysis identi/f_ied a core of 28 bene/f_its and 20 limitations with the usage of ASD. As for bene/f_its, we found that facilitates project monitoring and tracking as well as the interaction and collaboration. As for limitations, we found that it diﬃculty working with user stories and to work with large teams. Conclusion: /T_his study serves as a practical guide for so/f_tware com- panies interested in adopting and improving the use of ASD. KEYWORDS Agile So/f_tware Development, Agile Practices, Bene/f_its, Limitations, So/f_tware Industry, Interviews ACM Reference format: Fernando Kamei*, Gustavo Pinto†, Bruno Cartaxo‡∓, Alexandre Vasconcelos‡. 2017. On the Bene/f_its/Limitations of Agile So/f_tware Development: An Interview Study with Brazilian Companies. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017,6 pages. DOI: h/t_tp://dx.doi.org/10.1145/3084226.3084278 1 INTRODUCTION /T_he fast growth of the global so/f_tware industry, aligned with the in- creasing demand for robust methods to deal with existing so/f_tware, fostered the quest for excellence and continuous improvement of so/f_tware quality. Along the last years, many so/f_tware development practices have been proposed, implemented, and evaluated in the industry [7, 11, 15]. Among the notable solutions, Agile So/f_tware Development (ASD) stands as a solid, low eﬀort, and useful prac- tice [2, 3, 21]. /T_here is also evidence that ASD is invaluable to reduce the failure rate of so/f_tware development practice [10]. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or aﬃliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. EASE’17, Karlskrona, Sweden © 2017 ACM. 978-1-4503-4804-1/17/06. . . $15.00 DOI: h/t_tp://dx.doi.org/10.1145/3084226.3084278 However, some researchers argue that there is a lack of rigorous empirical studies aimed at evaluating Agile methods and practices. /T_herefore, such studies are necessary to assess their eﬀectiveness. As a result, li/t_tle is known about how these methods are employed in practice. Moreover, most of the studies are exploratory in na- ture [10], therefore questions such as “what are their bene/f_its?”, and “what are their limitations?”, are not easily answered. /T_his happens because the context of which the so/f_tware company is deployed (e.g., the number of developers in a team or how they are distributed) does ma/t_ter [1, 4]. Due to this complex scenario, most related studies do not cover the whole spectrum of context and usage of Agile methods. In this study, we aimed to reduce this gap by understanding the bene/f_its and limitations related to Agile methods in a speci/f_ic industrial context: so/f_tware companies based on the Porto Digital Technology Park1, in Recife, Brazil. Porto Digital is among the', 'industrial context: so/f_tware companies based on the Porto Digital Technology Park1, in Recife, Brazil. Porto Digital is among the largest technology parks in Brazil, hosting over two hundred highly innovative so/f_tware companies. More speci/f_ically, the research questions we are trying to answer are: • RQ1: What are the bene/f_its of using ASD? • RQ2: What are the limitations of using ASD? By de/f_inition, ASD is a way of working in an agile manner (ac- cording to Agile Principles, (e.g., Welcome changing requirements, and Self-organization team) using Agile Methods (e.g., Scrum, XP, and Kanban) or, more speci/f_ically, agile Practices (e.g., Pair Pro- gramming, Daily Meetings, and Collective Ownership) [9]. To answer these RQs, we employed qualitative research methods with semi-structured interviews. We found several bene/f_its and limitations in this particular context. Our results not only reinforce some of the already known /f_indings of Agile Methods in practice but also shed some light on areas that deserve further investigation. We present a detailed research context information (2). Next, we explain about how the interviews were carried out, and our criteria of extraction and analysis (3). /T_hen, we present the /f_indings regarding the bene/f_its and limitations (4), and a discussion about these (5). Next, we discussed some related works (6). And /f_inally, we present our conclusions (7). 2 CONTEXT DESCRIPTION In this paper, we argue that context is relevant for assessing the eﬀectiveness of Agile methods in so/f_tware development [4]. We based our study on the context of two so/f_tware companies: ALPHA and BETA2. 1h/t_tp://www.portodigital.org/ 2We refer to them as “ALPHA” and “BETA” to respect their con/f_identiality', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Kamei et al. We collected data from three so/f_tware projects under develop- ment. One from ALPHA and two from BETA, respectively named as A1, B1, and B2. /T_hese so/f_tware companies signed a Term of Authorization and Commitment, which granted researchers access to participants. ALPHA is a small company created in 2005 which deploys cus- tomized so/f_tware to Brazilian government agencies. It is certi/f_ied as MPS.BR level G (which is equivalent to CMMI level 1) and has about 60 employees. /T_hey employ Scrum as the main Agile method but also employs some XP practices, such as Pair Programming, Real Customer Involvement, Co-location Team, and TDD. Each Sprint lasts two weeks. Project A1 was under development approximately for one year and employ the technologies Java for a web, JSF, and the Demoiselle Framework. /T_his project aimed to development a /f_inancial control system to track and monitor expenses and government revenues. We interviewed 7 out of the 9 team members, who had on average 4.5 years of experience with so/f_tware development and 2.5 years with Agile methods. All interviewees considered this project to be of medium level of complexity. We summarized the characteristics of each interviewee in Table 1. Table 1: Characteristics of ALFA’s interviewees: Project A1 Role Age Dev. exp. Agile exp. So/f_tware Engineer / Scrum Master 28 4 years 1.5 year Project Manager 27 6 years 2.5 years System Analyst / PO 26 5 years 3 years Test Engineer 22 3 years 1.5 year So/f_tware Engineer 32 5 years 4 years IT Manager 26 6 years 3 years So/f_tware Engineer 27 2 years 1.5 year BETA is a medium-sized technology company created in 1996 with more than 700 employees distributed in three Brazilian oﬃces. It is CMMI level 3 certi/f_ied and delivers products and services to several business areas using Scrum, XP, and Lean So/f_tware Devel- opment. We selected two projects that use Scrum and some XP practices, such as Pair Programming, TDD, and Co-location team. Project B1 has been under development for over six months using the C and Java language. /T_his project aimed to develop protocols and network management systems without a GUI. Each Sprint lasts three weeks. /T_he customer could participate at any time during the Sprints. We interviewed 7 out of 9 team members, who had on average /f_ive years of experience in so/f_tware development and two years with Agile methods. All interviewees considered this project to be of high level of complexity. We summarized the characteristics of each interviewee in Table 2. Project B2 was under development over three years using Java to develop SaaS applications for a telecommunications company. It used Scrum from the very beginning. /T_he customer could also participate at any time during the Sprint using instant messaging tools or email. Two team members played the Product Owner (PO) role. One was a so/f_tware engineer at the BETA company and the other one was a so/f_tware engineer at the customer company. We Table 2: Characteristics of BETA’s interviewees: Project B1 Role Age Dev. exp. Agile exp. So/f_tware Engineer 25 2 years 1 year So/f_tware Engineer 27 3 years 2 years Team leader / Scrum Master 34 13 years 2 years So/f_tware Engineer 30 7 years 2 years So/f_tware Engineer 30 2 years 2 years So/f_tware Engineer 29 4 years 1 year So/f_tware Engineer 23 3 years 3 years did not interview the la/t_ter. We interviewed 8 out of 9 members, who had on average seven years of experience with so/f_tware de- velopment and three years of experience with Agile methods. All interviewees considered this project to be of high level of complex- ity. We summarized the characteristics of each interviewee in Table 3. Table 3: Characteristics of BETA’s interviewees: Project B2 Role Age Dev. exp. Agile exp. So/f_tware Engineer 27 4 years 4 years Team leader / Scrum Master 30 9 years 4 years Test Engineer / Product Owner 28 7 years 2 years', 'Role Age Dev. exp. Agile exp. So/f_tware Engineer 27 4 years 4 years Team leader / Scrum Master 30 9 years 4 years Test Engineer / Product Owner 28 7 years 2 years So/f_tware Engineer 23 3 years 2 years So/f_tware Engineer 25 2 years 2 years So/f_tware Engineer 38 18 years 3 years Test Engineer 29 7 years 3 years Project Manager / Scrum Master 28 8 years 6 years 3 INTERVIEWS We used interviews as our data collection procedure, conducted at the companies’ headquarters in October 2014. According to Merriam [17], interviews are eﬀective to elicit information about things that cannot be observed. We used semi-structured interviews with open-ended questions because this approach gathers richer responses when compared to structured interviews. /T_hese were conducted in Portuguese since it was the main language of the interviewer and interviewees. Interviewees: we contacted each participant in advance, and each interview occurred in a private meeting room. Interviewees ac- cepted voluntarily to participate in the research and had to agree with the Informed Consent Form, which guarantees the con/f_iden- tiality of the data provided, the anonymity of the participants and the right to withdraw from the research at any moment. Interviews: we conducted 22 semi-structured interviews. /T_he in- terview had six parts: (1) We explained the purpose of the study; (2) We asked questions regarding the participants’ background and experience;', 'On the Benefits/Limitations of Agile So/f_tware Development: An Interview Study with Brazilian Companies EASE’17, June 15-16, 2017, Karlskrona, Sweden (3) We asked questions to understand how participants deal with agile practices; (4–5) We collected the interviewees’ impressions on the (4) ben- e/f_its and (5) limitations of the use of ASD; (6) We ended the interview asking whether the participants had additional comments that were not covered by the previous questions. We collected general information about each project during the /f_irst interview, such as team size, the context of the project, the programming languages used, Agile methods, and practices used. Data Extraction:we recorded all interviews, totaling 23 hours and 19 minutes of audio time. /T_he /f_irst author transcribed them, and a form in MS ExcelTM was used to guide data extraction. Data Analysis and Synthesis:we used qualitative coding, simi- lar to open codingtechniques to identify factors. We used a post- formed code, so we labeled portions of text without any previous pre-formed code. Figure 1 shows two examples of the open cod- ing process used in two interview transcripts. /T_he categories and subset that emerged from the data were analyzed, compared and re-analyzed in parallel, using an approach similar to axial coding. Figure 2 shows an example of a category using of axial coding. Figure 1: Open coding process used in the interview tran- script. Figure 2: Example of how the category emerged from the initial codes. To avoid interpretation bias, a/f_ter the codi/f_ication phase (Fig- ure 1), we sent our interpretation and each transcript to the in- terviewees and ask them to highlight any misinterpretation. No problems were raised by the interviewees. 4 RESULTS When analyzing the interviews, we categorized 28 bene/f_its and 20 limitations. Due to space constraints, we provide discussions on the eight most common ones. When discussing the main bene/f_its (4.1) and limitations (4.2), we correlate them to agile practices, events or principles, quoting opinions from the interviews. Among similar opinions, we chose to only quote the one we considered the most representative for each case. 4.1 What are the bene/f_its of using ASD? In this section, we present a discussion on some of the bene/f_its found. Table 4 presents a subset of the most common occurrences. /T_he complete list of all bene/f_its and relation to ASD can be found at the companion website: h/t_tps://goo.gl/qrHo3N. Table 4: Summary of the bene/f_its. /T_he column # shows the total number of occurrences of a given category. Bene/f_its A1 B1 B2 # Facilitated project monitoring and tracking × × × 21 Facilitated interaction and collaboration × × × 18 Improved communication × × × 16 Improved project understanding × × × 16 Fosters knowledge sharing within the team members × × × 15 /Q_uick/Frequent feedback × × × 14 Facilitated problem-solving × × × 13 Improved quality × × × 13 Problems are discovered/solved earlier × × × 13 Improved so/f_tware development process × × × 8 Increased commitment × × × 8 Reduced the complexity × × × 7 More agreeable work environment × × 6 Increased team’s autonomy × × × 6 Reduced amount of defects or errors × × × 6 Improved estimation of so/f_tware project eﬀort × × × 6 Improved prioritization of requirements × × 5 Increased customer satisfaction × × 4 Reduced rework × × × 4 Reduced external interference × × 3 Facilitated project monitoring and tracking.Many agile prac- tices/events are useful to understand daily the health of the project. For example, theDaily Meetingspossibility the team members know what each member is doing, and what is done, as an interviewee stated: “/T_he daily meeting is valid because you follow up and you can get a general sense of the progress”. /T_he use ofTask boardwas also seen as bene/f_icial, because anyone in the team can easily see and', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Kamei et al. understand how well the team is doing during the Sprint, and if it is necessary to take special a/t_tention to some task. Recent literature also supports this bene/f_it [1, 8, 10, 22]. Facilitated interaction and collaboration.A subset of bene/f_its are related to this, as interaction and collaboration among the team, and a greater interaction between customers and team members. /T_he core practice wasCo-location Team, as one test engineer stated “As we sit together [co-located], the interaction is very close”and Daily Meeting as an interviewee pointed out “During the daily meetings I can talk about my diﬃculties, and someone can help me”. Recent literature also supports this bene/f_it [10, 12, 14, 19]. Improved communication.A subset of bene/f_its are related to this, as communication between team members, and among customer and team members. /T_he main practices wereDaily Meetingsand Co- location Team. One interviewee said: “[Daily meetings] are impor- tant because we can communicate more, creating a strong relationship. /T_hus, we are always communicating”. Recent literature also supports these bene/f_its [1, 8, 10, 19, 22]. On the other hand, there is evidence showing no positive impact of certain agile methods/practices from the perspective of customer satisfaction [5]. Improved project understanding.Scrum events as Sprint Plan- ning and Review, and principle of Real Customer Involvementwere important for this category. In total, we found two subsets: general improvement and help the understanding of User Stories. As a participant stated: “It serves [Sprint Planning 2] to validate some- thing we did not understand, or something misinterpreted”.Recent literature also supports this bene/f_it [7, 20]. Fosters knowledge sharing within the team members.Some ASD practices were mentioned to support this category, such asPair Programming, Sprint Planning, Daily Meetings, Co-location team, Sprint Review, and Task board. According to a so/f_tware engineer, “You see your pair doing something [coding], and you think ‘I had never thought about that’, or help you to use the IDE with new shortcuts. Moreover, you can help your pair if s/he misses something”.Recent literature also supports this bene/f_it [1, 8, 10, 19]. /Q_uick/Frequent feedback.Some ASD practices and principles were mentioned to support this category, mainly Iterative and in- cremental developmentand Sprint Review. One interviewee pointed out: “I think that delivering so/f_tware in small parts is very important, because we have a quick feedback from customer”. Recent literature also supports this bene/f_it [10, 19, 22]. Facilitated problem-solving.Some ASD practices were mentioned to support this category, as a Pair Programmingand Scrum Master, as one developer experienced: “when we do not know how to solve a problem, we contact the Scrum Master. S/he usually can help us”. Recent literature also supports this bene/f_it [16]. Improved quality./T_his bene/f_it was perceived mainly concerning the code and the project in general. /T_he main practices to support this bene/f_it werePair Programmingand Sprint Review. One inter- viewee pointed out: “[…] when we are working in a pair, we always do a code review”. 4.2 What are the limitations of using ASD? In this section, we present a discussion on some of the limitations found listed in Table 5. /T_he relation of each limitations to ASD can be found at the companion website: h/t_tps://goo.gl/3Ogyil. Table 5: Summary of the limitations. /T_he column # shows the total number of occurrences of a given category. Limitations A1 B1 B2 # Diﬃculty working with User Stories × × × 11 Diﬃculty working with large teams × × × 9 Increased specialization of team members × × × 9 Interference of Product Owner with technical skills × × × 7 Increased time of activities × × × 6 Diﬃculty working with a very closed person × × × 6 Diﬃculty applying/adapting the method/practice × × × 5', 'technical skills × × × 7 Increased time of activities × × × 6 Diﬃculty working with a very closed person × × × 6 Diﬃculty applying/adapting the method/practice × × × 5 Diﬃculty showing progress in projects that do not have GUI × 3 Diﬃculty working with non-agile people × 3 Requires maturity, commitment, and pro-activeness from team members × × 3 Diﬃculty concentrating in co-location teams × × 2 Diﬃculty identifying individual contributions × 2 Diﬃculty working with open scope contracts × 2 Increased costs of the project × × 2 Li/t_tle documentation × 2 Diﬃculty managing dependent requirements × 1 Diﬃculty monitoring distributed projects × 1 Formalism on meetings can inhibit good communication × 1 Increased pressure for delivery of the work × 1 Li/t_tle focus on architecture development × 1 Diﬃculty working with User Stories.A subset of limitations are related to this, as diﬃcult to estimate, to split into tasks, to be used in complex projects, and to understand the user stories, as interviewees pointed out: “the use of story points does not work well because it is subjective. […] I believe that estimate the eﬀort in hours is more accurate”and “It is diﬃcult to estimate in points when the technology is unknown.”. Recent literature also supports this limitation [6, 10]. Diﬃculty working with large teams.Although one of the prin- ciples of Agile Methods is to work with small teams, this limitation is not new in literature, and was found in many interviews mainly due to the events of Daily Meetingsand Sprint Planning, as an in- terviewee stated: “You lose focus when you have a Sprint Planning', 'On the Benefits/Limitations of Agile So/f_tware Development: An Interview Study with Brazilian Companies EASE’17, June 15-16, 2017, Karlskrona, Sweden or Daily Meeting with a large team”. Recent literature also supports this limitation [1, 10, 22]. Increased specialization of team members.Several agile princi- ples emphasize that teams working on the same project should work together at the same place (Co-location team). However, they faced problems when testers and developers worked together, which happened during Daily Meetings, Sprint Planning, and Sprint Re- view, as an interviewee stated: “Some discussions are exclusive to testers. Some discussions are exclusively to developers. Eventually, it is unnecessary for me, because I could not help”. Interference of Product Owner with technical skills.Agile meth- ods emphasize constant communication between customers and the development team. However, we found that the Product Owner can become a burden, as an interviewee stated: “the Product Owner had technical skills, but do not understand the core of the source code in greater details. He wants to push up many user stories”. Increased time of activities./T_his limitation is a subset on the increase on development time, on the time of meetings, and man- agement overhead, mainly caused by ASD events, such as Daily meetings, Sprint Planning and Review, and TDD. One interviewee pointed out: “TDD is quite good, but you are always evolving, result- ing in double of time. ” Diﬃculty working with a very closed person./T_his limitation occurs because the communication and collaboration among stake- holders may involve dealing with diﬀerent personality traits, as one interviewee stated: “[…] We have a highly skilled team member, but he is a very closed person. So, when I need some help, I do not feel comfortable to talk with him”. /T_his limitation highlights that non-technical skill are also valuable for so/f_tware developers. Recent literature also supports this limitation [1, 8, 10]. Diﬃculty applying/adapting the method/practice.A subset of limitations are related to this, as diﬃcult to implement TDD tech- nique, and diﬃcult to use Pair Programming. Although ASD em- braces changes, this was a problem for one interviewee: “[…] some- times the change is radical, and practically everything has changed. Some changes are quite diﬃcult to handle. ”. Diﬃculty showing progress in projects that do not have GUI. /T_his limitation occurs due to the particularity of B1 project during the Sprint Review, as a team leader stated: “[…] it is very diﬃcult to show something functional to the PO (Product Owner) due to the nature of the project”. 5 DISCUSSION In this section, we revisit the main /f_indings of this study (Section 5.1) and discuss important threats to validity (Section 5.2). 5.1 Revisiting /f_indings Our results suggest that the bene/f_its and limitations of ASD are not yet fully understood. We observed that one given agile practice could cause both bene/f_its and limitations. For example, although some interviewees agree that ASD can fosters knowledge sharing within the team members, other interviewees suggested that pair- ing with colleagues can also be a burden. /T_herefore, practitioners might place additional care when introducing agile practices in their projects. We also observed that practitioners need to have a be/t_ter understanding not only of the technical details of agile prac- tices, but also of the context in which the project is inserted, and the cultural characteristics of participants — as some limitations were related to personal traits. Bene/f_its & Limitations.In regards to the bene/f_its, the intervie- wees perceived the Sprint Planning as bene/f_icial (it was related to 14 bene/f_its). /T_his practice was most frequently on the bene/f_it Fosters knowledge sharing within the team members. Other practices most commons, related to 8 bene/f_its were:Daily Meetings, Itera-', 'Fosters knowledge sharing within the team members. Other practices most commons, related to 8 bene/f_its were:Daily Meetings, Itera- tive and Incremental Development, Real customer involvementand Short iterations. As regarding the limitations, the Sprint Planning meeting was the most cited practice. One limitation regarding is that Requires maturity, commitment, and pro-activeness from team members, since, to have a successful Sprint, it is important to have a self-management team [13]. Does context ma/t_ter?In this paper we studied two companies: a small one (60+ employees) and a large one (700+ employees). However, we observed that the developers’ characteristics and backgrounds are more relevant to the bene/f_its and limitations found, than the size of the company. /T_his is partially because, although the so/f_tware companies have diﬀerent sizes, the size of the teams were the same: they all have nine members. 5.2 /T_hreats to Validity First, to avoid inhibition during the interviews and thus compromis- ing our data collection, all interviewees had to sign the Informed Consent Form. In this form, we take responsibility in keeping con- /f_idential any information provided. Second, to foster diversity, we also collected data from participants playing diﬀerent roles. We also studied three projects under active development in two dis- tinct Brazilian so/f_tware companies. However, only one researcher performed the analysis and interpretation of data, which might introduce some bias due to the qualitative nature of this research. To mitigate this bias, we sent the results to our interviewees so that they might indicate whether we misinterpreted some discus- sions. We observed that some teams use Scrum with variations (e.g., some /f_indings suggested team members discussions during the daily meetings). However, in this study, we do not place em- phasis in diﬀerentiating Scrum variations, since we are interested in a broader understanding of the bene/f_its and limitations of this practice. Still, in addition to the detailed description of the results found, we also observed that our /f_indings are in line with the re- cent literature. Finally, to foster replicability, we made available all bene/f_its and limitations found. 6 RELATED WORK Begel and Nagapan [ 1] conducted a web survey at Microso/f_t to identify the perceptions of Agile Methods. /T_hey reported 25 bene/f_its and 14 limitations. /T_he /f_indings of our study overlap with this in 13 out of 28 bene/f_its (e.g., Improved communication, and /Q_uick/Frequent feedback), and 7 out of 20 limitations (e.g. Diﬃculty working with a very closed personand Li/t_tle documentation). Dyb˚a and Dingsøyr [10] conducted a systematic review targeting Agile methods. /T_hey included 36 primary studies. /T_hey found 37 bene/f_its and 12 limitations, primarily with the use of eXtreme Programming. On our research, however, all investigated projects', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Kamei et al. used Scrum, and some XP practices. /T_he /f_indings of our study overlap with this in 20 out of 28 bene/f_its (e.g., Facilitated project monitoring and tracking, and /Q_uick/Frequent feedback) and 5 out of 20 limitations (e.g., Increased time of activitiesand Diﬃculty working with a very closed person). Petersen and Wohlin [19] identi/f_ied the advantages and problems of Agile Methods in a large scale project. As in our study, they conducted 33 semi-structured interviews. /T_hey found 14 bene/f_its and 12 limitations. /T_he /f_indings of our study overlap with this study in 12 out of 28 bene/f_its (e.g., /Q_uick/Frequent feedback, Problems are discovered/solved earlier, and Improved communication), and 7 out of 20 limitations (e.g., Increased time of activities, and Increased pressure for delivery of the work). In a recent study, Solinski and Petersen [22] used the extended hi- erarchical voting analysis framework with 45 agile practitioners to investigate bene/f_it and limitation prioritization. /T_he /f_indings of our study overlap with this study in 10 out of 28 bene/f_its (e.g., Improved so/f_tware development process, Improved planning and management, and Increased eﬃciency of responses to changing requirements), and 5 out of 20 limitations (e.g., Diﬃculty working with large teams, In- creased pressure for delivery of the work, and Increased specialization of team members). Melo et al.[8] presented an overview of the evolution of the agile movement in Brazil. Although their work is both quantitative and qualitative, the /f_indings of our study overlap with this study in 8 out of 28 bene/f_its (e.g., Increased customer satisfaction, and Increased team motivation), and 5 out of 20 limitations (e.g., Requires maturity, commitment, and pro-activeness from team members, and Diﬃculty working with a very closed person). /T_he ”State of Agile” is an annual known research survey con- ducted by VersionOne. /T_he 11th edition [18] was conducted in 2016. /T_he /f_indings of our study overlap with this study in 8 out of 28 bene/f_its (e.g., Increased team productivity, and Improved quality), and 5 out of 20 limitations (e.g., Diﬃculty working with large teams, and Diﬃculty applying/adapting the method/practice). 7 CONCLUSION In this paper, we analyzed the bene/f_its and limitations of the use of Agile So/f_tware Development (ASD) in the context of two Brazilian so/f_tware companies. /T_hrough 22 semi-structured interviews, we created a curated list of 28 bene/f_its and 20 limitations. We found that several bene/f_its are related to (1) improving project monitoring and tracking, (2) improving interaction and collaboration, and (3) fosters sharing knowledge, whereas the limitations are (1) diﬃculty working with user stories, (2) diﬃculty working with large teams, and (3) increased specialization of team members. As a result, we believe that practitioners should be/t_ter understand not only ASD but also the context in which a project emerges, as well as the cultural characteristics of participants. For future work, we plan to correlate the /f_indings from diﬀerent so/f_tware companies and countries to be/t_ter understand whether the bene/f_its and limitations found can be generalized. We also plan to create an “Agile Practice Impact Model” to be/t_ter visualize and investigate which agile practice in/f_luences a given bene/f_it or limitation. Acknowledgements We wish to thank anonymous reviewers for helping improve this paper. /T_his research was partially funded by CNPq (406308/2016-0). REFERENCES [1] Andrew Begel and Nachiappan Nagappan. 2007. Usage and Perceptions of Agile So/f_tware Development in an Industrial Context: An Exploratory Study. In Proceedings of the International Symposium on Empirical So/f_tware Engineering and Measurement (ESEM ’07). 255–264. [2] Barry Boehm. 2002. Get ready for agile methods, with care. Computer 35, 1 (2002), 64–69.', 'and Measurement (ESEM ’07). 255–264. [2] Barry Boehm. 2002. Get ready for agile methods, with care. Computer 35, 1 (2002), 64–69. [3] Barry Boehm. 2006. A View of 20th and 21st Century So/f_tware Engineering. In Proceedings of the 28th International Conference on So/f_tware Engineering (ICSE ’06). 12–29. [4] Bruno Cartaxo, Adaulto Almeida, Emanoel Barreiros, Juliana Saraiva, Walde- mar Ferreira, and S´ergio Soares. 2015. Mechanisms to characterize context of empirical studies in so/f_tware engineering. InProceedings of the Workshop on Experimental So/f_tware Engineering (ESELAW ’15). [5] Bruno Cartaxo, Allan Arajo, Antonio Sa Barreto, and S´ergio Soares. 2013. /T_he Impact of Scrum on Customer Satisfaction: An Empirical Study. In Proceedings of the 27th Brazilian Symposium on So/f_tware Engineering (SBES ’13). 129–136. [6] Bruno Cartaxo, Gustavo Pinto, Danilo Monteiro Ribeiro, Fernando Kamei, Ron- nie E. S. Santos, S ´ergio Soares, and Fabio Q. B. Da Silva. 2017. Using Q&A Websites as a Method for Assessing Systematic Reviews. In Proceedings of the 14th International Conference on Mining So/f_tware Repositories (MSR ’17). [7] Tsun Chow and Dac-Buu Cao. 2008. A survey study of critical success factors in agile so/f_tware projects. Journal of Systems and So/f_tware81, 6 (Jun. 2008), 961–971. [8] Claudia de O. Melo, Viviane A. Santos, Eduardo T. Katayama, Hugo Corbucci, Rafael Prikladnicki, Alfredo Goldman, and Fabio Kon. 2013. /T_he evolution of agile so/f_tware development in Brazil - Education, research, and the state-of-the- practice. Journal of the Brazilian Computer Society,19, 4 (Nov. 2013), 523–552. [9] Philipp Diebold and /T_homas Zehler. 2016./T_he Right Degree of Agility in Rich Processes. Springer International Publishing, Cham, 15–37. DOI:h/t_tp://dx.doi. org/10.1007/978-3-319-31545-4 2 [10] Tore Dyb˚a and Torgeir Dingsøyr. 2008. Empirical Studies of Agile So/f_tware Development: A Systematic Review. Information and So/f_tware Technology50, 9-10 (Aug. 2008), 833–859. [11] Martin Fowler. 1999. Refactoring: Improving the Design of Existing Code. Addison- Wesley Longman Publishing Co., Inc., Boston, MA, USA. [12] Rashina Hoda, James Noble, and Stuart Marshall. 2010. Balancing Acts: Walking the Agile Tightrope. In Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of So/f_tware Engineering (CHASE ’10). 5–12. [13] Rashina Hoda, James Noble, and Stuart Marshall. 2010. Organizing Self- organizing Teams. In Proceedings of the 32nd ACM/IEEE International Conference on So/f_tware Engineering - Volume 1 (ICSE ’10). 285–294. [14] Emam Hossain, Muhammad Ali Babar, and June Verner. 2009. How Can Ag- ile Practices Minimize Global So/f_tware Development Co-ordination Risks?. In Proceedings of the 16th European Conference on So/f_tware Process Improvement (EuroSPI ’09). Berlin, Heidelberg, 81–92. [15] Wa/t_ts S Humphrey. 1995.A discipline for so/f_tware engineering. Addison-Wesley Longman Publishing Co., Inc. [16] Seiyoung Lee and Hwan-Seung Yong. 2010. Distributed agile: project manage- ment in a global environment. Empirical So/f_tware Engineering15, 2 (Apr. 2010), 204–217. [17] Sharan B Merriam and Elizabeth J Tisdell. 2015. /Q_ualitative research: A guide to design and implementation. John Wiley & Sons. [18] Version One. 2016. 11th Annual State of Agile Report. (March 2016). Retrieved April 26, 2017 from h/t_tps://explore.versionone.com/state-of-agile/versionone- 11th-annual-state-of-agile-report-2. [19] Kai Petersen and Claes Wohlin. 2009. A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case. Journal of Systems and So/f_tware82, 9 (Sep. 2009), 1479–1490. [20] Minna Pikkarainen, Jukka Haikara, Outi Salo, Pekka Abrahamsson, and Jari Still. 2008. /T_he Impact of Agile Practices on Communication in So/f_tware Development. Empirical So/f_tware Engineering13, 3 (Jun. 2008), 303–337.', '2008. /T_he Impact of Agile Practices on Communication in So/f_tware Development. Empirical So/f_tware Engineering13, 3 (Jun. 2008), 303–337. [21] D. J. Reifer. 2002. How good are agile methods? IEEE So/f_tware19, 4 (Jul. 2002), 16–18. [22] Adam Solinski and Kai Petersen. 2016. Prioritizing agile bene/f_its and limitations in relation to practice usage. So/f_tware /Q_uality Journal24, 2 (Jun. 2016), 447–482.']","['Keywords: Agile Softwae Development Agile Pawctcee Benefite Limitwtone Softwae Induetar Inteaviete Who is this briefin  or? Agile  pawcttoneae  wnd Reeewacheae  to  w  betea undeaetwnd  thich  benefite  wnd limitwtone cwn be found in epecific context of eoftwae paojecte  tith the  uee  of  Agile  Softwae Development. Where the fidiins come  rom? All findinge of thie baiefing teae  extawcted of quwlitwtve aeeewach  emplored eemi-etauctuaed  inteaviete conducted br Kwmei et  wl.  What is iicluded ii this briefin? The mwin findinge of the oaiginwl  etudr. What is iot iicluded ii this  briefin? Complete liet of benefite wnd  limitwtone. Exwmplee of tawnecaipt poatone  thwt explwin of thich eituwton the  findinge teae found. EVIDENCES OF BENEFITS AND LIMITATIONS OF AGILE SOFTWARE DEVELOPMENT ON BRAZILIAN SOFTWARE COMPANIES This  briefin  reports  empirical  evideice of beiefts aid limitatois fouid of Anile Software  Developmeit  of  ttwo  Braiiliai software compaiies- FINDINGS 10  MORE  COMMONS  PERCEPTIONS  OF BENEFITS OF  AGILE  SOFTWARE DEVELOPMENT: 1. Fwcilitwted paoject monitoaing wnd tawcking 2. Fwcilitwted  inteawcton  wnd collwboawton 3. Impaoved communicwton 4. Impaoved paoject undeaetwnding 5. Foeteae  knotledge  ehwaing  tithin the tewm membeae 6. Quick/Faequent feedbwck 7. Fwcilitwted paoblem-eolving 8. Impaoved quwlitr 9. Paobleme  wae  diecoveaed/eolved ewaliea 10. Impaoved  eoftwae  development paoceee 10  MORE  COMMONS  PERCEPTIONS  OF LIMITATIONS OF  AGILE  SOFTWARE DEVELOPMENT: 1. Difcultr toaking tith eeea Stoaiee 2. Difcultr toaking tith lwage tewme 3. Incaeweed  epeciwlizwton  of  tewm membeae 4. Inteafeaence  of  Paoduct  Otnea tith technicwl ekille 5. Incaeweed tme of wctvitee 6. Difcultr  toaking  tith  w  vear cloeed peaeon 7. Difcultr  wpplring/wdwptng  the method/pawctce 8. Difcultr  ehoting  paogaeee  in paojecte thwt do not hwve GeI 9. Difcultr  toaking  tith  non-wgile people 10. Requiaee  mwtuaitr,  commitment, wnd  pao-wctveneee  faom  tewm membeae 5  MORE  COMMON  AGILE  SOFTWARE PRACTICES AND METHODS RELATED TO BENEFITS: 1. Spaint Plwnning 2. Dwilr Scaum Meetnge 3. Pwia Paogawmming 4. Twek bowad 5. TDD 5  MORE  COMMON  AGILE  SOFTWARE PRACTICES AND METHODS RELATED TO LIMITATIONS: 1. Spaint Plwnning 2. Dwilr Scaum Meetnge 3. Shwaed Gowle wnd Paobleme 4. Spaint Reviet 5. TDD 6. Rewl Cuetomea Involvement ORIGINAL STUDY REFERENCE Link to pwpea: Kwmei, F. K. et wl. On the Benefite/Limitwtone of Agile Softwae Development: An Inteaviet Studr tith Bawziliwn Compwniee. EASE ’17, 2017.']","**Title:** Understanding the Benefits and Limitations of Agile Software Development in Brazilian Companies

**Introduction:**  
This Evidence Briefing summarizes findings from a study that explored the benefits and limitations of Agile Software Development (ASD) through interviews with software practitioners in Brazilian companies. The goal is to provide insights that can help organizations assess the applicability of Agile methods in their own contexts.

**Main Findings:**  
The study involved 22 semi-structured interviews with employees from two software companies located in Pernambuco’s Technology Park, Brazil. The analysis revealed 28 benefits and 20 limitations associated with the implementation of Agile practices. 

**Key Benefits:**
1. **Enhanced Project Monitoring:** Agile practices facilitate ongoing tracking of project progress, allowing teams to identify issues early.
2. **Improved Communication and Collaboration:** Daily meetings and co-location foster better interaction among team members and with customers, enhancing teamwork.
3. **Knowledge Sharing:** Agile practices such as Pair Programming and Sprint Reviews promote sharing of skills and insights among team members.
4. **Quick Feedback:** Iterative development allows for rapid customer feedback, enabling teams to adjust and improve continuously.
5. **Increased Team Autonomy:** Agile encourages self-management, empowering teams to make decisions and solve problems independently.

**Key Limitations:**
1. **Challenges with User Stories:** Interviewees reported difficulties in estimating and understanding user stories, which can hinder project progress.
2. **Large Team Dynamics:** Collaborating in large teams can complicate daily meetings and planning sessions, leading to decreased focus and effectiveness.
3. **Increased Specialization:** Team members may become overly specialized, which can limit flexibility and collaboration across roles.
4. **Product Owner Interference:** When the Product Owner has technical skills, there can be a tendency to push for more user stories than the team can handle, creating pressure.
5. **Time Consumption:** Agile practices, while beneficial, can also lead to increased time spent in meetings and management tasks, impacting overall productivity.

The findings suggest that while Agile methods can significantly improve software development processes, they also come with challenges that need to be managed carefully. Understanding the specific context of a project, including team composition and company culture, is crucial for successful implementation.

**Who is this briefing for?**  
This briefing is intended for software development practitioners, project managers, and decision-makers in organizations considering the adoption of Agile practices or seeking to improve their current Agile processes.

**Where the findings come from?**  
The insights presented in this briefing are derived from qualitative interviews conducted with software professionals at two companies in Pernambuco, Brazil, as part of research by Fernando Kamei, Gustavo Pinto, Bruno Cartaxo, and Alexandre Vasconcelos.

**What is included in this briefing?**  
This briefing includes a summary of the benefits and limitations of Agile Software Development as identified through interviews, providing actionable insights for practitioners.

**What is NOT included in this briefing?**  
Detailed statistical analyses or specific case studies beyond the summarized findings.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research conducted:**  
Fernando Kamei, Gustavo Pinto, Bruno Cartaxo, Alexandre Vasconcelos. ""On the Benefits/Limitations of Agile Software Development: An Interview Study with Brazilian Companies."" EASE’17, Karlskrona, Sweden, June 15-16, 2017. DOI: [http://dx.doi.org/10.1145/3084226.3084278](http://dx.doi.org/10.1145/3084226.3084278)"
"['Feature-Based Test Oracles to Categorize Synthetic 3D and 2D Images of Blood Vessels Misael C. Júnior ICMC/USP Universidade de São Paulo São Carlos, SP, Brazil misaeljr@usp.br Rafael A. P. Oliveira UTFPR Universidade Tecnológica Federal do Paraná Dois Vizinhos, PR, Brazil raoliveira@utfpr.edu.br Miguel A. G. Valverde IME/USP Universidade de São Paulo São Paulo, SP, Brazil miguelg@ime.usp.br Marcel P. Jackowski IME/USP Universidade de São Paulo São Paulo, SP, Brazil mjack@ime.usp.br Fátima L. S. Nunes EACH/USP Universidade de São Paulo São Paulo, SP, Brazil fatima.nunes@usp.br Márcio E. Delamaro ICMC/USP Universidade de São Paulo São Carlos, SP, Brazil delamaro@icmc.usp.br ABSTRACT Automated testing activities contribute significantly to reduce the cost and to increase the productivity during the software develop- ment process. Programs with complex outputs limit the applica- tion of automated testing strategies. A possible solution is the use of feature-based oracles. In this study, we use the framework O- FIm/CO (Oracle for Images and Complex Outputs), which uses CBIR (Content-based Image Retrieval) concepts to evaluate the similarity of synthetic images of blood vessels through the “feature-based test oracle” approach. In order to demonstrate the effectiveness of the approach, we evaluated the ability and accuracy of the test oracle in automated the process of categorization of synthetic images of blood vessels in 3D and 2D models through the similarity between features. Furthermore, we compared the accuracy of the catego- rization of the test oracle relative to random classifiers. The results obtained in two empirical studies revealed an AVG (average) of precision, recall, and specificity of, respectively, 77%, 100%, and 88% in the categorization performed by the test oracle for 3D images and 71%, 81%, and 93% in the categorization performed by the test oracle for 2D images. CCS CONCEPTS • General and reference → Empirical studies ; Experimenta- tion; • Software and its engineering → Software testing and debugging; KEYWORDS Software Testing, Test oracles, Angiography, Three-Dimensional Synthetic Vascular Networks Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SAST, September 18–19, 2017, Fortaleza, Brazil © 2017 Association for Computing Machinery. Conference ACM ISBN 978-1-4503-5302-1/17/09. . . $15.00 https://doi.org/10.1145/3128473.3128484 1 INTRODUCTION Complex-output systems are applications whose outputs are given in unusual formats limiting the application of automated software engineering activities[2]. Sounds [ 6], processed images [ 2], and three-dimensional user interfaces are some of the current complex- output domains. Thus, regarding the contemporary software, it is possible identify several complex-outputs systems. Among these systems, we highlight Text-To-Speech (TTS) systems, and systems that process three-dimensional medical images of blood vessels [7]. Due to the complexity associated with data produced by complex- output systems within the SUT (System Under Test), traditional testing techniques are inefficient and, often impracticable [6]. The complexity associated with the SUTs output leads testers to perform subjectively, and ad-hoc testing approaches. Generally, humans ex- ercise the SUT, observing its outputs and filling a protocol of aspects to be evaluated, making the testing activities difficult, tedious and time-consuming.', 'ercise the SUT, observing its outputs and filling a protocol of aspects to be evaluated, making the testing activities difficult, tedious and time-consuming. A promising approach to support automated testing activities in complex-output systems is the design based on test oracles [6]. In a testing environment, a test oracle [ 3] represents a structure adopted to decide on the correctness of an SUTs execution [ 1]. Automated test oracles check SUTs outputs, providing verdicts on their correction [5]. To do so, test oracles need for reliable source of information about SUTs behavior. This study presents a strategy to evaluate the similarity of syn- thetic images of blood vessels in 3D and 2D models through an approach called “feature-based test oracles”. The strategy is demon- strated through a case study that aims to automatically categorize synthetic images of blood vessels in 3D and 2D models from the similarity between features. Initially, we generated synthetic im- ages of blood vessels in 3D models based on the method proposed by Valverde et al. [7]. Next, we identified relevant features to extract from the images that may be source of information for the auto- mated oracle. The images are categorized into four groups, based on the similarity between the number of features. The feature ex- tractors and the similarity function are adapted to the context of the framework O-FIm/CO, which implements the concepts designed by feature-based test oracles.', 'SAST, September 18–19, 2017, Fortaleza, Brazil Misael C. Júnior et al. Aiming at extending the study, we generated images in 2D mod- els using screeshots of the 3D images taken with the BioImage Suite Tool1. After that, we created test oracles to assess the categoriza- tion of the images in 3D and 2D models, aiming at evaluating the accuracy of the test oracle in the judgment of similarity between the images. It is important to note that in order to implement the test oracles, we considered an image as a model image, aiming to measure the similarity of the model image with the other images of the dataset. In other words, the test oracle, through the definition of a threshold, judges which of the images in the dataset are similar to the model image and may be of the same category as the model image. The results show that the test oracle was highly accurate in categorizing 3D and 2D images. With respect to 3D images, the test oracle is more accurate, since 3D images allow a more detailed analysis of the image structures, facilitating the application of the feature-based test oracles approach. Additionally, we compared the categorization performed by the test oracle with random algorithms, demonstrating that the test oracle has a statistically significant difference in comparison with the random algorithms. The contributions associated with this paper are threefold: (1) the proposition of a workflow on how to develop and improve feature-based test oracles for a specific complex-output domain, highlighting its strengths and detecting its weakness; (2) the pre- sentation of a set of criteria to improve feature-based test oracles for systems that process medical images of blood vessels; and (3) an extensive discussion on future trends in automated test oracles for complex output domains. This study cooperates with the area of SE (Software Engineering), more specifically with the area of Software Testing, through the configuration and assessment of automated test oracles for systems whose outputs consist of synthetic medical images. 2 BACKGROUND Test oracles are mechanisms that support tester when determin- ing if the output of a test case is valid or not. Test oracles can be designed using difference sources of information: (i) based on asser- tion; (ii) test cases; (iii) tables; (iv) state machines; or (v) even human evaluation. The test oracle directly affects the test quality, and for that reason, the automation is required. In addition to improve the quality of the test, automation also improves the productivity of the test, reducing human efforts. Considering complexity and cost aspects, when a test oracle is required to only consider input data and expected output, the test activity tends to be quite simple. There are several frameworks available to handle this kind of oracle, such as the frameworks of xUnit family, allowing the execution of the unit test in SUTs (e.g., JUnit for Java application). On the other hand, the absence of an expected output makes the oracle definition a non-trivial task. The feature-based test oracles approach is based on the concept of exploiting features extracted from a reliable output set out by the tester as a reference, mitigating the absence of output [5]. The extracted features are used as a source of information to evaluate SUTs outputs. Thus, in a test environment, testers can define the reliable features of the reference output. Feature is any aspect of 1Link to the BioImage Suite tool: https://goo.gl/MjAEdD output that can be quantified by an algorithm, e.g., the number of bifurcations or segments in a synthetic blood vessel. Feature- based test oracles can provide resources for setting test oracles based on the difference between the features collected from the SUT and the same features obtained from the reference output. The difference between the output of the SUT and the reference output is calculated using a similarity function, i.e. a value to define', 'SUT and the same features obtained from the reference output. The difference between the output of the SUT and the reference output is calculated using a similarity function, i.e. a value to define the degree of similarity between the instances, according to its cohesion. Feature-based test oracles can be designed into the steps: (i) the group of features relevant to a given test; (ii) the similarity function to measure the distance between the SUT outputs and the predefined references; and (iii) the maximum acceptable distance value between the reference and the SUT outputs. Then, to provide a final verdict (approval/failure), a feature-based test oracle requires a threshold value that represents the maximum of the difference value between the SUTs outputs and the references, considering their resource values. Feature-based test oracles are recommended in the following cases: (i) when the SUT outputs are complex, and there is no stan- dard method to provide an adequate comparison of the SUT outputs with some similar object; and (ii) when it is difficult to define an expected output for the SUTs output. However, there are some trustful outputs from previous and similar implementation. The framework O-FIm/CO implements the concept of feature-based test oracles through CBIR concepts [2, 4, 6]. The approach is used as a novel test oracle paradigm to address to challenges of testing complex-output systems, eg, TTS systems and three-dimensional vascular networks. In this study, we have exploited O-FIm/CO to implement our test oracles described in Section 3. Modeling three-dimensional (3D) objects in computational sys- tems is complex and, because of the number of details, several applications demand geometric models or replicas to be generated from the characteristics of the external surfaces of the object. In this context, e.g., Valverde et al. [7] present a strategy to generate more realistic 3D synthetic blood vessel, consisting of three steps: (i) generation of the descriptive ’String’ of the vessel; (ii) generation of the synthetic vessel skeleton; and (iii) vessel discretization. The first step is generated through the input grammar, aString that rep- resents the execution of a given interactions grammar. This String represents a sequence of statements, and each character is asso- ciated with an action. These actions are taken in the second step, generating another sequence with some parameters, for instance, the vessel diameter. Finally, the third step of discretization creates a synthetic vessel, image with intensities that mimic real CT-angio. Systems that manipulate three-dimensional synthetic images of vascular networks are very useful for magnetic resonance angiog- raphy (MR angiography ) or computed tomography (CT angiogra- phy) based exams, enabling a thorough analysis of the vascular networks [7]. However, systems that generate or manipulate three- dimensional synthetic images have recurring quality issues due to the lack of automated test strategies. Using automated tools is es- sential to help health professionals to diagnose possible anomalies or pathologies.', 'Feature-Based Test Oracles to Categorize Synthetic 3D and 2D Images of Blood Vessels SAST, September 18–19, 2017, Fortaleza, Brazil 3 MATERIALS AND METHODS Using feature-based approach, we propose a strategy to evaluate systems with complex outputs, more specifically, systems that pro- cess synthetic images of blood vessels. In this section, we describe the materials and methods we have developed and used to conduct this study. Following the workflow required by the feature-based approach, we have studied some feature extractors to 3D blood vessels domain. During the definition of the feature extractors to be studied, we had the support of specialists in Medical Image Processing. The following feature extractors were developed: (i) density; (ii) bifur- cation; (iii) endpoints; and the (iv) segments. The density extractor measures how much the blood vessel fills the image by the ratio of the number of blood vessel points to the number of points in the image as a whole. Bifurcation points are characterized by the separation of a vessel into two or more branches. Endpoints, as well as bifurcation points, are determined by the number of neighbors at a given point of the image. Thus, if a certain point of the image has exactly one neighbor, it is defined as an endpoint. On the other hand, if a certain point of the image has more of two neighbor, it is defined as a bifurcation. Finally, the segment extractor returns the number of segments of the blood vessel. The extractors developed were made available as open-source code in an online repository for 3D and 2D images 2. Figure 1 shows the features extracted from a synthetic image blood ves- sel with aneurysm. Systems based on a single image attribute may not achieve adequate recovery levels, which is why systems use multiple image attributes for indexing and retrieval. Further, we develop feature extractors that complement each other and increase the level of significance between features and makes it easier to compare images. Features Dimensions: 120 120 120 ###########Density extractor########## Density: 1.03x10-4 #########Bifurcation extractor######### Number of bifurcations: 3 #########Endpoints extractor######### Number of endpoints: 5 ###########Segment extractor######### Number of segments: 7 Figure 1: Features extracted by extractors developed. After extraction of the features, data were normalized to the linear scale. This operation maps the values of an attribute to a closed range of 0 to 1. The features are stored in a vector of features, and the similarity between one image and another is measured by calculating the distance between the feature vectors of the images. To measure the distance between the characteristic vectors of the images, we used Euclidean Distance (ED). One of the steps in our study is to compare the accuracy obtained by the oracle in the correct categorization of 2D and 3D images with the accuracy obtained through a random classifier performing two of the same classifications. Random classifiers are methods of 2https://goo.gl/gqqzIM learning sets to randomly classify of objects into classes. To perform this activity in our study, we used the Random class of the Python programming language. Then, we ran the Random class twice for each image and measured the accuracy metrics, through precision, recall, and specificity metrics. To do so, we considered an image as a model and, through the choice method sample of the Random class, randomly obtained images of the same category from the model image. 4 CASE STUDY The main objective of our study is to evaluate the accuracy of feature-based test oracles, as measured by precision, recall and specificity metrics, in automated categorization of images of syn- thetic blood vessels in 3D and 2D models. After that, the Research Question (RQ), metrics, dataset, setting threshold, and the experi- mental conduction are presented and discussed. 4.1 Research Questions and Metrics', 'Question (RQ), metrics, dataset, setting threshold, and the experi- mental conduction are presented and discussed. 4.1 Research Questions and Metrics This study was defined towards answering a single RQ: – RQ1: Is classification of synthetic images of blood vessels in 3D and 2D models performed by feature-based test oracles more accurate compared to random classifiers? The RQ1 addresses the effectiveness of feature-based test oracles in categorizing synthetic images of blood vessels into 3D and 2D models in comparison with random classifiers. The accuracy of the tes oracle in the correct categorization of the images indicates the utility of the oracles in testing activities. In this case, the metrics for precision, recall and specificity of the automated oracles are evaluated to verify the accuracy in categorizing 3D and 2D images. The performance of categorization is measured by the metrics specified as follows: Precision = T P T P+ FP (1) Recall = T P T P+ F N (2) Speci f icity = T N T N+ FP (3) where: – True positive (TP): Image categorized as correct in the expected group; – False positive (FP): Image categorized as correct in the unexpected group; – True negative (TN): Image is categorized as incorrect in the unex- pected group; – False negative (FN): Image categorized as incorrect in the expected group. To measure the previously specified metrics, our oracle is defined based on the distance between the set of reference images R1 = {ImR1 1, ImR1 2, ..., ImR1n } with the set of reference images to be tested R2 = {ImR2 1, ImR2 2, ..., ImR2n }. Based on the expected categorization of the images, the oracle verifies whether the distance obtained in the categorization falls between an ImR1 1 and an ImR2 1. Oracle: If the image resulting from the execution of the test has a distance above the threshold for the reference image, then', 'SAST, September 18–19, 2017, Fortaleza, Brazil Misael C. Júnior et al. the execution is deemed incorrect: if ED (ImR1 i , ImR2 i ) > thr then execution with ImIi failed The metrics are measured according to the images that the test oracle judge as similar to the model image. As an analogy, by setting the threshold, the test oracle “returns” images that have a similarity value less than or equal to the threshold value. 4.2 Hypothesis Formulation The null hypothesis and the alternative hypothesis are described as follows: Null hypothesis (H 0): “There is no difference in the accuracy to categorize the synthetic images of blood vessels between the feature-based test oracle and the random classifiers. ” Alternative hypothesis (H 1): “The feature-based test oracle ob- tained a higher accuracy value in the categorization of synthetic images of blood vessels in comparison with random classifiers. ” The statistical test is performed using the binomial test, which is recommended to verify statistical evidence of data from binary results (success cases), being one of the reasons to use it. In the case of test oracles, considering the number of true verdicts, binomial tests can show the statistical significance of deviations from an expected distribution of observations into two categories (fail and pass). We use the binomial test with a level of significance equal to α = 0.05. Additionally, we performed the t-test, which is a statistical test, in two samples to evaluate the difference in the accuracy of the cat- egorizations performed by the test oracle and the random classifier. The two-sample t-test measures a confidence interval and performs a hypothesis test for difference between two population averages when standard deviations are unknown, and the samples are gener- ated independently of each other. To testH0, we performed a single unpaired t-test: Test oracle versus random with a significance level α = 0.05. 4.3 Dataset The dataset explored in our study is comprised of synthetic images of blood vessels in 3D and 2D models. The 3D images are generated by the method proposed by Valverde et al. [7], and the 2D images are obtained from screenshots of the 3D images using the BioImage Suite Tool. The images are generated from four categories of “pathologies” of blood vessels: (i) aneurysm; (ii) non-regular; (iii) normal; and (iv) stenosis. Aneurysm blood vessels are characterized by abnormal di- lation caused by weakened vessel walls, trauma, or vascular disease. Non-regular blood vessels are characterized by ’abnormal’ behavior, taking into account the visual aspect. Normal blood vessels present a visual appearance without the presence of dilatations or nar- rowing. Ultimately, blood vessels with stenosis have an abnormal narrowing. 4.4 Experimental Design and Procedure Seeking experimental evidence to answer RQ1, we performed our study in the following steps: Initially, we generated the 3D images through the method proposed by Valverde et al. [7]. The 2D images were obtained from screenshots. Then, in the second step, we car- ried out the activities related to the preprocessing of these images. In this step, we segmented and skeletonized the images using the Fiji tool3. Next, we sought to identify features that were relevant to the images. Therefore, we implemented the density, bifurcation, end- points, and segment extractors. The extractors were developed in Java programming language and made available in an online repos- itory as a contribution of our work and they were validated by specialists. The values of the features were normalized to the linear scale, taking into account samples of each feature obtained from the dataset. Then we have classified the images into four distinct categories, based on the number of features. The categorization was standard- ized for both image models, so we did not use the density feature in the categorization. On the other hand, the density feature was used', 'ized for both image models, so we did not use the density feature in the categorization. On the other hand, the density feature was used to measure the similarity of images within the same model. During this step, we had the support of the specialist in Medical image processing. Four classes we defined A (aneurism1, nonRegular1, nonRegular2, normal3, and stenose1), B (aneurism2, nonRegular3, normal4, and stenose2), C (nonRegular4 and normal5), and D (nor- mal1 and normal2). Next, we adapted the feature extractors, the similarity function, and carried out activities related to the framework adaptation to the context of our study. Trough this task, an image is considered as a model image for the test oracle, and other images in the dataset are considered as images to be tested. Thus, the test oracle measures the degree of similarity between the model image and the images to be tested from the definition of a threshold. Finally, we conducted the experiment for the 3D images and analyzed the results. Similarly, we replicated the experiment to the 2D images and the results were analyzed. 4.5 Threshold Setting The threshold value may be set according to the purpose of the application. For this study, we conducted an experiment on the 2D and 3D image dataset to evaluate the precision and recall metrics according to the threshold value. Thus, we vary the threshold value, starting with 0.1, incrementing steps from 0.1 to 0.7. The variation of the threshold value was performed using an increment of 0.05 at each step. In conducting initial studies, empirical studies are often used to evaluate the influence of the threshold value in the database under study. Figure 2 shows the AVG (average) value of the dataset for each value of the selected threshold. It is possible to notice that there is a trade-off between precision and recall. The higher the threshold value, the lower the precision and the higher the recall. The lower the threshold value, the higher the recall and the lower the precision. 5 RESULTS AND DISCUSSION Initially, we executed the feature extractors to get the feature values for the images in our dataset. Based on the number of features extracted from each image, the images were categorized. Table 1 shows the number of features of each image in our dataset. The values of the Bifurcation, Endpoints, and Segments features are similar for images in 3D and 2D models. However, the value of the 3Link containing information about the Fiji tool: http://imagej.net/Fiji', 'Feature-Based Test Oracles to Categorize Synthetic 3D and 2D Images of Blood Vessels SAST, September 18–19, 2017, Fortaleza, Brazil Figure 2: Images 3D – Comparison of precision and recall AVG values for the set of threshold values. density extractor is originally from the 3D images, so the density feature is not used to standard categorization of the images. Image Density Bifurcation Endpoints Segments aneurism1 1.03x10 −4 3 5 7 aneurism2 1.22x10 −4 7 9 15 nonRegular1 4.91x10 −5 2 4 5 nonRegular2 7.46x10 −5 2 4 5 nonRegular3 1.2x10 −4 6 8 13 nonRegular4 2.19x10 −4 20 18 37 normal1 1.9x10 −5 0 2 1 normal2 3.47x10 −5 1 3 3 normal3 5.96x10 −5 3 4 7 normal4 1.16x10 −4 7 9 15 normal5 1.9x10 −4 15 17 31 stenose1 6.46x10 −5 3 5 7 stenose2 7.87x10 −5 7 9 15 Table 1: Images with their respective numbers of features. During the analysis of the results, we observed that for images with larger numbers of features, high threshold values work well. However, for images with smaller numbers of features, low thresh- old values work better. Given that goal of the study is to obtain a larger number of correctly categorized images, but at the same time to ensure that the same category images are returned, the best choice would be 0.35 for the two image models. However, if the study aims to obtain a greater number of images of the same category without ensuring that the oracle was accurate in categorizing the images correctly, the best choice would be 0.5. As this study aims to verify if the oracle was necessary to categorize the images in the two models, we selected the threshold value of 0.35. Table 3 shows the results obtained by the automated oracles in the categorization of synthesized images of blood vessels in 3D model. The precision, recall and specificity values show the automated oracle accuracy in categorizing 3D images based on the categories previously defined (A, B, C, and D). The value of AVG represents a mean or percentage of the value of each metric. The automated oracle presented an AVG of 77% for precision, 100% for recall, and 88% for specificity. The automated oracle presented results with high accuracy in most 3D images. The AVG of 77% precision for 3D images means that the oracle has categorized most of the images as positive in the expected categories. The value of the recall equals to 100% in all images means that the oracle categorized as positive all images that are positive. On the other hand, an AVG of 88% specificity means that, for most images, the oracle did not categorize images that do not belong to the category of the image being analyzed. Regarding the normal1 and normal2 images, the oracle presented a low accuracy rate. The images normal1 and normal2, as shown in Table 1, are the images that have the lowest number of features. Thus, for the threshold value set, the oracle rated these images with low accuracy. On the other hand, for most images, the oracle categorization was highly accurate. Table 4 shows the results obtained by the automated oracles in the categorization of synthesized images of blood vessels in 2D model. In Table 4 are presented for the AVG of precision, recall and specificity. Precision presented an AVG of 72%, demonstrating that the automated oracle correctly categorized most of the images. Recall presented an AVG of 81%, demonstrating that for most of the images the automated oracle categorized as positive images that are positive. On the other hand, specificity presented an AVG of 93%, demonstrating that the automated oracle categorized the truly negative images as negative. Differently from the oracle performance when categorizing 3D images, the automated oracle in 2D images had a low recall value (81%). This may have occurred because of the threshold value used. Furthermore, for images with larger numbers of features, the auto- mated oracle presented low numbers of precision and recall. For instance, for the nonRegular4 and normal5 images, the oracle pre-', 'Furthermore, for images with larger numbers of features, the auto- mated oracle presented low numbers of precision and recall. For instance, for the nonRegular4 and normal5 images, the oracle pre- sented an AVG of precision and recall equal to 0, although it pre- sented an AVG of 100% for specificity. This may have occurred due to the difficulty in correctly extracting the features of 2D images. Complementing the answer to RQ1, we performed a statistical analysis aiming at demonstrating the efficiency of feature-based test oracles for the validation of systems that produce synthetic images of blood vessels. Thus, we performed the binomial test to verify the statistical evidence of data. The binomial test demonstrates the statistical significance of deviations from an expected distribution of observations in two categories (fail and pass). In this context, Table 2 presents the statistical analysis performed based on the numbers obtained in this study. Table 2 presents a global analysis including the number of false verdicts (FP/N), the number of true verdicts (VP/N) and the statistical test of H0 based on these numbers. In the meaning column of Table 2, a statistical interpretation of what the verdict numbers reveal about the results is presented. These interpretations show the probability of occurrence of the main events of the oracle combinations. The statistical interpre- tations are based on concepts of the one-tail binomial test, which describes the chance of occurrence of events on three different occasions: (i) the probability of exactly TP/N times out of 156 times; (ii) the probability of fewer than TP/N out of 156 times; and (iii) the probability of more than TP/N out of 156 times. Regarding the hypothesis tests presented, Table 2 reveals that the test oracles for 3D and 2D images reached a high statistical significance, causing H0 to be rejected. Out of the 156 ratings, these test oracles had many true verdicts reaching 142 for 3D and 2D im- ages. For random categorization, the number of true verdicts ranged from 100 to 108. In this context, test oracles had a performance that', 'SAST, September 18–19, 2017, Fortaleza, Brazil Misael C. Júnior et al. Image Metric Oracle O 1 P-Value H 0? meaning 3D Images False Positive 14 0.000000000001 Rej. The prob. of exactly 142 out of 156 is p = 0.000000000001 False Negative 0 The prob. of exactly, or fewer than, 142 out of 156 is p = 0.999999 True Positive 36 The prob. of exactly, or more than, 142 out of 156 is p = 0.000000000001 True Negative 106 The (one-tailed) prob. of exactly, or greater than, 142 out of 156 is p = 0.000000000001 2D Images False Positive 8 0.000000000001 Rej. The prob. of exactly 142 out of 156 is p = 0.000000000001 False Negative 6 The prob. of exactly, or fewer than, 142 out of 156 is p = 0.999999 True Positive 30 The prob. of exactly, or more than, 142 out of 156 is p = 0.000000000001 True Negative 112 The (one-tailed) prob. of exactly, or greater than, 142 out of 156 is p = 0.000000000001 Random 1 False Positive 24 0.000000000001 Rej. The prob. of exactly 108 out of 156 is p = 0.000000000001 False Negative 24 The prob. of exactly, or fewer than, 108 out of 156 is p = 0.999999 True Positive 12 The prob. of exactly, or more than, 108 out of 156 is p = 0.000000000001 True Negative 96 The (one-tailed) prob. of exactly, or greater than, 108 out of 156 is p = 0.000000000001 Random 2 False Positive 28 0.000000000001 Rej. The prob. of exactly 100 out of 156 is p = 0.000000000001 False Negative 28 The prob. of exactly, or fewer than, 100 out of 156 is p = 0.999999 True Positive 10 The prob. of exactly, or more than, 100 out of 156 is p = 0.000000000001 True Negative 90 The (one-tailed) prob. of exactly, or greater than, 100 out of 156 is p = 0.000000000001 Table 2: General results and metrics collected from the study. Image Threshold Precision Recall Specificity aneurism1 0.350 0.80 1.00 0.87 aneurism2 0.350 1.00 1.00 1.00 nonRegular1 0.350 0.67 1.00 0.75 nonRegular2 0.350 0.67 1.00 0.75 nonRegular3 0.350 0.75 1.00 0.90 nonRegular4 0.350 1.00 1.00 1.00 normal1 0.350 0.33 1.00 0.81 normal2 0.350 0.20 1.00 0.63 normal3 0.350 0.80 1.00 0.87 normal4 0.350 1.00 1.00 1.00 normal5 0.350 1.00 1.00 1.00 stenose1 0.350 0.80 1.00 0.87 stenose2 0.350 1.00 1.00 1.00 AVG 0.77 1 0.88 Table 3: Results returned by the oracle and values of preci- sion and recall for 3D images - threshold = 0.350. Image Threshold Precision Recall Specificity aneurism1 0.350 0.67 0.50 0.87 aneurism2 0.350 1.00 1.00 1.00 nonRegular1 0.350 0.80 1.00 0.87 nonRegular2 0.350 1.00 1.00 0.87 nonRegular3 0.350 1.00 1.00 1.00 nonRegular4 0.350 0.00 0.00 1 normal1 0.350 1.00 1.00 1.00 normal2 0.350 0.20 1.00 0.63 normal3 0.350 0.80 1.00 0.87 normal4 0.350 1.00 1.00 1.00 normal5 0.350 0.00 0.00 1 stenose1 0.350 1.00 1.00 1.00 stenose2 0.350 1.00 1.00 1 AVG 0.72 0.81 0.93 Table 4: Results returned by the oracle and precision and re- call values for 2D images - threshold = 0.350. allows them to be exploited in test environments. In addition to the numbers provided by this study, test oracle configurations showed that the number of true verdicts revealed that feature-based test oracles could be effectively exploited in test environments to evalu- ate applications that generate or process medical images of blood vessels in 3D and 2D. Regarding the t-test, the tests performed shown that by conven- tional criteria, test oracle versus random (p-value = 0.0109 ) has reached a difference considered to be statistically significant. 6 FINAL REMARKS AND FUTURE WORK This paper presents an empirical study to evaluate the ability and accuracy of test oracles to measure the similarity of synthetic im- ages of blood vessels in 2D and 3D models through the feature extraction. The strategy is demonstrated through a case study that aims to automatically categorize blood vessel images in 3D and 2D models through an approach called “feature-based test oracles” . As a tool to support the automation of feature-based test oracles, we use the software framework O-FIm/CO to automate the above mentioned process.', 'As a tool to support the automation of feature-based test oracles, we use the software framework O-FIm/CO to automate the above mentioned process. Based on the process applied when conducting the study, we observed that medical images in 3D model are complex and allow a more detailed analysis of the image. Furthermore, the develop- ment of feature extractors is less complex for 3D images. On the other hand, 2D images are limited, and the extractors development process is more complex. Thus, we can conclude that based on our empirical evidence, feature-based test oracles can be used as a promising strategy to automate validation activity of complex objects, specifically synthetic images of blood vessel in 3D and 2D models. Aiming at complementing the empirical evidence, we conducted the categorization of 3D and 2D images randomly and compared with the categorization performed by the test oracle. Therefore, the evidence has shown that, regarding the number of true verdicts, the oracle has a statistically significant difference in the categorization of the images 3D and 2D, compared to the random categorization. This demonstrates the effectiveness of the feature-based test oracle approach in categorizing synthetic images of blood vessels over random classifiers. ACKNOWLEDGMENTS This work is sponsored by FAPESP under G/N: 2015/11844-0 REFERENCES [1] E. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. I. Yoo. 2015. The oracle prob- lem in software testing: A survey. IEEE Transactions on Software Engineering (TSE) (2015), 507–525. [2] M. E. Delamaro, Fátima L. S. Nunes, and R. A. P. Oliveira. 2013. Using concepts of content-based image retrieval to implement graphical testing oracles. Software Testing, Verification and Reliability (STVR) (2013), 171–198. [3] W. E. Howden. 1978. Theoretical and empirical studies of program testing. IEEE Transactions on Software Engineering (TSE) (1978), 293–298. [4] R. A. P. Oliveira, M. E. Delamaro, and F. L. S. Nunes. 2009. O-FIm – Oracle for Im- ages. In Proceedings of the 23th Brazilian Symposium on Software Engineering (SBES) - XVI Tools Session of the SBES. 1–6. [5] R. A. P. Oliveira, U. Kanewala, and P. A. Nardi. 2015. Automated Test Oracles: State of the Art, Taxonomies, and Trends. Advances in Computers (2015), 113– 199. [6] R. A. P. Oliveira, A. M. Memon, V. N. Gil, F. L. S. Nunes, and M. E. Delamaro. 2014. An Extensible Framework to Implement Test Oracles for Non-Testable Programs. In Proceedings of the 26th International Conference on Software Engineering and Knowledge Engineering (SEKE). 199–204. [7] M. A. G. Valverde, M. M. Macedo, C. Mekkaoui, and M. P. Jackowski. 2013. Three- dimensional synthetic blood vessel generation using stochastic L-systems. In Proceedings of the Medical Imaging: Image Processing. 86691I–86691I–6.']","['Keywords Software Testting  Testt Oraclest Syithetc Vastcular Networsst Who is this briefin  or? Software eingiieeriing practtoierst who wait automate testt oraclest ii the procestst of validating 3D  aid 2D medical imangest. Where the fidiins come  rom? All  fidiingst  of  thist  briefing  were extracted  from  the  uste  of  the feature-basted  testt  oraclest approach to destcribe aid aialyze the  developmeit  of  automated testt oraclest for 3D aid 2D medical imangest. coiducted by JUNIOR, M. C et al.   What is iicluded ii this briefin? The  maii  fidiingst  oi  the  uste  of the  feature-basted  testt  oraclest approach. Evideice characteristtcst throungh a destcriptoi  of  ai  caste  sttudy, ideitfyiing  postitve  astpectst  aid limitatoist ii the approach. What is iot iicluded ii this  briefin? Iiformatoi  about  a  proposted approach to automate testt oraclest for 3D aid 2D medical imangest. To access other evideice  briefins oi sofware  einiieeriin: http:////www.lia.ufc.br//ccbstoft2017                                                                                                                         FEATURE-BASED TEST ORACLES TO CATEGORIZE IMAGES This  briefin  reports  scieitfc  evideice oi the use aid efcieicn of ai approach that automates test oracles to catenorize snithetc 3D aid 2D imanes blood vessels throunh the similaritn betweei features. FINDINGS The aim of thist paper wast to destcribe aid aialyze the efcieicy of ai approach that automatest testt oraclest to catengorize styithetc 3D aid 2D imangest blood  veststelst  throungh  the  stimilarity  betweei featurest. \uf0b7 Proposal: thist sttudy presteitst a sttratengy to stupport  automated  testting  actvitest  ii complex-output  stysttemst  ist  the  destingi basted oi testt oraclest. \uf0b7 Case studn: the sttratengy ist demoisttrated throungh  a  caste  sttudy  that  aimst  to automatcally catengorize styithetc imangest of  blood  veststelst  ii  3D  aid  2D  modelst from the stimilarity betweei featurest. \uf0b7 Methodolonn: iiitally,  we  ngeierated styithetc imangest  of  blood  veststelst  ii  3D modelst.  Next,  we  ideitfed  relevait featurest to extract from the imangest that may  be  stource  of  iiformatoi  for  the automated oracle.           The  imangest  are  catengorized  iito  four ngroupst, basted oi the stimilarity betweei the  iumber  of  featurest.  The  feature extractorst aid the stimilarity fuictoi are adapted to the coitext of the framewors O-FIm//CO,  which  implemeitst  the coiceptst destingied by feature-basted testt oraclest. \uf0b7 Exteisioi case studn: aimiing at exteidiing the  sttudy,  we  ngeierated  imangest  ii  2D modelst ustiing stcreesthotst of the 3D imangest tasei with the BioImange Suite Tool. After that, we created testt oraclest to aststestst the catengorizatoi of the imangest ii 3D aid 2D modelst, aimiing at evaluating the accuracy of  the  testt  oracle  ii  the  judngmeit  of stimilarity betweei the imangest. \uf0b7 Compare  method:  additoially,  we compared  the  catengorizatoi  performed by  the  testt  oracle  with  raidom alngorithmst,  demoisttrating  that  the  testt oracle  hast  a  sttatsttcally  stingiifcait differeice ii comparistoi with the raidom alngorithmst. From coiducted a caste sttudy, the followiing restultst were obtaiied \uf0b7 Rengardiing  the  catengorizatoi  of  3D imangest, the automated oracle presteited ai  averange  (AVG)  of  77%  for  precistioi, 100% for recall, aid 88% for stpecifcity. \uf0b7 The  automated  oracle  presteited  restultst with  hingh  accuracy  ii  mostt  3D  imangest. The AVG of 77% precistioi for 3D imangest meaist  that  the  oracle  hast  catengorized mostt  of  the  imangest  ast  postitve  ii  the expected catengoriest. \uf0b7 Rengardiing  the  catengorizatoi  of  2D imangest, the automated oracle presteited ai  AVG  of  72%  for  precistioi,  81%  for recall, aid 93% for stpecifcity. \uf0b7 The automated oracle ii 2D imangest had a low recall value. Thist may have occurred', 'ai  AVG  of  72%  for  precistioi,  81%  for recall, aid 93% for stpecifcity. \uf0b7 The automated oracle ii 2D imangest had a low recall value. Thist may have occurred becauste  of  the  thresthold  value  usted. Furthermore,  for  imangest  with  larnger iumberst  of  featurest,  the  automated oracle  presteited  low  iumberst  of precistioi aid recall.  \uf0b7 Rengardiing the raidom claststifer, the testtst performed  sthowi  that  by  coiveitoial criteria,  testt  oracle  verstust  raidom  (p- value = 0.0109) hast reached a differeice coistidered to be sttatsttcally stingiifcait. Throungh of the restultst obtaiied, we ideitfed  stome poiitst that cai be revistited ii the iext  verstioi of the sttudy, stuch ast: \uf0b7 Ast future worsst, we plai to iicreaste the iumber  of  imangest  ii  our  datastet  by varyiing  the  iumber  of  catengoriest  aid iiclude  imangest  with  larnger  iumberst  of featurest.  \uf0b7 Rengardiing extractorst featurest, we plai to ideitfy  aid  develop  more  feature extractorst, offeriing a larnger datastet ast the stource of iiformatoi for the testt oracle. \uf0b7 The sttudy offerst steveral ngapst that cai be explored with the purposte of performiing empirical  sttudiest  that  validate  styithetc imangest of blood veststelst. Basted oi the procestst applied whei coiducting the sttudy,  we  obsterved  that  medical  imangest  ii  3D model  are  complex  aid  allow  a  more  detailed aialystist of the imange. The  developmeit  of  feature  extractorst  ist  lestst complex  for  3D  imangest.  Oi  the  other  haid,  2D imangest  are  limited,  aid  the  extractorst developmeit procestst ist more complex. Feature-basted testt oraclest approach  cai be usted ast  a  promistiing  sttratengy  to  automate  validatoi actvity  of  complex  objectst,  stpecifcally styithetc imangest of blood veststel ii 3D aid 2D modelst.   ORIGINAL RESEARCH REFERENCE JUNIOR, M. C, OLIVEIRA, R. A. P. ; VALVERDE, M. A. G. ; JACKOWSKI, M. P. ; NUNES, F. L. S. ; DELAMARO, M. E.; Feature-Basted Testt Oraclest to Catengorize Syithetc 3D aid 2D Imangest of Blood Veststelst. Ii Proceediingst of the 2id Braziliai Sympostium oi Systtematc aid Automated Software Testting (SAST) @ VIII Braziliai Coifereice oi Software: Theory aid Practce (CBSoft 2017), Fortaleza, 2017.']","**Title: Enhancing Automated Testing with Feature-Based Test Oracles for 3D and 2D Blood Vessel Images**

**Introduction:**
This briefing presents key findings from a study on the effectiveness of feature-based test oracles in automating the categorization of synthetic images of blood vessels in both 3D and 2D formats. The goal is to provide insights for software engineering practitioners interested in improving automated testing processes for complex-output systems, particularly in medical imaging.

**Main Findings:**
The study utilized the O-FIm/CO framework to develop feature-based test oracles aimed at categorizing synthetic blood vessel images. Key findings include:

1. **Accuracy of Feature-Based Oracles:** The test oracles demonstrated high accuracy in categorizing 3D images, achieving an average precision of 77%, recall of 100%, and specificity of 88%. In contrast, the categorization of 2D images yielded slightly lower performance, with average precision at 72%, recall at 81%, and specificity at 93%.

2. **Comparison with Random Classifiers:** The study established that feature-based test oracles significantly outperformed random classifiers in categorizing both 3D and 2D images. Statistical analysis confirmed this, with the null hypothesis being rejected, indicating that the feature-based approach provides a more reliable and effective means of categorization.

3. **Threshold Impact on Performance:** The choice of threshold for similarity measurement influenced the accuracy of the categorization. A threshold value of 0.35 was identified as optimal for balancing precision and recall across both image types, highlighting the need for careful threshold selection in automated testing scenarios.

4. **Feature Extraction Complexity:** The development of feature extractors was found to be less complex for 3D images, allowing for a more detailed analysis of structures, while 2D images posed additional challenges in feature extraction.

5. **Practical Implications:** The findings suggest that implementing feature-based test oracles can significantly enhance the validation processes for medical imaging systems, reducing manual effort and improving testing efficiency.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, particularly those involved in automated testing, medical imaging software development, and researchers interested in innovative testing methodologies.

**Where the findings come from?**
All findings presented in this briefing are derived from the empirical study conducted by Misael C. Júnior et al., which evaluated the effectiveness of feature-based test oracles in categorizing synthetic images of blood vessels.

**What is included in this briefing?**
The briefing includes an overview of the study's methodology, key findings regarding the accuracy of feature-based test oracles, comparisons with random classifiers, and insights into the implications for automated testing in complex-output systems.

**Reference:**
Misael C. Júnior, Rafael A. P. Oliveira, Miguel A. G. Valverde, Marcel P. Jackowski, Fátima L. S. Nunes, Márcio E. Delamaro. Feature-Based Test Oracles to Categorize Synthetic 3D and 2D Images of Blood Vessels. SAST, September 18–19, 2017, Fortaleza, Brazil. https://doi.org/10.1145/3128473.3128484"
"['Better Similarity Coefficients to Identify Refactoring Opportunities Arthur F. Pinto Universidade Federal de Lavras Departamento de Ciência da Computação UFLA, Brasil fparthur@posgrad.ufla.br Ricardo Terra Universidade Federal de Lavras Departamento de Ciência da Computação UFLA, Brasil terra@dcc.ufla.br ABSTRACT Similarity coefficients are used by several techniques to identify refactoring opportunities. As an example, it is expected that a method is located in a class that is structurally similar to it. Howe- ver, the existing coefficients in Literature have not been designed for the structural analysis of software systems, which may not gua- rantee satisfactory accuracy. This paper, therefore, proposes new coefficients—based on genetic algorithms over a training set of ten systems—to improve the accuracy of the identification of Move Class, Move Method, and Extract Method refactoring opportunities. We conducted an empirical study comparing these proposed coeffi- cients with other 18 coefficients in other 101 systems. The results indicate, in relation to the best analyzed coefficient, an improve- ment of 10.57% for the identification of Move Method refactoring opportunities, 3.17% for Move Class, and 0.30% for Extract Method. Moreover, we implemented a tool that relies on the proposed coef- ficients to recommend refactoring opportunities. CCS CONCEPTS • Software and its engineering → Empirical software valida- tion; Maintaining software ; Software evolution; KEYWORDS Software Architecture, Structural Similarity, Code Refactoring, Move Class, Move Method, Extract Method ACM Reference Format: Arthur F. Pinto and Ricardo Terra. 2017. Better Similarity Coefficients to Identify Refactoring Opportunities. InProceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017, 10 pages. https://doi.org/10.1145/3132498.3132511 1 INTRODUÇÃO Durante o desenvolvimento de software, vários problemas estão sujeitos a ocorrer na arquitetura do software. Code Smells (também chamado de Bad Smells ) são definidos como quaisquer sintomas no código que possam indicar um desses problemas [5]. Muitas vezes, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5325-0/17/09. . . $15.00 https://doi.org/10.1145/3132498.3132511 tais Code Smells implicam no estabelecimento de dependências des- necessárias. No entanto, como garantir o projeto arquitetural é de suma importância para manutenibilidade, reusabilidade, escalabili- dade e portabilidade de sistemas de software [9], espera-se que os elementos de código de um projeto de software estejam localizados em entidades estruturalmente similares. Deve-se destacar que este artigo leva em consideração depen- dências estruturais entre os elementos de código de um sistema para o cálculo de similaridade entre pacotes, classes, métodos e blocos. Dependências estruturais ocorrem quando uma unidade de compilação depende de outra em tempo de compilação ou de vinculação [4]. Dentre os diversos tipos de dependência existentes em uma estrutura de código orientada a objetos, pode-se citar como mais relevantes: acesso de métodos e atributos (access), declaração de variáveis (declare), criação de objetos (create), extensão de clas- ses (extend), implementação de interfaces (implement), ativação de exceções (throw) e uso de anotações (useannotation).', 'de variáveis (declare), criação de objetos (create), extensão de clas- ses (extend), implementação de interfaces (implement), ativação de exceções (throw) e uso de anotações (useannotation). Uma estrutura de código com bons índices de similaridade influ- encia diretamente na qualidade de software, uma vez que evidencia o alto grau de coesão e baixo acoplamento, afetando positivamente na arquitetura e nas características de manutenção do software. A fim de assegurar a similaridade estrutural entre as entidades de código e evitar a ocorrência de Code Smells, deve-se verificar a similaridade das dependências entre seus elementos (seja no nível de pacote, classe, método ou bloco) e, quando necessário, conduzir refatorações. Isso implica no deslocamento de métodos e classes, e na extração de um bloco de código de um método (gerando um novo método), por intermédio de refatorações como Move Class, Move Method e Extract Method [5]. A Figura 1 ilustra um exemplo de um sistema que implementa uma arquitetura MVC (Model-View-Controller), composta por três camadas Modelo , Vis ˜ao e Controle . É possível perceber que C2 está mal localizada na camada de controle (Controle ), pois depende de elementos gráficos enquanto demais classes da camada possuem dependências a elementos de manipulação de requisição e respostas (e.g., HttpRequest e HttpResponse). Portanto, é possível sugerir mover tal classe (Move Class ) para uma camada estruturalmente similar que, nesse cenário, seria a camada de visão (Vis ˜ao). Tal simi- laridade estrutural ocorre devido ao fato deC2 e as classes da camada de visão (V1, . . . ,Vn) possuírem dependências a elementos gráficos em comum (e.g., JPanel e JLabel). Dessa forma, a refatoração não somente garantiria uma arquitetura com entidades devidamente localizadas, como passaria a respeitar uma arquitetura MVC. Diversos coeficientes foram propostos para o cálculo de similari- dade entre entidades de código. Entretanto, muitas vezes, a utiliza- ção de tais coeficientes não garante de fato uma precisão satisfatória.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Arthur F. Pinto and Ricardo Terra Figura 1: Exemplo de Refatoração Move Class Ademais, os principais coeficientes vigentes na literatura não foram projetados para a análise estrutural de um sistema de software. Por exemplo, o coeficiente Jaccard, um dos mais utilizados em Enge- nharia de Software, foi inicialmente concebido para comparar a similaridade entre espécies florais em diferentes distritos [7]. Diante disso, este artigo tem como principal objetivo propor novos coeficientes de similaridade para uma identificação mais pre- cisa de oportunidades de refatoração Move Class, Move Method e Extract Method. Isso possibilita (i) localizar, com maior precisão, entidades indevidamente posicionadas sobre a arquitetura de um sistema e (ii) alavancar a precisão de ferramentas de identificação de oportunidades de refatoração baseadas em similaridade estrutu- ral. É importante mencionar, entretanto que os novos coeficientes são específicos a essas três refatorações, uma vez que são as mais largamente utilizadas pelos desenvolvedores [13]. Primeiramente, é analisada a precisão de 18 coeficientes de simi- laridade em dez sistemas (training set) do Qualitas.class Corpus [16]. Em segundo lugar, o coeficiente Simple Matching é adaptado por meio de algoritmos genéticos de forma a gerar três novos coefi- cientes (PT MC, PT MMe PT EM) que possuam maior precisão na identificação de, respectivamente, oportunidades de refatoração Move Class,Move Method e Extract Method. Em terceiro lugar, os coeficientes propostos são comparados com os existentes em ou- tros 101 sistemas. Os resultados indicam, em relação ao melhor coeficiente analisado, uma melhoria de 10,57% para identificação de oportunidadesMove Method, 3,17% para Move Class e 0,30% para Extract Method. Por fim, é implementada uma ferramenta que identifica oportunidades de refatoração baseadas nos coeficientes propostos. O restante deste artigo está organizado como descrito a seguir. A Seção 2 introduz conceitos fundamentais ao estudo. A Seção 3 des- creve a metodologia utilizada. A Seção 4 apresenta uma análise dos coeficientes existentes, tendo como objetivo a seleção do coeficiente a ser adaptado. A Seção 5 propõe três novos coeficientes para iden- tificação de oportunidades de refatoração. A Seção 6 apresenta uma avaliação dos coeficientes propostos em 101 sistemas. A Seção 7 descreve a implementação de uma ferramenta para identificação de oportunidades de refatoração baseadas nos coeficientes propostos. A Seção 8 discute os trabalhos relacionados. Por fim, a Seção 9 apresenta as considerações finais, bem como trabalhos futuros. 2 BACKGROUND No intuito de prover o conhecimento necessário para a concepção e compreensão deste artigo, são apresentados os conceitos funda- mentais para o mesmo. A Seção 2.1 diz respeito ao processo de refatoração, o qual envolve métodos para reestruturação do código- fonte. A Seção 2.2 trata a respeito de similaridade estrutural, além de apresentar os principais coeficientes vigentes na literatura. Por fim, a Seção 2.3 introduz algoritmos de otimização, com foco em algoritmos genéticos, além de apresentar um exemplo ilustrativo de otimização. 2.1 Refatoração Refatoração de código é o processo de mudança de um sistema de software de forma que preserve o comportamento externo do código e aperfeiçoe sua estrutura interna [5]. Por meio da refatora- ção, torna-se possível tratar a ocorrência de diferentes Code Smells. Embora certos processos de refatorações levam em consideração outros aspectos (e.g., semântica), este estudo foca exclusivamente em Code Smells que se manifestem estruturalmente no código fonte. Diversas técnicas podem ser utilizadas para refatoração de código. Como este artigo foca sua análise exclusivamente em classes, méto- dos e blocos de um sistema, serão utilizadas as refatorações Move Class, Move Method e Extract Method a fim de tratar a ocorrência', 'Como este artigo foca sua análise exclusivamente em classes, méto- dos e blocos de um sistema, serão utilizadas as refatorações Move Class, Move Method e Extract Method a fim de tratar a ocorrência de Code Smells. Enquanto a aplicação de Move Class e Move Method consiste, respectivamente, na simples movimentação de uma classe para outro pacote e na movimentação de um método para outra classe, Extract Method é realizado por meio da extração de um bloco de código para um novo método que será gerado, substituindo o trecho extraído com uma chamada para o referente novo método. Embora a aplicação de Move Class para o reposicionamento de uma classe em seu devido pacote não trate de nenhum Code Smell específico, é fundamental para evitar a ocorrência de qualquerCode Smell, visto que a indevida localização de uma classe contribuirá para que sua estrutura interna também seja inadequada, ou seja, acarretará em métodos e blocos inapropriadamente posicionados. Já o Move Method possibilita tratar os seguintes Code Smells : Large Class, em que o reposicionamento dos métodos indevidos irá reduzir o tamanho da referente classe; Divergent Change e Shotgun Surgery, em que será possível posicionar os métodos que necessitem de alterações em uma classe específica que não ocasione na neces- sidade de alterações; Feature Envy, em que os métodos que fazem uso excessivo de propriedades internas de outra classe poderão ser movidos para a outra classe em questão; e Refused Bequest, em que os métodos da superclasse, que não possuírem propriedades em comum com todas as subclasses, poderão ser movidos para as subclasses que os utilizam. A refatoração Extract Method, por sua vez, proporciona meios de se tratar de Long Method, onde a extração dos blocos de código em novos métodos irá reduzir o tamanho do método em questão. Ademais, ainda que Extract Method trate diretamente apenas de Long Method, indiretamente pode ser utilizado para o tratamento de outros Code Smells (e.g., Large Class , Divergent Change, Shotgun Surgery, Feature Envy e Refused Bequest ), visto que após a extração do bloco de código em um novo método, esse método pode ser movido para outra classe por meio do Move Method. Dessa forma, é possível tratar dos referentes Code Smells em situações em que ocorrem em apenas determinados blocos de código.', 'Better Similarity Coefficients to Identify Refactoring Opportunities SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil 2.2 Similaridade Estrutural A fim de identificar a ocorrência de Code Smells e, consequente- mente, oportunidades de refatoração de código, é fundamental a análise da similaridade estrutural das entidades de código presentes em um projeto de software. Similaridade, no contexto de arquitetura de software, refere-se à relação entre as propriedades compartilha- das entre duas ou mais entidades presentes na estrutura do código. Para o cálculo dos índices de similaridade, diversos coeficientes foram propostos. A Tabela 1 representa os principais coeficientes de similaridade propostos na literatura [17]. Tabela 1: Coeficientes de similaridade Co eficiente Definição Escala Bar oni-Urbani and Buser [a + (ad) 1 2 ]/[a + b + c + (ad) 1 2 ] [0-1*] Dot-pr oduct a/(b + c + 2a) [0-1*] Hamann [(a + d) −(b + c)]/[(a + d) + (b + c)] [ -1-1*] Jaccar d a/(a + b + c) [0-1*] Kulczynski 1 2 [a/(a + b) + a/(a + c)] [0-1*] Ochiai a/[(a + b) (a + c)] 1 2 [0-1*] P hi (ad −bc)/[(a + b) (a + c) (b + d) (c + d)] 1 2 [ -1-1*] PSC a2/[(b + a) (c + a)] [0-1*] Relativ e Matching [a + (ad) 1 2 ]/[a + b + c + d + (ad) 1 2 ] [0-1*] Rogers and Tanimoto (a + d)/[a + 2(b + c) + d] [0-1*] Russell and Rao a/(a + b + c + d) [0-1*] Simple Matching (a + d)/(a + b + c + d) [0-1*] Sokal and Sneath 2(a + d)/[2(a + d) + b + c] [0-1*] Sokal and Sneath 2 a/[a + 2(b + c)] [0-1*] Sokal and Sneath 4 1 4 [a/(a + b) + a/(a + c) + d/(b + d) + d/(c + d)] [0-1*] Sokal binary distance [(b + c)/(a + b + c + d)] 1 2 [0*-1] Sor enson 2a/(2a + b + c) [0-1*] Y ule (ad −bc)/(ad + bc) [0-1*] O símbolo * indica a similaridade máxima Para a compreensão de cada coeficiente de similaridade, con- sidere duas entidades de código A e B. Levando em consideração que este artigo analisa dependências estruturais entre entidades de código, tem-se as seguintes variáveis: a = quantidade de dependências em ambas entidades, b = quantidade de dependências exclusivas da entidade A, c = quantidade de dependências exclusivas da entidade B, e d = quantidade do restante do universo de dependências considerado. Com o objetivo de ilustrar o cálculo de similaridade estru- tural entre duas entidades de código, o coeficiente Jaccard e Simple Matching foram aplicados em dois métodos do sistema MyAppointments, um sistema de controle e gerenciamento de compromissos [ 9]. Dessa forma, foram selecionados os métodos loadAppointments, responsável por carregar os compromissos do sistema, e getAppointmentRowAsDate, responsável por retornar a data do compromisso de uma linha específica do conjunto de dados. Ambos os métodos estão presentes na mesma classe de controle ( AgendaController). O Código 1 apresenta a implementação de ambos os métodos loadAppointments e getAppointmentRowAsDate. Com base na análise de ambos os métodos e as dependências estruturais presentes em suas estruturas, tem-se que: •a = 2, visto que ambos os métodos acessam métodos de AgendaView e DateUtils (destacados pela cor vermelha); •b = 4, visto que o método loadAppointments possui as seguintes dependências exclusivas: o lançamento de uma ex- ceção do tipo Exception, a declaração de List, a declaração do tipo Appointment e o acesso aos métodos de AgendaDAO (destacadas pela cor azul); 1 public class AgendaController 2 ... 3 4 public void loadAppointments() throws Exception { 5 List<Appointment> apps = agendaDAO.getAppointments 6 (DateUtils.getCurrentDay(), 7 DateUtils.getCurrentMonth(), 8 DateUtils.getCurrentYear()); 9 10 int i = 0 ; 11 for(Appointment app : apps) { 12 agendaView.insertAppointRow 13 (i, DateUtils.toString( 14 app.getDate(), 15 DateUtils.HOUR_FMT), 16 app.getTitle()); 17 i++ ; 18 } 19 } 20 21 private Date getAppointmentRowAsDate(int row) { 22 String[] appHour = 23 agendaView.getAppointmentRow(row)[0].split("":""); 24 return DateUtils.newDate 25 (DateUtils.getCurrentDay(), 26 DateUtils.getCurrentMonth(),', '22 String[] appHour = 23 agendaView.getAppointmentRow(row)[0].split("":""); 24 return DateUtils.newDate 25 (DateUtils.getCurrentDay(), 26 DateUtils.getCurrentMonth(), 27 DateUtils.getCurrentYear(), 28 Integer.parseInt(appHour[0]), 29 Integer.parseInt(appHour[1])); 30 } 31 } Código 1: Fragmento da classe AgendaController •c = 1, visto que a única dependência exclusiva do método getAppointmentRowAsDate é a declaração do tipo Date como retorno (destacada pela cor verde); •d = 32, visto que ao considerar o sistema como um todo, são utilizados outros 32 diferentes tipos de dependências distintas. Deve-se ressaltar que dependências a tipos primitivos (e.g., int, char, byte, etc.), a classes Wrappers (e.g., Integer, Long, Character, etc.) e ao tipo String são desconsideradas durante a análise, uma vez que praticamente todas as classes estabelecem dependências com esses tipos, não contribuindo para o cálculo de similaridade. Portanto, ao se aplicar, como exemplo, o coeficiente de Jaccard e Simple Matching, encontra-se os seguintes índices de similaridade: Jaccard = a a + b + c = 2 2 + 4 + 1 = 0,28 (1) Simple Matching = a + d a + b + c + d = 2 + 32 2 + 4 + 1 + 32 = 0,87 (2) De tal forma, é importante mencionar que somente o valor bruto não indica o nível de similaridade com exatidão, pois devem-se observar e comparar os demais valores de similaridade do sistema por completo. Por exemplo, 0,28 (Jaccard) pode ser considerado um alto valor de similaridade se a média de similaridade do sistema for 0,12. Da mesma forma, 0,87 (Simple Matching) pode ser considerado um baixo valor. 2.3 Algoritmos de Otimização Otimização diz respeito ao processo de buscar e comparar soluções para determinado problema, maximizando e/ou minimizando os valores das variáveis que compõem a função objetivo até que seja encontrada a melhor solução possível [1]. Entretanto, em muitos casos, um problema não possui solução ótima, o que resulta na busca por soluções que atendam o objetivo desejado de forma satisfatória.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Arthur F. Pinto and Ricardo Terra Esse conceito, por sua vez, pode ser aplicado para adaptar e melhorar os coeficientes de similaridade abordados na seção anterior. Para a busca da solução de problemas de otimização, diversas abordagens podem ser aplicadas, envolvendo diferentes tipos de algoritmos. Dentre as diversas abordagens, a aplicação de um algo- ritmo genético simples mostrou ser capaz de encontrar soluções satisfatórias de forma eficaz. Portanto, tornou-se a abordagem es- colhida para aplicação neste artigo. 2.3.1 Algoritmos Genéticos. Um algoritmo genético define um conjunto de candidatos à solução proposta e, a cada iteração (cada geração do algoritmo genético), seleciona e combina os candidatos mais aptos, podendo sofrer leves alterações. Assim, ao fim de toda execução, o conjunto solução será composto por candidatos com relativo grande potencial de otimizar a função objetivo [14]. Considerando o problema de otimizar os coeficientes de simi- laridade, o algoritmo define pesos a cada variável da fórmula do coeficiente escolhido e a cada iteração busca adaptar cada peso vi- sando otimizar o resultado da função objetivo, selecionando por fim o conjunto dos melhores resultados. Por exemplo, considerando o coeficiente Jaccard (apresentado na Tabela 1), e os pesosPa′, Pa′′, Pb e Pc correspondentes, respectivamente, às variáveisa do numerador e a, b e c do denominador. Assim, tem-se como resultado: Coeficiente Resultante = (Pa′∗a′) (Pa′′∗a′′) + (Pb ∗b) + (Pc ∗c) (3) Durante a execução de um algoritmo genético, são definidos parâmetros como função objetivo (ou função de fitness), população inicial, número de gerações, operador de seleção, operador de cru- zamento e operador de mutação. A função objetivo representa o dado ou função que se pretende otimizar. A população inicial esti- pula o número inicial de possíveis candidatos ao pesos pretendidos em busca de otimizar a função objetivo. Número de gerações diz respeito a quantidade de vezes o algoritmo genético irá repetir, i.e., iterar adaptando os pesos almejados. O operador de seleção irá es- colher candidatos mais aptos para a realização do cruzamento entre eles, gerando assim um ou mais candidatos que possam apresentar maior aptidão para o conjunto solução. Por fim, o(s) candidato(s) gerado(s) pode(m) sofrer uma pequena mutação, i.e., uma leve alte- ração em seu valor, o que previne a estagnação do mesmo, além de possibilitar que se chegue em qualquer ponto do espaço de busca. Dentre os diferentes operadores de seleção, cruzamento e mu- tação, este artigo utiliza respectivamente, Torneio Binário, Cruza- mento Binário Simulado e Mutação Polinomial. O método Torneio Binário seleciona, dentre todos os possíveis candidatos gerados até o momento, dois dos indivíduos que apresentam maior resultado em relação à função objetivo para que os mesmos sejam cruzados. O Cruzamento Binário Simulado ocorre por meio do cruzamento de dois indivíduos, combinando suas representações binárias, de forma a gerar dois novos indivíduos. Tal combinação considera uma probabilidade definida, analisando se cada índice binário deve ser combinado ou não. Por fim, a Mutação Polinomial também consi- dera uma probabilidade definida a fim de alterar um ou mais índices binários dos indivíduos resultantes do cruzamento. Em ambos os operadores de cruzamento e mutação, deve ser estabelecido um índice de distribuição com o propósito de avaliar a diversidade das soluções selecionadas no espaço de busca, o que garante a seleção de indivíduos mais heterogêneos. 2.3.2 Exemplo Ilustrativo. Para melhor compreensão da abor- dagem de algoritmos de otimização e dos conceitos de algoritmos genéticos, esta seção apresenta a aplicação de um algoritmo gené- tico visando otimizar o coeficiente de similaridade Jaccard em um pequeno exemplo de refatoração Move Class. De tal modo, suponha um sistema com dois pacotes pkд1 e', 'tico visando otimizar o coeficiente de similaridade Jaccard em um pequeno exemplo de refatoração Move Class. De tal modo, suponha um sistema com dois pacotes pkд1 e pkд2 tendo seu conjunto de classes e suas dependências, respec- tivamente, como pkд1 = {A={X,Y,W,Z},B={X,Y,Z,L},C={X,Y,W,K}} e pkд2 = {D={X,W,R,T},E={R,T,Z}, F={X,Z,R,T,M}}, conforme Figura 2. Figura 2: Exemplo Ilustrativo Suponha que sabe-se que a arquitetura atual do sistema é a ideal e pretende-se utilizar um coeficiente de similaridade em que não haja sugestões de refatoração Move Class, ou seja, mantenha a arquite- tura atual e garanta bons índices de similaridade entre as classes de um mesmo pacote. Para isso, pretende-se adaptar um coeficiente de forma a maximizar a similaridade desim(A, B), sim(A, C), sim(B, C), sim(D, E), sim(D, F ) e sim(E, F ). Ao mesmo tempo, espera-se mini- mizar a similaridade de sim(A, D), sim(A, E), sim(A, F ), sim(B, D), sim(B, E), sim(B, F ), sim(C, D), sim(C, E) e sim(C, F ). Entretanto, somente o valor bruto não indica o nível de similaridade com exati- dão, pois deve-se observar e comparar os demais valores de simila- ridade do sistema por completo. Dessa forma, pretende-se obter a maior diferença possível entre a média aritmética das similaridades que deseja-se maximizar e da que deseja-se minimizar. Levando em consideração o coeficiente Jaccard como exemplo, decide-se aplicar o algoritmo genético definindo pesos para cada va- riável do coeficiente, conforme previamente descrito na Equação 3. Dessa forma, foi utilizada as configurações apresentadas na Ta- bela 2, a qual foi obtida após uma série de tentativas que buscavam melhorar os coeficientes, levando em consideração a disposição dos recursos computacionais e o tempo de execução. Tabela 2: Configuração do Algoritmo Genético Função objetivo: Quantidade de similaridades otimizadas Operador de cruza- mento: Cruzamento Binário Simulado Tamanho da po- pulação: 1.200 Probabilidade de muta- ção: 0,6 Representação da população: {x ∈R |−2 ≤x ≤2} Probabilidade de cruza- mento: 0,9 Número de gera- ções: 150 Índice de distribuição de mutação: 20,0 Operador de sele- ção: Torneio Binário Índice de distribuição de cruzamento: 20,0 Operador de mu- tação: Mutação Polinomial Nesse cenário, foi encontrado como possível conjunto solução, os pesos 6, 07 (Pa′), 0, 01 (Pa′′), 9, 1 (Pb ) e 9, 1 (Pc ), respectivamente às variáveis a do numerador e a, b e c do denominador. O comparativo das similaridades decorrentes do coeficiente Jaccard e o coeficiente resultante é apresentado na Tabela 3.', 'Better Similarity Coefficients to Identify Refactoring Opportunities SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Tabela 3: Comparativo de similaridade entre Jaccard e Coe- ficiente Resultante Maximizar Objetiv o Jaccar d Similaridade Co eficiente Resultante Similaridade sim( A,B) = 3 3 + 1 + 1 = 0,6 6,07∗3 0,01∗3 + 9,1∗1 + 9,1∗1 = 0,9989 sim( A,C) = 3 3 + 1 + 1 = 0,6 6,07∗3 0,01∗3 + 9,1∗1 + 9,1∗1 = 0,9989 sim(B, C) = 2 2 + 2 + 2 = 0,3333 6,07∗2 0,01∗2 + 9,1∗2 + 9,1∗2 = 0,3333 sim(D ,E) = 2 2 + 2 + 1 = 0,4 6,07∗2 0,01∗2 + 9,1∗2 + 9,1∗1 = 0,4444 sim(D ,F) = 3 3 + 1 + 2 = 0,5 6,07∗3 0,01∗3 + 9,1∗1 + 9,1∗2 = 0,6663 sim(E,F) = 3 3 + 0 + 2 = 0,6 6,07∗3 0,01∗3 + 9,1∗0 + 9,1∗2 = 0,9939 Mé dia 0,5055 Mé dia 0,7393 Minimizar Objetiv o Jaccar d Similaridade Co eficiente Resultante Similaridade sim( A,D) = 2 2 + 2 + 2 = 0,3333 6,07∗2 0,01∗2 + 9,1∗2 + 9,1∗2 = 0,3333 sim( A,E) = 1 1 + 3 + 2 = 0,1667 6,07∗1 0,01∗1 + 9,1∗3 + 9,1∗2 = 0,1333 sim( A,F) = 2 2 + 2 + 3 = 0,2857 6,07∗2 0,01∗2 + 9,1∗2 + 9,1∗3 = 0,2667 sim(B,D ) = 2 2 + 2 + 2 = 0,3333 6,07∗2 0,01∗2 + 9,1∗2 + 9,1∗2 = 0,3333 sim(B,E) = 0 0 + 4 + 3 = 0,0 6,07∗0 0,01∗0 + 9,1∗4 + 9,1∗3 = 0,0 sim(B,F) = 1 1 + 3 + 4 = 0,125 6,07∗1 0,01∗1 + 9,1∗3 + 9,1∗4 = 0,0953 sim( C,D) = 1 1 + 3 + 3 = 0,1429 6,07∗1 0,01∗1 + 9,1∗3 + 9,1∗3 = 0,1111 sim( C,E) = 1 1 + 3 + 2 = 0,1667 6,07∗1 0,01∗1 + 9,1∗3 + 9,1∗2 = 0,1334 sim( C,F) = 2 2 + 2 + 3 = 0,2857 6,07∗2 0,01∗2 + 9,1∗2 + 9,1∗3 = 0,2667 Mé dia 0,2044 Mé dia 0,1859 É possível observar que o coeficiente resultante da otimização mostrou ser muito mais eficiente que o coeficiente Jaccard, visto que obteve uma diferença entre a média das similaridades que deve- riam ser maximizadas e a média das similaridades que deveriam ser minimizadas de 0, 55, em contrapartida ao Jaccard que obteve uma diferença de apenas 0, 30. Portanto, o coeficiente resultante demons- trou ser o mais adequado e preciso para a análise de similaridade no exemplo apresentado. 3 METODOLOGIA Tendo como objetivo a criação de três novos coeficientes de simila- ridade que sejam mais eficientes na identificação, respectivamente, de oportunidades de refatoração Move Class, Move Method e Extract Method, este artigo adota uma metodologia baseada na seleção de um coeficiente para ser adaptado por meio da análise dos princi- pais coeficientes de similaridade existentes (Seção 4), na proposta dos três novos coeficientes após a adaptação do respectivo coefi- ciente selecionado (Seção 5) e na avaliação dos mesmos (Seção 6). Por fim, os coeficientes propostos são aplicados por meio de uma implementação de um plug-in para o IDE Eclipse (Seção 7). Para isso, o cálculo de similaridade, utilizado em cada etapa da metodologia, é realizado comparando determinada classe a de- terminado pacote, bem como comparando determinado método a determinada classe, além de analisar a similaridade resultante de uma determinada classe após a extração de determinado bloco para geração de um novo método. Tal processo é detalhado a seguir. 3.1 Cálculo de Similaridade Classe-Pacote: A similaridade de uma classe com um pacote é dada por meio da média aritmética da similaridade entre a respec- tiva classe e as demais classes presentes no pacote. Isso ocorre devido ao fato de que caso fosse calculada apenas a similaridade da classe com o pacote de forma geral, o conjunto de dependências do pacote poderia resultar em uma elevada similaridade ainda que as classes não fossem similares, apresentando baixos índices. Consequentemente, é possível sugerir refatorações Move Class para pacotes que possuam de fato classes similares. Método-Classe: A mesma abordagem utilizada para identificação de oportunidades de refatoração Move Class é aplicada para a análise de similaridade entre métodos e classes, em que é considerada a média da similaridade entre determinado método e os demais métodos da classe. Bloco-Método: Inicialmente é calculada a similaridade de cada método de uma classe com os demais métodos da mesma classe,', 'os demais métodos da classe. Bloco-Método: Inicialmente é calculada a similaridade de cada método de uma classe com os demais métodos da mesma classe, obtendo assim, as similaridades internas da classe. Em seguida, para cada método serão analisadas todas as possibilidades de extração de seus blocos a fim de gerar um novo método (Extract Method), ou seja, para cada bloco extraído, serão recalculadas as similaridades da classe após extração, incluindo o novo método gerado. Por fim, as médias aritméticas de ambos os conjuntos de valores são comparadas a fim de verificar se deve ou não ser sugerida a refatoração Extract Method. Dessa forma, os coeficientes são analisados e avaliados utilizando os sistemas do Qualitas.class Corpus [16], base de dados contendo 111 sistemas orientados a objetos, mais de 18 milhões de LOC (Lines Of Code ), 200 mil classes compiladas e 1,5 milhão de métodos compilados. Tendo como base o fato de que o Qualitas.class Corpus é composto por sistemas mais maduros e estáveis, assume-se que a estrutura atual dos sistemas possua o mais elevado índice de similaridade em comparação às possíveis refatorações de código. Em suma, pretende-se que a estrutura seja mantida e nenhuma refatoração seja realizada. Assim, torna-se possível avaliar a precisão de cada coeficiente de similaridade tendo em vista cada sistema analisado. Regras de Execução: Para evitar a ocorrência de falsos positivos nas recomendações de refatoração resultantes, após uma série de testes e execuções experimentais, foram definidas nove regras de execução para a implementação da solução proposta: (1) A entidade sob análise é desconsiderada enquanto o sistema busca por oportunidades de refatoração. Quando é calculada a similaridade entre uma classe A e seu respectivo pacote Pkд, o sistema considera Pkд como sendo Pkд - {A}. Nesse caso, entidades localizadas isoladamente são totalmente descartadas; (2) Pacotes e classes de teste não são consideradas, visto que a maioria dos sistemas organizam suas classes de teste em um único pacote. Logo, esse pacote contém classes relacionadas a diferentes partes do sistema, isto é, não estão estruturalmente relacionados. Para tal, é utilizada uma abordagem que desconsidera pacotes e classes que contenham o texto “ test” (caixa alta ou caixa baixa) em qualquer parte de seu nome. E, embora nem todo pacote ou classe de teste contenha o texto “test” e nem todo pacote ou classe que o contenha é de fato um pacote ou classe de teste, o ganho em precisão é maior do que a perda quando pacotes ou classes de teste forem erroneamente detectados;', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Arthur F. Pinto and Ricardo Terra (3) Dependências triviais são ignoradas. São filtradas depen- dências como as estabelecidas com tipos primitivos e de wrappers(e.g., int e java.lang.Integer), dependências de java.lang.String e Java.lang.Object, etc. Uma vez que a grande maioria dos elementos de código estabelecem dependências com esses tipos, eles não contribuem de fato para o cálculo de similaridade; (4) Não são analisadas entidades de classe, método ou bloco que estabelecem dependências com menos de três tipos. Embora possam existir entidades com poucas dependências que deveriam ser refatoradas, essas entidades contém pouca informação para o cálculo de similaridade ou para fazer qualquer inferência com base nas suas dependências estruturais. Entretanto, deve-se ressaltar que, embora essas entidades não são analisadas se devem ser refato- radas, elas ainda são consideradas como possível destino de refatoração, caso possuam ao menos alguma dependência; (5) Não são analisadas entidades que não estejam co-localizadas com pelo menos duas outras entidades analisáveis. Por exemplo, um pacote que possua apenas duas classes não provê informações estruturais suficientes para recomendar ou não a movimentação de suas classes. Novamente, esse critério desconsidera apenas a análise de tais entidades, mas ainda podem ser consideradas como destino de refatoração; (6) Não será possível extrair o primeiro bloco de um método. A extração do primeiro bloco de um método não ocasionará uma mudança de similaridade. Uma vez que a extração de um bloco resultará na extração de seus blocos internos, tal operação apenas recriará o respectivo método. Nessa situação, o ideal seria mover o método; (7) Qualquer dependência presente em uma entidade interna de classe, método ou bloco será atribuída às suas entidades externas. Tendo em vista que uma entidade de código interna (e.g., classes aninhadas, métodos e blocos internos, etc.) está presente dentro do escopo da(s) entidade(s) externa(s), qualquer dependência estabelecida deve ser atribuída às entidades externas; (8) Métodos e blocos pertencentes a um tipo Interface serão desconsiderados. Devido à própria natureza de interfaces serem compostas por membros abstratos, essas não devem ser movidas ou extraídas, pois tal ação acarretaria em uma série de conflitos na estrutura do projeto; e (9) Métodos construtores e seus respectivos blocos serão descon- siderados. Considerando a obrigatoriedade de construtores em uma classe, a movimentação desse tipo de método é des- considerada, embora pretende-se avaliar a possibilidade de extração de seus blocos internos. 4 ANÁLISE E COMPARAÇÃO DE COEFICIENTES Esta seção analisa e compara os coeficientes de similaridade existen- tes a fim de encontrar o coeficiente mais adequado a ser adaptado. Dessa forma, pretende-se propor novos coeficientes de similaridade que obtenham maiores taxas de precisão. Assim, são analisados 18 coeficientes de similaridade (Tabela 1) em dez sistemas selecionados de forma completamente aleatória (training set ) do Qualitas.class Corpus, conforme pode ser observado na Tabela 4. Tabela 4: Sistemas-Alvo Sistema Versão Sistema Versão Ant 1.8.2 JFreeChart 1.0.13 ArgoUML 0.34 JHotDraw 7.5.1 Collections 3.2.1 JMeter 2.5.1 Hibernate 4.2.0 JUnit 4.1 JEdit 4.3.2 Tomcat 7.0.2 Todos os coeficientes de similaridade analisados foram aplicados para cada entidade dos sistemas-alvo selecionados, analisando se uma entidade possui o maior grau de similaridade encontrado com a entidade a qual deveria estar posicionada. Por exemplo, para uma classe A, cada coeficiente é aplicado comparando A com cada pacote no sistema, buscando verificar se a maior taxa de similaridade (Top #1) encontrada corresponde ao pacote em que A está, de fato, posicionada. Essa avaliação também analisa a segunda (Top #2 ) e a terceira (Top #3 ) maior taxa a fim de verificar se o coeficiente', '(Top #1) encontrada corresponde ao pacote em que A está, de fato, posicionada. Essa avaliação também analisa a segunda (Top #2 ) e a terceira (Top #3 ) maior taxa a fim de verificar se o coeficiente aplicado possui ao menos resultados próximos ao desejado. Por fim, é calculada a média aritmética nos 10 sistemas do training set considerando os Top #1, Top #2 e Top #3. Cada coeficiente foi avaliado separadamente em relação aos tipos de refatoração de código Move Class, Move Method e Extract Method. A Tabela 5 apresenta a precisão da similaridade dos coeficiente abor- dados em relação a identificação de oportunidades de refatoração Move Class, Move Method e Extract Method. Refatorações Extract Method levam em consideração apenas uma única taxa de acertos, visto que para refatorações Extract Method é analisado somente se o método deve ser extraído ou não, portanto é descartada a possibi- lidade acertar na segunda ou terceira tentativa (Top #2 ou Top #3). Por restrições de espaço, a tabela detalhada com os resultados de cada sistema está publicamente disponível em [10]. Após a análise e comparação dos principais coeficientes exis- tentes, Simple Matching foi selecionado para ser adaptado a fim de gerar um novo coeficiente. Tal escolha se deu devido ao fato de que o coeficienteSimple Matching possui uma estrutura fácil de ser adaptada, definindo pesos para as variáveis, o que contribui para a obtenção de bons resultados na proposta de novos coefici- entes. Em contrapartida, grande parte dos coeficientes analisados já possuíam pesos definidos. Embora não tenha sido o coeficiente com melhores resultados, a possível definição de pesos do Simple Matching permite simular coeficientes como Sokal and Sneath 2 que foi o melhor coeficiente em Move Method e segundo melhor em Move Class 1, bem como Russell and Rao, que obteve maior precisão em Extract Method. Ademais, o coeficiente considera o universo de dependências, ao contrário dos coeficientes Jaccard, Sorenson, Ochiai, PSC, Dot-product e Sokal and Sneath 2. 1PSC foi o melhor coeficiente para Move Class, contudo não foi selecionado por não ser linear.', 'Better Similarity Coefficients to Identify Refactoring Opportunities SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Tabela 5: Precisão de similaridade dos 18 coeficientes de si- milaridade em relação ao training set MOVE CLASS MédiaCoeficiente Top1 Top2 Top3 Baroni-Urbani and Buser 27,83% 41,87% 47,63% Dot-product 43,59% 55,9% 62,26% Hamann 20,35% 26,13% 29,41% Jaccard 45,66% 58,3% 64,26% Kulczynski 40,85% 55,1% 62,16% Ochiai 43,47% 56,58% 62,83% Phi 43,28% 56,65% 62,64% PSC 49,19% 62,18% 69,82% Relative Matching 34,20% 46,73% 53,52% Rogers and Tanimoto 20,37% 26,31% 29,56% Russell and Rao 39,30% 53,28% 60,07% Simple matching 20,07% 26,03% 29,39% Sokal and Sneath 19,70% 25,65% 28,94% Sokal and Sneath 2 47,00% 59,69% 65,51% Sokal and Sneath 4 41,15% 55,08% 62,18% Sokal binary distance 21,87% 28,35% 31,93% Sorenson 43,59% 55,9% 62,26% Yule 20,00% 29,84% 35,69% MOVE METHOD MédiaCoeficiente Top1 Top2 Top3 Baroni-Urbani and Buser 16,06% 22,39% 27,24% Dot-product 29,09% 38,2% 44,78% Hamann 17,86% 24,07% 28,54% Jaccard 32,71% 42,55% 48,89% Kulczynski 23,36% 32,61% 39,45% Ochiai 27,26% 36,52% 43,19% Phi 27,23% 36,52% 43,08% PSC 33,70% 44,22% 51,16% Relative Matching 23,36% 31,66% 36,94% Rogers and Tanimoto 18,07% 24,25% 28,85% Russell and Rao 30,33% 40,12% 46,29% Simple matching 17,94% 24,15% 28,48% Sokal and Sneath 16,88% 23,31% 27,76% Sokal and Sneath 2 35,74% 45,65% 51,74% Sokal and Sneath 4 23,33% 32,49% 39,32% Sokal binary distance 21,20% 27,96% 33,25% Sorenson 29,09% 38,2% 44,78% Yule 9,22% 15,13% 20,1% EXTRACT METHOD MédiaCoeficiente Acertos Baroni-Urbani and Buser 52,57% Dot-product 58,00% Hamann 15,59% Jaccard 62,09% Kulczynski 66,52% Ochiai 62,00% Phi 61,94% PSC 66,18% Relative Matching 69,17% Rogers and Tanimoto 15,56% Russell and Rao 74,79% Simple matching 15,57% Sokal and Sneath 15,47% Sokal and Sneath 2 67,49% Sokal and Sneath 4 66,28% Sokal binary distance 21,75% Sorenson 58,00% Yule 55,44% 5 PROPOSTA DE NOVOS COEFICIENTES Após a seleção do coeficiente Simple Matching para ser adaptado, é aplicado um algoritmo genético ao referente coeficiente, tendo como training set dez sistemas do Qualitas.class Corpus. Para a execução da proposta deste artigo, foi definido como ob- jetivo de otimização, maximizar a precisão de acerto do algoritmo selecionado. A configuração utilizada pelo algoritmo genético é a mesma aplicada no exemplo ilustrativo da Seção 2.3.2 (ver Ta- bela 2). É importante clarificar que cada novo coeficiente proposto é resultante de uma execução diferente do algoritmo genético. Após gerado o conjunto de soluções, na maioria dos casos são apresentadas várias possibilidades que resultam na mesma taxa de precisão. Logo, é selecionada uma única solução que apresente maior distância entre a média dos índices de similaridade que deseja maximizar com a média do índices que deseja minimizar, visto que uma maior distância indica que o coeficiente tenderá a maximizar e minimizar as similaridades de forma correta em outros sistemas. Assim, considerando a possibilidade de definição de pesos às variáveis do Simple Matching, foi simulado inicialmente como can- didatos do algoritmo genético os pesos do coeficiente Sokal and Sneath 2 para refatorações Move Class e Move Method, bem como o Russell and Rao para refatorações Extract Method, o que facilita a seleção de novos candidatos, visto que estavam entre os melhores resultados em suas respectivas refatorações. Posteriormente, o algo- ritmo genético foi executado sobre os dez sistemas do training set a fim de obter pesos mais adequados para cada variável do coeficiente e formar os novos coeficientes a serem avaliados nos demais 101 sistemas (test set ). Dessa forma, foram propostos os seguintes coeficientes, onde cada um corresponde a um tipo de refatoração de código: PT MC para operações de Move Class, PT MM para operações de Move Method e PT EMpara operações de Extract Method. A execução do algoritmo genético sobre o Simple Matching no training set definido', 'para operações de Move Class, PT MM para operações de Move Method e PT EMpara operações de Extract Method. A execução do algoritmo genético sobre o Simple Matching no training set definido resultou na primeira versão dos três coeficientes almejados. Os resultantes pesos Pa′, Pd′, Pa′′, Pb , Pc e Pd′′, correspondentes às variáveis a e d do numerador e a, b, c e d do denominador, são reportados na Tabela 6. Tabela 6: Pesos dos coeficientes propostos Co eficiente Pa′ Pd′ Pa′ ′ Pb Pc Pd′ ′ PT MC 5,13e-7 3,73e-7 8,71e-6 2,0 0,17 3,9e-7 P T MM 1,62e-10 1,15e-10 6,18e-7 1,62 8.16e-4 3,55e-10 P T EM 1,63 0,08 1,0 9,93 0,03 1,48 Dessa forma, ao atribuir cada peso à variável correspondente do Simple Matching, tem-se os seguintes coeficientes propostos: PT MC = 5, 13e−7a + 3, 73e−7d 8, 71e−6a + 2b + 0, 17c + 3, 9e−7d (4) PT MM = 1, 62e−10a + 1, 15e−10d 6, 18e−7a + 1, 62b + 8, 16e−4c + 3, 55e−10d (5) PT EM = 1, 63a + 0, 08d a + 9, 93b + 0, 03c + 1, 48d (6) 6 AVALIAÇÃO DOS COEFICIENTES PROPOSTOS A fim de avaliar a eficiência dos coeficientes propostos, foi realizada uma avaliação em que a precisão dos respectivos coeficientes é comparada com outros 18 coeficientes existentes na literatura (Tabela 1), envolvendo os demais 101 sistemas da baseQualitas.class Corpus (test set ). Cada coeficiente proposto é analisado e comparado de acordo com sua respectiva refatoração de código, ou seja, esta seção apresenta uma comparação diferente para Move Class, Move Method e Extract Method.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Arthur F. Pinto and Ricardo Terra Para a avaliação dos resultados, foi adotada a mesma abordagem de análise descrita na Seção 4. Dessa forma, a Tabela 7 apresenta os resultados das médias das taxas de precisão dos coeficientes analisados para refatorações Move Class (incluindo PT MC), Move Method (incluindo PT MM) e Extract Method (incluindo PT EM). Novamente, por restrições de espaço, a tabela detalhada abran- gendo os resultados de cada um dos 101 sistemas está publicamente disponível em [10]. Os resultados são reportados a seguir. Move Class : Pode-se observar que o coeficiente proposto se destacou aos demais ao atingir média aproximada de 58,20%, 70,82% e 77,22% em relação às taxas de similaridade Top #1, Top #2 e Top #3. PSC apresentou a segunda melhor média de precisão dentre os coeficientes analisados. Para Top #1, Top #2 e Top #3, PSC obteve, respectivamente, os valores de precisão de 55,03%, 69,20% e 76,36%. Mais importante, PT MCmostrou ser 3,17% superior em relação ao Top #1, 1,62% em relação ao Top #2 e 0,86% em relação ao Top #3 do PSC. É possível observar, também acréscimo de 31,61%, 34,56% e 33,52% em relação aos valores de, respectivamente, Top #1, Top #2 e Top #3 do Simple Matching, coeficiente do qualPT MCfoi adaptado. Move Method: Pode-se observar que PT MMsuperou os demais coeficientes nos três casos de taxa de similaridade (Top #1, Top #2 e Top #3), alcançando, respectivamente, média de aproximadamente 51,36%, 60,93% e 66,41%. O segundo melhor coeficiente foiSokal and Sneath 2 em que considerando os três casos de taxa de similaridade (Top #1, Top #2 e Top #3), o coeficiente atingiu, respectivamente, os valores de precisão de 40,79%, 51,86% e 58,58%. Portanto pode-se concluir que PT MMfoi 10,57% melhor em relação ao Top #1, 9,07% melhor em relação ao Top #2 e 7,83% melhor em relação ao Top #3. Além disso, mais uma vez ocorreu um acréscimo significativo no coeficiente proposto em relação ao Simple Matching, havendo aumento de 28,44%, 30,90% e 32,21% em relação aos valores de Top #1, Top #2 e Top #3, respectivamente. Extract Method : Pode-se observar que PT EMapresentou uma média de aproximadamente 86,88%, superando o segundo melhor coeficiente (Russell and Rao ), o qual apresentou uma média de aproximadamente 86,58%. Dessa forma, PT EMmostrou ser 0,30% melhor em na precisão de identificação de oportunidades de refatoraçãoExtract Method. E, embora tal diferença na média seja relativamente pequena, PT EMnão foi o melhor em apenas cinco dos 101 sistemas analisados. Essa pequena melhora é esperada devido ao fato de o cálculo da similaridade de blocos e métodos considerarem apenas os métodos da própria classe (como mencionado na Seção 3.1) o que implica em um espectro de avaliação reduzido. É relevante observar também o acréscimo de 75,05% em relação a média obtida pelo Simple Matching, coeficiente do qual PT EMfoi geneticamente adaptado. Conforme observado, os coeficientes PT MC, PT MM, e PT EM apresentaram melhor eficiência aos demais coeficientes, entretanto deve-se ressaltar que é possível aumentar ainda mais as precisões dos coeficientes propostos. Tal fato é possível por meio de abordagens que visam aprimorar o conjunto solução resultante do algoritmo genético. Devido ao fato de o algoritmo genético ser não-determinístico, planeja-se como trabalho futuro aplicar o Teste de Tukey [ 6] sobre sua execução, aplicando repetitivas execuções do Tabela 7: Precisão de similaridade dos 19 coeficientes de si- milaridade em relação ao test set MOVE CLASS Top1 Top2 Top3Coeficiente Média Baroni-Urbani and Buser 39,32% 52,24% 60,42% Dot-product 50,50% 64,50% 72,29% Hamann 26,43% 36,16% 43,43% Jaccard 52,62% 66,53% 73,98% Kulczynski 48,58% 63,33% 71,48% Ochiai 50,40% 64,48% 72,32% Phi 50,56% 64,39% 72,31% PSC 55,03% 69,20% 76,36% Relative Matching 39,65% 55,29% 64,70% Rogers and Tanimoto 26,78% 36,37% 43,55% Russell and Rao 45,11% 61,79% 70,81%', 'Ochiai 50,40% 64,48% 72,32% Phi 50,56% 64,39% 72,31% PSC 55,03% 69,20% 76,36% Relative Matching 39,65% 55,29% 64,70% Rogers and Tanimoto 26,78% 36,37% 43,55% Russell and Rao 45,11% 61,79% 70,81% Simple matching 26,59% 36,26% 43,70% Sokal and Sneath 26,23% 35,92% 43,09% Sokal and Sneath 2 54,32% 68,02% 75,22% Sokal and Sneath 4 48,80% 63,53% 71,54% Sokal binary distance 29,39% 38,72% 46,09% Sorenson 50,69% 64,76% 72,61% Yule 30,79% 43,01% 51,62% PTMC 58,20% 70,82% 77,22% MOVE METHOD Top1 Top2 Top3Coeficiente Média Baroni-Urbani and Buser 21,97% 29,98% 35,43% Dot-product 34,24% 45,34% 52,32% Hamann 22,96% 30,05% 34,19% Jaccard 37,84% 49,01% 55,94% Kulczynski 28,61% 40,16% 47,54% Ochiai 32,61% 43,98% 51,13% Phi 32,50% 43,92% 51,01% PSC 38,98% 51,06% 58,31% Relative Matching 27,59% 37,43% 43,64% Rogers and Tanimoto 23,17% 30,31% 34,54% Russell and Rao 34,62% 45,74% 52,49% Simple matching 22,92% 30,03% 34,20% Sokal and Sneath 22,32% 29,23% 33,33% Sokal and Sneath 2 40,79% 51,86% 58,58% Sokal and Sneath 4 28,47% 40,07% 47,54% Sokal binary distance 26,18% 33,64% 38,21% Sorenson 34,24% 45,34% 52,32% Yule 14,21% 22,07% 27,83% PTMM 51,36% 60,93% 66,41% EXTRACT METHOD AcertosCoeficiente Média Baroni-Urbani and Buser 61,74% Dot-product 69,48% Hamann 11,86% Jaccard 71,74% Kulczynski 77,62% Ochiai 73,11% Phi 72,70% PSC 75,35% Relative Matching 81,51% Rogers and Tanimoto 12,02% Russell and Rao 86,58% Simple matching 11,83% Sokal and Sneath 11,66% Sokal and Sneath 2 74,37% Sokal and Sneath 4 76,66% Sokal binary distance 18,03% Sorenson 69,48% Yule 65,50% PTEM 86,88% algoritmo em diferentes grupos de configurações do mesmo, dimi- nuindo assim, a taxa de erro e aumentando o intervalo de confiança. Ameaças à Validade : Embora as regras de execução foram pro- postas após uma série de testes e execuções experimentais, não se pode afirmar que as regras de execução descritas na Seção 3.1', 'Better Similarity Coefficients to Identify Refactoring Opportunities SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil são completas, visto que ainda é possível a ocorrência de falsos positivos (validade interna). Ademais, como é comum em estudos empíricos em Engenharia de Software, os resultados não podem ser extrapolados (validade externa). Embora o test set conte com um número razoável de 101 sistemas, é importante a avaliação dos novos coeficientes em cenários reais de desenvolvimento. 7 SISTEMA DE RECOMENDAÇÃO Tendo como objetivo aplicar os novos coeficientes propostos neste artigo para a identificação de oportunidades de refatoração, desenvolveu-se um protótipo de um plug-in para o IDE Eclipse. A ferramenta, denominada AIRP 2, é composta por uma arquitetura baseada em quatro módulos principais: •Módulo de Extração de Dependências: Responsável por identificar e armazenar todas as dependências de uma classe, método ou bloco. Ou seja, identifica o conjunto de tipos com os quais uma entidade de código estabelece dependência estrutural. Isso inclui chamada de métodos, acesso a atributos, instanciação, declaração de variáveis, anotações, etc.; •Módulo de Cálculo de Similaridade: Realiza o cálculo da similaridade estrutural de determinada entidade de código presente no sistema-alvo. Tal cálculo é efetuado utilizando a fórmula do coeficiente de similaridade escolhido, realizando a comparação entre as dependências previamente armazenadas de determinada classe, método ou bloco com a entidade (pacote, classe ou método) na qual se encontra; •Módulo de Recomendação: Após o cálculo de similaridade, seu resultado é comparado com um índice mínimo de aceitação especificado pelo usuário. Caso o resultado seja menor que essethreshold definido pelo usuário, é apresentada uma lista de sugestões de possíveis refatorações Move Class, Move Method ou Extract Method. Em seguida, as recomendações são reportadas para a análise dos usuários, sendo ordenadas pelo seu índice de similaridade; e •Módulo de Visualização: Com o objetivo de fornecer mais detalhes a respeito da organização estrutural do sistema alvo, é gerado um grafo da arquitetura implementada. Tal grafo reportará possíveis refatorações de Move Class, Move Method e Extract Method, bem como a similaridade resultantes de cada refatoração. Dessa forma, torna possível uma maior compreensão por parte do usuário a respeito do processo realizado pela ferramenta, podendo observar o reposicionamento das entidades envolvidas e a arquitetura resultante de cada recomendação. A Figura 3 ilustra um protótipo do grafo de oportunidades de refatoração. A Figura 3 ilustra um cenário hipotético onde a classe B possui um valor da média de similaridade 0,12 maior com as classes depkд2, o m ´etodoA1 possui um valor da média de similaridade 0,2 maior com os métodos da classe C, m ´etodoC 1 possui um valor de média 0,23 maior com a classeD e, por fim, ao ser extraído o segundo bloco de m ´etodoA1, a média de similaridade do respectivo método aumen- taria em 0,3. Assim, AIRP realiza sugestões de refatoração Move 2Publicamente disponível em: https://github.com/rterrabh/AIRP Figura 3: Grafo de oportunidades de refatoração Class (mover a classe B para pkд2), Move Method (mover m ´etodoA1 para a classe C e mover m ´etodoC 1 para a classe D) e Extract Method (extrair o segundo bloco de m ´etodoA1 para um novo método). 8 TRABALHOS RELACIONADOS Ainda que apenas um estudo a respeito da proposta de novos coeficientes de similaridade foi encontrado, alguns trabalhos apresentam metodologias e técnicas para a identificação de oportunidades de refatoração, adaptações de métricas existentes ou estudos empíricos a respeito dos conceitos abordados neste artigo. Como o objetivo deste artigo é aprimorar a identificação de oportunidades de refatoração por meio de coeficientes de similaridade, não são considerados trabalhos que não utilizem tais coeficientes. Os trabalhos mais relevantes a este artigo são', 'de oportunidades de refatoração por meio de coeficientes de similaridade, não são considerados trabalhos que não utilizem tais coeficientes. Os trabalhos mais relevantes a este artigo são apresentados, de acordo com suas categorias, a seguir. Proposta de Coeficientes de Similaridade Estrutural: Naseem et al. propõem um novo coeficiente de similaridade voltado para sua aplicação em algoritmos de clusterização [8]. O novo coeficiente proposto, por sua vez, é uma adaptação do coeficiente Jaccard, denominado Jaccard-NM, cuja principal diferença é a adição de uma nova variável que leva em consideração o universo de fatores analisados, i.e., todos os fatores existentes no conjunto em que as entidades analisadas estão presentes. É importante ressaltar que o novo coeficiente de similaridade proposto é comparado somente ao coeficiente Jaccard, do qual foi adaptado e em apenas três sistemas. Em contrapartida, este artigo conta com uma análise aprofundada dos coeficientes propostos para uma identificação mais precisa em relação aos demais coeficientes existentes. Estudos Empíricos: Terra et al. realizam uma robusta avaliação em 111 sistemas da base Qualitas.class Corpus, envolvendo 18 dos prin- cipais coeficientes de similaridade [17]. Tendo em vista o propósito do estudo, o mesmo torna-se fundamental para a realização deste artigo, considerando que os coeficientes utilizados e suas análises, bem como a abordagem envolvendo dependências estruturais, atu- aram como o principal embasamento para a proposta dos novos coeficientes, assim como suas avaliações. Szõke et al. apresentam um estudo de caso onde é discutido se refatorações automáticas de código realmente melhoram a', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Arthur F. Pinto and Ricardo Terra manutenibilidade de sistemas de software [15]. Embora o estudo reporte que refatorações usualmente impactam positivamente na manutenibilidade, efetuá-las sem a devida análise pode impactar negativamente o sistema. Ainda que seja de grande importância investigar o real uso de refatorações automáticas, este artigo propõe coeficientes que permitem identificar de forma mais precisa oportunidades de refatoração que podem ou não ser passíveis de refatoração automática. Revisões Sistemáticas de Literatura: Dallal apresenta uma revisão sistemática da literatura a respeito de refatoração de código [ 2]. O estudo aborda 47 trabalhos sobre os tipos de atividades de refatoração, as diferentes abordagens para identificar opor- tunidades de refatoração, bem como os data sets e os meios utilizados para avaliá-las. Essa revisão teve grande importância neste artigo, visto que as análises realizadas permitem detectar as abordagens mais adequadas a serem utilizadas, além de compreender melhor o processo de identificação de oportunidades de refatoração de código, assim como sua realização e sua avaliação. Ferramentas para Identificação de Oportunidades de Refatoração: Sales et al. descreveram a proposta da ferramenta JMove, um plug- in do IDE Eclipse que indica oportunidades de refatoração Move Method baseado no coeficiente de similaridade estrutural Sokal and Sneath 2 [11]. A ferramenta, entretanto, envolve análises apenas de Move Method. Mais importante, o uso do coeficiente PT MM, pro- posto neste artigo, poderia melhorar a precisão do JMove em 10,57%. Silva et al. propuseram a ferramenta JExtract, um plug-in do IDE Eclipse voltado para a identificação de oportunidades de re- fatoraçãoExtract Method baseado no coeficiente de similaridade Kulczynski [12]. JExtract, por sua vez, foca apenas na refatoração Extract Method. Similarmente, o uso do coeficiente PT EM, proposto neste artigo, poderia melhorar a precisão do JExtract em 9,26%. Fokaefs et al. apresentam a ferramenta JDeodorant, que identifica Code Smells e aplicam diferentes técnicas de refatoração de código a fim de tratá-los [ 3]. JDeodorant não utiliza a compa- ração de similaridade entre as dependências de um projeto. Em contrapartida, utiliza o coeficienteJaccard para calcular apenas a similaridade entre os atributos e métodos da classes analisadas, o que pode afetar sua eficiência uma vez queJaccard—ainda que seja um dos coeficientes mais utilizados em Engenharia de Software—não apresenta boa precisão quando comparado aos demais coeficientes existentes. Embora os trabalhos analisados demonstrem foco na identifica- ção de oportunidades de refatoração de código ou em abordagens que envolvam coeficientes de similaridade estrutural, não foram encontrados estudos que propusessem novos coeficientes visando maior precisão na identificação de tais oportunidades. Dessa forma, a proposta de novos coeficientes, tendo, como embasamento esta- tístico, robustas análises e comparações entre os principais coefici- entes de similaridade existentes focados em três tipos diferentes de refatoração, ressalta a originalidade deste estudo. 9 CONCLUSÃO Este artigo apresenta a proposta de três novos coeficientes de si- milaridade estrutural a fim de prover meios mais precisos na iden- tificação de oportunidades de refatoração de código: PT MC, para identificação de oportunidades de refatoração Move Class ; PT MM, para Move Method ; e PT EM, para Extract Method. Os coeficientes propostos apresentam melhor precisão em relação aos coeficientes até então mais precisos utilizados para identificação de oportunida- des de refatoração. Em termos numéricos, PT MC, PT MMe PT EM apresentam melhores precisões, respectivamente, de 3,17%, 10,57% e 0,30% em relação aos segundos melhores coeficientes. Como trabalhos futuros, pretende-se: (i) considerar pesos nos expoentes das variáveis; (ii) utilizar mais coeficientes na etapa de', 'e 0,30% em relação aos segundos melhores coeficientes. Como trabalhos futuros, pretende-se: (i) considerar pesos nos expoentes das variáveis; (ii) utilizar mais coeficientes na etapa de análise; (iii) aplicar técnicas de validação cruzada; (iv) realizar um estudo comparativo entre técnicas para identificação de oportuni- dades de refatoração que usem ou não coeficientes de similaridade; (v) propor um coeficiente único que atenda a outros tipos de refato- ração; e (vi) medir o grau de aceitação dos novos coeficientes em cenários reais. AGRADECIMENTOS Este trabalho é apoiado pelo CNPq (projeto n o 460401/2014-9), CAPES e FAPEMIG. REFERÊNCIAS [1] Monica Chis. 2010. Evolutionary Computation and Optimization Algorithms in Software Engineering: Applications and Techniques . Information Science Refe- rence. [2] Jehad Al Dallal. 2015. Identifying Refactoring Opportunities in Object-Oriented Code: A Systematic Literature Review. Information and Software Technology 58 (2015), 231–249. [3] Marios Fokaefs, Nikolaos Tsantalis, Eleni Stroulia, and Alexander Chatzigeorgiou. 2011. JDeodorant: Identification and Application of Extract Class Refactorings. In 33rd International Conference on Software Engineering (ICSE). 1037–1039. [4] Martin Fowler. 2004. UML Distilled: a Brief Guide to the Standard Object Modeling Language. Addison-Wesley. [5] Martin Fowler, Kent Beck, John Brant, William Opdyke, and Don Roberts. 1999. Refactoring: Improving the Design of Existing Code. Addison-Wesley. [6] Winston Haynes. 2013. Tukey’s Test. Springer New York, USA. 2303–2304 pages. [7] Paul Jaccard. 1912. The Distribution of the Flora in the Alpine Zone. New Phytologist 11, 2 (1912), 37–50. [8] Rashid Naseem, Onaiza Maqbool, and Siraj Muhammad. 2010. An Improved Si- milarity Measure for Binary Features in Software Clustering. In 2nd International Conference on Computational Intelligence, Modelling and Simulation (CIMSiM). 111–116. [9] Leonardo Passos, Ricardo Terra, Renato Diniz, Marco Tulio Valente, and Nabor Mendonça. 2010. Static Architecture Conformance Checking: An Illustrative Overview. IEEE Software 27, 5 (2010), 132–151. [10] Arthur F. Pinto and Ricardo Terra. 2017. Dados das Avaliações Reportadas no Artigo Submetido ao SBCARS’17. (2017). https://github.com/rterrabh/2017_sbcars [11] Vitor Sales, Ricardo Terra, Luis Fernando Miranda, and Marco Tulio Valente. 2013. Recommending Move Method Refactorings Using Dependency Sets. In 20th Working Conference on Reverse Engineering (WCRE). 232–241. [12] Danilo Silva, Ricardo Terra, and Marco Tulio Valente. 2014. Recommending Automated Extract Method Refactorings. In 22nd International Conference on Program Comprehension (ICPC). 146–156. [13] Danilo Silva, Nikolaos Tsantalis, and Marco Tulio Valente. 2016. Why we re- factor? confessions of GitHub contributors. In 24th International Symposium on Foundations of Software Engineering (FSE). 858–870. [14] S.N. Sivanandam and S. N. Deepa. 2007. Introduction to Genetic Algorithms. Springer Science & Business Media. [15] Gábor Szőke, Csaba Nagy, Péter Hegedűs, Rudolf Ferenc, and Tibor Gyimóthy. 2015. Do automatic refactorings improve maintainability? An industrial case study. In31st International Conference on Software Maintenance and Evolution (ICSME). 429–438. [16] Ewan Tempero, Craig Anslow, Jens Dietrich, Ted Han, Jing Li, Markus Lumpe, Hayden Melton, and James Noble. 2010. The Qualitas Corpus: A Curated Collec- tion of Java Code for Empirical Studies. In 17th Asia Pacific Software Engineering Conference (APSEC). 336–345. [17] Ricardo Terra, Joao Brunet, Luis Miranda, Marco Tulio Valente, Dalton Serey, Douglas Castilho, and Roberto S. Bigonha. 2013. Measuring the Structural Simi- larity between Source Code Entities. In 25th International Conference on Software Engineering and Knowledge Engineering (SEKE). 753–758.']","['FINDINGS ON SIMILARITY COEFFICIE NTS TO  IDENTIFY REFACTORING  OPPORTUNITIES     This briefing reports scientific  evidence on the proposal of better  similarity coefficients to identify  code refactoring opportunities.      FINDINGS    The results presented in this briefing were  obtained through the analysis of the  accuracy of the proposed similarity  coefficients along with oth er 18  coefficients in 101 software  systems. This  analysis considered similarities values for  Move Class, Move Method and Extract  Method refactoring operations.      \uf0b7 The research results show that it is  indeed possible to apply optimization  algorithms to adapt the weights of the  formula of  an existing coefficient to  generate new coefficients with a higher  accuracy.    \uf0b7 As the research objective, three new  similarity coefficients were proposed.  PTMC for Move Class operations,  PTMM for Move Method operations  and PTEM for Extract Method  operations.       Findings about the major 18  previously existing coefficients in  literature analyzed:    \uf0b7 Considering the analyzed coefficients  and target systems, PSC was the  coefficient with the best precision for  Move Class operations, in which it  achieved an average accuracy of  55.03% in the first attempt, 69.20% in  the second, and 76.36% in the third.    \uf0b7 Sokal and Sneath 2 reached the best  precision among the coefficients for  Move Method operations. It achieved  an average accuracy of, respectively,  40.79%, 51.86% and 58.58% for the  first, second, and third attempts.              \uf0b7 For Extract Method operations, which  consider only one attempt, Russell and  Rao presented an accuracy of 86.58%,  the best precision among the  previously existing coefficients.      Findings about the coefficients  proposed in the research:     \uf0b7 It can be observed, through the  analysis of the results, that the  proposed coefficient PTMC was  superior to the  previously best  coefficient for Move Class  operations  (PSC), achieving an average accuracy of  58.20% in the first attempt, 70.82% in  the second , and 77.22% in the third,  and having therefore an accuracy,   respectively, 3.17%, 1.62%, and 0.86%  higher.    \uf0b7 For Move Method operations, the  proposed coefficient PTMM obtained  an average accuracy  of 51.36% in the  first attempt, 60.93% in the second  ,and 66.41% in the third . Therefore, it  was superior to Sokal and Sneath 2  (coefficient with the  previous highest  precision), achieving an accuracy ,  respectively, 10.57%, 9.07% and 7.83%  higher.    \uf0b7 The proposed coefficient PTEM  reached a n average accuracy  of  86.88%, and therefore 0.3% higher  than the Russell and Rao, previously  best coefficient for Extract Method  operations. And although such  difference in the accuracy  is relatively  small, PTEM was not superior  in only  five of the 101 analyzed systems.      Who is this briefing for?    Software engineering  practitioners who want to  find more suitable similarity  coefficients to identify code  refactoring opportunities  based on scientific evidence.    Where the findings come  from?    All findings of this briefing  were extracted from the  empirical study conducted by  Arthur F. Pinto and Ricardo  Terra.      What is included in this  briefing?    The main findings of the  original empirical study  related to the proposed  coefficients.    What is not included in this  briefing?    Additional information not  supported by the findings of  the original empirical study as  well as descriptions about the  applied methodology and  details of the developed tool  features.      ORIGINAL RESEARCH REFERENCE  Arthur F. Pinto and Ricardo Terra. Better Similarity Coefficients to Identify Refactoring Opportunities. In Proceedings of SBCARS, 2017.']","**Title:** Enhancing Refactoring Opportunities with Improved Similarity Coefficients

**Introduction:**
This evidence briefing summarizes the findings from a study aimed at improving the identification of refactoring opportunities in software systems through the development of new similarity coefficients. This research addresses the limitations of existing coefficients that are not tailored for structural analysis, which can lead to inadequate accuracy in identifying when to perform refactorings such as Move Class, Move Method, and Extract Method.

**Main Findings:**
The study proposes three new similarity coefficients, derived from genetic algorithms, specifically designed to enhance the accuracy of identifying refactoring opportunities. These coefficients, labeled PT MC, PT MM, and PT EM, were tested against 18 existing coefficients across 101 software systems. Key findings include:

1. **Move Method Refactoring:** The new coefficient PT MM demonstrated a significant improvement of 10.57% in identifying Move Method opportunities compared to the best existing coefficient.
   
2. **Move Class Refactoring:** PT MC showed a 3.17% increase in accuracy for identifying Move Class opportunities over the best alternative.

3. **Extract Method Refactoring:** The improvement for Extract Method was modest at 0.30%, but it still positions PT EM as a competitive option.

4. **Tool Implementation:** A tool was developed based on these new coefficients, which provides recommendations for refactoring opportunities, thereby facilitating practical application in software development environments.

These findings highlight the potential of the proposed coefficients to improve software maintainability and reduce technical debt by ensuring that classes and methods are structurally aligned within the architecture of the software system.

**Who is this briefing for?**
This briefing is intended for software engineers, developers, and architects who are involved in code maintenance and refactoring processes. It is particularly relevant for those seeking to implement more effective methods for identifying refactoring opportunities in their projects.

**Where the findings come from?**
The findings are based on empirical evaluations conducted by Arthur F. Pinto and Ricardo Terra, analyzing the performance of new similarity coefficients across a diverse set of software systems from the Qualitas.class corpus.

**What is included in this briefing?**
This briefing includes a summary of the main findings related to the new similarity coefficients, their comparative performance against existing methods, and insights into the practical implications of using these coefficients in real-world software development scenarios.

To access other evidence briefings on software engineering:  
[http://www.lia.ufc.br/~cbsoft2017/](http://www.lia.ufc.br/~cbsoft2017/)

For additional information about the research group:  
[http://www.great.ufc.br](http://www.great.ufc.br)

**Original Research Reference:**
Pinto, A. F., & Terra, R. (2017). Better Similarity Coefficients to Identify Refactoring Opportunities. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil. https://doi.org/10.1145/3132498.3132511"
"['How Has the Health of Software Ecosystems Been Evaluated? A Systematic Review Simone da Silva Amorim Federal Institute of Bahia Federal University of Bahia Salvador, Brazil simone.amorim@ifba.edu.br Félix Simas S. Neto Federal Institute of Bahia Salvador, Brazil felixneto@ifba.edu.br John D. McGregor Clemson University Clemson, USA johnmc@clemson.edu Eduardo Santana de Almeida Federal University of Bahia Salvador, Brazil esa@dcc.ufba.br Christina von Flach G. Chavez Federal University of Bahia Salvador, Brazil flach@ufba.br ABSTRACT The health of the software ecosystems concerns to the growing and continuity to exist remaining variable and productive over time. Research on this area is becoming more important. Even today, no studies have been available summarizing the research on evaluation approaches for the health of software ecosystems. The objective of this study is to structure and analyze the available literature on this field identifying the state-of-the-art of the research. We conducted a systematic literature review to obtain an overview of the existing studies in this area. 23 studies were selected as primary studies by applying inclusion, exclusion and quality criteria. The findings show that the research area is quite immature. There are few approaches and tools to support the evaluation work. In these studies, only 3 reported a complete evaluation of the health of ecosystems, 5 studies were considered as initial proposals, and the others evaluated the health partially. CCS CONCEPTS • Software and its engineering → Operational analysis; Em- pirical software validation; Software evolution; Open source model; KEYWORDS Software Ecosystems Health, Systematic Literature Review, Soft- ware Evaluation ACM Reference format: Simone da Silva Amorim, Félix Simas S. Neto, John D. McGregor, Eduardo Santana de Almeida, and Christina von Flach G. Chavez. 2017. How Has the Health of Software Ecosystems Been Evaluated? A Systematic Review. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. https://doi.org/10.1145/3131151.3131174 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5326-7/17/09. . . $15.00 https://doi.org/10.1145/3131151.3131174 1 INTRODUCTION Software ecosystems have gained significant attention over recent years. There are several concepts for software ecosystems, for ex- ample Messerschmitt et al.[16] states that “Traditionally, a software ecosystem refers to a collection of software products that have some given degree of symbiotic relationships” . It is claimed that software ecosystems provide several benefits to organizations that adopt this approach. They increase the attractiveness for new users, accelerate innovation, share cost of the innovation, and decrease the cost of software maintenance by sharing this activity with third-party[3]. In this scenario many organizations are migrating to this approach. We can see the success of many open source and commercial or- ganizations such as Hadoop, Eclipse, Apple, and so on. Hadoop market “is growing at almost 55 per cent a year and is expected to be worth $20.9 billion in 2018” [19]. As well as, we see an icon of the closed code, the giant Microsoft, opens their boundaries in recent years adopting the open source ecosystems approach[17]. Nowa-', 'worth $20.9 billion in 2018” [19]. As well as, we see an icon of the closed code, the giant Microsoft, opens their boundaries in recent years adopting the open source ecosystems approach[17]. Nowa- days, Microsoft is on the top of the list of organizations with the most contributors in the open source approach[8]. In this context, where several organizations are adopting the soft- ware ecosystem approach, the health of the software ecosystems is a factor of extreme importance. Iansiti and Levien[10] introduced the idea of what a healthy ecosystem provides “durably growing opportunities for its members and for those who depend on it” . Eval- uating the health is a fundamental activity considered as one key feature for organizations, partners, or even common developers. This allows to know the real health state for all stakeholders that de- sire to engage into the ecosystem. However, evaluating this health involves complex and challenging tasks. These tasks should also be supported by appropriate approaches, techniques, and tools. Furthermore, there are no efforts to systematically analyze how the health of software ecosystems have been evaluated. There is not a systematic proposal to collect and synthesize existing health evaluation approaches. These approaches should investigate what benefits and limitations can contribute to improve the health of ecosystems. Besides, the research outcomes should be published to software ecosystem community. Several organizations can use a consolidated evaluation process to consider participating, and investing resources in the ecosystem. Moreover, the results of an effective health evaluation can help the organization to evaluate 14', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil S. Amorim et al. and compare platforms before basing new products on the assets available in the ecosystem. A clear and accurate health evaluation is an important input for investment decisions. In face of the importance to get an effective evaluation process for the health of ecosystems, we decided to conduct a systematic literature review to know the scenario of existing approaches and to provide a picture of the state-of-the-art of the existing approaches. The outcomes reported in this work can be reference to support new evaluating approaches. The remainder of the work is organized as follows: Section 2 presents some background and related work on the health of ecosys- tems. The objectives and methodology are described in Section 3. Section 4 describes the outcomes gathered from the systematic review. Section 5 presents the summary of study findings and the implications from research and practice. Section 6 describes the limitations of the methodology used. Finally, Section 7 offers a brief description of the outcomes and some concluding remarks. 2 BACKGROUND AND RELATED WORK In this section, we describe a short background and some related work for the health of software ecosystems. We start with the sys- tematic review of Hyrynsalmi et al.[9]. Considering this work and our own observation, there is not an agreed upon definition for “health” in an ecosystem context. Different studies use different con- cepts. We added a research question to gather different definitions in the studies. The concept of Iansiti and Levien[10], described in previous section, serves as basis for the majority of the primary studies. They introduced an evaluation framework for ecosystem health and defined indicators such as: robustness, productivity, and niche creation. KDE ecosystem1 is an example of healthy ecosystem. They have a non-profit organization that supports business and financial issues to improving their robustness. KDE also have some practices to increase the productivity providing a set of tools for development and tests. Last, they have specific policies to attract new developers and receive new projects, increasing their niche creation capacity. In addition, Manikas and Hansen[15] have taken a different per- spective on the health of software ecosystems. They state that soft- ware ecosystems are different from business ecosystems because the actors are different from the products. The software component and the actor have different influence on the ecosystem. A software component can have a positive influence, but the actor responsible for that component may not have the same positive influence on the ecosystem health. They proposed a health framework composed by three main elements that affects the health of software ecosys- tems, they are: the actors, the software and the orchestration. Other concepts can be seen in this study later. Equally important, in 2014, Fotrousi et al.[6] conducted a map- ping study. They found 34 studies published from 2004 on the use of KPI for software-based ecosystems. The goal of this study was to gather existing research on KPI-based software ecosystem as- sessment. Key performance indicators (KPI) are initial indicators that can help to foreseen the sustainability and the health state of software ecosystems. This mapping study provides an overview of the literature on KPI for software ecosystems, explaining how 1http://www.kde.org KPI are used in the management of software ecosystems. Despite introducing several health indicators, this study did not mention any approach used to evaluate the health of software ecosystems, as well as key areas, practices and tools to support the evaluation process. Similarly, a systematic review was conducted by Franco-Bedoya et al. in 2014[7]. They introduced a quality model for the quality assessment of OSS ecosystems (QuESo). Building this model, they', 'Similarly, a systematic review was conducted by Franco-Bedoya et al. in 2014[7]. They introduced a quality model for the quality assessment of OSS ecosystems (QuESo). Building this model, they gathered several quality measures from the systematic review, clas- sified and organized the set of measures to create the QuESo. The main question was: “What measures or attributes are defined to as- sess or evaluate open source software ecosystems?” . They reviewed 53 studies of which 17 provided relevant measures to evaluate the quality of open source ecosystems. After using two criteria to select measures, they chose 68 different measures to compose the QuESo. This systematic review focused only on obtaining health indica- tors, without provide other information about ecosystem health evaluation. Finally, another systematic review was conducted by Hyryn- salmi et al. in 2015[ 9]. This review aimed to support a common sense for the health of ecosystems definition. They tried to charac- terize this concept through empirical evidences. Their focus was to understanding how the health of ecosystems has been defined. Our work is similar to this work when we also raised a common understanding for the concept of health of ecosystems. However, the similarity is only this. Our focus is on research approaches to evaluate the health of ecosystems. We investigated deeply the existing approaches to discover key areas, practices, metrics, and tools that compose and support these approaches. Despite these three literature reviews present valuable information about the health of ecosystems, they did not investigate existing evaluation approaches and their details. All of studies cited here supported our work providing references for the definition of health of ecosystems and their indicators. 3 RESEARCH METHOD This study aims to obtain an overview of the research literature on evaluation of the health of software ecosystems. Performing the systematic review, we followed three phases: planning, conducting, and reporting based on a systematic review protocol. We conducted our study based on the guidelines of Kitchenham et al.[ 13]. The remainder of this section describes our protocol of the review. 3.1 Research Questions Identifying evaluating approaches for the health of software ecosys- tems, we defined a high-level research question:How have the health of software ecosystems been evaluated? This research question was divided into 6 specific research questions. Table 1 shows all research questions of this study with the motivation for each one. 3.2 Scope of Study The goal of this systematic review is to clarify the research area around evaluation of the health of software ecosystems. There are several studies in this area with different means for health concept. The scope of our study is focused on approaches used to evaluate 15', 'How Has the Health of Software Ecosystems Been Evaluated? A Systematic ReviewSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Table 1: Research Questions Nr. Research question Motivation RQ1 What definitions of the term “health of software ecosys- tem” exist? There are several definitions for the health of software ecosystems. We aim to find out which concepts each study is using in its design. RQ2 What are the approaches to evaluating the health of soft- ware ecosystems? With this question, we intend to iden- tify the approaches, methods, mod- els or frameworks which are used for evaluating the health of software ecosystems. RQ3 What key ares are empha- sized by existing approaches as having an influence on evaluating the health of soft- ware ecosystems? By a key area, we mean a characteristic that is common across a set of studies or processes that has an influence on the health of software ecosystems. RQ4 Which business, technical and social practices are used by existing approaches to evaluate the health of software ecosystems? Through this question, we hope to un- derstand which practices have a sub- stantial role in the software ecosystem health that is not addressed by the exist- ing approaches of evaluating the health nowadays. RQ5 What are the metrics used to measure the health of soft- ware ecosystems? In this question, we intend to identify metrics commonly used in evaluation of the health of software ecosystems. RQ6 Are there tools that support the evaluation process of the health of software ecosys- tems? There are several tools that support dif- ferent models or approaches of evalua- tion. In this sense, we would like to in- vestigate which tools are used to sup- port the evaluation processes of the health of software ecosystems. the health of software ecosystems. The concept of software ecosys- tems health must be included in the study. We did not consider approaches describing and assessing process structures of other nature for software ecosystems. There are some papers that do not present evaluation approaches explicitly defined. However, we also examined studies that addresses some metrics to evaluate some aspect of the health of software ecosystems. 3.3 Search Strategy Following the review protocol, we built our research string. We included the term “open source systems” to get studies that evaluate the health of open sources ecosystems. Our intention was also to capture studies that do not use explicitly the term “software ecosystem” in their description. Open source systems can also be classified as software ecosystems[12]. The goal was to encompass all types of software ecosystems. We adapted the string to work with each research database mechanism. The string is: (evaluation OR evaluate OR measurement OR evaluating OR measure OR assessment OR assess) AND (approach OR method OR framework OR model OR practices) AND (health OR healthy) AND (software ecosystem OR open source ecosystem OR open source systems) The following electronic databases were searched: • ACM Digital Library • IEEEXplore • ISI Web of Science • ScienceDirect • SpringerLink • Scopus In addition, we used the snowballing technique [22] and hand- searched relevant papers for two events: international conference proceedings on software business (ICSOB) and international work- shop on software ecosystems (IWSECO). The goal was to get papers not captured by the electronic research. Following the guidelines of Kitchenham et al.[13], two authors analyzed the papers separately. After, they cross checked their results to reduce the likelihood of bias. Also, during data extraction, another author performed a data extraction. He got a random sample of a primary studies to analyze. Following, their results cross-checked. The search process is shown in Fig. 1. Figure 1: Search process Initially, to conduct the search process, we used a tool called StArt (State of the Art through Systematic Review)2. This tool imported', 'in Fig. 1. Figure 1: Search process Initially, to conduct the search process, we used a tool called StArt (State of the Art through Systematic Review)2. This tool imported the files generated from databases, and also allowed the inclusion of papers manually. In the first phase, after getting all research papers from databases and conferences, we collected 2280 papers. In the phase 2, 70 papers were removed as duplicates. In the phase 3, after applying the inclusion/exclusion criteria, we excluded studies based on abstract, title, and key words, resulting in 34 papers to analyze. In the phase 4, we scanned the whole papers and excluded studies do not related to evaluation of the health of software ecosystems, based on the exclusion criteria. We also excluded one paper that we consider duplicated, because it described the same approach with similar data to another paper. The final result was 23 papers relevant for the detailed quality assessment, see A. In spite of the concept of “Health” for ecosystems having been introduced in 2002, only lately this topic has attracted attention of the research community. Figure 2 illustrates the number of papers by year of publication. Furthermore the Table 2 lists the paper distribution by source. 3.4 Exclusion and Inclusion Criteria Exclusion and inclusion criteria were defined to ensure that relevant papers will be analyzed. They were defined based on the research topic, singleness, language, period of time, and work format. We use the following inclusion criteria: (i) The study must explore practices, theory, approaches or issues related to evaluation of the health on software ecosystems; (ii) The study must be unique, i.e. 2http://lapes.dc.ufscar.br/tools/start_tool 16', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil S. Amorim et al. Figure 2: Timeline of publications in evaluation ap- proaches(number of studies). Table 2: Paper distribution Source Count Percent(%) International Conference on Software Business (IC- SOB) 3 13.00 International Conference on Management of Emer- gent Digital EcoSystems (MEDES) 1 4.35 European Conference on Software Architecture Work- shops (ECSAW) 2 8.70 International Conference on Research Challenges in Information Science (RCIS) 1 4.35 International Workshop on Building Sustainable Open Source Communities (OSCOMM) 1 4.35 International Conference on Malicious and Unwanted Software (MALWARE) 1 4.35 International Workshop on Software Ecosystems (IWSECO) 1 4.35 International Conference on Open Source Systems (OSS) 2 8.70 ACM international workshop on Software-defined ecosystems (BigSystem) 1 4.35 International Conference on Software Technologies (ICSOFT) 1 4.35 International Symposium on Empirical Software Engi- neering and Measurement (ESEM) 2 8.70 European Network on Chaos and Complexity Re- search and Management Practice Meeting 1 4.35 Journal of Theoretical and Applied Electronic Com- merce Research 1 4.35 International Journal of Open Source Software and Processes 1 4.35 Information and Software Technology 1 4.35 International Journal of Web Information Systems 1 4.35 Harvard Business School 1 4.35 Journal Computer 1 4.35 Total 23 100 when a study has been published in more than one venue, the most complete version will be used; (iii) Papers must be written in English. In the other hand, we defined the following exclusion criteria: (i) Studies that do not address health and evaluation in software ecosystems are going to be excluded; (ii) Studies that mention health or evaluation in software ecosystems, but do not discuss any type of method, activity, experience, or approach concerning evaluation of software ecosystems health; (iii) Studies that were only available as abstracts or Power Point presentations; (iv) Short papers (less than three pages long); (v) Studies that were not published in the period between 2001 and 2016 are going to be excluded; (vi) Studies presented in languages other than English. 3.5 Quality Assessment One of the steps of Kitchenham’s guidelines is the quality assess- ment of the primary studies[ 13]. We developed a quality check- list to ensure that minimal parts of a research study are present in the primary study. The 10 quality criteria that we used are: purpose, methodology, motivation, technology definition, limita- tions, data collection, data analysis, sampling, sampling justification, and completeness. The quality checklist questions are available at http://www.professores.ifba.edu.br/simoneamorim/review/. First of all, our quality instrument was used to support the selec- tion process providing a detailed inclusion/exclusion criteria. The quality data were collected during data extraction using our quality checklist. The answers of the checklist could be “yes” or “no”. Three criteria (Q2, Q3, Q4) were used as the basis to support the decision of including or excluding a primary study. We assessed 23 studies for quality and did not exclude any paper. However, we observed that 4 papers had less than 50% of points in the quality assessment. The reason was that some papers only introduced a proposal, but they did not validate the evaluating approach. We decided to keep these studies in the systematic review because we consider the idea proposed relevant for our study goals. Table 3 shows the results of the quality assessment for each paper. Table 3: Results of the Quality Assessment Study Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Total% [S1] Y Y Y Y N Y Y Y Y Y 90 [S2] Y Y Y Y Y Y Y Y Y Y 100 [S3] Y Y Y Y Y Y Y Y Y Y 100 [S4] Y Y Y Y N N N N N N 40 [S5] Y Y Y Y Y Y Y Y Y Y 100 [S6] Y Y Y Y N Y Y Y Y Y 90 [S7] N Y Y Y N N N N N Y 40 [S8] Y Y Y Y Y Y Y Y Y Y 100 [S9] Y Y Y Y Y Y Y Y Y Y 100 [S10] Y Y Y Y Y Y Y Y Y Y 100', '[S5] Y Y Y Y Y Y Y Y Y Y 100 [S6] Y Y Y Y N Y Y Y Y Y 90 [S7] N Y Y Y N N N N N Y 40 [S8] Y Y Y Y Y Y Y Y Y Y 100 [S9] Y Y Y Y Y Y Y Y Y Y 100 [S10] Y Y Y Y Y Y Y Y Y Y 100 [S11] N Y Y Y N N N N Y Y 50 [S12] Y Y Y Y Y Y Y Y Y Y 100 [S13] Y Y Y Y Y Y Y Y Y Y 100 [S14] Y Y Y Y N Y Y Y N Y 80 [S15] Y Y Y Y Y N Y Y Y Y 90 [S16] Y Y Y Y Y Y Y Y Y Y 100 [S17] N Y Y Y N N N N N Y 40 [S18] Y Y Y Y N Y Y Y Y Y 90 [S19] Y Y Y Y Y Y Y Y Y Y 100 [S20] Y Y Y Y Y Y Y Y Y Y 100 [S21] Y Y Y Y Y Y Y Y Y Y 100 [S22] Y Y Y Y N Y Y Y Y Y 90 [S23] N Y Y Y N N N Y Y N 40 3.6 Data Extraction In this phase, we extracted data from 23 primary studies and in- cluded data in a predefined extraction form. The data included were 17', 'How Has the Health of Software Ecosystems Been Evaluated? A Systematic ReviewSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil related to bibliography, objectives, problems, results, and the an- swers of the 6 research questions. The data extraction available at http://www.professores.ifba.edu.br/simoneamorim/review/. 3.7 Synthesis of Findings According to Cruzes et al. the research synthesis is the process of summarize, integrate, combine, and compare the findings of differ- ent studies on a specific topic or research question. This process is the heart of systematic reviews and much attention must be given to choose the appropriate method of synthesis[ 4]. Based on the nature of our research questions, we chose the method of narrative synthesis[20]. Narrative synthesis is a form of story telling based on evidence as a way of assuring through trustworthy arguments the obtainment of plausible outcomes. In this study, we adopted the narrative synthesis based on guidelines provided by Popay et al.[18]. The results of this narrative synthesis is described in the next section. 4 FINDINGS This section presents the results to answer each research question listed in Table 1. 4.1 RQ1: What definitions of the term “health of software ecosystem” exist? From the studies found that reported some kind of evaluation of the health of software ecosystems, we gathered several different concepts of health adopted by them. In 2015, Hyrynsalmi conducted a systematic review that reported different concepts for ecosystem health[9]. Additionally, Manikas and Hansen discussed the types of components for health of ecosystems[ 15]. These work cover a large range of possible definitions. However, to contextualize and improve the understanding of our results, we also provide the health concepts found in our primary studies. The first health concept among primary studies comes from Ian- siti and Levien. They consider health as the ability to grow the ecosystem and keep it attractive for all members of the community. They also provide three health indicators: robustness, productiv- ity, and niche creation[10]. Moreover, we captured the concept of Manikas and Hansen. They state that software ecosystem health is composed of the health of their components. The interaction among these components affects the entire software ecosystem. Finally, the general concept adopted to open source software ecosystems is the survivability over time. The majority of the authors consider that longevity is synonymous of good health for a software ecosystem. In summary, several papers provide varying views for the health concept. They performed some kind of health evaluation regarding these concepts. However, we found three papers that evaluate the health, but we cannot identify exactly what the term “health” means to them. This diversity of concepts influences the arrangement of the elements that compose the software ecosystem evaluating approaches. Table 4 shows the concepts and the papers related to them. Based on the concepts presented by studies, we can identify the main features of the health of software ecosystems. They are: Table 4: Concepts of Health of Software Ecosystems Identifier Health Concept Studies C1 A healthy software ecosystem provides durably grow- ing opportunities for its members and for those who depend on it. They have three indicators for ecosys- tem health: robustness, productivity, and niche cre- ation. [S9] [S22] [S18] [S4] [S2] [S20] [S12] [S15] C2 The software ecosystem health is the ability of the ecosystem to endure and remain variable and pro- ductive over time. This health concept is composed of elements interacting to build the health for the whole ecosystem. They are: each individual actor, network of actors, each individual software component, plat- form, software network, and orchestrator . [S5] [S11] C3 The health for OSS software ecosystems is composed of the ability of the project to survive throughout', 'form, software network, and orchestrator . [S5] [S11] C3 The health for OSS software ecosystems is composed of the ability of the project to survive throughout time. This means the survivability of the ecosystem for the next several time periods . [S1] [S6] [S21] [S19] [S21] [S10] [S16] [S17] C4 Ecosystem health is defined as “long-term financial well-being of the business ecosystem and the long- term strength of the network” . [S3] [S13] C5 Ecosystem health refers to the global condition of an ecosystem, provides a powerful theoretical and prac- tical framework for monitoring system activity, iden- tifying and predicting areas for improvement, and evaluating changes in ecosystems . [S8] C6 No concept presented . [S7] [S14] [S23] productivity, attractiveness, survivability, longevity, successful, im- provement, connectedness, interactions, and dependencies. Com- bining these features, we can generate a synthesized concept for the health of software ecosystems: “A healthy software ecosystem has the capacity of keeping their productivity and attractiveness, fac- ing problems, disruptions and junctions. At the same time, they also monitor and implement advances of their strategies to achieve the success over time” . This success should include all their internal elements considering their interactions and dependencies. 4.2 RQ2: What are the approaches to evaluating the health of software ecosystems? This research has identified 23 studies reported to deal with health evaluation for software ecosystems. The goal of this study is to gather the main characteristics of the primary studies considering different perspectives such as issues addressed by them, empirical research methods used, evaluation scenarios, studied objects, and evidence validated in some real-world software ecosystem. The scope of this study does not include a detailed analysis and discus- sion on the different aspects of the published evaluation. Table 5 shows the characteristics of the approaches. We observed that only 5 approaches are formally defined as an entire evaluation proposal for an ecosystem. Other 18 approaches evaluate only some kind of feature or health metric for software ecosystems. Based on data from primary studies, we observed that the evalu- ation scenarios are composed mainly by open source ecosystems. Only 7 studies are conducted in scenarios with commercial software ecosystems. Only an unique scenario is applied to commercial and open source ecosystems at the same time. Open source ecosystems 18', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil S. Amorim et al. Table 5: Overview of the approaches Study Software Ecosystem Type Domain Issues Addressed Name Validated S1 Vuforia Commercial Augmented Reality The developers network and applications yes S2 Axis Commercial Embedded systems Governance activities yes S3 Azure, Cloud Foundry, dot- Cloud, Engine Yard, Google App Engine, Heroku, Node- jitsu and OpenShift OSS PaaS providers Developer and Project activities yes S4 Creation, monitoring and evaluation for digital business ecosystems no S5 Apache Cordova OSS Mobile framework Software network; Keystone and dominator ac- tivities yes S6 Markmail, Jira and GIT OSS Service Oriented Computing and Quality of Service Performance indicators SALMonOSS partially S7 OSS Interactions among stakeholders no S8 Antivirus Commercial Antivirus Diversity, stability, and activity yes S9 WordPress, Joomla and Dru- pal OSS Content Management Sys- tem Health metrics yes S10 Topcased and Papyrus OSS Model Driven Development Activity, company influence in ecosystems, and interaction between ecosystems yes S11 Cytoscape Consortium OSS Genetics and biology Health metrics yes S12 GNOME OSS Linux desktop environment Quality of the platform, community, and network QuESo partially S13 Magento, PrestaShop, and WooCommerce Commercial E-commerce systems Health metrics yes S14 90 OSS projects OSS OSS Social network analysis yes S15 Dutch IT companies Commercial IT systems Health metrics yes S16 Nagios OSS IT infrastructure Social network activity yes S17 OSS OSS systems Sustainability, process maturity, and mainte- nance capacity QualOSS no S18 Apple, Google, BlackBerry and Nokia Commercial and OSS Mobile systems Business process and Health metrics yes S19 Apache Lenya, Apache log4J, Apahce Excalibur, and Apache OJB OSS OSS systems Health metrics FOSSDA yes S20 Python, Gnome, Wordpress, Joomla, Drupal OSS OSS systems Health metrics OSEHO yes S21 Apache Tomcat, Apache HTTP Server, Apache Xindice, and Apache Slide OSS OSS systems Health metrics yes S22 Commercial IT systems Health metrics partially S23 Commercial OSS systems Relationship among stakeholders no have free data available increasing the number of software ecosys- tems evaluated. Issues addressed by evaluations approaches have different focus, but the majority are concentrated in evaluating health metrics. These metrics vary in accordance with the author. Often they mention the health indicators proposed by Iansiti and Levien[10] and aggregate other metrics to complement their study. Another focus addressed by studies are social activities and network influencing the health. Moreover, we observed that almost all approaches are validated by data collected in real-world ecosystems. Only 4 approaches do not have any kind of validation. They are studies that propose a model to be built in the future. For our understanding, a validated approach present some data to illustrate or prove the metric pro- posed by the study. The majority of the studies evaluate a set of health metrics considering some aspects of the health concept. No approach evaluates the health for the entire ecosystem. There are only two approaches that introduce this proposal, but the studies are just proposals without data and research methods. Other stud- ies present a wealth of data such as the OSEHO [S20], FOSSDA [S19], and Wahyudin’s work [S21]. However, they also explain the challenges that prevent performing a complete evaluation of the health of ecosystems. Regarding empirical methods, we found 5 types of empirical approaches used in primary studies. We gathered the types of em- pirical studies from the work of Easterbrook et al.[5] and Zannier et al.[23]. Our results pointed out that Experience and Example appli- cation are the most frequently used means of evaluation followed by Case studies . Three papers indicated that this research topic gained the attention of the researchers. Lately, there are different', 'cation are the most frequently used means of evaluation followed by Case studies . Three papers indicated that this research topic gained the attention of the researchers. Lately, there are different evaluation approaches been developed for the health of ecosystems. Table 6 shows different empirical research methods used in the primary studies. 4.3 RQ3: What key areas are emphasized by existing approaches as having an influence on evaluating the health of software ecosystems? Key areas can be defined as sets of fields investigated and included in the evaluation of the health of ecosystems. It is important to know 19', 'How Has the Health of Software Ecosystems Been Evaluated? A Systematic ReviewSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Table 6: Empirical Research Methods Empirical Methods Concepts Studies Case Study An empirical inquiry that investigates a contempo- rary phenomenon within its real-life context, espe- cially when the boundaries between phenomenon and context are not clearly evident [5]. [S2] [S10] [S18] Discussion Provides some qualitative, textual, opinion-oriented evaluation. E.g. compare and contrast, oral discus- sion of advantages and disadvantages [23]. [S23] Example applica- tion Describes an application and provide an example to assist in the description, but the example is ""used to validate"" or ""evaluate"" as far as the authors sug- gest[23]. [S5] [S6] [S12] [S15] [S19] [S20] [S22] Experience Relates the use of a previously-reported tool or tech- nique in a practical software project. The conclu- sions are supported by comparative data and/or sta- tistics[21]. [S1] [S3] [S8] [S9] [S11] [S13] [S14] [S21] Proposal Presents and justifies the need to study a research problem and to present the practical ways in which this research should be conducted [14]. [S4] [S7] [S17] the directions and subjects that researchers are considering to build the evaluation approaches. An analysis of the studies showed 4 key areas addressed by research: productivity, community, business, and quality. Among the main topics studied by authors, productivity in the community was the most explored. Productivity is one of the health indicators introduced by Iansiti and Levien[10]. Other authors presented different methods to measure the productivity of software ecosystems. They used several metrics to estimate a value for productivity [S1] [S2] [S3] [S5] [S7] [S9] [S10] [S11] [S19] [S20] [S21]. Another key area was raised frequently in the primary studies - issues of the community such as satisfaction, relationship, diversity, and roles in the community. These issues addressed many metrics that represented part of the health of software ecosystems [S1] [S3] [S7] [S19] [S10] [S14] [S16] [S20] [S23]. Some studies addressed issues related to business side such as market share, governance, financial management, and others[S2] [S4] [S9] [S20] [S22]. Some studies also addressed performance indicators, process maturity and sustainability as well as quality issues[S6] [S12] [S17]. Finally, we cannot identify specific key areas in some studies [S8] [S13] [S15] [S18]. 4.4 RQ4: Which business, technical and social practices are used by existing approaches to evaluate the health of software ecosystems? Jacobson states that a practice provides a way to systematically and verifiable address a particular aspect of a problem . They address a particular aspect of a problem, instead of addressing the entire problem[11]. We consider that practices have directly influenced the health of software ecosystems. A practice produces a result, good or bad, that can be expressed by a metric. The way to guide a software ecosystem to achieve a good health probably will be through practices. However, practically all approaches found in primary studies directed their efforts to evaluate the health by metrics. Only two studies included practices directly in their evalu- ation [S2] [S17]. In [S2], Wnuk et al. investigated what governance activities can improve the health of ecosystems. They connected governance and health, considering governance as all activities of management of the ecosystem[12]. Aside from that, health is the capacity of growing and prosperity[10]. The governance is used to achieve health. They classified practices in accordance with health indicators defined by Iansiti and Levien[10]. We describe these 19 practices below: (1) Robustness - Create partnership model, Do marketing, Grow profits, Establish partner development programs, Form alliances, Stabilize APIs, Raise entry barriers, Make partners explicit, and Propagate operation knowledge.', 'Grow profits, Establish partner development programs, Form alliances, Stabilize APIs, Raise entry barriers, Make partners explicit, and Propagate operation knowledge. (2) Productivity - Organize developer days, Collaborative marketing, Create sales partner program, and Create new sales channels. (3) Niche Creation - Expand applicability, Make strategy ex- plicit, Create API, Co-development, Develop complemen- tary platforms, and Develop new business models. An another study, [S17] proposed to include the health evalu- ation into the process maturity evaluation for ecosystems. They described some processes that will be covered by the proposed model. Some example of these processes are: change submission and review, peer review of changes, propose significant enhance- ments, test the programs produced by the project, and plan releases. The authors did not describe the detailed practices in these pro- cesses. Furthermore, they did not explain how the processes would be used to evaluate the maturity of a software ecosystem. In sum- mary, practices have an influential role in the health of ecosystems. In addition, this role is not addressed by existing approaches for evaluating the health nowadays. 4.5 RQ5: What are the metrics used to measure the health of software ecosystems? The majority of primary studies evaluate the health of ecosystems using metrics. There are tens of metrics defined by different au- thors, including a systematic review conducted by Franco-Bedoya et al.[S12]. This study aimed to know what measures or attributes are defined to assess or evaluate open source software ecosystems. They found 68 different measures used to build their evaluating model. Practically, all primary studies presented some kind of met- ric in their evaluation approach. We observed that some metrics are repeated in other studies, or exist similar metrics with different names. Some metrics have the same name, but they are calculated per different period of time (day, week, month, year). Other metrics were defined specifically for the ecosystem context or approach. These metrics cannot be easily operationalized for other ecosys- tems. Answering this RQ5, we raised 211 metrics from all primary studies. We started working from [S12] study and included met- rics captured in other primary studies. We did not distinguish the type of ecosystem that the metric can be applied. As well as, we did not consider if these metrics can be measured in the prac- tice of real-world ecosystems. Metric’s names were kept in ac- cordance with the original study to avoid misunderstanding. Many studies introduced metrics and focused on measuring their val- ues in the ecosystem. However, we observed that there is a lack of critical work to investigate issues such as: which metrics are 20', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil S. Amorim et al. better to be applied in the health evaluation, which metrics can be really measured in real-world ecosystems, which implications of the interactions among these metrics, how a metric can influ- ence another metric, and so on. There are many open research issues regarding the work with health metrics. Table 7 shows an example for 5 health metrics. All metrics collected are available at http://www.professores.ifba.edu.br/simoneamorim/review/. Table 7: Health Metrics for Evaluation Approaches Name Description Studies Files per Version Number of files per version. [S12] Files Changed Number of files that has been changed. [S12] Member Effort The effort of member m in community c. [S12] Value Creation The overall value of new options created. [S20] Zeta Model Bankruptcy classification score model. [S12] 4.6 RQ6: Are there tools that support the evaluation process of the health of software ecosystems? There are many ways of performing an evaluation of software ecosystems. Existing tools can help and make the process of eval- uation faster. We gathered from the primary studies tools used to support the evaluation process. There is not a tool that support the whole evaluation process. We observed that 9 primary studies described explicitly some kind of tool used in some point of the evaluation process. The majority of tools perform data extraction from repositories. Many tools were developed by the authors based on GIThub3 repositories from which they extracted data. One study, [S23], extracted data from a website specialized to analyze open source projects, the FLOSSmole4. It also used the CVSAnalY 5 to extract statistical information out of CVS repositories. Furthermore, [S19] study used the ProMonCo, a project monitoring cockpit that provides integrated indicators to analyze the project status[2]. In summary, the results for Question RQ6 show that only 4 tools devel- oped by third parties were used, and 5 authors developed their own tool. None of them informed any functionality of these tools. Other work did not describe anything about tools. There are not tools to support important evaluation issues such as: data analysis and data visualization, conducting and maintenance of the evaluation, and even provide post-evaluation activities. Table 8 shows the studies and tools used in the evaluation process. 5 DISCUSSION This section discusses and summarizes contributions of this sys- tematic review to the software ecosystems research community. 5.1 Summary of Study Findings The evaluation of the health of software ecosystems has produced few approaches. The majority of the approaches expect some kind of rigorous and formal evaluation. Quite a few studies provide some data validation, other focus only in parts of the ecosystem, 3https://github.com/ 4http://ossmole.sourceforge.net 5http://cvsanaly-web.stage.tigris.org/ Table 8: Tools used in the evaluation process Study Tools [S1] Import.io(https://www.import. io/) [S3] Authors developed their own tool based on Ruby [S6] Authors developed their own tool [S8] Microsoft Software Removal Tool [S9] Authors developed their own tool based on Java and PH [S13] Authors developed their own tool based on Python and Scrapy(https://scrapy.org/) [S19] Project Monitoring Cockpit (ProMonCo)[2] [S21] Authors developed their own tool for mining the web-based devel- opers mailing list [S23] FLOSSmole and CVSAnalY and many of them do not present a “formal” evaluation approach. A large majority of the studies can be categorized as “example application” or “experience”. They present a lack of certain pieces of information to be considered a complete study. They propose several health metrics and show data extracted for these metrics. However, they do not show how these metrics can be part of a complete evaluation approach. They do not also exhibit the health state of the entire ecosystem.', 'However, they do not show how these metrics can be part of a complete evaluation approach. They do not also exhibit the health state of the entire ecosystem. The mass of the studies are in early stages of development, ex- plaining the lack of robust assessments. We have identified only three complete approaches: FOSSDA [S19], OSEHO [S20], and Wahyudin’s approach [S22]. Other approaches are focused on mea- suring specific health aspects, or define only a proposal. However, they did not present data evaluation. Furthermore, the results also exhibit a general lack of studies replication. Many studies use con- cepts proposed by other authors, but they did not perform a repli- cation of the original study. Two studies, [S11] and [S13], present a partial application of the original study. However, they also intro- duced extensions and discussed issues with the primary approach. Regarding the studies quality, some topics were not well de- scribed. Aspects such as issues of bias, limitation, validity, and reliability were not always addressed. None of them presented and explained data collection. We consider that these topics should be treated by the studies, mainly to avoid the risk of bias, subjectivity and/or confounding. Despite the quality assessment of the studies 78% present more than 80% of quality (see table 3). In spite of the problems and absence of information, the outcomes related here have several implications for researchers and practitioners. They can help to identify important research areas. So, the empirical software engineering community can be encouraged to improve the evaluation of the health of software ecosystems. 5.2 Implications for Research and Practice This work shows a set of research gaps and the necessity of more empirical studies for evaluating the health of ecosystems. In our opinion, issues raised by this work could guide new studies in the near future. We found some important research gaps such as the lack of practices, metrics and tools adequate to conduct the process of health evaluation. Starting with the proper concept of the health of software ecosys- tems, we found some discrepancies. There is not a common sense about health definition. The RQ1 captured various concepts adopted 21', 'How Has the Health of Software Ecosystems Been Evaluated? A Systematic ReviewSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil into primary studies. It would be useful to have an agreed upon concept on which to build solid evaluation approaches. Practically all studies have concentrated their efforts on getting metrics for health evaluation. They discovered several metrics, but there is not a consensus about which metrics are better to evaluate. Working with metrics has challenges described by Jansen’s work, in [S20]. There are some problems such as: difficulty in defining which met- rics can be measured, how to work with highly abstract metrics, which is the better way get value for a metric considering different ways of measuring, and so on. Other research areas on evaluating of the health of ecosystems are in the initial state, or even nonexistent. For example, the study of appropriate practices. One unique approach addressed explicitly practices on the health evaluation. Wnuk et al., in [S2], connected the health of ecosystems with some governance activities. They evaluated the governance model of an ecosystem. There are other studies[1, 12] that provide ecosystems governance models. These studies consider processes and practices to preserver and improve the ecosystem health. However, these models do not address health evaluations. They present models for ecosystems management that aim to achieve the health. The influence of the practices on the health of ecosystems should be more deeply explored. They inter- fere directly on the actions that guide the ecosystem. Metrics will only describe results of the direct application of practices. In addition, this study shows the absence of tools to support the evaluation process. Some approaches developed their own tool to extracted data. Few tools existing in the market were also used. The majority of the studies did not reveal what tools their used in the evaluation process. There is a necessity of robust assessment to gather adequate tools to support all parts of he evaluation. Data analysis tools, visualization tools, and a registry of evaluation tool are required. An evaluation in a ecosystem should not be done manually. This can bring many mistakes and bias to the evaluation. The construction of a suitable environment to support all stages of the process is essential to obtain a good quality assessment. For practitioners, this systematic review shows that many en- couraging studies of evaluating the health of ecosystems have been reported. Although the evaluations are immature, the review re- sults suggest that the metrics’ analysis can identify problems and improve their health. In order to increase the usefulness of the research in the practice, it is necessary to have a sufficient number of high quality studies. These studies will provide a solid basis of knowledge for problems investigation and improvement proposals affording the evaluation. Additionally, we emphasize that to per- form research in a large environments such as an ecosystem is a big challenge. Many people in different roles should be mobilized and with a large range of variables. Researchers and practitioners need to work together in collaboration to overcome challenges through the mutual support. This will contribute to achieve a good level of maturity in the future. 6 LIMITATIONS Although we have followed Kitchenham’s guidelines[13] to conduct this systematic review, the process can suffer by some limitations. Bias may have been introduced during the selection process of primary studies causing problems such as: errors in data extraction, omission on the approaches classification, and flaws in the quality assessment process. To avoid bias, the papers selection was perform by two researchers in parallel. After selection, the results were compared to resolve differences. In other phases, variations were discussed to find a consensus. In addition, there are some situations beyond our control that', 'compared to resolve differences. In other phases, variations were discussed to find a consensus. In addition, there are some situations beyond our control that can interfere in the research quality. For example, research string terms can be confused with terms with similar meanings but that belong to another research topic. Also, search engines quality could have influenced the completeness of the identified primary studies. In addition, there are papers with different problems such as: lack of sufficient details about the design and execution of the reported studies; some approaches that are not described adequately; issues of validity are not addressed; and data collection and analysis meth- ods do not reveal important details. These problems interfered in our process of analysis, so some subjective decisions were taken by the researchers to perform the data synthesis. To minimize these limitations, all the doubts were discussed by researchers to resolve all ambiguities. 7 CONCLUSION Software ecosystems have attracted increased attention in recent years. However, research in this area is awakening and many re- search gaps should be filled. Mainly concerning the health of soft- ware ecosystems and its implications. There has been no compre- hensive attempt to systematically investigate how the health of ecosystems has been evaluated and what approaches are available. In order to fill this gap, we conducted a systematic review to inves- tigate evaluating approaches for the health of ecosystems. In the beginning, we identified 2280 studies from searching the literature. 23 primary studies were qualified for the data extraction phase after being filtered by inclusion/exclusion criteria. Only 3 studies reported a complete health evaluation for software ecosys- tems. 5 studies were considered proposals to be developed in the future. The majority of the studies addressed partially some aspect of the health, exploring metrics into evaluation. In summary, we identified 6 different concepts for the health of software ecosystems, 211 health metrics, 5 approaches with a name formally defined, 4 key areas addressed by research, 19 governance practices, and 9 support tools were used by studies. Our systematic review revealed an absence of formal studies with scientific rigor as case studies and experiments. Some pub- lished studies did not show a robust explication of limitations or validation or a detailed data collection and analysis process. So, we perceived the immature state of the evaluation approaches. Our findings provide important contributions to academic researchers and practitioners. We described many research gaps that should be explored by researchers, as well we provided useful information about different aspects of the evaluation approaches. For practition- ers, we emphasize that an analysis of metrics can help to identify problems and improve the ecosystem health. However, there is the necessity of more high quality studies to consolidate the evalua- tion approaches. We believe that research in this area have high potential for industrial adoption and will bring plenty of benefits for all. 22', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil S. Amorim et al. A STUDIES INCLUDED IN THE REVIEW [S1] L. Soussi, Z. Spijkerman, S. Jansen, A case study of the health of an aug- mented reality software ecosystem: Vuforia, in: proceedings of the 7th International Conference on Software Business (ICSOB), 2016, pp. 145-152. [S2] K. Wnuk, K. Manikas, P. Runeson, M. Lantz, O. Weijden, H. Munir, Evaluat- ing the governance model of hardware-dependent software ecosystems a case study of the axis ecosystem, in: proceedings of the 4th International Conference on Software Business (ICSOB), 2014, pp. 212-226. [S3] G. Lucassen, K. van Rooij, S. Jansen, Ecosystem health of cloud paas providers, in: proceedings of the 4th International Conference on Software Business (ICSOB), 2013, pp. 183-194. [S4] A. D’Andrea, F. Ferri, P. Grifoni, T. Guzzo, Digital eco-systems: the next generation of business services, in: Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems (MEDES), 2013, pp. 40-44. [S5] K. Manikas, D. Kontogiorgos, Characterizing software activity: The influ- ence of software to ecosystem health, in: Proceedings of the 2015 European Conference on Software Architecture Workshops (ECSAW), 2015. [S6] M. Oriol, O. Franco-Bedoya, X. Franch, J. Marco, Assessing open source communitiesÃŢ health using service oriented computing concepts, in: Pro- ceedings of the 2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS), IEEE, 2014, pp. 1-6. [S7] B. Lundell, B. Forssten, J. Gamalielsson, H. Gustavsson, R. Karlsson, C. Lennerholt, B. Lings, A. Mattsson, E. Olsson, Exploring health within oss ecosystems, in: Proceedings of the 1stInternational Workshop on Building Sustainable Open Source Communities (OSCOMM), 2009, pp. 1-5. [S8] F. L. Levesque, A. Somayaji, D. Batchelder, J. M. Fernandez, Measuring the health of antivirus ecosystems, in: Proceedings of the 10th International Conference on Malicious and Unwanted Software (MALWARE), IEEE, 2015, pp. 101-109. [S9] G. L. Sonny Van Lingen, Adrien Palomba, On the software ecosystem health of open source content management systems, in: proceedings of the 5th International Workshop on Software Ecosystems (IWSECO), 2013, pp. 45-56. [S10] A. M. Jonas Gamalielsson, Bjrn Lundell, Open source software for model driven development: A case study, in: Proceedings of the 7th IFIP WG 2.13 International Conference (OSS), 2011, pp. 348-367. [S11] J. Y. Monteith, J. D. McGregor, J. E. Ingram, Proposed metrics on ecosys- tem health, in: Proceedings of the 2014 ACM international work- shop on Software-defined ecosystems (BigSystem), 2014, pp. 33-36. [S12] O. Franco-Bedoya, D. Ameller, D. Costal, X. Franch, Measuring the quality of open source software ecosystems using queso, in: Proceedings of the 10th International Conference on Software Technologies (ICSOFT), 2015, pp. 39-62. [S13] D. Alami, M. Rodrguez, S. Jansen, Relating health to platform success: Ex- ploring three e-commerce ecosystems, in: Proceedings of the 2015 European Conference on Software Architecture Workshops (ECSAW), 2015. [S14] S. Onoue, H. Hata, K. Matsumoto, Software population pyramids: the cur- rent and the future of oss development communities, in: Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineer- ing and Measurement (ESEM), 2014. [S15] E. den Hartigh, M. Tol, W. Visscher, The health measurement of a busi- ness ecosystem, in: Proceedings of the European Network on Chaos and Complexity Research and Management Practice Meeting (ECCON), 2006, p. 139. [S16] J. Gamalielsson, B. Lundell, B. Lings, The nagios community: An extended quantitative analysis, in: proceedings of the 6th International IFIP WG 2.13 Conference on Open Source Systems (OSS), 2010, pp. 85-96. [S17] M. Soto, M. Ciolkowski, The qualoss open source assessment model mea- suring the performance of open source communities, in: proceedings of', '[S17] M. Soto, M. Ciolkowski, The qualoss open source assessment model mea- suring the performance of open source communities, in: proceedings of the 3rd International Symposium on Empirical Software Engineering and Measurement (ESEM), 2009, pp. 498-501. [S18] P. R. J. Campbell, F. Ahmed, An assessment of mobile os-centric ecosystems, Journal of Theoretical and Applied Electronic Commerce Research 6 (2011) 50-62. [S19] T. Moser, S. Biffl, D. Winkler, W. D. Sunindyo, Analyzing oss project health with heterogeneous data sources, International Journal of Open Source Software and Processes 3 (2011) 1-23. [S20] S. Jansen, Measuring the health of open source software ecosystems: Beyond the scope of project health, Information and Software Technology 56 (2014) 1508-1519. [S21] D. Wahyudin, K. Mustofa, A. Schatten, S. Biffl, A. M. Tjoa, Monitoring the health status of open source web engineering projects, International Journal of Web Information Systems 3 (2007) 116-139. [S22] M. Iansiti and R. Levien, Keystones and Dominators: Framing Operating and Technology Strategy in a Business Ecosystem.Harvard Business School 03-061 (November 2002). [S23] K. Crowston, J. Howison, Assessing the health of open source communities, Journal Computer 39 (2006) 89-91. REFERENCES [1] Alfred Baars and Slinger Jansen. 2012. A Framework for Software Ecosystem Governance. (June 2012), 168-180 pages. [2] Stefan Biffl, Wikan Danar Sunindyo, and Thomas Moser. 2010. A Project Mon- itoring Cockpit Based On Integrating Data Sources in Open Source Software Development. (July 2010), 620-627 pages. [3] Jan Bosch. 2009. From Software Product Lines to Software Ecosystems. In Proceedings of the 13th International Software Product Line Conference (SPLC) (SPLC ’09) . 111–119. [4] Daniela S. Cruzes and Tore Dybå. 2011. Research synthesis in software engineer- ing: A tertiary study. Information and Software Technology 53 (2011), 440–455. https://doi.org/10.1016/j.infsof.2011.01.004 [5] Steve Easterbrook, Janice Singer, Margaret-Anne Storey, and Daniela Damian. 2008. Selecting Empirical Methods for Software Engineering Research. In Guide to Advanced Empirical Software Engineering , Forrest Shull, Janice Singer, and Dag I.K. Sjøberg (Eds.). Springer, Chapter 11, 285–311. https://doi.org/10.1007/ 978-1-84800-044-5_11 [6] Farnaz Fotrousi, Samuel A. Fricker, Markus Fiedler, and Franck Le-Gall. 2014. KPIs for Software Ecosystems: A Systematic Mapping Study. (June 2014), 194- 211 pages. https://doi.org/10.1007/978-3-319-08738-2_14 [7] Oscar Franco-Bedoya, David Ameller, Dolors Costal, and Xavier Franch. 2014. Measuring the Quality of Open Source Software Ecosystems Using QuESo. (Au- gust 2014), 39-62 pages. https://doi.org/10.1007/978-3-319-25579-8_3 [8] Abhimanyu Ghoshal. 2016. Microsoft has more open source contributors on GitHub than Facebook and Google. http://thenextweb.com/microsoft/2016/09/ 15/in-your-face-google/. (2016). Online; accessed 9 October 2016. [9] Sami Hyrynsalmi, Marko Seppänen, Tiina Nokkala, Arho Suominen, and Antero Järvi. 2015. Wealthy, Healthy and/or Happy - What does ’Ecosys- tem Health’ Stand for? (June 2015), 272-287 pages. https://doi.org/10.1007/ 978-3-319-19593-3_24 [10] M. Iansiti and R. Levien. 2002. Keystones and Dominators: Framing Operating and Technology Strategy in a Business Ecosystem. Harvard Business School 03-061 (November 2002). [11] Ivar Jacobson, Pan Wei Ng, and Ian Spence. 2007. Enough of Processes - Lets do Practices. Journal of Object Technology 6, 6 (July-August 2007), 41–66. [12] Slinger Jansen and Michael A. Cusumano. 2013. Defning Software Ecosystems: A Survey of Software Platforms and Business Network Governance. In Software Ecosystems: Analyzing and Managing Business Networks in the Software Industry , Slinger Jansen, Sjaak Brinkkemper, and Michael Cusumano (Eds.). Edward Elgar Publishing, Chapter 1, 13–28. https://doi.org/10.4337/9781781955635.00008', 'Slinger Jansen, Sjaak Brinkkemper, and Michael Cusumano (Eds.). Edward Elgar Publishing, Chapter 1, 13–28. https://doi.org/10.4337/9781781955635.00008 [13] Barbara Kitchenham and Stuart Charters. 2007. Guidelines for performing System- atic Literature Reviews in Software Engineering . Technical Report EBSE 2007-001. Keele University and Durham University Joint Report. [14] USC Libraries. 2005. Research Guide - Organizing Your Social Sciences Re- search Paper: Writing a Research Proposal. http://libguides.usc.edu/writingguide/ researchproposal. (2005). Online; accessed 7 October 2016. [15] Konstantinos Manikas and Klaus Marius Hansen. 2013. Reviewing the Health of Software Ecosystems - A Conceptual Framework Proposal. (June 2013), 33- 44 pages. https://doi.org/10.1.1.415.7578 [16] D. Messerschmitt and C. Szyperki. 2003. Software Ecosystem - Understanding an Indispensable Technology and Industry . MIT Press Cambridge. [17] Microsoft. 2016. Openness at Microsoft. https://www.microsoft.com/en-us/ Openness/OpennessAtMicrosoft. (2016). Online; accessed 9 October 2016. [18] Jennie Popay, Helen Roberts, Amanda Sowden, Mark Petticrew, Lisa Arai, Mark Rodgers, Nicky Britten, Katrina Roen, and Steven Duffy. 2006. Guidance on the conduct of narrative synthesis in systematic Reviews. A Product from the ESRC Methods Programme . Technical Report. Lancaster University. https://doi.org/10. 13140/2.1.1018.4643 [19] Transparency Market Research. 2013. Global Hadoop Market – Industry Analysis, Size, Share, Growth, Trends and Forecast 2012 – 2018. White Paper. (August 2013). http://www.transparencymarketresearch.com/hadoop-market.html [20] M. Rodgers, A. Sowden, M. Petticrew, L. Arai, H. Roberts, N. Britten, and J. Popay. 2009. Testing methodological guidance on the conduct of narrative synthesis in systematic reviews. Evaluation 15 (2009), 047–071. https://doi.org/10.1177/ 1356389008097871 [21] Mary Shaw. 2003. Writing Good Software Engineering Research Papers. (June 2003), 726-736 pages. https://doi.org/10.1007/s10009-002-0083-4 [22] Claes Wohlin. 2014. Guidelines for snowballing in systematic literature studies and a replication in software engineering. (May 2014), 321-330 pages. https: //doi.org/10.1145/2601248.2601268 [23] Carmen Zannier, Grigori Melnik, and Frank Maurer. 2006. On the Success of Empirical Studies in the International Conference on Software Engineering. (may 2006), 20-28 pages. https://doi.org/10.1145/1134285.1134333 23']","['FINDINGS ON THE EVALUATION OF THE HEALTH OF SOFTWARE ECOSYSTEMS This briefin reports scieitfc evideices oi fidiins ii iiitatves of evaluaatoi the healuth of softare ecosystems. FINDINGS \uf0b7 The evaluatin if the health if siftaae  ecisystems has paiduced fet  appaiaches. The majiaity if the  appaiaches expect sime kind if aigiaius  and fiamal evaluatin.  \uf0b7 Quite a fet studies paivide sime data  validatinn ithea ficus inly in paats if the ecisystemn and many if them di nit  paesent a “fiamal” evaluatin appaiach.  \uf0b7 The studies paipise seveaal health  metaics and shit data extaacted fia these metaics. Hitevean they di nit shit hit  these metaics can be paat if a cimplete  evaluatin appaiach.  \uf0b7 Ni ine study exhibits the health state if  the entae ecisystem. The mass if the  studies is in eaaly stages if develipmentn  explaining the lack if aibust assessments. \uf0b7 Only thaee studies paesent cimplete  appaiaches if evaluatin if the health if  Siftaae Ecisystems. \uf0b7 Sime evaluatins aae ficused in  measuaing specifc health aspectsn ia  defne inly a paipisal. Hitevean they did  nit paesent data evaluatin. \uf0b7 Theae is a geneaal lack if studies  aeplicatin. Many studies use cincepts  paipised by ithea authiasn but they did  nit peafiam a aeplicatin if the iaiginal  study. \uf0b7 Tti studiesn paesent a paatal applicatin  if the iaiginal study. Hitevean they alsi  intaiduced extensiins and discussed  issues tith the paimaay appaiach.  \uf0b7 Regaading the studies qualityn sime tipics teae nit tell descaibed. Aspects such as  issues if biasn limitatinn validityn and  aeliability teae nit altays addaessed.  Nine if them paesented and explained  data cillectin.  \uf0b7 Despite if the quality assessment if the  studies 78% paesent miae than 80% if  quality. Theae aae aeseaach gaps such as the lack if  paactcesn metaics and tiils adequate ti cinduct  the paicess if health evaluatin. Let’s see: \uf0b7 Theae is nit a cimmin sense abiut  health defnitin. It tiuld be useful ti  have an agaeed upin cincept in thich ti build silid evaluatin appaiaches.  \uf0b7 Paactcally all studies have cincentaated  theia efiats in getng metaics fia health  evaluatin. They disciveaed seveaal  metaicsn but theae is nit a cinsensus  abiut thich metaics aae betea ti  evaluate.  \uf0b7 Theae aae sime paiblems ti tiak tith  metaics such as: difculty in defning  thich metaics can be measuaedn hit ti  tiak tith highly abstaact metaicsn thich  is the betea tay get value fia a metaic  cinsideaing difeaent tays if measuaingn  and si in.  \uf0b7 Othea aeseaach aaeas in evaluatng if the  health if ecisystems aae in the inital  staten ia even ninexistent.  The infuence  if the paactces in the health if  ecisystems shiuld be miae deeply  expliaed.  \uf0b7 This study shits the absence if tiils ti  suppiat the evaluatin paicess. Sime  appaiaches develiped theia itn tiil ti  extaacted data. Fet tiils existng in the  maaket teae alsi used.  See Table 1. \uf0b7 Althiugh the evaluatins aae immatuaen  the aeviet aesults suggest that the  metaics’ analysis can identfy paiblems  and impaive theia health. In iadea ti  incaease the usefulness if the aeseaach in  the paactcen it is necessaay ti have a  sufcient numbea if high quality studies. Table 1 Tools used in the evaluaton process Who is this briefin  or? Siftaae engineeaing paacttineas thi tant ti make decisiins abiut  evaluatin if the health if Siftaae  Ecisystems based in scientfc  evidence. Where the fidiins come  rom? All fndings if this baiefng teae  extaacted faim the Systematc Reviet  cinducted by Simine da Silva Amiaim  et al.   What is iicluded ii this briefin? The main fndings if the Systematc  Reviet. What is iot iicluded ii this briefin? Additinal infiamatin nit paesented in the iaiginal systematc aeviet.  Detailed infiamatin abiut the iaiginal  studies analyzed in the systematc  aeviet. To access other evideice briefins  oi software einiieeriin: htp:////ttt.lia.ufc.ba//ccbsif2017// For additoial ii ormatoi about: htp:////tiki.dcc.uba.ba//Aside//', 'aeviet. To access other evideice briefins  oi software einiieeriin: htp:////ttt.lia.ufc.ba//ccbsif2017// For additoial ii ormatoi about: htp:////tiki.dcc.uba.ba//Aside//  ORIGINAL RESEARCH REFERENCE Simine da Silva Amiaim et al. Hot Has the Healuth of Softare Ecosystems Beei Evaluaated? A Systematc Reviet. Paiceedings if SBES’17n Fiatalezan CEn Baaziln Septembea  20–22n 2017n 10 pages. htps:////dii.iag//10.1145//3131151.3131174  .']","**Title: Evaluating the Health of Software Ecosystems: Current Insights and Gaps**

**Introduction:**
This Evidence Briefing aims to summarize findings from a systematic review conducted by Amorim et al. (2017) on the evaluation approaches for the health of software ecosystems. As organizations increasingly adopt software ecosystem models, understanding their health is crucial for sustainability and growth. This briefing highlights the current state of research, identifies gaps, and suggests implications for practitioners.

**Main Findings:**
1. **State of Research:**
   - The review found that the field of software ecosystem health evaluation is still immature, with only 23 studies qualifying for detailed analysis.
   - Only three studies provided a complete evaluation of ecosystem health, while the majority addressed specific aspects or proposed initial frameworks.

2. **Diverse Definitions and Metrics:**
   - The term ""health"" lacks a universally accepted definition, leading to varied interpretations across studies. Key concepts include robustness, productivity, and longevity.
   - A total of 211 health metrics were identified, but there is no consensus on which metrics are most effective for evaluation.

3. **Evaluation Approaches:**
   - Five formal evaluation proposals were noted, but most studies focused on specific health metrics rather than comprehensive evaluations.
   - Evaluations predominantly targeted open-source ecosystems, with limited focus on commercial software ecosystems.

4. **Key Areas of Influence:**
   - The studies emphasized four key areas affecting ecosystem health: productivity, community dynamics, business practices, and quality metrics.
   - Productivity was the most explored area, with various methods proposed to measure it.

5. **Tools and Practices:**
   - Nine studies mentioned tools used for evaluation, but none provided comprehensive solutions for the entire evaluation process.
   - Practices influencing health were underexplored, with only two studies explicitly connecting governance activities to ecosystem health.

6. **Research Gaps:**
   - There is a need for more empirical studies that rigorously validate evaluation approaches.
   - The exploration of practices that can enhance ecosystem health remains limited.

**Who is this briefing for?**
This briefing is intended for researchers, software engineers, and practitioners involved in software ecosystem development and management. It provides insights into current evaluation practices and highlights areas for further investigation.

**Where the findings come from?**
The findings are derived from a systematic literature review conducted by Amorim et al. (2017), which analyzed 23 primary studies on software ecosystem health evaluation.

**What is included in this briefing?**
This briefing includes an overview of the current state of research on software ecosystem health evaluation, key findings regarding definitions, metrics, evaluation approaches, and identified research gaps.

To access other evidence briefings on software engineering, visit: [ESEG Evidence Briefings](http://cin.ufpe.br/eseg/evidence-briefngs)

**Original Research Reference:**
Amorim, S. D. S., Simas S. Neto, F., McGregor, J. D., de Almeida, E. S., & von Flach G. Chavez, C. (2017). How Has the Health of Software Ecosystems Been Evaluated? A Systematic Review. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017. https://doi.org/10.1145/3131151.3131174"
"[""Hearing the Voice of Developers in Mobile Software  Ecosystems  Awdren Fontão, Fabricio Lima,  Bruno Ábia  Federal University of Amazonas  Manaus-AM, Brazil  {awdren, fsl, abia}@icomp.ufam.edu.br  Rodrigo Pereira dos Santos  Federal University of the State of  Rio de Janeiro  Rio de Janeiro – RJ, Brazil  rps@uniriotec.br  Arilo Claudio Dias-Neto  Federal University of Amazonas  Manaus – AM, Brazil  arilo@icomp.ufam.edu.br  ABSTRACT  In a Mobile Software E cosystem (MSECO), there is no  direct  communication between the organizations that maintain mobile   platforms (e.g. Apple, Google , and Microsoft) and developers to  solve technical questions. Thus, Q&A repositories can serve as a  mechanism to understand and define strategies to support  developers. In this paper, we mined 13,515,636 posts from Stack  Overflow by identifying 1,568,377 questions related to Android,  iOS, and Windows Phone  platforms. Next, we performed   comparisons among th ose three MSECOs regarding: (i)  developers’ activity intensity, (ii) hot -topics (using Latent  Dirichlet al location algorithm) in all and more  commented/viewed questions , and (iii) relationship among  questions and official developer events. From the results , we  identified four key insig hts: recruiting, educ ating, and  monitoring strategies; barrier reduction;  management of  technology insertion; and fostering of relationships.   CCS CONCEPTS  • Software and its engineering  → Software creation and  management → Collaboration in software development  KEYWORDS  Software ecosystems,  mobile appli cation development, mining  software repository, Stack Overflow  ACM Reference format:  A. Fontão, F. Lima, B. Ábia, R. Santos, and A. Dias-Neto. 2017. Hearing  the Voice of Developers in Mobile Software Ecosystems. In Proceedings of  Brazilian Symposium on Software Engineering , Fortaleza, Ceará BRAZIL ,  September 2017 (SBES’17), 10 pages.  DOI: 10.1145/3131151.3131167  _______________________________________  Permission to make digital or hard copies of all or  part of this work for personal or  classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice and  the full citation on the first page. Copyrights for components of this work owned  by others than ACM must be honored. Abstracting with credit is permitted. To  copy otherwise, or republish, to post on servers or to redistribute to lists, requires  prior specific permission and/or a fee. Request permissions from  Permissions@acm.org.  SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil   © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503-5326-7/17/09$15.00   https://doi.org/10.1145/3131151.3131167    1 INTRODUCTION  The relationship among developers and an organization  responsible for a technological platform ( i.e. keystone) over a  common platform, which involves cooperation and competition,  is known as a Software Ecosystem  (SECO) [1] [3]. It serves  a  strategy to meet users’ de mands that is essential for the  maintenance of the keystone’s business [ 2]. In the mobile  application (app) scenario, this context refers to a specific type of  SECO, known as Mobile Software Ecosystem (MSECO) [4] [5].   The developer is an essential actor to sustain MSECO  contributions, such as apps and technical documentation  [1].  Such contributions are usually stored in an official , internal  MSECO repository, such as Android and Apple Developers  repositories. Moreover, there are links from MSECOs with  repositories outside the ecosystem ( i.e. external), such as code  (e.g. GitHub) [6] and questions and answers  (Q&A) repositories.  As an example of a n external Q&A repository, Stack Overflow  has a set of technical questions/a nswers that arise from the use  of APIs, SDKs, and development tools, among others [7].  Such external repositories help to maintain the interaction"", 'has a set of technical questions/a nswers that arise from the use  of APIs, SDKs, and development tools, among others [7].  Such external repositories help to maintain the interaction  among developers over a common platform, resulting in a set of  contributions and influencing directly or indirectly the  ecosystem as a whole [8]. Therefore, a Q&A repository such as  Stack Overflow has archived communications among ecosystem  developers and it can be used to investigate some MSECO  aspects, for example, developer engagement and code snippets.   Manikas [ 9] argues that more in -depth (instead of in-width)  studies are necessary. Focusing on a specific subset or type of  ecosystem in depth would arguably be a challenge that is easier  to tackle and bring results that are more realistic  rather than  wide ecosystem studies focusing on a single aspect (e.g.  architecture). Considering the existing literature reviews on  SECO [9] [11] [12], MSECO [ 5], and Q&A sites [ 13], there is no  indication of studies that investigate the use of a Q&A repository  to understand an ecosystem. However, there is an indication of  mining software repositories as a way to extract information  about the socio-technical perspective of a SECO [9].  Stack Overflow is a community of five million registered  developers and 3.9 billion  visits. This reality puts us at the  forefront of a research question: “ What can be understood from  the three main MSECOs ( i.e. Android, iOS, and Windows Phone)  based on tec hnical questions at Stack Overflow?”. It can help a  keystone to get  a good overview of the ecosystem providi ng  4', 'SBES’17, September 2017, Fortaleza, Brazil A. Fontão et al.      effective measurements to analyze the developers’ performance  [10] and to assist in developers’ engagement serving as an  instrument for ecosystem governance.  In our study, we mined Stack Overflow to investigate this  research question. Our work was inspired by studies performed  by Bajaj et al. [27] to analyze technical questions asked by web  developers on Stack Overflow, and Kochhar [ 14] to analyze  repositories to understand common discussion topics as well as  challenges faced by developers during testing.  The contributions of this paper is a set of  four key insights  that can help to understand  the involvement of developers in   MSECOs as follows : 1) commonly used tags in recently added  questions may serve as input to developer monitoring strategy;  2) the most viewed topics as well as  topics in which developers  are most committed to respond can  indicate a community of  experts who can help to reduce frequent barriers to participation  in MSECO; 3) questions posted in a Q&A repository next to an  official MSECO announcement period can help the keystone to  manage strategies to add new MSECO resources.   This paper is organized as follow s. Section 2 presents  background. Related work are discussed in Section 3. In Section 4  we describe  the empirical study. Section 5  presents r esults’  analysis while the threats to validity are discussed in Section 6.  Section 7 concludes the paper and points out future work.  2 BACKGROUND  2.1 Mobile Software Ecosystems  A MSECO consists in a category of SECO  [9] that comprises  several elements around the app [5]. An important part is the  relationship among elements (e.g.  developers, keystone , and  users) that result in technical (e.g. apps, sample codes) and non - technical (e.g. user reviews of an app) contributions [ 15]. The  activity of each developer in a MSECO is motivated by value  creation for both the developer and the ecosystem.  A MSECO from the knowledge perspective has a hybrid  business structure, i.e., the ecosystem support s both proprietary  and open source contributions [ 9]. For instance, in the Windows  Phone MSECO, the official support site indicates two sources  working as technical forums: MSDN Forums (proprietary) and  Stack Overflow (open source). As pointed out by Souza et al.  [16], Q&A sites as Stack Overflow support the interaction among  developers from hybrid ecosystems (e.g. Android, iOS or  Windows Phone, or simply WP, in this study).  From a technical dimension,  a large amount of data is often  readily available in those Q&A repositories, and the data is stable  and is not influenced by researchers [ 18]. In this scenario, a   method used to conduct empirical stud ies in ecosystem field is  mining software repositories [13].  2.2 Mining Questions & Answers Sites  Software repositories can be a valuable source of information  since they contain (or may allow to extract) information about  the technical and social perspect ives of the  project, such as the  sources of  developer communications [ 19]. Mining Software  Repositories (MSR) area focuses on uncovering useful  information about software by extracting and analyzing data  from different software repositories [ 20]. MSR appro aches have  been used for different goals, e.g.  analyses of contribution and  developer behavior. In this scenario, Questions & Answers  (Q&A) sites are an important object of analysis.  Q&A repositories are web, collaborative, social computing  platforms that aims at supporting crowdsourcing knowledge by  allowing users to post and answer questions. They not only  provide a platform for experts to share their knowledge and be  identified as community members , but also help newcomers to  solve their problems effecti vely [ 21]. According to Shah et al.  [22], the Stack Overflow would be an example.  2.3 Stack Overflow  Stack Overflow ( SO) is a community -driven Q&A website used', 'solve their problems effecti vely [ 21]. According to Shah et al.  [22], the Stack Overflow would be an example.  2.3 Stack Overflow  Stack Overflow ( SO) is a community -driven Q&A website used  by developers who post and answer questions related to  computer programming [ 21] (approximately 3.7M questions and  4.6M answers1). The questions and answers within the SO may  receive users’ votes (against/in favor of ). Such votes become  reputation points that allow developer to have some privileges ,  such as releasing restric tions on creating a public ation and   editing questions and answers from other users.  Another privilege mechanism involves the assignment of  badges i.e., developers’ achievements while using SO. Developers  can get badges from several activities, for instance, a developer  can receive a badge if he/she has asked a question that reached  more than a thousand visits.   Zagalsky et al. [24] identified that developers us es SO for  several reasons: (a) the ability to gain peer recognition; (b) its  rich and user -friendly interface; (c) answers are straight to the  point; (d) questions are usually answered faster on Stack  Overflow than on other forums ; and (e) it is easy to sear ch for  previous questions and answers.  2.4 Silhouette and LDA  Similar to some analyses based on SO mining, we used  unsupervised methods to extract topics from SO questions. Our  methodology is based on Latent Dirichlet allocation (LDA) [25],  a statistical t opic model used to automatically recover topics in  several domains from a corpus of text documents. We choose  LDA because it is able to model topics in large corpus ; in our  case, the body of developers’ questions related to MSECOs.  Moreover, in order to id entify the appropriate number of  topics (and not a random choice ) we applied a partitioning  technique called Silhouette [ 26]. Such number provided by  silhouette was applied as input to LDA . Each cluster is  represented by a so -called silhouette, which is based on the  comparison of its tightness and separation.  This silhouette shows  which objects lie well within their cluster, and which ones are  merely somewhere else in between those clusters.   The average silhouette wid th provides an evaluation of  clustering validity . It might be used to select an appropriated                                                                    1 http://stackoverflow.com/company/about . Accessed in May 01, 2017  5', 'Hearing the Voice of Developers in Mobile Software Ecosystems SBES’17, September 2017, Fortaleza, Brazil      number of clusters. Silhouette provides values in the range of -1  to 1, where 1 means that the samples belonging to the cluster are  far from the other clusters, 0 means that the division among the  clusters is already at the edge of the separation, and -1 means  that there is a possibility of some samples are ass igned to the  wrong cluster.  3 RELATED WORK  Bajaj et al. [27] presented a study of common challenges and  misconceptions among web developers, by mining related  questions over Stack Overflow. The authors used unsupervised  learning (LDA) to categorize the mined questions and define a  ranking algorithm to rank all the  Stack Overflow questions  based on their importance. The results indicated for example that  the overall share of web development related discussions  is  increasing among developers.  Barua et al. [28] used (LDA) to automatically discover the  main topics from Stack Overflow dataset (July 2008 to September  2010) regarding developer discussions. Their analysis allowed to  make a number of interesting observations: the topics of interest  to developers range widely from jobs and version control  systems to C# syntax ; questions in some topics lead to  discussions in other topics; and the topics becoming more  popular over time are web development (especially jQuery), apps  (especially Android), Git, and MySQL.   Rosen and Shihab [ 29] investigate what issues mobile  developers ask about using data from Stack Overflow (updated  in March 2013). The authors used LDA to summarize the mobile - related questions . Some findings were: app distribution, mobile  APIs, data management, sensors , and context. They focused on  identify the cha llenges facing mobile developers. The authors  motivate more research in this area as a way to improve mobile  development processes.   Considering the existing literature reviews on SECO [9] [11],  MSECO [ 5], and Q&A repositories [ 12] [13], there is no  indication of studies that investigate the use of a Q&A repository  to understand a n ecosystem (and specifically a MSECO) .  However, there is an indication of mining software repositories  as a way to extract information on the social-technical  perspective in  of ecosystems. This study also contributes to  evaluate another source of information to analyze SECO and its  elements (e.g. developers, repositories, platforms, and keystone).  4 EMPIRICAL STUDY  4.1 Research Questions  As a way to support the main research question, we defined a set  of sub -research questions . Our questions were inspired by the  studies of Bajaj et al. [3] (focus: web developers), Barua et al. [28]  (focus: testing), and Rosen  and Shihab [ 29] (focus : mobile  developers). We are currently investigating detailed insights  regarding how to identify and support MSECO developers’  governance mechanisms from Q&A repositories. Those insights  are very important to come up with rich information to aid  decision-making based on the huge amount of available data.  RQ1. What is the developer activity intensity from MSECO   data available in Stack Overflow?  Rationale: The answer for this RQ can help us to analyze how  developers’ activity is evolving in relation to number of questions,  number of answers, and response time . The activity intensity  corresponds to the frequency to which questions and answers are  posted, including the average time for topic answering.  RQ2. What are the hot -topics extracted from technical  questions asked by MSECOs’ developers?  Rationale: The answer for this RQ can help us to get an  overview of what topics are covered and whether there is a ny  difference among the analyzed MSECOs.  RQ3. What are the platforms’ questions on which developers  are more engaged?  Rationale: The answer for this RQ can help us to u nderstand  how much involvement in certain topics contributes to explore', 'are more engaged?  Rationale: The answer for this RQ can help us to u nderstand  how much involvement in certain topics contributes to explore  knowledge flow within a MSECO. As such, we can identify the  most committed developers based on the most commented/viewed  questions among them.  RQ4. Is there a ny relation between questions and official  events?  Rationale: We u se time series to identify the frequent  ecosystem questions  in order to u nderstand if topics have any  relation with official events, such as platform launch. As such, it is  important to know how to analyze the effect of external events in  the community.  RQ5. What is the ranking of number of badges received by  developers of each platform?  Rationale: The answer for this RQ can help us to  obtain  information on the MSECO developers’ badges as well as to explore  information about top developers.   4.2 Datasets  Stack Overflow makes its data publicly available in XML format  licensed under CC BY -SA 3.0 license. For our purposes, we use  posts.xml, which contains current posts’ text contents, as well as  the answers/view count, tags, favorite count, and creation date .  Our dataset contains information from Mar 26 , 2017  at 6:35.  Since the goal was to retrieve datasets from three MSECOs, we  performed t he mapping of tags that could represent Android,  iOs, and Windows Phone MSECOs . This analysis allowed us to  adopt the tags: android, windows-phone, and ios. In the next  section, for each resear ch question presented in Section 4.1 , we  analyze the results.  We pre-processed the textual content (Body) of the extracted  posts in four steps. First, we discarded any code snippets that are  present in the posts (i.e.  enclosed in <code> HTML tags), because  source code syntax (e. g. “if” statements and “for” loops)  introduces noise into the analysis phase. Next, we remove d all  HTML tags (e.g.  <p> and <a href=""..."">), since these are not the  focus of our analysis. Third, we remove d common English - language stop words such as “a”, “the” and “is”, which do not  help to create meaningful topics . We used Spark as a framework  that support s the analysis of big data.  Data mining procedure  was automated from the dataset construction to topic analysis.  6', ""SBES’17, September 2017, Fortaleza, Brazil A. Fontão et al.      5 RESULTS’ ANALYSIS  The Stack Overflow  dataset contains 13,515,636 rec ords. When  filtering by MSECO tags, a total of 1,568,377 (11.6%) records were  extracted in order to compose the dataset from January 2008 to  March 2017, containing data related to each MSECO: Android  62.9% (986,099), iOS 34.2% (535,876) , and Windows Phone 2.9%  (46,402). Part of the data obtained from each question dataset  and an analysis of the available data are presented in this paper.  At the end of each analysis, we present key insights as a set of  notes that can guide researchers to use data to study MSECOs  and that can be evaluated in future studies.  In order to ans wer some questions, we used LDA . To  generate an LDA model, we need to understand how frequently  each term occurs within each document. As such, we  constructed a document -term matrix and our  dictionary was   converted into a bag-of-words (a common representation used in  natural language processing and information retrieval). LDA was  applied to each dataset related to the questions and the  silhouette method was used to evaluate the quality of clusters.  5.1 (RQ1) What is the developer activity  intensity from MSECO data available in  Stack Overflow?  Regarding the number of questions by year (2008 to 2016) (Fig.1),  the dataset allowed us to define a growth function for each  MSECO as follows: Android : 𝑎(𝑥)  =  16284𝑥 +  9045.8; iOS:  𝑖(𝑥)  =  9245.1𝑥 +  2739.4 ; and Windows Phone: 𝑤(𝑥)  =  358.25𝑥 +  2669.8. The Android function 𝑎(𝑥) is 43% greater  than iOS  function and 97% greater than Windows P hone  function 𝑤(𝑥). The iOS function 𝑖(𝑥) is 96% greater than 𝑤(𝑥).    Figure 1: New questions by year.  We also analyzed the following null hypothesis “ There is no  difference between the amount of developers’ posts among  MSECOs”. The Mann-Whitney test was applied  to verify the  normality of the three samples with confidence level of 95%. We  identified that the samples follow the normal distribution. There  was a statistically significant difference among groups as  determined by one -way ANOVA, p = .001. A Tukey post hoc test  revealed that the amount of Windows Phone questions was  statistically significant lower than Android (93969± 22638, p =  .0096). There was no statistically significant difference between  the Android and iOS questions (p = .134), and between Windows   Phone and iOS questions (p = .096). The Windows Phone began  to be discontinued by Microsoft in 2015, which has affected the  community's involvement (Fig.1). We can perceive that Android  and iOS are the main MSECOs in the market.  Regarding the number of an swers by year (Fig.2), the dataset  also allowed us to defined a growth function for each MSECO as  follows: Android: 𝑎(𝑥)  =  31061𝑥 +  3728,1 ; iOS: 𝑖(𝑥)  =  19136𝑥 +  1017,7; and Windows Phone: 𝑤(𝑥)  =  869,07𝑥 + 2462,6 . Android function 𝑎(𝑥) is 38.4% greater than iOS  function and 97 .2% greater than Windows Phone function 𝑤(𝑥).  iOS function 𝑖(𝑥) is 95.3% greater than 𝑤(𝑥).      Figure 2: Number of answers by year.  We also analyzed the tags that represent questions that take  more time to be answered and the questions that are quickly  answered by each MSECO, as follows. We ranked 500 questions  that take more time to be answered and that are quickly  answered (each question has information about related tags and  time to answer) . Five Android Tags with more than 500  questions that take more time to be answered are described next:  android-syncadapter ( 26.4 hours ) – a service that  synchronizes data between an Android device and a server;   android-espresso (18.5 hours) – espresso is a library which is  used to write Android UI  (User Interface) tests; android-testing  (18.3 hours ) – Android tes ting framework  that provides an  architecture and tools to test at eve ry level from unit to  framework; google-drive-android-api ( 16.3 hours) – Drive"", '(18.3 hours ) – Android tes ting framework  that provides an  architecture and tools to test at eve ry level from unit to  framework; google-drive-android-api ( 16.3 hours) – Drive  Android API is a native API simplifying many common  associated tasks using D rive service on mobile devices;  and  android-source (15.5 hours) – questions about source code and  related themes: how to contribute and/or porting etc.  Five Android Tags with more than 500 questions that are  quickly answered are described next:  android-context (15 min) – interface to global information  about an app environment. It allows ac cess to up -calls for app - level operations such as launching activi ties, broadcasting and  receiving intents etc .; android-button (16.5 min) – this tag is  for questions about Buttons over Android platform . android- alertdialog (17 min) – a subclass of Dialog that can display one,  two or three buttons ; android-asynctask (19  min) -   AsyncTask enables proper , easy use of UI thread. AsyncTasks  should be ideally used for short operations (few seco nds, at  most); and android-textview (19 min) – Android user interface  component that displays text to the user.  7', ""Hearing the Voice of Developers in Mobile Software Ecosystems SBES’17, September 2017, Fortaleza, Brazil      Five iOS Tags with more  than 500 questions that take more  time to be answered are described next:   ios-app-extension (59.2 hours) – a feature introduced in iOS  8 that was created to do a specific task, such as to enable sharing  Safari pages through an app, or to display an interface in  Notification Center for an app; ios-ui-atomation (47.4 hours)  – this tag specifically focuses on using this functionality in the  iOS development, and questions related to scripts can be used to  automate interaction between user and app; facebook-ios-sdk  (12.9 hours ) – Facebook's SDK for developing Facebook - connected apps for iOS devices ; google-maps-sdk-ios ( 9.5  hours) – Google Maps SDK for iOS allows users to view and  interact with a Google map; and ios10 (5.9 hours) – iOS 10 is  the tenth version of Apple's iOS mobile operating system.  Five iOS Tags with more than 500 questions that are faster to  be answered are described next:   ios4 (35 min) – iOS 4 was made publicly available for iPhone  and iPod Touch on June 21, 2010 . It has been succeeded by ios5   (tag ios5 – 48 min) which was released on October 12, 2011; ios- autolayout (1 hour) – auto Layout dynamically calculates the  size and position of all the views in a view hierarchy, based on  constraints placed on those views;  ios-provisioning (1 hour) –  the process of preparing an app to run on an iOS device; and  ios6 (1.17 hours) – related to iOS platform that provides more  than 200 new features, including a new Maps app, Siri updates,  Siri for iPad (3rd generation) etc.  Five Windows Phone Tags with more than 500 questions that  take more time to be answered are described next:  windows-phone-voip ( 53.4 hours ) – related to Windows  Phone integration with phone services, and the ability to retrieve  incoming VoIP calls in the background using push messaging;  windows-phone-emulator (16.3 hours ) – Windows Phone  Emulator presents Windows Phone Interface on an Windows PC;  windows-phone-silverlight (8.5 hours) – Microsoft Silverlight  is a free web -browser plug -in that enables inter active media  experiences and  rich business apps; and windows-phone-8- emulator (8.3 hours ) and windows -phone-7-emulator ( 7.8  hours) – the emulator allows for the development and testing of  Windows Phone 7.x and 8 apps out of a hardware device.  Five Windows Phone Tags with more than 500 questions that  are faster to be answered  are related to Windows Phone  platforms versions: windows-phone-7 ( 1 hour), windows- phone-7.8 (1.1 hours), windows-phone-8 (2 hours), windows- phone-7.1 (2.2 hours), and windows-phone-7.1.1 (3.6 hours).  5.2 (RQ2) What are the hot-topics extracted  from technical questions asked by  MSECOs’ developers?  Regarding all the questions related to MSECOs , the number of  topics and silhouette value was obtained through the silhouette  method for each MSECO : Android – n=4 (0.64), iOS – n=4 (0.86)  e Windows Phone – n=3 (0.87). We used the number of topics as  input to LDA algorithm. Table 1 shows the results.  In the Android MSECO, questions related to Project topic  involve the basic of A ndroid projects , such a s starting new  projects, importing/exporting projects , and  creating/manipulating activities . User Interface  topic covers  questions about the placement, alignment and justification of  objects with respect to a c ontainer element. Questions related to  Exceptions topic  covers doubts related to a  condition that  requires deviation  from the A ndroid program's normal flow .  Finally, Notifications topic covers technical questions related to  a user interface element that a developer can display outside the  app's normal UI to indicate that an event has occurred. Users can  choose to visualize the notification while using other apps and  respond to it according to their convenience.    Table 1. Extracted topics from all questions"", 'choose to visualize the notification while using other apps and  respond to it according to their convenience.    Table 1. Extracted topics from all questions  Android Project file, project, activity, class  User Interface activity, view, layout, xml  Notifications service, device, data, notification,  Exceptions androidruntime, lang, thread, method  iOS User Interface cell, table, tableview,  uitableview  Data Binding view, data, controller, screen  Project xcode, project, device, iphone  WP Services wsdl, keys, resx, reminder  Data Binding page, data, xaml, control  Frameworks xna, dll, native, reflection    Regarding the iOS MSECO, the Data Binding topic covers  mechanisms used to synchronize a n UI with an underlying data  model. User Interface topic  in iOS covers the  user interface  control, and adaptation  to any size changes. Project topic  involves similar questions within Android community with  focus in the build of apps to hardware devices.  Questions in the Windows Phone MSECO have the following  topics: Services – that involves web and data services that use an  open XML -based language to describe their web -based API;  Data Binding – a connection/binding be tween UI and a data  object allows data flow between such tiers; and Frameworks –  questions referring to dynamic-link libraries, frameworks to  support game development, native functionalities of the system   etc.  There are common points in the intersection between topics  of different MSECO: data binding mechanisms , user interface  (UI) programming, and development support infrastructure. This  leads us to the following key insight:    Key Insight #1: The most commonly used tags in recently added  questions may in dicate the most frequent barriers faced by  developers willing to participate in a MSECO . This scenario can  serves a monitoring strategy to support a keystone in recruiting  and educating developers.  5.3 (RQ3) What are the platforms’ questions on  which developers are more engaged?  In order to analyze this research question, we define d two  perspectives: 1) to analyze engagement by number of answers of  developers who participate in MSECOs and Stack Overflow; and  8', 'SBES’17, September 2017, Fortaleza, Brazil A. Fontão et al.      2) analyze the engagement by number of views of a question, i.e.,  developers who do not participate in Stack Overflow but  visualize questions and include answers.  Table 2  shows five records with the highest numbers of  answers obtained from each platform dataset, and Table 3  presents the most viewed answ ers by developers. For each case  (NumAnswers and ViewCount), we used LDA method to identify  in which topics  developers are more engaged . In turn, the  amount of topic clusters was defined using the silhouette  algorithm.    Table 2. Questions ordered by number of answers  Android  R cannot be resolved - Android error  Why is Android emulator so slow? How can we speed up the  Android emulator?  ""Conversion to Dalvik format failed with error 1"" on external JAR  Developing for Android in Eclipse: R.java not regenerating  Close/hide Android Soft Keyboard  iOS  How to make an UITextField move up when keyboard is present?  What does this mean? ""\'NSUnknownKeyException\', reason: … this  class is not key value coding-compliant for the key X""  Placeholder in UITextView  Applications are expected to have a root view controller at the  end of application launch  How to change Status Bar text color in iOS 7  Windows Phone  Windows Phone 7 closes application  Windows Phone 8 emulator can\'t connect to the internet  Resources for Windows Phone 7 development  Unable to create the virtual machine  Windows Phone 8 Emulator not launching. Error code 0x80131500    Regarding developer engagement in Stack Overflow and the  most popular answered questions , the number of topics and  silhouette value w ere obtained through the silhouette method  for each MSECO: Android – n=4 (0.64), iOS – n=4 (0.86) , and  Windows Phone – n=3 (0.87). We used the  number of topics as  input to LDA algorithm. Results are shown in Table 4.  In Android, the most frequently answered topics were related  to: Data Binding – to write declarative layouts and minimize  glue code necessary to bind app logic and layouts; IDE – the use  of code e diting, debugging and performance tooling; User  Interface – to create a dynamic and multi -pane user interface to  encapsulate UI components and activity behaviors into modules  of activities; and Back-end infrastructure  – the use of a  platform that helps to grow user base and monetize the app.  In Windows Phone MS ECO, the topics were related to:   Event Handler  – the use of handling manipulation events  methods for processing touch input; IDE – tools to support the  app development, including the emulators and the use of  migration tools (WP7.X to WP8.X); and User Interface control  guidelines.  In iOS MSECO, some topics were the same of the previous  MSECOs. The other topics were: User Interface; Data Binding;  Notification Services – local and push notifications for keeping  users informed with relevant content, whether the app is  running in the background , or inactive; and Programming  Language – the use of Swift and Objective -C in XCode to  develop apps.    Table 3. Questions ordered by number of views  Android  Can\'t start Eclipse - Java was started but returned exit code=13  Get screen dimensions in pixels  Close/hide Android Soft Keyboard  What is the difference between ""px"", ""dp"", ""dip"" and ""sp"" on  Android?  R cannot be resolved - Android error  iOS  How can I develop for iPhone using a Windows development  machine?  What does this mean? ""\'NSUnknownKeyException\', reason: … this  class is not key value coding-compliant for the key X""  How to make an UITextField move up when keyboard is present?  How to change Status Bar text color in iOS 7  Vertically align text to top within a UILabel  Windows Phone .Net - DateTime.ToString(""MM/dd/yyyy HH:mm:ss.fff"") resulted in  something like ""09/14/2013 07.20.31.371""  Install Visual Studio 2013 on Windows 7  Can we install Android OS on any Windows Phone and vice versa,  and same with iPhone and vice versa?', 'something like ""09/14/2013 07.20.31.371""  Install Visual Studio 2013 on Windows 7  Can we install Android OS on any Windows Phone and vice versa,  and same with iPhone and vice versa?  How to Install Windows Phone 8 SDK on Windows 7?  How to trigger event when a variable\'s value is changed?    Table 4. Hot-topics – the most answered questions   Topic Words  Android Data Binding Android, Text, Java, API  IDE Studio, File, Adb, Device  User Interface Activity, Fragment, View, Image  Back-end Infrastructure Firebase, Data, Notification, Time  WP Event Handler Event, Develop, Silverlight, Visual  IDE WP7, file, service, emulator  User Interface Listbox, control, page, image  iOS  User Interface Swift, View, Change, Image  Data Binding Data, View, Control, Swift  Notification services Can, Call, Work, Notification  Programming Language Swift, Object, Xcode, Value    Regarding the engagement from the questions that are most  viewed, the number of topics and silhouette value (obtained by  the silhouette method) respectively for each MSECO are:  Android – n=4 (0.62), iOS – n=3 (0.81) , and Windows Phone –  n=4 (0.75).  We use d the values indicated for the amount of  topics. Results are shown in Table 5.  In Windows Phone MSECO, developers work to upgrade  their apps to the newest platform as a way to support new  features (e.g . sensor data). An Android developer must decide  whether to build a single app or multiple versions to run on top  of the broad range of devices by the use of fragments. In iOS  MSECO, the most viewed questions refer to  the use of integrated  development envir onment, main programming , and  design/development of user interface.  9', 'Hearing the Voice of Developers in Mobile Software Ecosystems SBES’17, September 2017, Fortaleza, Brazil          Table 5. Hot-topics – the most viewed questions   Topic Words  Android IDE Device, android, java, string  User Interface View, set, button, listview  Basic Steps File, create, project, use  Interface behavior Activity, fragment, call, service  WP  Platform App, 8, 1, T, work  Notification Services Data, call, service, notification,  method  Data Binding Image, item, bind, listbox, control  Page Navigation File, page, button, navigation, wp7  iOS IDE Xcode, Imag, View, iPhone, Chang  Programming Language Object, Swift, c, text, io  User Interface Uitableview, view, uiview, cell, anim    The most viewed topics can indicate frequent barriers faced  by app developers because those questions can be found by any  developer using a search engine as Google, for example. The  analysis of engagement from the perspective of the most  commented/viewed questions allowed us to define the following  key insight:    Key Insight #2: The most viewed topics as well as the  topics in  which developers are most committed to respond can indicate a  community of experts who can help to reduce frequent barriers  to participation in MSECO.  5.4 (RQ4) Is there any relation between  questions and official events?  In order t o answer this question, a period between  February/2015 and January/2016 was chosen since it covers  official announcements of the MSECO organizations’ official  channels. The first analysis allowed us to veri fy whether there  was a similar behavior in the posting frequency among the  MSECOs ( Table 6). For statistical analysis, the data were  normalized to a range [0,1]. W e calculate the posting frequency  for each day of the year and then we divided each element by the  maximum element. Finally, we calculated the average for each  month.    Table 6. Frequency of posts in a specific year  (Feb/15 – Jan/16)  Month/Year Windows Phone  Android iOS  February/15 0.43 ˄ 0.62 0.59  March/15 0.34 0.71 0.66  April/15 0.39 0.72 0.67  May/15 0.37 0.70 0.65  June/15 0.35 0.69 0.67  July/15 0.39 0.74 0.73 ˄  August/15 0.32 0.71 0.70  September/15 0.31 0.67 0.71  October/15 0.22  0.70 0.69  November/15 0.33 0.67 0.62  December/15 0.27 0.72 0.66  January/16 0.32 0.77 ˄ 0.70    We analyzed the following null hypothesis “ There is no  difference between the frequency of developers’ posts among  MSECOs in a selected period of time ”. The selected period  was  between February/15 and January/16.  The Mann -Whitney test  was applied to verify normality of the three samples with  confidence level of 95%. We identified that the samples follow  the normal distribution. There was a statistically significant  difference between groups as determined by one-way ANOVA, p  = .0001. A Tukey  post hoc test revealed that the frequency to  which Windows Phone developers post questions was  statistically significant lower than iOS developers (.334  ± .018, p  = .0001) and Android developers (.365 ± .018, p = .0001). There  was no statistically significant difference between Android and  iOS developers (p = .233).  From Table 6, we can identify seasonal points within the time  series formed by the posting frequency. The highest point for  Windows Phone within the studied period was the first month  (February/2015); it was the last month of the series for Android - related posts (January/16); and  it was the sixth month for iOS  (July/15).         Figure 3: Posting frequency during the first 12 months. 10', 'In February/15, Microsoft announced improvements for  Windows Phone developers: the Windows Phone download and  in-app purchase reports have been optimized to deliver  information faste r. Microsoft also announced a Windows App  Studio Beta bringing new features like a full -featured logo and  image wizard with built -in image controls and conversion tools,  plus improved Facebook and YouTube DataSources matching  their latest API releases.  In the iOS MSECO (July/15), the announcements englobe:  Apple Previews iOS 9, News App for iPhone & iPad, OS X El  Capitan, New Apple Watch Software watchOS 2 (Native Third - Party Apps, New Watch Faces & Enhanced Communications  Features), and Expanding Benefits with Merchant Rewards &  Store Cards (Apple Pay).  With the announcement of iOS 9, developers have posted  questions related to: XCode 7, Swift 2.0 and interface settings,  which involves updating SDKs, font -rendering crashes, failures  when trying to launch em ulators, use of TouchID, and  deprecated methods. In the case of Apple Watch, questions relate  to the implementation of features, use of gestures, testing the  XCode emulator, user interface, and how to use the sensors.  While using Apple Pay, developers ques tioned crashes involving  Swift Apple Pay, how to use Apple Pay with a PayPal SDK  (BrainTree), and integration with Apple Passport.  Fig. 3 shows that the use of tags related to the  announcements maintain s an accumulated growth of questions  until the fourth month. After that, the behavior stays almost  constant based on the difference between the  last month and the  current one . From the analysis, we can perceive that technical  questions emerge when a keystone delivers new technologies. A  keystone must effectively deliver new technologies, processes or  ideas to the ecosystem’s participants.   The analysis led us to the following key insight:    Key Insight #3: Questions posted in a Q&A repository next to  official MSECO announcement periods can help a keystone to  manage strategies to add new MSECO resources (e.g. platforms,  SDKs, APIs, programming languages). When such new  technologies are released to the market, a keystone should be  able to manage them easily.    From this key insight, we can perceive a difference between  IT governance where b usiness strategies are not necessarily  reflected in the IT decisions  [30] and MSECOs in which business  strategies affect the developers’ communities  (e.g. APIs and  SDKs announcements).   5.5 (RQ5) What is the ranking of number of  badges received by developers of each  platform?  For this research question, we created a ranking of badges  received by developers within the three MSECOs . We used  dataset information about 9,795 developers from those MSECOs   ranked by reputation (i .e. number of conquered badges) . Table 7  shows this ranking . Due to statistical analysis purposes, we  normalized the data following the procedures adopted in RQ4.  We investigated 9 ,795 developers with badges based on the  following h ypothesis: “ There is no difference between the number  of badges received by developers from the MSECOs”.   Applying One -way ANOVA  test, it was perceived that the  significance value is p = .0001, which is below 0.05. Therefore,  there is a statistically signi ficant difference in the number of  badges among MSECOs. A Tukey post hoc test revealed that th e  number of badges acquired by Windows P hone developers was  statistically significant lower than iOS developers  (.00052 ±  .00019, p = .015) and Android developers (.00089 ± .00019, p =  .0001). There was no statistically significant difference between  Android and iOS groups (p = .139).    Table 7. Ranking – number of conquered badges  Ranking NumBadges  (Android)  NumBadges  (iOS)  NumBadges  (Windows Phone)  1º 14,779 14,779 14,779  2º 5,576 4,074 5,166  3º 4,202 3,093 4,202  4º 4,074 2,588 3,093  5º 3,093 2,130 3,035  6º 3,035 2,062 2,755  7º 2,588 1,865 2,732  8º 2,421 1,650 2,421', '(Windows Phone)  1º 14,779 14,779 14,779  2º 5,576 4,074 5,166  3º 4,202 3,093 4,202  4º 4,074 2,588 3,093  5º 3,093 2,130 3,035  6º 3,035 2,062 2,755  7º 2,588 1,865 2,732  8º 2,421 1,650 2,421  9º 2,150 1,623 2,279  10º 2,062 1,621 2,130    From the first ten developers in the ranking, it was possible  to identify that some of them act as multi -homing, for example,  the first one – Jon Skeet (Fig. 4), i.e., they play in more than one  MSECO, helping to answer questions and manage communities  in Stack Overflow. From the ecosystem perspective, such  developer profile is important because it fosters the exchange of  knowledge acquired from the interactions  of ecosystems and  developers. We also analyzed the conquered badges by  developers in each MSECO and created a ranking with the five  most frequent badges as shown in Table 8.  The Mortarboard badge is the only one present in Android  (1º), iOS (1º) and Windo ws Phone (3º) . It is a bronze  participation badge earned when developers conquer  at least  200 reputation points in a single day  (200 is the daily maximum  points).  Analyzing the Android MSECO, second badge in the ranking  is Multithread, i.e., a participation badge earned  when at least  400 total score for at least 80 non -community wiki answers  is  conquered. The third badge , Legendary, is a gold participation  badge earned when 200 daily reputation is conquered 150 times.  In turn, Quorum is a bronze participati on badge earned when a  developer reaches one  post with score of two on Meta Stack  Exchange (i.e. part of the site where users discuss the workings  and policies of Stack Overflow ). Finally, Great Answer is a gold  answer badge when an a nswer’s score of 100 or more  is  conquered. The five most frequent badges on MSECO Android  are participation-related and one is focused on answers.      11', 'Hearing the Voice of Developers in Mobile Software Ecosystems SBES’17, September 2017, Fortaleza, Brazil      Table 8. Badges earned in each MSECO – top five  Rankin g  Android iOS Windows Phone  1º Mortarboard Mortarboard Enthusiast  2º Multithread Reviewer Good Answer  3º Legendary Great Answer Mortarboard  4º Quorum Editor Talkative  5º Great Answer Cleanup Excavator    For reviews, we have expanded the filter for a period of an  extra month because some announcements occurred at the end  of such indicated month.  In the iOS MSECO, Reviewer is a silver moderation badge  earned when the developer complete at least 250 review tasks.  Next, Great Answer is a gold answer badge as explained above .  The fourth badge, Editor, is a bronze moderation badge  conquered when the developer make some editions for the first  time. Finally, the cleanup is also a bronze moderation badge  when the developer made his/her first rollback. The five most  earned badges on the MSECO iOS are related to moderation and  one of them refers to answers.    Figure 4: Veen diagram – multi-homing.  In Windows Phone MSECO , the first more frequent badge is  Enthusiast, i.e., a silver  participation badge earned when  developer visits the Stack Overflow every day for 30 consecutive  days. The second badge is Good Answer, i.e., a silver answer  badge earned when an answer gets a score of 25 or more.  Talkative badge is in participation catego ry – this badge is  earned when the developer posts ten message evaluated with  one or more star s. The last badge is Excavator, i.e., a bronze  moderation badge earned when a developer edit a first post that  was inactive for six months.  The five most frequent badges in  the MSECO Windows Phone are related to participation,  response, and moderation.  The last proposition refers to the identification of developers  and technical communities within Stack Overflow that can play  as “an extension” of the keystone role. This extension rises from  the technical knowledge flow and community control:  Key Insight #4: The badges can help a keystone to manage  strategies related to technical resource exploration, active  developer in the community , and community control by  fostering relationships with top developers in the ecosystem.  6 THREATS TO VALIDITY  Below we present the possible threats to validity involved in this  study, and how we mitigated it.  Constructo validity: the theoretical basis of this study  considered the weaknesses pointed out in recent literature  reviews published in the SECO field, i.e., in -depth studies. The  choice for Stack Overflow as a Q&A repository is due to the  presence of developers who also pos t questions and answers  related to the mobile platform domain.  Internal validity: datasets were not selected randomly, but  they were related to the studied MSECOs. To reduce the effect of  the experimenters’ expectation, the study’s analyses followed the  procedures indicated by algorithms or statistical analyses.  External validity: the environment is not different from the  real one since Stack Overflow is a repository with questions  from developers who are somehow participating in a MSECO. In  addition, our a nalysis considered the three main MSECOs in the  market: Android, iOS, and Windows Phone.  Conclusion validity: The statistical analyses and/or result  interpretation were based on algorithms for topic extraction  (LDA), word counting, and procedures for hypot hesis testing  with a confidence level of 95%.  7 CONCLUSION  App developers use Q&A repositories as a way to solve technical  questions that arise throughout the app (and platform)  development process. An example of a Q&A repository is the  Stack Overflow, wit h more than 3.9 billion visits 2 only in 2015  (latest report).   Developers, technical resources, apps and other elements  have been studied as MSECO s. MSECO can be treated as a  hybrid ecosystem, since it has a proprietary platform structure', '(latest report).   Developers, technical resources, apps and other elements  have been studied as MSECO s. MSECO can be treated as a  hybrid ecosystem, since it has a proprietary platform structure  but it is influenced by the use of external repositories controlled  by communities. In this scenario, Stack Overflow holds relevant  information about the developer and their participation in  MSECO. In this study, we analyze d the three main MSECOs:  Android, iOS, and Windows Phone.   We mined  13,515,636 technical questions in  Stack Overflow  repository aiming to identify what can be understood about the  ecosystem. We found relevant information involving the most  viewed and the most answered questions, developer  engagement, relation between questions and official events , and  developer reputation. After analyzing the results obtained to  each question, we identified a set of four key insights , or  propositions, that can help to understand  the involvement of  developers in MSECOs.  In addition to the propositions, we share d a set of datasets   containing data f rom 2008 to March, 201 7 that can be used by  researchers as a way to study the community of developers  in                                                                    2 http://Stack Overflow.com/company/about  12', 'SBES’17, September 2017, Fortaleza, Brazil A. Fontão et al.      other types of ecosystems. The existing set of information serves  both t he support of the developer community and the  organization itself that can evaluate the effect of adopting SDKs,  for example. We concluded that a keystone can use  Stack  Overflow as an external repository since it is a source of  information for the creation  and adaptation of ecosystem  strategies. As such, data extracted from a Q&A repository can be  used as input to support ecosystem’s information visualization.   As future work, we intend to investigate how MSECO  developers’ governance mechanisms can be ident ified and  supported by Q&A repositories. To do so, we are exploring  complex network analysis, fine -grained emotion detection, and  MSECO lifecycle through the analysis of questions and answers.  It is also important to understand the correlation between data   from Q&A repositories and information retrieved from other  repositories like Apps’ Store, Social Sites (Facebook and Twitter),  Github, and CodePlex.  ACKNOWLEDGMENTS  The authors would like to thank FAPEAM , CAPES and CNPq for  the financial support . The fourth author also thanks to  DPq/PROPG/UNIRIO for partially support this research.  REFERENCES  [1] A. Fontão, R.  Santos, J. F. Filho, and A. C. Dias-Neto. 2016. MSECO- DEV: Application development process in mobile software  ecosystems. In Proceedings of the International Conference on  Software Engineering and Knowledge Engineering, 317-322.  [2] S. Jansen and E. Bloemendal. 2013. Defining App Stores: The Role of  Curated Marketplaces in Software Ecosystems. In Proceedings of the  International Conference of Software Business,195–206.  [3] J. Bosch. 2009. From Software Product Lines to Software  Ecosystems. In Proceedings of the International Software Product Line  Conference, 111–119.  [4] F.  Lin and W. Ye. 2009. Operating System Battle in the Ecosystem of  Smartphone Industry. In Proceedings of the International Symposium  on Information Engineering and Electronic Commerce, 617-621.  [5] A. Fontão, R. P. Santos, and A. C. Dias-Neto. 2015. Mobile Software  Ecosystem (MSECO): A Systematic Mapping Study. In Proceedings of  the Annual International Computers, Software & Applications  Conference, 653-658.  [6] C. Casalnuovo, B. Vasilescu, P. Devanbu, and V. Filkov. 2015.  Developer Onboarding in GitHub: The Role of Prior Social Links  and Language Experience. In Proceedings of the Joint Meeting on  Foundations of Software Engineering, 817–828.  [7] B. Lin and A. Serebrenik. 2016. Recognizing Gender of Stack  Overflow Users. In Proceedings of the International Conference on  Mining Software Repositories, 425-429.  [8] R. Santos and C. Werner. 2012. ReuseECOS: An Approach to  Support Global Software Development through Software  Ecosystems. In Proceedings of the IEEE International Conference on  Global Software Engineering Workshops, 60–65.  [9] K. Manikas. 2016. Revisiting software ecosystems Research: A  longitudinal literature study. Journal of Systems and Software 117,  84–103.   E. Eckhardt, E. Kaats, S. Jansen, and C. Alves. 2014. The merits of a  meritocracy in open source software ecosystems. In Proceedings of  the European Conference on Software Architecture, 7.  [11] O. Barbosa and C. Alves. 2011. A Systematic Mapping Study on  Software Ecosystems. In Proceedings of the Third International  Workshop on Software Ecosystems, 15–26.  [12] K. Manikas and K. M. Hansen. 2013. Software ecosystems – A  systematic literature review. Journal of Systems and Software 86, 5  (2013), 1294–1306.  [13] M.  Farias, R. Novais, M. Colaço, L. Carvalho, M. Mendonça, and R.  Spínola. 2016. A Systematic Mapping Study on Mining Software  Repositories. In Proceedings of the ACM/SIGAPP Symposium on  Applied Computing, 1472–1479.  [14] P.  Kochhar, F. Thung, N. Nagappan, T. Zimmermann, and D. Lo.  2015. Understanding the test automation culture of app developers.', 'Applied Computing, 1472–1479.  [14] P.  Kochhar, F. Thung, N. Nagappan, T. Zimmermann, and D. Lo.  2015. Understanding the test automation culture of app developers.   In  Proceedings of the International Conference on Software Testing,  Verification and Validation, 1-10.  [15] D. German, B. Adams, and A. E. Hassan. 2013. The evolution of the  R software ecosystem. In Proceedings of the European Conference on  Software Maintenance and Reengineering, 243–252.  [16] C. de Souza, F. Filho, M. Miranda, R. Ferreira, C. Treude, and L.  Singer. 2016. The Social Side of Software Platform Ecosystems. In  Proceedings of the International Conference on Human Factors in  Computing Systems, 3204–3214.  [17] A. Fontão, R. Pereira, and A. Dias-Neto. 2015. Research  Opportunities for Mobile Software Ecosystems. In Proceedings of the  Workshop on Distributed Software Development, Software Ecosystems  and Systems-of-Systems, 4–5.  [18] F. Shull, J. Singer, and D. I. K. Sjøberg. 2008. Guide to advanced  empirical software engineering.  [19] N. Genc-Nayebi and A. Abran. 2016. A Systematic Literature  Review: Opinion Mining Studies from Mobile App Store User  Reviews. Journal of Systems and Software 125, 207-2019.  [20] H. Ahmed. 2008. The road ahead for mining software repositories. In  Proceedings of the Frontiers of Software Maintenance, 48–57.  [21] V. Bhat. 2014. Min(e)d Your Tags\u202f: Analysis of Question Response  Time in Stack Overflow.  In Proceedings of the International  Conference on Advances in Social Network Analysis and Mining, 328– 335.  [22] C. Shah, V. Kitzie, and E. Choi. 2014. Questioning the question -  Addressing the answerability of questions in community question- answering. In Proceedings of the Annual Hawaii International  Conference on System Sciences, 1386–1395.  [23] B. Vasilescu, A. Serebrenik, P. Devanbu, and V. Filkov. 2014. How  social Q&A sites are changing knowledge sharing in open source  software communities. In Proceedings of the ACM Conference on  Computer Supported Cooperative Work and Social Computing, 342– 354.  [24] A. Zagalsky, C. G. Teshima, D. M. German, M. Storey, and G. Poo- caamaño. 2016. How the R Community Creates and Curates  Knowledge\u202f: A Comparative Study of Stack Overflow and Mailing  Lists. In Proceedings of the International Conference on Mining  Software Repositories, 441–451.  [25] R. Krestel, P. Fankhauser, and W. Nejdl. 2009. Latent dirichlet  allocation for tag recommendation. In Proceedings of the third ACM  conference on Recommender systems, 61–68.  [26] P. J. Rousseeuw. 1987. Silhouettes: A graphical aid to the  interpretation and validation of cluster analysis. Journal of  Computational and Applied Mathematics 20, 53–65.  [27] K. Bajaj and A. Mesbah. 2014. Mining Questions Asked by Web  Developers. In Proceedings of the International Conference on Mining  Software Repositories, 112–121.  [28] Barua, Anton, Stephen W. Thomas, and Ahmed E. Hassan. 2014.  What are developers talking about? an analysis of topics and trends  in stack overflow. Empirical Software Engineering, 619-654.  [29] C. Rosen and E. Shihab. 2016. What are mobile developers asking  about? A large scale study using stack overflow. Empirical Software  Engineering 21, 3 (2016), 1192–1223.  [30] K. Manikas, K. Wnuk, and A. Shollo. 2015.  Defining decision  making strategies in software ecosystem governance. Department of  Computer Science, University of Copenhagen.    13']","['Research Group on Experimental Softare Engineering and  Softare Tesing HEARING THE VOICE OF APP DEVELOPERS This  briefin  reports  scieitfc  evideice oi  the  use  of  Q&A  repositories  as  a mechaiism  to  uiderstaid  aid  defie stratenies  to  support  developers  ii Mobile  Softare  Ecosystems  (e.n., Aidroid, iOS aid Wiidots Phoie). FINDINGS \uf0b7 Evidence  related  to  developer  acivitt  (i.e., developers’  posts)  suggests  that  there  is  no diference eetteen the amount of developers’ posts among MSECOS (Fig. 1) Figure 1. New questios by year \uf0b7 Regarding  the  numeer  of  ansters  et  tear, considering  a  grotth  funcion  for  each MSECO. Android funcion a(x) is 38.4% greater than  iOS  funcion  and  97.2%  greater  than Windots  Phone  funcion  t(x).  iOS  funcion i(x) is 95.3% greater than t(x). \uf0b7 The  fndings  presented  in  this  eriefng  also consider four ket insights constructed from a data  mining  process  in  Stack  Overfot,  a Quesions & Ansters repositort.  \uf0b7 Ket Insight #1: The most commonlt used tags in recentlt added quesions mat indicate the most  frequent  earriers  faced  et  developers tilling  to  paricipate  in  a  MSECO.  This scenario can serves a monitoring strategt to support a ketstone in recruiing and educaing developers. Table 1. Extracted tipics frim all questios Aidroid Project User Iiterface Notfcatois Exceptois iOS User Iiterface Data Biidiin Project WP Services Data Biidiin Frametorks \uf0b7 There are common points in the intersecion eetteen  topics  of  diferent  MSECO:  data einding  mechanisms,  user  interface  (UI) programming,  and  development  support infrastructure. \uf0b7 Ket Insight #2: The most vieted topics as tell as  the  topics  in  thich  developers  are  most committed  to  respond  can  indicate  a communitt of experts tho can help to reduce frequent earriers to paricipaion in MSECO. Table 2. Hit-tipics: develiper eogagemeot Most vieted Most aistered Topic Aidroid IDE Data Binding User Interface IDE Basic Steps User Interface Interface eehavior Back-end Infrastructure WP Platform Event Handler Noifcaion Services IDE Data Binding User InterfacePage Navigaion iOS IDE User Interface Programming Language Data Binding User Interface Noifcaion services Programming Language \uf0b7 Ket  Insight  #3:  Quesions  posted  in  a  Q&A repositort  next  to  ofcial  MSECO announcement periods can help a ketstone to manage  strategies  to  add  net  MSECO resources  (e.g.  platforms,  SDKs,  APIs, programming  languages).  When  such  net technologies  are  released  to  the  market,  a ketstone  should  ee  aele  to  manage  them easilt. \uf0b7 The use of tags related to the announcements maintains  an  accumulated  grotth  of quesions unil the fourth month. Afer that, the eehavior stats almost constant eased on the  diference  eetteen  the  last  month  and the current one.  \uf0b7 Technical quesions emerge then a ketstone delivers  net  technologies.  A  ketstone  must efecivelt  deliver  net  technologies, processes  or  ideas  to  the  ecoststem’s paricipants. \uf0b7 There is a diference eetteen IT governance there eusiness strategies are not necessarilt refected in the IT  decisions and  MSECOs  in thich  eusiness  strategies  afect  the developers’ communiies (e.g. APIs and SDKs announcements). \uf0b7 Ket  Insight  #4:  The  eadges  can  help  a ketstone  to  manage  strategies  related  to technical  resource  exploraion,  acive developer in the communitt, and communitt control  et  fostering  relaionships  tith  top developers in the ecoststem. \uf0b7 The  fve  most  frequent  eadges  on  MSECO Android  are  paricipaion-related  and  one  is focused on ansters \uf0b7 The fve most earned eadges on the MSECO iOS  are  related  to  moderaion  and  one  of them refers to ansters \uf0b7 The fve most frequent eadges in the MSECO Windots Phone are related to paricipaion, response, and moderaion   Keywirds: Softare ecoststems Moeile applicaion development Mining Softare Repositort Stack Overfot Whi is this briefog fir?', 'Windots Phone are related to paricipaion, response, and moderaion   Keywirds: Softare ecoststems Moeile applicaion development Mining Softare Repositort Stack Overfot Whi is this briefog fir? Softare engineering praciioners tho tant to make decisions aeout  developers’ governance in Moeile  Softare Ecoststem eased on scienifc  evidence. Where the fodiogs cime frim? All fndings of this eriefng tere  extracted from the data mining of all  moeile related quesions on Stack  Overfot conducted et Fontão et al. What is oit iocluded io this briefog? The staisical analtsis, threats to  validitt and graphs of the original  empirical studt.  Detailed descripions aeout the  research quesions analtzed in the  original invesigaion. Origioal Empirical Study Refereoce: Atdren Fontão, Faerício Lima, Bruno  Áeia, Rodrigo Santos and Arilo Dias- Neto. Hearing the voice of developers  in Moeile Softare Ecoststems.  Brazilian Stmposium on Softare  Engineering. 2017. Fir additioal iofirmatio abiut  this research: Experts.icomp.ufam.edu.er atdren [at] icomp [dot] ufam [dot] edu  [dot] er']","**Title: Understanding Developer Engagement in Mobile Software Ecosystems through Q&A Repositories**

**Introduction:**
This Evidence Briefing aims to summarize key findings from a study that investigates the interactions and challenges faced by developers in Mobile Software Ecosystems (MSECOs) using data mined from Stack Overflow. The insights derived from this research can inform strategies for enhancing developer engagement, education, and support within these ecosystems.

**Core Findings:**
1. **Developer Activity Intensity:** The study analyzed over 1.5 million questions related to Android, iOS, and Windows Phone on Stack Overflow. It found that Android developers were significantly more active than their iOS and Windows Phone counterparts. This highlights the need for tailored support strategies for less active ecosystems.

2. **Hot Topics Identification:** By employing Latent Dirichlet Allocation (LDA), the researchers identified key topics in developer questions. Common themes included user interface design, data binding, and project management across all platforms. These insights can guide educational content and resources targeted at developers facing specific challenges.

3. **Engagement and Community Dynamics:** The study revealed that the most viewed questions often indicate barriers faced by developers. Engaging with these topics can foster a community of experts who can assist newcomers, thereby enhancing overall developer participation in MSECOs.

4. **Relationship with Official Events:** The research found a correlation between developer questions and official announcements from platform providers. Questions tend to spike following new releases or updates, suggesting that timely communication from keystone organizations can help manage developer expectations and resource allocation.

5. **Developer Recognition through Badges:** The analysis of badges awarded on Stack Overflow indicated that developers who contribute actively across multiple platforms enhance knowledge flow within the ecosystem. Recognizing and supporting these multi-homing developers can strengthen community ties and improve resource sharing.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, platform managers, and educators who aim to support and enhance developer engagement in mobile software ecosystems.

**Where the findings come from:**
The findings presented in this briefing are derived from the analysis of 13,515,636 posts on Stack Overflow, specifically focusing on 1,568,377 questions related to Android, iOS, and Windows Phone platforms.

**What is included in this briefing?**
This briefing includes insights on developer activity, hot topics in developer questions, engagement dynamics within the community, relationships with platform announcements, and the impact of developer recognition through badges.

To access other evidence briefings on software engineering:
[http://www.lia.ufc.br/~cbsoft2017/xxxi-sbes/sbes-cfp/](http://www.lia.ufc.br/~cbsoft2017/xxxi-sbes/sbes-cfp/)

For additional information about the research group:
[http://www.icomp.ufam.edu.br](http://www.icomp.ufam.edu.br)

**Original Research Reference:**
Fontão, A., Lima, F., Ábia, B., Santos, R. P., & Dias-Neto, A. C. (2017). Hearing the Voice of Developers in Mobile Software Ecosystems. In Proceedings of the Brazilian Symposium on Software Engineering (SBES’17), Fortaleza, Ceará, Brazil. DOI: [10.1145/3131151.3131167](https://doi.org/10.1145/3131151.3131167)"
"['KDM-RE: A Model-Driven Refactoring Tool for KDM Rafael S. Durelli1, Bruno M. Santos2, Raphael R. Honda2, Márcio E. Delamaro1 and Valter V . de Camargo2 1Computer Systems Department University of São Paulo - ICMC São Carlos, SP, Brazil. 2Computing Departament Federal University of São Carlos - UFSCAR São Carlos, SP, Brazil. {rdurelli, delamaro}@icmc.usp.br1, {valter, bruno.santos, raphael.honda}@dc.ufscar.br2 Abstract. Architecture-Driven Modernization (ADM) advocates the use of mod- els as the main artifacts during modernization of legacy systems. Knowledge Discovery Metamodel (KDM) is the main ADM metamodel and its two most outstanding characteristics are the capacity of representing both i) all system details, ranging from lower level to higher level elements, and ii) the dependen- cies along this spectrum. Although there exist tools, which allow the application of refactorings in class diagrams, none of them uses KDM as their underlying metamodel. As UML is not so complete as KDM in terms of abstraction lev- els and its main focus is on representing diagrams, it is not the best metamodel for modernizations, since modiﬁcations in lower levels cannot be propagated to higher levels. To fulﬁll this lack, in this paper we present a tool that allows the application of seventeen ﬁne-grained refactorings in class diagrams. The main difference from other tools is that the class diagrams uses KDM as their under- lying metamodel and all refactorings are applied on this metamodel. Therefore, the modernizer engineer can detect ""model smells"" in these diagrams and apply the refactorings. 1. Introduction Architecture-Driven Modernization (ADM) is an initiative which advocates for the appli- cation of Model Driven Architecture (MDA) principles to formalize the software reengi- neering process. According to the OMG the most important artifact provided by ADM is the Knowledge Discovery Metamodel (KDM). KDM is an OMG speciﬁcation adopted as ISO/IEC 19506 by the International Standards Organization for representing information related to existing software systems. KDM is structured in a hierarchy of four layers; In- frastructure Layer, Program Elements Layer, Runtime Resource Layer, and Abstractions Layer. We are specially interested in the Program Elements Layer because it deﬁnes the Code and Action packages which are widely used by our tool. The Code package deﬁnes a set of meta-classes that represents the common elements in the source-code supported by different programming languages such as: ( i) ClassUnit and InterfaceUnit which represent classes and interface, respectively, ( ii) StorableUnit which illus- trates attributes and ( iii) MethodUnit to represent methods, etc. The Action package represents behavior descriptions and control-and-data-ﬂow relationships between code', 'elements. Refactoring has been known and highly used both industrially and academi- cally. It is a form of transformation that was initially deﬁned by Opdyke [Opdyke 1992] as “a change made to the internal structure of the software while preserving its external behavior at the same level of abstraction”. In the area of object-oriented programming, refactorings are the technique of choice for improving the structure of existing code with- out changing its external behavior [Fowler et al. 2000]. Refactorings have been proved to be useful to improve the quality attributes of source code, and thus, to increase its main- tainability. It is possible to identify several catalogs of refactoring for different languages and the most complete and inﬂuential was published by Fowler in [Fowler et al. 2000]. Nowadays, there are researches been carried out about apply refactoring in model instead of source code[Ulrich and Newcomb 2010]. Nevertheless, although ADM provides the process for refactoring legacy systems by means of KDM, there is a lack of an Integrated Development Environment (IDE) to lead engineers to apply refactorings as such exist in others object-oriented languages. In the same direction, Model-Driven Modernization (MDM) is a special kind of model transformation that allows us to improve the structure of the model while preserving its internal quality characteristics. MDM is a considerably new area of research which still needs to reach the level of maturity attained by source code refactoring [Misbhauddin and Alshayeb 2012]. In order to enable MDM in the context of ADM, refactorings for the KDM meta- model are required. In this context, in a parallel research line of the same group, we developed a catalogue of refactorings for the KDM [Durelli et al. 2014]. We argue that devising a refactoring catalogue for KDM makes this catalogue language-independent and standardized. However, the KDM metamodel was not created with the goal of being the basis for diagrams, as is the case of UML metamodel. Thereby, in order to make pos- sible to apply ﬁne-grained refactoring in the KDM metamodel, it is necessary to devise a way to view the KDM instance graphically. Furthermore, although there exist tools, which allow the application of refactorings in class diagrams, none of them uses KDM as their underlying metamodel. As UML is not so complete as KDM in terms of abstraction levels and its main focus is on representing diagrams, it is not the best metamodel for modernizations, since modiﬁcations in lower levels cannot be propagated to higher levels Hence, the main contribution of this paper is the provision of a plug-in on the top of the Eclipse Platform named Knowledge Discovery Model-Refactoring Environment (KDM-RE). This plug-in can be used to lead engineers to apply refactorings in KDM, which are based on seventeen well known refactorings[Fowler et al. 2000]. The IDE as well as the adapted catalogue are based on our experience as model-driven engineering. Also, by using this plug-in the modernizer engineer can visualize the Code package as an UML class diagram, allowing engineers to detect model smells in that diagram. One hypothetical case study was developed in order to exemplify the use of the plug-in. This paper is organized as followed: Section 2 provides the background to fully understand our plug-in - Section 3 depicts information upon the plug-in KDM-RE and an case study - in Section 4 there are related works and in Section 5 we conclude the paper with some remarks and future directions. 2. ADM and KDM OMG deﬁned ADM initiative [Perez-Castillo et al. 2009] which advocates carrying out the reengineering process considering MDA principles. ADM is the concept of modern-', 'Conceptual Build Structure Platform Event UIData Core,  KDM,  source Code Action Abstraction  Layer Resource  Layer Program  Elements  Layer Infrastructure  Layer Figure 1. Layers, packages, and separation of concerns in KDM (Adapted from [OMG 2012]) izing existing systems with a focus on all aspects of the current systems architecture. It also provides the ability to transform current architectures to target architectures by using all principles of MDA [Ulrich and Newcomb 2010]. To perform a system modernization, ADM introduces Knowledge Discovery meta-model (KDM). KDM is an OMG speciﬁcation adopted as ISO/IEC 19506 by the International Standards Organization for representing information related to existing soft- ware systems. According to [Perez-Castillo et al. 2009] the goal of the KDM is to deﬁne a meta-model to represent all the different legacy software artifacts involved in a legacy in- formation system (e.g. source code, user interfaces, databases, etc.). The KDM provides a comprehensive high-level view of the behavior, structure and data of legacy information systems by means of a set of meta-models. The main purpose of the KDM speciﬁcation is not the representation of models related strictly to the source code nature such as Uniﬁed Modeling Language (UML). While UML can be used to mainly to visualize the system “as-is”, an ADM-based process using KDM starts from the different legacy software ar- tifacts and builds higher-abstraction level models in a bottom-up manner through reverse engineering techniques. As outlined before, the KDM consists of four abstraction layers: (i) Infrastructure Layer, (ii) Program Elements Layer, (iii) Runtime Resource Layer, and (iv) Abstractions Layer. Each layer is further organized into packages, as can be seen in Figure 1. Each package deﬁnes a set of meta-model elements whose purpose is to repre- sent a certain independent facet of knowledge related to existing software systems. We are specially interested in the Program Elements Layer because it deﬁnes the Code and Action packages which are widely used by our catalogue. The Code package deﬁnes a set of meta-classes that represents the common elements in the source code supported by dif- ferent programming languages. In Table 1 is depicted some of them. This table identiﬁes KDM meta-classes possessing similar characteristics to the static structure of the source code. Some meta-classes can be direct mapped, such as Class from object-oriented lan- guage, which can be easily mapped to the ClassUnit meta-class from KDM. 3. Refactoring for KDM by means of KDM-RE This sections describes KDM-RE. In Figure 2 we depicted the main window of our plug- in. For explanation purpose, we highlight two main regions, i.e.,a⃝, and b⃝. It supports 17', ""Table 1. Meta-classes for Modeling the Static Structure of the Source-code Source'Code*Element* KDM*Meta'Classes* Class* ClassUnit* Interface* InterfaceUnit* Method* MethodUnit* Field* StorableUnit* Local*Variable* Member* Parameter* ParameterUnit* Association* KdmRelationShip* * refactorings adapted to KDM. These refactorings are based on some ﬁne-grained refactor- ings proposed by Fowler [Fowler et al. 2000]. All the refactorings are shown in Table 2. We chose the Fowler’s refactorings because they are well known, basic and ﬁne-grained refactorings. Please, not that KDM-RE uses MoDisco 1 once it provides an extensible framework to transform an speciﬁc source-code to KDM models. In Figure 2 is presented Table 2. Refactorings Adapted to KDM Rename Feature Moving Features Between Objects Organing Data Dealing with Generalization  Rename ClassUnit Move MethodUnit Replace data value with Object Push Down MethodUnit  Rename StorableUnit Move StorableUnit Encapsulate StorableUnit Push Down StorableUnit      Rename MethodUnit  Extract ClassUnit Replace Type Code with ClassUnit Pull Up StorableUnit    Inline ClassUnit  Replace Type Code with SubClass Pull Up MethodUnit    Replace Type Code with State/Strategy  Extract SubClass  Extract SuperClass  Collapse Hierarchy  ! Figure 2. Snippets KDM-RE’s Interface just a snippet of KDM-RE. Starting from the popup menu named “Refactoring KDM”, in this model browser, see Figure 2 a⃝, either the software developer or software modernizer can interact with the KDM model and choose which refactoring must be carried out in the KDM. In the region a⃝ can be seen all 17 refactorings that have been implemented in KDM-RE. For illustration purposes only we drew rectangles to separate the refactorings 1http://www.eclipse.org/MoDisco/"", 'into three groups. The black rectangle represents refactorings that deal with generaliza- tion, the blue rectangle stand for refactorings to organize data and the red one symbolize refactoring to assist the moving features between objects. The region b⃝ on Figure 2 shows an UML class diagram. This diagram can be used before to apply some refactorings to assist the modernizer to decide where/when to apply the refactorings. This UML class diagram also can be useful as the modernizer per- forms the refactorings in KDM model. For instance, changes are reproduced on the ﬂy in a class diagram. We claim that the latter use of this diagram is important once it provides an abstract view of the system, hence, the modernizer can visually check the system’s changes after applying a set of refactorings. Furthermore, in the context of modernization usually the source-code is the only available artifact of a legacy system. Therefore, creat- ing an UML class diagram makes, both the legacy system and the generated software to have a new type of artifact (i.e., UML class models), improving their documentation. 3.1. Case Study In this section, we motivate KDM-RE by analyzing one hypothetical case study. This case study is a small part of the university domain. Figure 2 b⃝ (left side) shows a class diagram used for modeling a small part of the university domain. In an university there are several Persons, more speciﬁcally Professors, their Assistants, and Students. Each Person has RG, CPF, and address (of type String). Moreover, classes Professor, Assistant, and Student have an attribute name of type String each. The software modernizer or the software developer found out by looking at the UML class diagram (see Figure 2 b⃝ left side) this redundantly, i.e., equal attributes in sibling classes. Therefore, he/she must apply the refactoring “Pull Up Field’. Similarly, he/she also found out by looking at the UML class diagram that one class is doing work that should be done by two or more. For example, he/she found that the attributes RG and CPF should be modularized to a class. Similarly, it is necessary to provide more information about they address, such as number, city, country, etc. Therefore, he/she must apply the refactoring “Extract Class” to the attributes “RG”, “CPF” and “rua”. Due space limitation it is depicted just the extraction of the attributes “RG” and “CPF”. The ﬁrst step is to select the meta-class that he/she identiﬁed as a bad smell, i.e., the meta-class to be extracted into a separate one. This step is illustrated in Figure 3(a). After selecting the meta-class, a right-click opens the context menu where the refactoring is accessible. After the click, the system displays the “RefactoringWizard” to the engineer, Figure 3(b) depicts the Extract Class Wizard. In this wizard, the name of the new meta-class can be set. Also a preview of all detected StorableUnits and MethodUnits that can be chosen to put into the new meta-class. Further, the engineer can select if either the new meta-class will be a top level meta-class or a nested meta-class. The engineer also can select if the KDM-RE must create instances of MethodUnits to represent accessors methods (gets and sets). Finally, the engineer can set the name of the StorableUnit that represent the link between the two meta-classes (the old meta-class and the new one). After all of the required inputs have been made, the engineer can click on the button “Finish” and the refactoring “Extract Class” is performed by KDM-RE. As can be seen in Figure 3(c) a new instance ofClassUnit named “Document” was created - two StorableUnit from “Pessoa”, i.e., “rg” and “CPF” were moved', '(a)  (b) (c) Figure 3. Extract Class Wizard to the new ClassUnit - instances of MethodUnits were also created to represent the gets and sets. In addition, the instance of ClassUnit named “Pessoa” owns a new instance of StorableUnit that represent the link between both ClassUnits. Due space limitation the other StorableUnits of ClassUnit named “Pessoa” are not shown in Figure 3(c). After the engineer realize the refactorings, an UML class diagram is created on the ﬂy to mirror graphically all changes performed in the KDM model, see Figure 2 b⃝right side. 4. Related Work Van Gorp et al. [Gorp et al. 2003] proposed a UML proﬁle to express pre and post con- ditions of source code refactorings using Object Constraint Language (OCL) constraints. The proposed proﬁle allows that a CASE tool: ( i) verify pre and post conditions for the composition of sequences of refactorings; and ( ii) use the OCL consulting mechanism to detect bad smells such as crosscutting concerns. Reimann et al. [Reimann et al. 2010] present an approach for EMF model refactoring. They propose the deﬁnition of EMF- based refactoring in a generic way. Another approach for EMF model refactoring is pre- sented in [Thorsten Arendt 2013]. They propose EMF Refactor 2, which is a new Eclipse incubation project in the Eclipse Modeling Project consisting of three main components. Besides a code generation module and a refactoring application module, it comes along with a suite of predeﬁned EMF model refactorings for UML and Ecore models. 2http://www.eclipse.org/emf-refactor/', '5. Concluding Remarks In this paper is presented the KDM-RE which is a plug-in on the top of the Eclipse Platform to provide support to model-driven refactoring based on ADM and uses the KDM standard. More speciﬁcally, this plug-in supports 17 refactorings adapted to KDM. These refactorings are based on some ﬁne-grained refactorings proposed by Fowler [Fowler et al. 2000]. As stated in the case study the engineer/modernizer by using KDM-RE can apply a set refactorings in a KDM. Also, on the ﬂy the engineer can check all changes realized in this KDM replicated into a class diagram - the engineer can visu- ally verify the system’s changes after applying a set of refactorings. In addition, usually the source code is the only available artifact of the legacy software. Therefore, creating an UML class diagram makes, both the legacy software and the generated software to have a new type of artifact (i.e., UML class models), improving their documentation. Also, we claim that as we have deﬁned all refactoring based on the KDM, they can be easily reused by others researchers. It is important to notice that the application of refactorings in UML class diagrams is not a new research as stated before. However, all of the works we found on literature perform the refactoring directly on the UML metamodel. Although UML is also an ISO standard, its primary intention is just to represent diagrams and not all the characteristics of a system. As KDM has been created to represent all artifacts and all characteristics of a system, refactorings performed on its ﬁner-grained elements can be propagated to higher level elements. This propitiates a more complete and manageable model-driven modernization process because all information is concentrated in just one metamodel. In terms of the the users who uses modernization tools like ours, the difference is not noticeable; that is, whether the refactorings are performed over UML or KDM. However, there are two main beneﬁts of developing a refactoring catalogue for KDM. The ﬁrst one is in terms of reusability. Other modernizer engineers can take advantage of our catalogue to conduct modernizations in their systems. The second beneﬁt is that, unlikely UML, a catalogue for KDM can be extended to higher abstractions levels, such as architecture and conceptual, propitiating a good traceability among these layers. We believe that KDM-RE makes a contribution to the challenges of Software Engineering which focuses on mechanisms to support the automation of model-driven refactoring. Future work involves implementing more refactorings and conducting exper- iments to evaluate all refactorings provided by KDM-RE. Doing so, we hope to address a broader audience with respect to using, maintaining, and evaluating our tools. Currently, KDM-RE generates only class diagrams to assist the modernization engineer to perform refactorings, however, as future work, we intend to: ( i) extend this computational sup- port to enable the achievement of other diagrams, e.g., the sequence diagram, (ii) perform structural check of the software after the application of refactorings; and (iii) carry out the assessment tool, as well as refactorings proposed by controlled experiments. A work that is already underway is to check how other parts of the highest level of KDM are affected after the application of certain refactorings. For example, assume that there are two pack- ages P1 and P2. Suppose there is a class in P1, named C1, and within the P2 there is a class named C2. Assume that C1 owns an attribute A1 of the type C2., i.e., there is an association relationship between these classes of different packages. P1 and P2 represent architectural layers, i.e., P1 = Model and P2 = View. Thus, the relationship that exists is undesirable. When we make a ﬁne-grained refactoring such as moving the attribute A1', 'of the class C1, it should be reﬂected to the architectural level, eliminating the existing relationship between the two architectural layers. 6. Acknowledgements Rafael S. Durelli would like to thank the ﬁnancial support provided by FAPESP, process number 2012/05168-4. Bruno Santos and Raphael Honda also would like to thank CNPq for sponsoring our research. References Durelli, R. S., Santibáñez, D. S. M., Delamaro, M. E., and Camargo, V . V . (2014). To- wards a refactoring catalogue for knowledge discovery metamodel. In IEEE 15th In- ternational Conference on Information Reuse and Integration (IRI). Fowler, M., Beck, K., Brant, J., Opdyke, W., and Roberts, D. (2000). Refactoring: Im- proving the Design of Existing Code. Addison-Wesley. Gorp, P. V ., Stenten, H., Mens, T., and Demeyer, S. (2003). Towards automating source- consistent uml refactorings. In International Conference on UML - The Uniﬁed Mod- eling Language, pages 144–158. Springer. Misbhauddin, M. and Alshayeb, M. (2012). Model-driven refactoring approaches: A comparison criteria. In Sofware Engineering and Applied Computing (ACSEAC), 2012 African Conference on. OMG (2012). Object Management Group (OMG) Architecture-Driven Modernisation. Disponível em: http://www.omgwiki.org/admtf/doku.php?id=start. (Acessado 2 de Agosto de 2012). Opdyke, W. F. (1992). Refactoring Object-Oriented Frameworks. Ph.D. Thesis, Univer- sity of Illinois. Perez-Castillo, R., de Guzman, I. G.-R., Avila-Garcia, O., and Piattini, M. (2009). On the use of adm to contextualize data on legacy source code for software modernization. In Proceedings of the 2009 16th Working Conference on Reverse Engineering, WCRE ’09, pages 128–132, Washington, DC, USA. IEEE Computer Society. Reimann, J., Seifert, M., and Abmann, U. (2010). Role-based generic model refactor- ing. In In ACM/IEEE 13th International Conference on Model Driven Engineering Languages and Systems (MoDELS 2013). Springer. Thorsten Arendt, Timo Kehrer, G. T. (2013). Understanding complex changes and im- proving the quality of uml and domain-speciﬁc models. In In ACM/IEEE 16th Inter- national Conference on Model Driven Engineering Languages and Systems (MoDELS 2013). Ulrich, W. M. and Newcomb, P. (2010). Information Systems Transformation: Architecture-Driven Modernization Case Studies. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.']","['REFACTORING SYSTEMS FROM KDM INSTANCES This  briefin  reports  scieitfc  evideice oi  the  efectveiess  of  applyiin refactoriins  to  KDM  iistaices  of software systess via KDM-RRE toolm FINDINGS Our finiigs suggessv vhav vhes KDM-RsE vool cai  heslp  softares  esigiiesesrs  vo  apply resfacvoriigs  vhav  improves  vhes  qualivy  of softares sysvesms. KDM-RsE  facilivavess  vhes  cresatoi  of Kiotlesnges  Discovesry  Mesvamonesl  (KDM) iisvaicess  of  sysvesms  by  proviniig  a meschaiism vhav resans vhes sysvesm sources cones ain gesiesravess ivs KDM vesrsioi. Thes  vool  heslp  softares  esigiiesesrs  vo aialyzes  ain  inesitfy  ban-smeslls  by proviniig a graphical visualizatoi of vhes KDM  iisvaicess,  thich  illusvravess  vhes sysvesms  classess  ain  vhesir  atribuvess  ain mesvhons. Thes maii anvaivages is vhav vhes resfacvoriigs  cai  bes  pesrformesn  oi  vhis graphical  monesl  ain  vhesi  ares auvomatcally resfescvesn vo all lesvesls of vhes KDM iisvaices. Wes carriesn ouv ai esxpesrimesiv vo esvaluaves vhes  anvaivagess  of  usiig  KDM-RsE  vo resfacvor sesvesi resal-torln sysvesms vhrough vhesir  KDM  iisvaicess.  This  esxpesrimesiv shotesn  vhav,  afesr  applyiig  sesvesral resfacvoriigs,  covesriig  eslesvesi  nifesresiv vypess,  all  sesvesi  sysvesms  han  ai improvesmesiv oi vhes qualivy atribuvess of resusabilivy  (12.77%),  fesxibilivy  (6.37%), uinesrsvainabilivy  (17.79%),  ain esfesctvesiesss (..29%). Accorniig vo vhes ressulvs of vhes svatstcal vessvs  tes  pesrformesn,  vhes  resfacvoriigs appliesn  tivh  vhes  supporv  of  KDM-RsE iicresasesn  sigiifcaivly  vhreses  qualivy atribuvess  (resusabilivy,  fesxibilivy,  ain uinesrsvainabilivy). Finure 1m Graphs of the experiseit datam Alvhough  vhesres  tas  also  ai  iicresases  ii esfesctvesiesss,  vhes  svatstcal  vessvs iinicavesn  vhav  iv  tas  iov  sigiifcaiv.  Iv happesiesn bescauses uinesrsvainabilivy ain esfesctvesiesss ares opposives atribuvess, i.es., thesi  oies  improvess,  vhes  ovhesr  vesins  vo gesv  torses.  Ii  our  esxpesrimesiv, uinesrsvainabilivy  tas  vhes  qualivy atribuves  tivh  vhes  highessv  iicresases (17.79\\%),  thiles  esfesctvesiesss  tas  vhes oies tivh vhes lotessv iicresases (..29\\%). Wes esmphasizes vhav vhis sorv of vranes-of has vo  bes  aialyzesn  by  vhes  nesveslopesrs,  tho haves vo kesesp ii miin vhes prioritess of vhesir sysvesms thesi a qualivy atribuves improves vo vhes nesvrimesiv of aiovhesr oies. Who is this briefin  or? Softares esigiiesesriig practtoiesrs tho taiv  vo  makes  nescisiois  applyiig resfacvoriigs  oi  KDM  iisvaicess  basesn oi sciesitfc esvinesices. Where the fidiins come  rom? All  finiigs  of  vhis  briesfig  tesres esxvracvesn  from  vhes  esxpesrimesiv coinucvesn by Dureslli esv al.   What is iicluded ii this briefin? Thes  maii  finiigs  of  vhes  esxpesrimesiv, ain briesf coivesxvual iiformatoi abouv vhes coivesxv of vhes finiigs. What is iot iicluded ii this briefin? Annitoial  iiformatoi  iov  supporvesn by vhes finiigs of vhes origiial ressesarch as  tesll  as  nesscriptois  abouv  vhes ressesarch  mesvhon  or  nesvails  abouv  vhes esxpesrimesiv ain vhes KDM-RsE vool. To access other evideice briefins  oi software einiieeriin: htp:////ttt.lia.ufc.br//ccbsof2017//pro gramacao-sbess// ORIGINAL RESEARCH REFERENCE Dureslli esv al. Improving the structure of KDM instances via refactorings: an experimental study using KDM-RE. Braziliai Symposium oi Softares Eigiiesesriig (SBES), 2017.']","**Title: Enhancing Legacy System Refactoring with KDM-RE Tool**

**Introduction:**
This briefing presents the findings from a study on the Knowledge Discovery Model-Refactoring Environment (KDM-RE), a model-driven refactoring tool designed to modernize legacy systems using the Knowledge Discovery Metamodel (KDM). The goal is to provide insights into how this tool can facilitate the refactoring process, improve software maintainability, and enhance the overall modernization of software architectures.

**Main Findings:**
The KDM-RE plug-in, developed for the Eclipse platform, supports the application of seventeen fine-grained refactorings tailored for KDM. Unlike traditional tools that utilize UML, KDM-RE leverages KDM’s comprehensive structure, which encompasses multiple abstraction layers and provides a more holistic view of software systems. This allows for better propagation of changes across different levels of abstraction.

1. **Refactoring Capabilities**: KDM-RE enables engineers to apply 17 specific refactorings, such as renaming, moving features between objects, and extracting classes, directly on KDM models. These refactorings are based on established practices by Fowler and are adapted to fit the KDM structure.

2. **Visual Feedback**: The tool provides a graphical representation of KDM models as UML class diagrams, allowing engineers to visualize and verify modifications in real-time. This feature helps in identifying ""model smells"" and facilitates better decision-making during the refactoring process.

3. **Case Study Application**: A hypothetical case study illustrated the effectiveness of KDM-RE in a university domain scenario. The refactoring process helped eliminate redundancy in class attributes and modularize data, showcasing how KDM-RE can streamline the modernization of legacy systems.

4. **Documentation Improvement**: By generating UML class diagrams from KDM models, KDM-RE enhances the documentation of both legacy and modernized systems, providing a new artifact that aids in understanding system structures.

5. **Future Directions**: The study indicates plans to extend KDM-RE's functionality to support additional refactorings and diagrams, conduct structural checks post-refactoring, and evaluate the tool's effectiveness through controlled experiments.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, particularly those involved in legacy system modernization and refactoring processes. It is also relevant for researchers interested in model-driven engineering and software architecture.

**Where the findings come from?**
The findings are derived from the development and evaluation of KDM-RE, as detailed in the original research conducted by Rafael S. Durelli et al. The study emphasizes the application of KDM in refactoring practices, highlighting the tool's capabilities and potential benefits.

**What is included in this briefing?**
This briefing includes an overview of the KDM-RE tool, its refactoring capabilities, a case study demonstrating its application, and insights into its future development.

**What is NOT included in this briefing?**
This briefing does not cover detailed statistical analyses or comprehensive discussions of all related works in the field. It focuses on the practical implications and functionalities of KDM-RE.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research group:**
[http://www.icmc.usp.br](http://www.icmc.usp.br)

**ORIGINAL RESEARCH REFERENCE:**
Durelli, R. S., Santos, B. M., Honda, R. R., Delamaro, M. E., & Camargo, V. V. (2017). KDM-RE: A Model-Driven Refactoring Tool for KDM. Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering."
"['Incremental Strategy for Applying Mutation Operators Emphasizing Faults Diﬀicult to be Detected by Automated Static Analyser Vinícius Barcelos Silva Departament of Computing Federal University of São Carlos Rod. Washington Luís, Km 235 São Carlos, SP 13565-905 vinicius.silva@dc.ufscar.br Cláudio Antonio Araujo Tribunal Regional do Trabalho da 18ª Região R. T-51, 1403 – Qd T-22, Lt 7-22 Goiânia, GO 74215-901 claudionoar@gmail.com Edmundo Sérgio Spoto Instituto de Informática Universidade Federal de Goiás Alameda Palmeiras, Quadra D Goiânia, GO 74690-900 edmundo@inf.ufg.br Auri M R Vincenzi Departament of Computing Federal University of São Carlos Rod. Washington Luís, Km 235 São Carlos, SP 13565-905 auri@dc.ufscar.br ABSTRACT To ensure software quality, we can use static and dynamic analysis techniques. Both have advantages and disadvantages and should be used together to improve their performance. In this paper, we present a strategy for applying a set of mutation operators for soft- ware testing, which represents a dynamic technique, based on the diﬃculty an automated static analyzer has on detecting their mo- deled faults. In other words, we investigated which sets of faults, represented by mutation operators, an automated static analyzer was able to recognize and prioritize the mutation testing conside- ring only the set of mutation operators whose set of faults are dif- /f_icult to be detected by such static analyzer. We compare our set of mutation operators with others, and the statical analysis shows no diﬀerence in the mutation score and costs regarding the num- ber of generated and equivalent mutants among the diﬀerent stra- tegies. Nevertheless, we consider our proposal attractive once it uses operators with lower overlapping with faults detected by the automated static analyzer we have used. CCS CONCEPTS •Software and its engineering→Automated static analysis; Software defect analysis; Software testing and debugging; KEYWORDS Software Testing, Static Analysis, Mutation Testing, Selective Mu- tation, Incremental Testing Strategy Permission to make digital or hard copies of all or part of this work f or personal or classroom use is granted without fee provided that copies are not ma de or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full cita- tion on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy oth erwise, or re- publish, to post on servers or to redistribute to lists, requires prio r speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 123-4567-24-567/08/06. . . $15.00 DOI: 10.1145/3131151.3131169 ACM Reference format: Vinícius Barcelos Silva, Cláudio Antonio Araujo, Edmundo Sérgio Spoto, and Auri M R Vincenzi. 2017. Incremental Strategy for Applying Mutation Operators Emphasizing Faults Diﬃcult to be Detected by Automated Static Analyser. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. DOI: 10.1145/3131151.3131169 1 INTRODUÇÃO Nas atividades de desenvolvimento de software é comum a utili- zação de técnicas e ferramentas para obtenção de um produto de qualidade e de baixo custo. Entre as atividades que podem contri- buir para um produto de qualidade tem-se a aplicação de técnicas e critérios de testes e a utilização de analisadores estáticos. Uma técnica de teste bastante e/f_icaz na detecção e eliminação dos defeitos presentes no software é o Teste de Mutação [16]. Sendo considerado um excelente modelo de defeitos, o Teste de Mutação é, em geral, empregado para avaliar a qualidade de con- juntos de teste ou critérios de teste, tornado-o um muito utilizado em experimentação [1]. Eme geral, defeitos são introduzidos no software devido a enganos cometidos por pessoas ao interpretar erroneamente informações sobre o que deve ser feito ou ao utilizar', 'em experimentação [1]. Eme geral, defeitos são introduzidos no software devido a enganos cometidos por pessoas ao interpretar erroneamente informações sobre o que deve ser feito ou ao utilizar determinada linguagem de programação. Um comando ou uma instrução incorreta presente no código fonte é um exemplo de de- feito [19]. O Teste de Mutação gera, por meio dos operadores de mutação, versões do programa em teste contendo possíveis defeitos. Assu- mindo que o programa original está correto, é necessário construir testes que mostrem que o mutante e o programa original se com- portem de maneira diferente e, nesse caso, descartando a possibili- dade do programa possuir o defeito representado pelo mutante. Um dos problemas do Teste de Mutação é o alto custo computa- cional devido ao grande número de mutantes gerados, que deman- dam tempo para a execução, e posterior análise de mutantes vivos para identi/f_icação de mutantes equivalentes. Desse modo, diferen- tes alternativas são empregadas para reduzir esses custos. Uma 24', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Silva et al.. delas é diminuir o número de operadores de mutação que são uti- lizados, gerando-se apenas um subconjunto de todos os possíveis mutantes. Por outro lado, as ferramentas de análise estática automatizadas podem ser empregadas para auxiliar no reconhecimento de certos tipos de defeitos e vulnerabilidades. Exemplos de ocorrências que podem ser detectadas por essas ferramentas são: prováveis laçosin- /f_initos, estouro debuﬀer, erro no gerenciamento de memória, den- tre outros. Essas ferramentas emitem uma grande quantidade de avisos, alertando o testador sobre possíveis problemas que podem estar presentes no código fonte, sem a necessidade de execução do mesmo. A alta taxa de avisos falsos positivos relatados por ferra- mentas de análise estática é apontada como sua principal desvan- tagem, ou seja, avisos relatados que não correspondem a defeitos reais, mas que vão demandar tempo de investigação por parte da equipe para sua análise [10, 11, 20]. Os verdadeiros positivos são avisos emitidos que realmente indicam a presença de um defeito real no código fonte. Há ainda os avisos classi/f_icados como falsos negativos, que correspondem a defeitos existentes no código fonte, mas que devido à ausência de regras sintáticas implementadas, tais ferramentas de análise estáticas ainda não são capazes de detectá- los. Existem diversas ferramentas de análise estática automatizadas disponíveis para diversas linguagens de programação, alguns exem- plos são: a StyleCop [25] e a FxCop [24] para sistemas em .NET, aFindBugs [18] e a PMD [9] para Java, a Lint [28], Splint [17] e Frama-C [21] para C/C++. Apesar do interesse acadêmico e industrial no uso das ferramen- tas de análise estática automatizadas [5, 18, 22], principalmente de- vido ao seu baixo custo de utilização, ainda existe uma resistência devido à falta de evidências das contribuições reais que tais ferra- mentas agregam na produção de um software de qualidade [4]. Em estudo recente, Araujo et al. [2, 3] fez um uso alternativo do teste de mutação para a avaliação da qualidade de analisado- res estáticos automatizados para a linguagem Java. Nesse estudo, foi medida a taxa de correspondência entre avisos emitidos pelos analisadores estáticos e defeitos representados por meio de mutan- tes. Em Araujo et al. [2, 3], os mutantes foram gerados por meio da ferramenta de mutaçãoµJava e a ferramenta de análise estática utilizada foi a FindBugs. Observou-se que existem algumas catego- rias de defeitos, representadas pelos operadores de mutação, que são mais facilmente detectáveis pelos analisadores estáticos auto- matizados, enquanto há outras categorias de defeitos que são quase imperceptíveis por tais analisadores, evidenciando o aspecto com- plementar da análise estática e dinâmica. O objetivo deste artigo é replicar o estudo de Araujo et al. [2, 3], de/f_inindo uma estratégia incremental de aplicação dos operadores de mutação para C, com base nas informações de análise estática. E estender o estudo de Araujo et al. [2, 3] realizando um estudo expe- rimental avaliando a efetividade da estratégia incremental de/f_inida em relação a duas outras estratégias já de/f_inidas na literatura. A comparação das estratégias foi realizada segundo os aspectos de es- core de mutação, custo em termos do número de mutantes gerados, e custo em termos do número de mutantes equivalentes gerados pelas estratégias. Os resultados já obtidos indicam que não houve diferença es- tatística entre as diferentes estratégias investigadas, entretanto, a estratégia proposta é a que aplica de forma incremental operado- res de mutação com menor sobreposição com os tipos de defeitos detectáveis pelo analisador estático investigado. O restante deste artigo encontra organizado da seguinte forma: na Seção 2 são apresentadas as terminologias e conceitos básicos para compreensão deste estudo; na Seção 3 são apresentados tra-', 'O restante deste artigo encontra organizado da seguinte forma: na Seção 2 são apresentadas as terminologias e conceitos básicos para compreensão deste estudo; na Seção 3 são apresentados tra- balhos relacionados; na Seção 4 são apresentados os programas, ferramentas e passos empregados nesse estudo; na Seção 5 foi rea- lizada uma breve apresentação sobre os conjuntos utilizados para comparação; na Seção 6 os dados estatísticos são analisados; na Seção 7 é feita a discussão dos resultados obtidos. Finalmente, a conclusão e perspectivas de trabalhos futuros são apresentadas na Seção 9. 2 TERMINOLOGIA E CONCEITOS BÁSICOS As atividades relacionadas à veri/f_icação e validação de software podem ser divididas em dois tipos: atividades de análise dinâmica e atividades de análise estática. A análise dinâmica requer a execução do código, podendo assim ter um custo mais elevado do que a análise estática de acordo com as técnicas e critérios de testes adotados. Isso ocorre porque para a realização da análise dinâmica podem ser necessários diversos casos de teste até que os objetivos de teste sejam atingidos [32, 33]. Já as atividades de análise estática avaliam o produto em questão sem a necessidade de execução do produto. O teste de mutação foi concebido baseado na hipótese do progra- mador competente e no efeito de acoplamento [16]. A hipótese do programador competente parte da premissa de que programadores experientes escrevem programas bem próximos de estarem corre- tos. Já o efeito de acoplamento parte da premissa de que defeitos mais complexos estão relacionados a defeitos mais simples. Com base nessas hipóteses, defeitos arti/f_iciais são inseridos no programa original gerando os chamados mutantes. No teste de mu- tação tradicional, cada mutante contém apenas um defeito. As al- terações sintáticas realizadas no programa original para a geração dos mutantes são feitas por meio dos chamados operadores de mu- tação (op), que simulam os defeitos mais frequentes introduzidos pelos programadores. Para que um op gere mutantes é necessário que o programa ori- ginal contenha as estruturas sintáticas exigidas pelo operador para criar os mutantes. Por exemplo, o operador ORRN (Relational Ope- rator Mutation) substitui a ocorrência de um operador relacional no programa original por todos os outros operadores relacionais existentes na linguagem C. Já o operador SSDL (Statement Dele- tion) remove um comando de cada vez do programa original para dar origem aos mutantes. Desse modo, o número de mutantes gera- dos com a aplicação de um operador varia em função da presença das estruturas sintáticas e do número de vezes que essas estruturas aparecem no programa em teste. Inicialmente, considerando um conjunto de teste T e um pro- grama P, cada caso de teste t ∈ T é executado em P e observa-se se o resultado obtido condiz com o resultado esperado. Caso os resul- tados divirjam, uma falha foi exposta eP deve ser corrigido para a eliminação do(s) defeito(s) que levaram à ocorrência da falha. 25', 'Incremental Strategy for Applying Mutation Operators Emphasizing Faults Diﬀicul t to be Detected by Automated Static AnalyserSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Entretanto, se nenhuma falha for observada não signi/f_ica que, necessariamente, P não tenha defeitos. Pode ser que o conjunto de teste T seja de baixa qualidade. Para avaliar a qualidade de T pode ser utilizado o Teste de Mutação. A execução do Teste de Mutação se inicia quando um ou mais operadores são aplicados à P, gerando o conjunto M de mutantes de P. Cada mutante m ∈ M difere de P apenas em um defeito simples. Em seguida, cada caso de teste t ∈ T é executado com cada mutante m e o resultado é comparado com o resultado obtido quando t foi executado com P. Se os resultados forem os mesmos, é dito que o mutante permanece vivo, do contrário é dito que o mutante é morto. Um mutante ser morto signi/f_ica simplesmente que o caso de teste em questão foi capaz de expor a diferença de comportamento entre ele e o programa original e, nesse caso, con/f_irma que o pro- grama original não contém o defeito modelado pelo mutante. Já os mutantes vivos precisam ser analisados. Pode ser que um mutante m esteja vivo porque T é de baixa qualidade e ainda não possui um teste capaz de matá-lo, ou então, m pode estar vivo por ser equiva- lente à P e, nesse caso, pode ser descartado. Do ponto de vista de melhoria do conjunto de teste T o que mais interessa são os mutantes vivos não equivalentes pois são esses que contribuem para adicionar aT novos casos de teste. Por meio do cálculo do escore de mutação, que é a razão en- tre o número de mutantes mortos e o número de mutantes não- equivalentes (do total de mutantes criados desconsidera-se o nú- mero de mutantes equivalentes), avalia-se qual a qualidade deT para o teste de P considerando o conjunto de mutantes gerados. O valor obtido do escore de mutação varia entre 0 e 1, sendo que quanto mais próximo de 1, melhor a qualidade deT no teste de P. Em outras palavras, se T obtiver um escore de 1, signi/f_ica que ele possui casos de teste capaz de expor todas as possíveis falhas modeladas pelos operadores de mutação utilizados. Do ponto de vista teórico, cada mutante representa um possível defeito que pode estar presente no programa original [13]. Em es- tudo realizado por Andrews et al. [1] demonstrou-se que o teste de mutação pode ser considerado um excelente modelo de falhas e é empregado frequentemente na avaliação da qualidade de conjun- tos e critérios de teste. Em virtude do alto custo de utilização do Teste de Mutação, prin- cipalmente devido ao grande número de mutantes gerados, pes- quisas na linha de reduzir o número de operadores utilizados são conduzidas. Uma dessas estratégias é denominada de Mutação Se- letiva [27] e tem por objetivo utilizar apenas um subconjunto de operadores de mutação, em vez de empregar todo o conjunto de operadores. Conjuntos essenciais de operadores de mutação são um tipo de mutação seletiva e já existem conjuntos essenciais de- terminados para algumas linguagens de programação como For- tran [26] e C [7]. Já a análise estática realizada por analisadores estáticos automa- tizados costuma ser de menor custo pois não exige execução do código e a análise é realizada com base em regras e na representa- ção abstrata do comportamento do código do produto [17]. Com base na análise realizada são emitidos avisos sobre os possíveis pro- blemas identi/f_icados. Muitos deles, podem ser falsos positivos. Araujo et al. [2, 3] fez um uso alternativo do teste de mutação para a avaliação da qualidade de analisadores estáticos, ou seja, ao invés de avaliar a qualidade de conjuntos de teste, os autores optaram por utilizar os mutantes com possíveis defeitos e mediram qual a taxa de avisos que seriam capazes de detectar tais defeitos. Neste trabalho o estudo de Araujo et al. [2, 3] foi repetido con- siderando o conjunto de operadores de mutação implementados', 'qual a taxa de avisos que seriam capazes de detectar tais defeitos. Neste trabalho o estudo de Araujo et al. [2, 3] foi repetido con- siderando o conjunto de operadores de mutação implementados na FerramentaProteum/IM [14], que modelam defeitos em progra- mas escritos na Linguagem C. No caso foi comparada a capacidade da Ferramenta de análise estáticaSplint em detectar os 78 tipos de defeitos modelados pelos operadores de teste unitário implementa- dos na FerramentaProteum/IM [14, 15]. Os operadores de mutação de unidade são divididos em 4 grupos: 4 operadores de Mutação de Constante, 47 operadores de Mutação de Operador, 15 operadores de Mutação de Comandos e 12 operadores de Mutação de Variáveis. Mais detalhes são apresentados na Seção 4. 3 TRABALHOS RELACIONADOS Oﬀutt et al. [26] conduziu um estudo que visava determinar um conjunto essencial de operadores de mutação para testes em Pro- gramas Fortran a partir de operadores utilizados pela Mothra. A Mothra é uma ferramenta de apoio ao critério de Análise de Mu- tantes para programas em Linguagem Fortran, ela possui 22 ope- radores de mutação. Os resultados obtidos demonstraram que uti- lizando apenas cinco operadores já seria possível aplicar o teste de mutação de forma e/f_iciente. Wong et al. [30] realizou um estudo preliminar semelhante, no qual foi realizada a comparação entre Mutação Seletiva em Pro- gramas C e Fortran, resultando na seleção de um subconjunto de operadores de mutação da FerramentaProteum/IM. O subconjunto obtido possibilitou reduzir o número de mutantes gerados e manter a e/f_icácia do critério em revelar a presença de defeitos. A seleção de tais operadores por parte de Wong et al. [30] se baseou na experiência dos autores, os resultados motivaram a con- dução do trabalho de Barbosa et al. [7]. Os experimentos foram conduzidos com o objetivo de investigar alternativas pragmáticas para a aplicação do Teste de Mutação e, nesse contexto, foi pro- posto o procedimento Essencial para determinação do conjunto es- sencial de operadores de mutação para Linguagem C, baseado nos operadores de mutação implementados na FerramentaProteum/IM. A validação do procedimento proposto foi feita a partir de dois estudos experimentais. No primeiro utilizou-se um grupo de 27 programas, os quais compõem um editor de texto simpli/f_icado; no segundo, utilizou-se um grupo de 5 programas utilitários do Unix (vide Seção 4). Os resultados obtidos a partir de ambos conjuntos mostraram um alto grau de adequação em relação ao Teste de Mu- tação, tendo escores de mutação acima de 0,995, proporcionando em média reduções de custo superiores a 65% [7]. Delamaro et al. [15] conduziu um estudo a /f_im de validar a se- guinte questão: se a exclusão de declarações é uma maneira econô- mica de projetar casos de teste, a exclusão de outros elementos do programa também será e/f_icaz? Foi testada a exclusão de variáveis, operadores e constantes, tal abordagem se mostrou e/f_iciente e e/f_i- caz. A partir desse estudo foram criados mais três operadores de mutação para a FerramentaProteum/IM com o objetivo de reduzir o número de mutantes gerados que podem ser mortos com os mes- mos casos de teste, tentado tornar o teste de mutação mais barato e mantendo sua e/f_icácia. 26', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Silva et al.. O estudo conduzido por Araujo et al. [2, 3] teve como objetivo investigar a correspondência entre mutantes e avisos emitidos por ferramentas de análise estática automatizada. Os operadores de mutação foram utilizados como um modelo de defeitos para ava- liar a correspondência entre mutantes e avisos estáticos. Foi cons- tatada a correspondência entre alguns operadores de mutação da µJava [23] e alguns tipos de avisos da FindBugs [5]. Os autores sugerem que os resultados obtidos podem ser utilizados de duas maneiras: • Caso decida-se por realizar análise estática com a Find- Bugs, os autores sugerem que, inicialmente, utilizasse aque- les tipos de aviso que possuem maior correspondência com os mutantes pois, desse modo, aumenta-se a chance de tais avisos corresponderem a verdadeiro positivos. A premissa é que se os avisos foram capazes de perceber as mutações possivelmente sejam realmente bons em detectar a pre- sença de determinados tipos de defeitos no código fonte. • Caso decida-se por realizar o Teste de Mutação a recomen- dação dada é a inversa, ou seja, deveria dar prioridade para aqueles operadores que geram mutantes que são pratica- mente imperceptíveis ao analisador estático. Tais mutan- tes corresponderiam aos falso negativos, ou seja, possíveis defeitos que o analisador estático é incapaz de detectar. Observa-se que no estudo de Araujo et al. [2, 3] as estratégias propostas não foram avaliadas uma vez que o objetivo daquele tra- balho era o de evidenciar a existência das correspondências e não a efetividade das estratégias de priorização de/f_inidas. Com base nessas recomendações, este artigo visa a reproduzir e estender o estudo de Araujo et al. [2, 3] no contexto de Programas C e investigar se a estratégia de utilizar os operadores de muta- ção que possuem menor correspondência com os avisos do anali- sador estático permite obter bons resultados em termos do escore de mutação, número de mutantes gerados e número de mutantes equivalentes. As seções a seguir detalham os resultados obtidos. 4 ESTRATÉGIA INCREMENTAL COM BASE EM ANÁLISE ESTÁTICA PARA C Visando inicialmente estender as análise realizando uma avaliação dinâmica das estratégias propostas dada a disponibilidade de con- juntos de casos de teste adequados para o teste de mutação em Programa C, foi realizada a replica o estudo de Araujo et al. [2, 3] para Programas C. A Ferramenta Splint foi escolhida como analisador estático e a Ferramenta Proteum/IM como a ferramenta de mutação. No caso da Splint foram utilizadas todas as possíveis regras de análise está- tica1. No caso da Proteum/IM todos os 78 operadores de unidade implementados2. A escolha da Splint como ferramenta de análise estática é devido ao fato da mesma ser uma das ferramentas mais conhecidas para Linguagem C, e ser a evolução da Ferramenta Lint [28], que é uma das ferramentas referência para Linguagem C. ASplint também 1Utilizou-se a Splint no modo strict, por meio do seguinte comando: splint -strict <arquivo.c>. 2Para gerar os mutantes da Proteum/IM utilizou-se o seguinte comando: muta-gen -u 100 0 <arquivo.c> foi usada no trabalho de Evans e Larochelle [17], para detectar fa- lhas de segurança. A escolha da FerramentaProteum/IM é devido ao fato dessa ser a mais popular ferramenta de apoio ao teste de mutação para Programas C e ser utilizada extensamente outros es- tudos experimentais [14]. Selecionaram-se para a condução do estudo os mesmos 5 pro- gramas UNIX já utilizados por outros pesquisadores [7, 12, 29–31]. Além disso, devido ao seu uso intenso, tais programas possuem uma baixa probabilidade de apresentarem defeitos naturais em seu código. Na Tabela 1 são apresentadas mais informações sobre os mesmos. Por exemplo, o Programa Cal é o utilitário do UNIX res- ponsável pela geração de calendário via linha de comando. Este programa possui 142 linhas de código (LOC), a FerramentaSplint–', 'mesmos. Por exemplo, o Programa Cal é o utilitário do UNIX res- ponsável pela geração de calendário via linha de comando. Este programa possui 142 linhas de código (LOC), a FerramentaSplint– con/f_igurada para utilizar todos os tipos de avisos possíveis –, emi- tiu 129 avisos durante sua execução, que dá uma taxa de 0,91 avisos por linha de código (W/LOC). Já a FerramentaProteum/IM– con/f_i- gurada para utilizar todos os seus operadores de mutação para o teste de unidade –, gerou 4.881 mutantes, que corresponde a uma taxa de 34,37 mutantes por linha de código (M/LOC). No total, os cinco programas somam 621 linhas de código, 508 avisos e 15.136 mutantes, conforme a Tabela 1, representando uma taxa de avisos por linha de código de 0,82 e taxa de mutante por li- nha de código de 24,37. Destaca-se ainda que, do total de mutantes gerados, 15.136, 1.381 mutantes (9,12%) são equivalentes e foram identi/f_icados manualmente por parte de um dos autores deste ar- tigo. Tanto os conjuntos de teste adequados quanto os equivalentes foram obtidos a partir do trabalho de mestrado de Vincenzi [29]. A análise dos equivalentes foi feita, naquele momento, de forma manual e consumiu em torno de 2 semanas de trabalho. Ressalta- se, entretanto, que para a estratégia em questão, não é necessária a determinação dos equivalentes. Os dados foram aqui incluídos apenas para evidenciar o reduzido custo de equivalentes da STAT. Para se determinar a correspondência entre avisos e mutantes empregou-se a Correspondência Direta por Linha (CDL) por ope- rador de/f_inida em [2, 3]. O objetivo da CDL é quanti/f_icar o número de mutantes identi/f_icados pelaSplint, ou seja, CDL é a quantidade de avisos emitida no mutante mas que não estão presentes no pro- grama original. Uma vez computada a CDL de cada mutante é pos- sível determinar a CDL de cada operador de mutação. A CDL é obtida da seguinte maneira, segundo Araujo et al. [2, 3]: • TM ( ok ) – quantidade total de mutantes gerados pelo ope- rador ok ; • CDLA( ok ) – quantidade absoluta de mutantes gerados pelo operador ok com pelo menos um aviso relatado no ponto de mutação, mas que o mesmo aviso não exista, na mesma linha, no arquivo original; • CDLR( ok ) = CDLA( ok )/ TM ( ok ) – quantidade relativa da CDLA( ok ) sobre TM ( ok ) . O valor de CDLR( ok ) , que va- ria entre 0 e 1 (100%), representa a probabilidade das mu- tações produzidas pelo operadorok de serem detectadas pela ferramenta de análise estática. A de/f_inição dos valores de TM ( ok ) , CDLA( ok ) e CDLR( ok ) per- mitem classi/f_icar os operadores de mutação de acordo com a capa- cidade que o analisador estático tem em detectá-lo. A classi/f_icação 27', 'Incremental Strategy for Applying Mutation Operators Emphasizing Faults Diﬀicul t to be Detected by Automated Static AnalyserSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Tabela 1: Sistemas Utilizados no Estudo Experimental Sistema Descrição LOC Avisos (W ) Mutantes (M) W /LOC M/LOC Cal Apresenta um calendário para o ano ou mês especi/f_icado 142 129 4.881 0,91 34,37 Checkeq Informa os delimitadores ausentes ou desbalanceados e pares .EQ/.EN 92 108 4.492 1,17 48,82 Comm Seleciona ou rejeita linhas comuns entre dois arquivos 148 134 1.966 0,91 13,28 Look Procura palavras em um dicionário ou linhas em uma lista ordenada 115 33 2.074 0,29 18,03 Uniq Informa ou remove linhas adjacentes duplicadas 124 104 1.723 0,84 13,90 Total 621 508 15.136 0,82 24,37 representa, teoricamente, que os operadores de mutação com mai- ores taxasCDLR( ok ) , modelam defeitos mais fáceis de serem de- tectados pela Splint. Por outro lado, operadores de mutação com menores taxas CDLR( ok ) modelam os defeitos mais difíceis de se- rem detectados pelo analisador estático. Na Tabela 2 são apresentados os dados dos operadores da Pro- teum/IM em ordem crescente pelo valor CDL_R, ou seja, dos ope- radores que geram os mutantes mais difíceis de serem percebidos pelaSplint para os mais fáceis. Dos 78 operadores de unidade da Proteum/IM, 16 não gera- ram mutantes para os programas utilizados. São eles, u-OBAA, u-OBBA, u-OBEA, u-OSAA, u-OSAN, u-OSBA, u-OSBN, u-OSEA, u-OSLN, u-OSRN, u-OSSA, u-OSSN, u-SCRn, u-VLTR, u-VGTR, u- SGLR. Nesse caso, a geração de mutantes não ocorreu pois os pro- gramas não possuem as estruturas sintáticas exigidas pelos opera- dores. Para mais informações sobre os operadores de mutação da Proteum/IM, o leitor interessado pode consultar [14]. Dos 62 operadores de mutação que geraram ao menos um mu- tante para os programas utilizados, pode-se observar na Tabela 2 que o 62º operador, o u-SSWM, teve 23 mutantes gerados e que des- tes, 50% são identi/f_icados pelo analisador estático automatizado, ou seja, supostamente esse operador modela defeitos que aSplint tem mais facilidade de detecção, se comparado ao 1º operador da lista. O operador u-OAAA teve um total de 50 mutantes gerados, e a Splintfoi capaz de identi/f_icar apenas 1,4% dos mutantes gerados por esse operador. Desse modo, o uso da CDL_R permite determi- nar uma ordem incremental de aplicação dos operadores de muta- ção cuja qualidade será avaliada a seguir. As demais colunas da Tabela 2 serão explicadas na próxima seção. 5 AVALIAÇÃO DA ESTRATÉGIA INCREMENTAL Visando avaliar a qualidade da estratégia incremental de aplicação dos operadores de mutação sugerida, foi realizada uma compara- ção dessa estratégia, denominada STAT, com os conjuntos de ope- radores de mutação essenciais de/f_inidos para a Linguagem C [7]. Os conjuntos essenciais foram de/f_inidos por Barbosa et al. [7] a partir da execução de dois experimentos, sendo o Experimento-I que gerou um conjunto essencial, denominado E27, a partir de um grupo de 27 programas que compõem um editor de texto simpli- /f_icado e, o Experimento-II que gerou um conjunto essencial, de- nominado E5, a partir do grupo de 5 programas utilitários Unix. Para mais informações sobre como os conjuntos essenciais foram determinado, o leitor interessado pode consultar [7]. O resultado da aplicação incremental do conjuntos essenciais obtidos por Barbosa et al. [7] são apresentados nas Tabelas 3 e 4. A ordem de aplicação dos operadores dos conjuntos essenciais E5 e E27 prioriza, basicamente, o incremento no escore de mutação [7], sem levar em consideração a representatividade dos defeitos mo- delados pelos operadores. A título de ilustração, nas Tabelas 3 e 4 também é apresentada a taxa de correspondência entre avisos e os mutantes de cada um dos operadores dos conjuntos essenciais E5 e E27. De maneira diferente da estratégia de aplicação incremental dos operadores essenciais, no contexto deste trabalho, a sequência de', 'dos operadores dos conjuntos essenciais E5 e E27. De maneira diferente da estratégia de aplicação incremental dos operadores essenciais, no contexto deste trabalho, a sequência de aplicação dos operadores obtida é baseada na taxaCDLR( ok ) , prio- rizando aqueles com as menores taxas, ou seja, aqueles operadores cujos defeitos modelados tiveram uma menor taxa de identi/f_icação por parte daSplint. Determinada essa ordem de aplicação, a ques- tão que se coloca é se tal sequência gera resultados tão bons quanto aqueles determinados pelos conjuntos essenciais considerando os aspectos de escore de mutação, total de mutantes gerados e total de mutantes equivalentes gerados (vide Tabela 6). Conforme mencionado na introdução, o objetivo principal do nosso experimento é de/f_inir uma estratégia incremental de aplica- ção de operadores de mutação para C, com base nas informações de análise estática [3]. E realizar uma avaliação da qualidade da es- tratégia proposta em relação aos conjuntos essenciais [7], segundo os aspectos de escore de mutação, custo em termos de mutantes gerados, e custo em termos do número de mutantes equivalentes gerados pelas estratégias. Nesse sentido, a Tabela 5 apresenta as hipóteses de/f_inidas vi- sando a avaliação dos conjuntos de operadores sob esses aspectos. Para avaliação das hipóteses, foram computados o escore de mu- tação, a redução de custo e a redução de custo equivalente dos três conjuntos em análise sob as mesmas condições, possibilitando a comparação estatística dos resultados para determinar se existe ou não uma diferença entre os conjuntos. Todas estas medidas podem ser in/f_luenciadas pelas ferramentas de teste adotadas; e pelo tama- nho e complexidade dos programas sob análise. Um resumo dos dados apresentados nas Tabelas 2, 3 e 4 são apre- sentados na Tabela 6. O detalhamento da análise desses dados é realizado na próxima seção. 6 COLETA E ANÁLISE DOS DADOS O primeiro passo foi reavaliar os conjunto essenciais E5 e E27 no contexto dos 5 programas UNIX, considerando a versão atual da FerramentaProteum/IM. Os dados obtidos são apresentados na Ta- bela 6. Como pode ser observado, os conjuntos essenciais E5 e E27 de- terminam escores de mutação de 0,99738 e 0,99869, respectivamente. Utilizando-se apenas os operadores de/f_inidos nos conjuntos E5 e 28', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Silva et al.. Tabela 2: Dados agregados da correspondência dos avisos daSplint com operadores unitários daProteum/IM E5 E27 STAT Operador CDLR( %) #Mut #Eq Escore CC CCEq RC RCEq 1º u-OAAA 1,4 50 0 0,63228 50 0 99,7 100,0 2º u-OLLN 1,6 27 0 0,82537 77 0 99,5 100,0 3º u-VSCR 2,0 28 0 0,82537 105 0 99,3 100,0 4º u-OLNG 2,1 81 0 0,87350 186 0 98,8 100,0 5º u-VLPR 2,9 58 1 0,91167 244 1 98,4 99,9 1º 6º u-SWDD 3,2 18 2 0,91305 262 3 98,3 99,8 4º 6º 7º u-ORRN 3,6 490 65 0,96707 752 68 95,0 95,1 8º u-OIDO 5,3 71 2 0,97441 823 70 94,6 94,9 9º u-OAAN 5,8 179 1 0,97615 1002 71 93,4 94,9 10º u-OAEA 6,3 12 0 0,97615 1014 71 93,3 94,9 11º u-OBAN 7,7 15 5 0,97615 1029 76 93,2 94,5 12º u-OBNG 7,7 9 2 0,97615 1038 78 93,1 94,4 13º u-OBSN 7,7 6 2 0,97615 1044 80 93,1 94,2 14º u-OBBN 7,7 6 2 0,97615 1050 82 93,1 94,1 15º u-OLRN 7,9 162 9 0,98292 1212 91 92,0 93,4 16º u-OBSA 8,3 6 6 0,98292 1218 97 92,0 93,0 17º u-OCNG 9,6 109 0 0,98517 1327 97 91,2 93,0 18º u-VGPR 10,2 47 3 0,98517 1374 100 90,9 92,8 19º u-SMVB 12,1 15 0 0,98517 1389 100 90,8 92,8 20º u-OABA 12,5 36 1 0,98517 1425 101 90,6 92,7 21º u-OASA 12,5 24 0 0,98517 1449 101 90,4 92,7 2º 3º 22º u-SSDL 13,6 458 34 0,99171 1907 135 87,4 90,2 23º u-VGSR 13,8 528 10 0,99506 2435 145 83,9 89,5 24º u-SBRC 14,3 8 1 0,99506 2443 146 83,9 89,4 25º u-OBRN 15,4 18 3 0,99506 2461 149 83,7 89,2 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 5º 29º u-OASN 16,5 82 1 0,99695 3701 204 75,5 85,2 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 10º 33º u-CCSR 22,0 2290 10 0,99782 7107 250 53,0 81,9 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 9º 39º u-CCCR 26,5 2374 236 0,99833 10525 585 30,5 57,6 40º u-SMTT 27,3 33 0 0,99833 10558 585 30,2 57,6 1º 2º 41º u-SMTC 27,3 33 3 0,99847 10591 588 30,0 57,4 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 4º 53º u-OLBN 41,3 81 30 0,99855 12869 895 15,0 35,2 54º u-OLSN 41,3 54 2 0,99855 12923 897 14,6 35,0 6º 8º 55º u-VDTR 41,9 636 324 0,99985 13559 1221 10,4 11,6 5º 7º 56º u-VTWD 41,9 424 43 0,99993 13983 1264 7,6 8,5 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 3º 60º u-OEBA 45,5 252 84 1,00000 14945 1366 1,3 1,1 61º u-OESA 45,5 168 14 1,00000 15113 1380 0,2 0,1 62º u-SSWM 50,0 23 1 1,00000 15136 1381 0,0 0,0 E27, a redução de custo em termos do número de mutantes gera- dos é de 84,9% e 54,5%, respectivamente. Em termos de número de mutantes equivalentes, a redução proporcionada pelo E5 é de 59,9% e do E27 é de 45,8%. Nas Tabelas 2, 3 e 4 são apresentados os dados sobre a ordem de aplicação dos operadores obtidos, a taxa CDLR( ok ) , a quantidade de mutantes gerados por cada operador, a quantidade de mutan- tes equivalentes em cada operador, o escore de mutação, o custo cumulativo em termos do número de mutantes gerados da estraté- gia (CC) e o custo cumulativo em termos do número de mutantes equivalente da estratégia (CCEq). É possível observar que a STAT mesmo tendo 23 operadores de mutação, gera 6,2% mais mutantes que E5, que possui 6 operadores de mutação, e 64,6% menos mutantes que a E27, que possui 10 ope- radores de mutação. A quantidade de mutantes gerados por cada operador se dá em função da presença ou não das estruturas sintá- ticas exigidas para a sua aplicação. A STAT gerou um total de 2435 29', 'Incremental Strategy for Applying Mutation Operators Emphasizing Faults Diﬀicul t to be Detected by Automated Static AnalyserSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Tabela 3: Dados agregados da correspondência dos avisos daSplint com operadores essenciais do conjunto E5 E5 Op CDLR( %) #Mut #Eq Escore CC CCEq RC RCEq 1º u-SMTC 27,3 33 3 0,78684 33 3 99,782 99,783 2º u-SSDL 13,6 458 34 0,95674 491 37 96,756 97,321 3º u-OEBA 45,5 252 84 0,95798 743 121 95,091 91,238 4º u-ORRN 3,6 490 65 0,98408 1233 186 91,854 86,531 5º u-VTWD 41,9 424 43 0,99535 1657 229 89,053 83,418 6º u-VDTR 41,9 636 324 0,99738 2293 553 84,851 59,957 Tabela 4: Dados agregados da correspondência dos avisos daSplint com operadores essenciais do conjunto E27 E27 Op CDLR( %) #Mut #Eq Escore CC CCEq RC RCEq 1º u-SWDD 3,2 18 2 0,46579 18 2 99,881 99,855 2º u-SMTC 27,3 33 3 0,79353 51 5 99,663 99,638 3º u-SSDL 13,6 458 34 0,95682 509 39 96,637 97,176 4º u-OLBN 41,3 81 30 0,96205 590 69 96,102 95,004 5º u-OASN 16,5 82 1 0,96372 672 70 95,560 94,931 6º u-ORRN 3,6 490 65 0,98466 1162 135 92,323 90,224 7º u-VTWD 41,9 424 43 0,99600 1586 178 89,522 87,111 8º u-VDTR 41,9 636 324 0,99804 2222 502 85,320 63,650 9º u-Cccr 26,5 2374 236 0,99840 4596 738 69,635 46,560 10º u-Ccsr 22,0 2290 10 0,99869 6886 748 54,506 45,836 Tabela 5: Formalização das Hipóteses Investigadas Hipótese Nula Hipótese Alternativa Não há diferença no Escore de Mutação entre o conjunto STAT(S) e o ESS(ES).H10: Es- core de Mutação(S) = Escore de Mutação(ES) Existe uma diferença no Es- core de Mutação entre o con- junto STAT(S) e o ESS(ES). H11: Escore de Mutação(S) = Escore de Mutação(ES) Não há diferença na Redu- ção de Custo entre o conjunto STAT(S) e o ESS(ES).H20: Re- dução de Custo(S) = Redução de Custo(ES) Existe uma diferença na Redu- ção de Custo entre o conjunto STAT(S) e o ESS(ES).H21: Re- dução de Custo(S) = Redução de Custo(ES) Não há diferença na Redução de Custo Equivalente entre o conjunto STAT(S) e o ESS(ES). H30: Redução de Custo Equi- valente(S) = Redução de Custo Equivalente(ES) Existe uma diferença na Re- dução de Custo Equivalente entre o conjunto STAT(S) e o ESS(ES).H31: Redução de Custo Equivalente(S) = Redu- ção de Custo Equivalente(ES) Tabela 6: Escore, Redução de Custo (RC) e Redução de Custo de Equivalentes (RCE) dos Conjuntos Essenciais [7] Estratégia Escore RC(%) RCEq(%) STAT 0,99506 83,9 89,5 E5 0,99738 84,9 59,9 E27 0,99869 54,5 45,8 mutantes, a E5 gerou um total de 2293 mutantes e a E27 gerou um total de 6886 mutantes. Na terceira coluna da Tabela 2 é apresentada a ordem dos ope- radores de mutação da estratégia STAT. O primeiro operador su- gerido pela STAT é o u-OAAA, que corresponde ao operador com menor CDL_R (1,4%). O u-OAAA gera 50 mutantes (coluna #Mut), considerando os 5 programas UNIX utilizados e não há mutante equivalente desse operador (coluna #Eq). Ao encontrar um con- junto de teste que mate todos os mutantes do u-OAAA, esse con- junto de teste determina um escore de mutação de 0,63228 em rela- ção ao conjunto completo de mutantes (15.136 mutantes). Conside- rando, portanto, uma estratégia que utilize apenas esse operador, o custo da estratégia (CC) seria de apenas 50 mutantes, represen- tando uma redução (RC) de 99,7% em relação ao total de mutantes gerados. O custo de equivalentes da estratégia (CCEq) seria 0, que equivale a 100% de redução no número de equivalentes (RCEq). Ao se utilizar os cinco primeiros operadores da STAT (u-OAAA, u-OLLN, u-VSCR, u-OLNG e u-VLPR) o escore de mutação em rela- ção ao total supera os 0,91 e o custo da estratégia em termos do nú- mero total de mutantes gerados e equivalentes é de 244 e 1, respec- tivamente, o que corresponde a uma redução de custo em temos do total gerado e de equivalentes de 98,4% e 99,9%, respectivamente. Para se obter resultados em termos do escore de mutação seme- lhantes ao obtido pelos conjuntos essenciais E5 e E27 deveriam ser', 'total gerado e de equivalentes de 98,4% e 99,9%, respectivamente. Para se obter resultados em termos do escore de mutação seme- lhantes ao obtido pelos conjuntos essenciais E5 e E27 deveriam ser utilizados os 23 primeiros operadores da estratégia STAT. Nessa situação, o escore de mutação determinado seria superior a 0,995 com redução de custo em termos do número de mutantes gerados e equivalentes de 83,9% e 89,5%, respectivamente. Considerando as hipóteses estabelecidas, aplicou-se o teste de normalidade de Shapiro-Wilk baseado no Escore de Mutação, Re- dução de Custo e Redução de Custo Equivalente, considerando os 30', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Silva et al.. dados individuais de cada estratégia em cada programa. Por exem- plo, a Tabela 7 apresenta os dados referentes ao escore de mutação de cada estratégia em relação a cada programa analisado. Tabela 7: Escore de Mutação das Estratégias para os Progra- mas Unix Programas E5 E27 STAT Cal 0,999334 0,99978 0,998447 Checkeq 0,997880 0,99976 0,994348 Comm 0,992468 0,99652 0,988992 Look 0,995030 0,99503 0,990613 Uniq 0,998636 0,99932 0,999318 Os resultados dos testes estatísticos indicam que os dados não possuem uma distribuição normal com nível de con/f_iança de 95% (p-value ≤ 0,05). Isso sugere o uso de teste não paramétrico para veri/f_icar se existe diferença entre os grupos, para tal foi utilizado o teste de Wilcoxon [8] considerando nível de con/f_iança de 95% ( α = 0,05). As colunas nas Tabelas 8, 10 e 12, apresentam os resultados do teste para cada par de conjuntos de operadores. Como são re- alizados testes múltiplas hipóteses, deve-se aplicar o método de correção de Holm-Bonferroni, que resultou nos valores apresenta- dos [8]. Tabela 8: Escore de Mutação:p-value ajustado Estratégias ESS.5 ESS.27 ESS.27 0.19 - STAT 0.94 0.19 Tabela 9: Porcentagem de Redução de Custo das Estratégias para os Programas Unix Programas E5(%) E27(%) STAT(%) Cal 87,216 51,301 88,629 Checkeq 87,177 42,253 89,804 Comm 81,129 62,767 76,399 Look 81,389 70,395 78,206 Uniq 80,499 66,976 70,633 Tabela 10: Redução de Custo:p-value ajustado Estratégia ESS.5 ESS.27 ESS.27 0.19 - STAT 0.94 0.19 Como pode ser observado nas Tabelas 8, 10 e 12, o teste esta- tístico con/f_irma que não há diferença entre quaisquer estratégias quando comparadas par a par em termos de escore de mutação, redução de custo e redução de custo em termos de número de mu- tantes equivalentes. Desse modo, devem ser aceitas as hipóteses Tabela 11: Porcentagem de Redução de Custo Equivalente das Estratégias para os Programas Unix Programas E5(%) E27(%) STAT(%) Cal 54,400 34.400 93,333 Checkeq 45,528 48,374 93,902 Comm 73,333 50,833 90,417 Look 62,357 49,430 86,692 Uniq 66,926 51,751 81,712 Tabela 12: Redução de Custo Equivalente:p-value ajustado Estratégia ESS.5 ESS.27 ESS.27 0.38 - STAT 0.19 0.19 nulas H10, H20, H30, respectivamente. Observa-se que mesmo na Tabela 12 que, aparentemente, apresenta as maiores discrepâncias de valores, devido ao número de amostras, o teste estatístico não é capaz de con/f_irmar as diferenças entre as estratégias. 7 DISCUSSÃO Não havendo diferença estatística entre as estratégias, todas po- dem ser consideradas boas alternativas para a redução do custo do teste de mutação atingindo-se bons escores de mutação. Entretanto, quando analisadas as estratégias à luz do aspecto complementar entre análise estática e análise dinâmica, e conside- rando a ferramentaSplint, as duas primeiras colunas da Tabela 2 trazem informações sobre qual capacidade da Splint em detectar os defeitos modelados pelos operadores das estratégias E5 e E27. Na Tabela 2 a coluna STAT tem-se a ordem de aplicação dos operadores obtidos nesse experimento, nas colunas E5 e E27 tem- se a comparação da ordem de aplicação dos conjuntos obtidos por Barbosa et al. [7]. É possível notar que, no conjunto E5 o primeiro operador a ser aplicado por essa estratégia corresponde ao 41º ope- rador de acordo com a taxa CDLR( ok ) , o qual possui uma taxa CDLR( ok ) de 27,3%, ou seja, esse operador é mais facilmente iden- ti/f_icadoSplint em uma análise inicial do que o primeiro operador sugerido nesse trabalho. Observa-se que, dentre os 23 operadores da STAT estão apenas dois dos seis operadores da estratégia E5 e apenas três dos dez ope - radores da estratégia E27. Isso demonstra que, embora todos os conjuntos apresentem bons escores e reduções de custo, os con- juntos essenciais possuem pouca intersecção com os 23 primeiros operadores da STAT, mas a maioria dos seus operadores modelam', 'conjuntos apresentem bons escores e reduções de custo, os con- juntos essenciais possuem pouca intersecção com os 23 primeiros operadores da STAT, mas a maioria dos seus operadores modelam defeitos que são mais perceptíveis pelaSplint. Já os 23 primeiros operadores da STAT apresentam taxas de correspondência com a Splintabaixo de 14%. 8 AMEAÇAS À VALIDADE Nesta seção são apresentadas as ameaças observadas bem como as limitações relacionadas ao processo utilizado neste trabalho. 31', 'Incremental Strategy for Applying Mutation Operators Emphasizing Faults Diﬀicul t to be Detected by Automated Static AnalyserSBES’17, September 20–22, 2017, Fortaleza, CE, Brazil 8.1 Validade Externa A validade externa se deve ao risco da generalização dos resultados, pois em se tratando de um estudo experimental, alguns fatores cau- sam ameaças à validade do estudo e não permitem a generalização dos resultados obtidos. Algumas limitações do presente estudo estão relacionados com utilização da linguagem de programação C, da ferramenta de mu- tação e da ferramenta de análise estática. Nesse estudo foi empregada apenas a linguagem C e, assim, tais resultados não podem ser estendidos automaticamente para outras linguagens de programação. Em relação à ferramentas de análise estática e de mutação, fo- ram empregadas, respectivamente, apenas a Splint e Proteum/IM, e, deste modo, os resultados não podem ser automaticamente es- tendidos para outras ferramentas. De toda forma, os resultados demonstram que há correspondência entre os avisos emitidos por esse analisador e os tipos de defeitos modelados pelos operado- res daProteum/IM, resultado semelhante ao obtido no contexto de Java [2, 3]. Outra ameaça é o número de programas utilizados, bem como o domínio de aplicação destes programas. Entretanto, tais progra- mas continuam sendo extensivamente empregados em outros es- tudos realizados por outros pesquisadores. 8.2 Validade Interna A validade interna se deve ao controle do processo de experimen- tação para a coleta dos dados analisados. Inicialmente, foram ge- rados os mutantes, por meio da ferramentaProteum/IM, dos pro- gramas apresentados na Tabela 1. Todos os operadores de unidade daProteum/IM foram selecionados para gerar a maior quantidade possível de mutantes. Nem todos os tipos de defeitos modelados pelos operadores fo- ram avaliados, uma vez que 16 operadores não geraram mutantes pois os programas não possuíam as estruturas sintáticas exigidas pelos mesmos. Entretanto, para a determinação dos conjuntos E5 e E27 essa mesma ameaça existiu e novos estudos envolvendo outros programas precisam ser identi/f_icados para minimizar essa ameaça. A Splint foi utilizada no seu modo mais completo( -strict), para que fosse relatado o maior número de avisos possíveis. Para deter- minar o local onde ocorreu a mutação, durante a coleta dos dados foi realizada a diferença textual(diﬀ ) entre cada mutante e o pro- grama original. 8.3 Validade de Conclusão A validade de conclusão diz respeito diz respeito à capacidade de se chegar a conclusão correta entre os relacionamentos entre o tra- tamento e o resultado. Mesmo com um conjunto reduzido de dados, os testes realizados são condizentes e sustentam os resultados obtidos. Com a expansão do estudo para um grupo maior de programas espera-se aumentar ainda mais a con/f_iança estatística nos resulta- dos. 8.4 Validade de Construção A validade de construção está relacionada com a teoria e a observa- ção. Como a coleta dos dados foi realizada de forma automatizada por meio de scripts construídos por um dos autores e esses scripts foram revisados e validados por outro autor, problemas decorren- tes da intervenção humana na geração e manipulação dos dados do experimento foram minimizados. 9 CONCLUSÃO O objetivo inicial deste artigo foi dar continuidade ao trabalho de Araujo et al. [2, 3] e estabelecer uma estratégia incremental de apli- cação de operadores de mutação para C, com base nas informa- ções de análise estática. A estratégia STAT foi de/f_inida e avaliada em relação aos conjuntos essenciais E5 e E27 obtidos por Barbosa et al. [7] segundo valores de escore mutação, custo em termos de números de mutantes gerados, e custo em termos do número de mutantes equivalentes gerados pelas estratégias. O conjunto de operadores da estratégia STAT é composto pelos primeiros 23 operadores da Tabela 2, considerando que o ponto de corte para determinar tal conjunto foi o escore de mutação superior', 'O conjunto de operadores da estratégia STAT é composto pelos primeiros 23 operadores da Tabela 2, considerando que o ponto de corte para determinar tal conjunto foi o escore de mutação superior a 0,995, valor próximo ao valor médio relatado por Barbosa et al. [7] em seu estudo. Os testes estatísticos realizados demonstraram que segundo os aspectos investigados, não há diferença entre a STAT e os conjun- tos essenciais E5 e E27 do ponto de vista de escore de mutação, e reduções de custo em termos do número de mutantes gerados e equivalentes. Isso indica que sob a ótica do teste de mutação, que esses subconjuntos de operadores de mutação são bons candidatos para redução de custo e manutenção da e/f_icácia do critério. Entretanto, considerando o aspecto complementar entre análise estática e o teste de mutação, foi possível constatar que os operado - res dos conjuntos essenciais E5 e E27, em sua maioria, não fazem intersecção com os 23 primeiros operadores do conjunto STAT. O primeiro operador a ser executado no conjunto E5 por exemplo, corresponde ao 41º operador de acordo com a taxaCDLR( ok ) , ou seja, esse operador representa um defeito que a Ferramenta Splint consegue identi/f_icar mais facilmente do que o primeiro operador de/f_inido pela STAT. É importante mencionar que apenas dois dos seis operadores da estratégia E5 e apenas três dos dez operadores da estratégia E27 fazem intersecção com os 23 primeiros operado- res da STAT. Nesse sentido, considera-se uma vantagem o uso da STAT pois esta utiliza, inicialmente, os operadores que modelam defeitos mais difíceis de serem detectados por meio de análise estática. Como trabalhos futuros ainda é necessário realizar a mesma aná- lise em um grupo maior de programas e para diferentes domínios de aplicação. Além disso, pretende-se introduzir outros analisado- res estáticos e veri/f_icar quais outros tipos de defeitos podem ser melhor identi/f_icados por tais analisadores. Finalmente, pretende- se realizar uma análise qualitativa dos dados visando a identi/f_icar categorias de defeitos identi/f_icadas e as causas que proporcionam essa identi/f_icação. REFERÊNCIAS [1] J. H. Andrews, L. C. Briand, and Y. Labiche. 2005. Is mutation an ap propri- ate tool for testing experiments?. In XXVII International Conference on Soft- ware Engineering – ICSE’05 . ACM Press, New York, NY, USA, 402–411. DOI: https://doi.org/10.1145/1062455.1062530 [2] Cláudio Antonio Araujo, Márcio Eduardo Delamaro, José Carlos Maldonado, and Auri Marcelo Rizzo Vincenzi. 2016. Correlating automatic static ana ly- sis and mutation testing: towards incremental strategies. Journal of Software 32', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Silva et al.. Engineering Research and Development – JSERD 4, 1 (Nov. 2016), 1–32. DOI: https://doi.org/10.1186/s40411-016-0031-8 Artigo em aval iação. Versão esten- dida do artigo “Investigating the Corresponcende between Mutations and Static Warnings” premiado entre os melhores artigos do SBES’2015. [3] Cláudio Antônio Araújo, Márcio Eduardo Delamaro, José Carlos Maldonado, and Auri Marcelo Rizzo Vincenzi. 2015. Investigating the Corresponcende between Mutations and Static Warnings. InXXIX Simpósio Brasileiro de Enge- nharia de Software – SBES’2015 . SBC, SBC, Belo Horizonte, MG, 1–10. DOI: https://doi.org/10.1109/SBES.2015.23 Premiado como um dos melhores artigos do SBES’2015. [4] Nathaniel Ayewah, William Pugh, J. David Morgenthaler, John Pe nix, and YuQian Zhou. 2007. Evaluating Static Analysis Defect Warnings on Produc tion Software. In Proceedings of the 7th ACM SIGPLAN-SIGSOFT Workshop on Pro- gram Analysis for Software Tools and Engineering (PASTE’07) . ACM, New York, NY, USA, 1–8. DOI:https://doi.org/10.1145/1251535.1251536 [5] Nathaniel Ayewah, William Pugh, J. David Morgenthaler, John Pe nix, and YuQian Zhou. 2007. Using FindBugs on Production Software. In Companion to the 22Nd ACM SIGPLAN Conference on Object-oriented Programmi ng Systems and Applications Companion (OOPSLA ’07) . ACM, New York, NY, USA, 805–806. DOI:https://doi.org/10.1145/1297846.1297897 [6] E. F. Barbosa. 1998. Uma Contribuição para a Determinação de um Conjunto Essencial de Operadores de Mutação no Teste de Programas C . Master’s thesis. ICMC-USP, São Carlos – SP. [7] E. F. Barbosa, J. C. Maldonado, and A. M. R. Vincenzi. 2001. Towar ds the Determi- nation of Suﬃcient Mutant Operators for C. STVR – Software Testing, Veri/f_ication and Reliability 11, 2 (June 2001), 113–136. DOI:https://doi.org/10.1002/stvr.226 [8] George E. P. Box, J. Stuart Hunter, and William G. Hunter. 2005. Statistics for Experimenters: Design, Innovation, and Discovery (2 ed.). Wiley-Interscience. [9] Tom Copeland. 2005. PMD Applied: An Easy-to-use Guide for Developers . Cen- tennial Books. [10] Cesar Couto, João Eduardo Montandon, Christofer Silva, and Ma rco Tulio Va- lente. 2013. Static Correspondence and Correlation Between Field D efects and Warnings Reported by a Bug Finding Tool. Software Quality Control 21, 2 (June 2013), 241–257. DOI:https://doi.org/10.1007/s11219-011-9172-5 [11] J. E. de Araújo Filho, Cesar Francisco de Moura Couto, Sílvio J osé de Souza, and Marco Túlio Valente. 2010. Um Estudo Sobre a Correlação Entre Def eitos de Campo e Warnings Reportados por Uma Ferramenta de Análise Estática. In IX Simpósio Brasileiro de Qualidade de Software – SBQS’2010 . Belém, PA, 9–23. [12] M. E. Delamaro. 1997. Mutação de Interface: Um Critério de Adequação Inter- procedimental para o Teste de Integração . Ph.D. Dissertation. Instituto de Física de São Carlos – Universidade de São Paulo, São Carlos, SP. [13] M. E. Delamaro, J. C. Maldonado, and M. Jino. 2016. Introdução ao Teste de Software (2 ed.). Elsevier, Rio de Janeiro, RJ. [14] M. E. Delamaro, J. C. Maldonado, and A. M. R. Vincenzi. 2000. Pro teum/IM 2.0: An Integrated Mutation Testing Environment. In Mutation 2000 Symposium . Kluwer Academic Publishers, San Jose, CA, 91–101. [15] Marcio Eduardo Delamaro, Jeﬀ Oﬀutt, and Paul Ammann. 2014. Des igning Dele- tion Mutation Operators. In Proceedings of the 2014 IEEE International Conference on Software Testing, Veri/f_ication, and Validation (ICST ’14) . IEEE Computer Soci- ety, Washington, DC, USA, 11–20. DOI:https://doi.org/10.1109/ICST.2014.12 [16] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. 1978. Hints on Tes t Data Selection: Help for the Practicing Programmer. IEEE Computer 11, 4 (April 1978), 34–43. [17] David Evans and David Larochelle. 2002. Improving Security Using Extensi- ble Lightweight Static Analysis. IEEE Software 19, 1 (Jan. 2002), 42–51. DOI: https://doi.org/10.1109/52.976940', '[17] David Evans and David Larochelle. 2002. Improving Security Using Extensi- ble Lightweight Static Analysis. IEEE Software 19, 1 (Jan. 2002), 42–51. DOI: https://doi.org/10.1109/52.976940 [18] David Hovemeyer and William Pugh. 2004. Finding Bugs is Easy. SIGPLAN Not. 39, 12 (Dec. 2004), 92–106. DOI:https://doi.org/10.1145/1052883.1052895 [19] ISO/IEC/IEEE. 2010. Systems and software engineering – vocabul ary. (Dec. 2010), 418 pages. DOI:https://doi.org/10.1109/IEEESTD.2010.5733835 [20] Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge. 2013. Why don’t software developers use static analysis tools to /f_ind bugs?. In Proceedings of the 2013 International Conference on Software En gineering (ICSE ’13). IEEE Press, Piscataway, NJ, USA, 672–681. [21] Florent Kirchner Virgile Prevosto Armand Puccetti Julien Signol es Loic Corren- son, Pascal Cuoq and Boris Yakobowski. 2014. Frama-C Software Ana lyzers. Página Web. (March 2014). Disponível em: http://frama-c.com/. A ccesso em: 03/05/2017. [22] Panagiotis Louridas. 2006. Static Code Analysis. IEEE Software 23, 4 (July 2006), 58–61. DOI:https://doi.org/10.1109/MS.2006.114 [23] Y.-S. Ma, J. Oﬀutt, and Y. R. Kwon. 2005. MuJava: an automated class mutation system: Research Articles. STVR – Software Testing, Veri/f_ication and Reliability 15, 2 (2005), 97–133. DOI:https://doi.org/10.1002/stvr.v15:2 [24] Microsoft. 2011. FxCop. Página WEB. (2011). Disponível em: http://msdn.microsoft.com/en-us/library/bb429476.aspx. Aces so em: 01/08/2011. [25] Microsoft. 2014. StyleCop. Página Web. (March 2014). Dispo nível em: https://stylecop.codeplex.com/. Accesso em: 07/02/2014 . [26] A. J. Oﬀutt, A. Lee, G. Rothermel, R. H. Untch, and C. Zapf. 1996 . An Expe- rimental Determination of Suﬃcient Mutant Operators. ACM Transactions on Software Engineering Methodology 5, 2 (April 1996), 99–118. [27] A. J. Oﬀutt, G. Rothermel, and C. Zapf. 1993. An Experimental Ev aluation of Se- lective Mutation. In 15th International Conference on Software Engineering . IEEE Computer Society Press, Baltimore, MD, 100–107. [28] Jochen Pohl. 2001. Lint – C program veri/f_ier. Página Web. (May 2 001). Disponível em: http://www.unix.com/man-page/FreeBSD/1/lint. Ac cesso em: 02/05/2017. [29] A. M. R. Vincenzi. 1998. Subsídios para o Estabelecimento de Es- tratégias de Teste Baseadas na Técnica de Mutação . Master’s thesis. ICMC-USP, São Carlos, SP, Brasil. Disponível em: http://www.teses.usp.br/teses/disponiveis/55/55134/tde-06022001-182640. Acesso em: 21/10/2004. [30] W.E. Wong, J.C. Maldonado, M.E. Delamaro, and S.R.S. Souza . 1997. A Compari- son of Selective Mutation in C and FORTRAN. In Workshop do Projeto Validação e Teste de Sistemas de Operação . Águas de Lindóia, SP, 71–80. [31] W. E. Wong. 1993. On Mutation and Data Flow . Ph.D. Dissertation. Department of Computer Science, Purdue University, W. Lafayette, IN. [32] Z. Zhioua, S. Short, and Y. Roudier. 2014. Static Code Analysis fo r Software Security Veri/f_ication: Problems and Approaches. In 2014 IEEE 38th Internatio- nal Computer Software and Applications Conference Worksho ps. 102–109. DOI: https://doi.org/10.1109/COMPSACW.2014.22 [33] Zeineb Zhioua, Stuart Short, and Yves Roudier. 2014. Towards th e veri/f_ication and validation of software security properties using static code anal ysis. Com- puter Science: Theory and Application 2, 2 (Dec. 2014), 23–34. 33']","['INCREMENTAL STRATEGU FOR APPLYING OPERATORS EMPHASIZING FAULTS DIFFICULT TO BE DETECTED BY AUTOMATED STATIC ANALYSER This  briefin  reports  scieitfc  evideice oi Mutatoi Testin Strateny.i FINDINGS \uf0b7 Um subcoijuito de operadores de mutação selecioiados  a  partr  de  dados  de  aiálise estátca.i \uf0b7 O coijuito STAT se mostrou atraeite dado o aspecto da combiiação eitre aiálise estátca e aiálise diiâmica.i \uf0b7 Teido  operadores  de  mutação  com  meior sobreposição  com  defeitos  passíveis  de detecção  pelo  aialisador  estátco automatiado emprenado,a a STAT se mostra uma boa opção para a aplicação do Teste de Mutação.i \uf0b7 A correspoidêicia eitre avisos  e mutaites foi  baseada  ia  taxa  de  Correspoidêicia Direta  por  Liiha  (CDL)  ios  permitu quaitfcar  o  iúmero  de  mutaites ideitfcados  pelo  aialisador  estátco  e determiiar a STAT.i \uf0b7 Foi  ideitfcado  um  coijuito  dos  tpos  de avisos  do  aialisador  estátco  que  mais ideitfcaram  as  mutações  produiidas  ios mutaites.i \uf0b7 Foi  ideitfcado  um  coijuito  dos  tpos  de avisos  do  aialisador  estátco  que  meios ideitfcaram  as  mutações  produiidas  ios mutaites.i \uf0b7 Foi ideitfcado um coijuito de operadores de  mutação  que  mais  tveram correspoidêicia com os avisos do aialisador estátco.i \uf0b7 Foi ideitfcado um coijuito de operadores de  mutação  que  meios  tveram  mutações ideitfcadas pelo aialisador estátco.i \uf0b7 A  avaliação  qualitatva  dos  dados  nerados permitem a evolução do aialisador estátco e da ferrameita de mutação utliiados.i \uf0b7 A taxa de Correspoidêicia Direta por Liiha (CDL)  em  ordem  cresceite  permitu  que chenasse  io subcoijuito de operadores da STAT.i \uf0b7 Os operadores com as meiores taxas CDL são aqueles que neram mutaites mais difceis de serem  percebidos  pelo  aialisador  estátco utliiado.i \uf0b7 Foi  feita  a  comparação  estatstca  eitre  os operadores  da STAT,a E5 e E27 par a par e foi coistatado  que  ião  existe  difereiça estatstca eitre as estratginias.i \uf0b7 A  difereiça  estatstca  foi  baseada  ia comparação par a par em termos de escore de mutação,a redução de custo e redução de custo  em  termos  de  iúmero  de  mutaites equivaleites.i \uf0b7 Mesmo  ião  existido  difereiça  estatstca eitre  as  estratginias,a  foi  possível  coistatar que  houve  pouca  iitersecção  eitre  os operadores da STAT e os operadores da E5 e da E27.i \uf0b7 Deitre  os  23  operadores  da  STAT  estão apeias  dois  dos  seis  operadores  da estratginia  E5  e  apeias  três  dos  dei operadores da estratginia E27.i \uf0b7 O coijuito esseicial STAT foi baseado ios 23 primeiros  operadores  de  mutação  e apreseitam uma taxa CDL abaixo de 14%.i \uf0b7 Mesmo  a  STAT  teido  mais  operadores mutação,a nerou 6,a2% mais  mutaites que o E5,a e 64,a6% meios mutaites que a E27.i \uf0b7 Os resultados obtdos possibilitam que sejam propostas melhorias taito para o aialisador estátco  quaito  para  a  ferrameita  de mutação utliiada.i \uf0b7 Para o aialisador estátco gi possível sunerir a criação  de  iovas  renras  de  detecção  que toriem o aialisador mais robusto.i \uf0b7 Para  a  ferrameita  de  mutação  gi  possível sunerir  quais  operadores  de  mutação emprenar  ou  ião  duraite  o  teste  de mutação.i       Who is this briefin  or? Para  profiisooasi  de  eogeoharsa  de ioftare a fi de toroar iasi eficseote a atisdade de teitea Where the finiins come  rom? Todai  ai  deiicobertai  deite  Brsefog forai extraídai do trabalho icooduzsdo por Sslia et ala What is iiclunen ii this briefin? Ioforiaçõei báisicai iobre ai prsoicspasi icootrsbusçõei do trabalho e uia breie apreieotação do icoojuoto STATa To access other evineice briefins  oi softare einiieeriine http://tttalsaauficabr/cicbioft2017//xxxs -ibei/ For annitoial ii ormatoi about  LaPESe http://lapeiadicaufiicarabr/']","**Title:** Enhancing Software Testing with Incremental Mutation Strategies

**Introduction:**  
This evidence briefing summarizes findings from a study focused on improving software testing through an incremental strategy that applies mutation operators. The aim is to enhance the detection of faults that are difficult for automated static analyzers to identify, thereby improving overall software quality.

**Main Findings:**  
1. **Incremental Strategy Development:** The proposed strategy prioritizes mutation operators that introduce faults less detectable by automated static analyzers. This approach is based on empirical evidence demonstrating that certain faults are consistently overlooked by static analysis tools.

2. **Comparison of Strategies:** The study compared the incremental mutation strategy against two established strategies (E5 and E27) regarding mutation scores and costs associated with generated mutants. The results indicated no significant statistical differences in mutation effectiveness or costs across the strategies.

3. **Cost Efficiency:** While all strategies yielded similar mutation scores, the incremental strategy was found to generate fewer mutants overall, leading to reduced testing costs. This reduction is particularly beneficial when considering the computational resources required for testing.

4. **Complementarity of Techniques:** The findings highlight the complementary nature of static analysis and dynamic mutation testing. The incremental strategy focuses on faults that static analyzers struggle to detect, thus optimizing the testing process by targeting potential weaknesses in software.

5. **Practical Implications:** Practitioners can implement the incremental mutation strategy to enhance their testing frameworks, especially in environments where static analysis tools produce high rates of false positives. By focusing on harder-to-detect faults, teams can improve their defect detection capabilities and allocate resources more effectively.

**Who is this briefing for?**  
This briefing is intended for software engineers, quality assurance professionals, and researchers interested in enhancing software testing methodologies, particularly those exploring the integration of static and dynamic analysis techniques.

**Where the findings come from?**  
All findings in this briefing are derived from the experimental evaluation conducted by Vinícius Barcelos Silva et al. (2017) at the SBES’17 conference, which investigated the effectiveness of an incremental strategy for applying mutation operators in software testing.

**What is included in this briefing?**  
The briefing includes a summary of the incremental mutation strategy, its comparative analysis with other strategies, and its implications for software testing practices.

**What is NOT included in this briefing?**  
Detailed statistical metrics and exhaustive technical descriptions of the methodologies used in the original study are not included. The focus is on practical implications and findings relevant to practitioners.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research group:**  
Contact Vinícius Barcelos Silva at vinicius.silva@dc.ufscar.br.

**Original Research Reference:**  
Vinícius Barcelos Silva, Cláudio Antonio Araujo, Edmundo Sérgio Spoto, and Auri M R Vincenzi. (2017). Incremental Strategy for Applying Mutation Operators Emphasizing Faults Difficult to be Detected by Automated Static Analyser. In Proceedings of SBES’17, Fortaleza, CE, Brazil. DOI: [10.1145/3131151.3131169](https://doi.org/10.1145/3131151.3131169)"
"['Is There a Demand of Software Transparency?  Roxana Lisette Quintanilla Portugal1, Priscila Engiel1,2, Hugo Roque1,   Julio Cesar Sampaio do Prado Leite1  1Departamento de Informática, PUC-Rio, Rio de Janeiro, Brasil  2Dept. of Information Engineering and Computer Science, University of Trento, Italy  {rportugal, pengiel, julio}@inf.puc-rio.br  hugo.roque@aluno.puc-rio.br  ABSTRACT  This paper uses data about bills proposed to the Brazilian  Congress to better understand transparency demand. The  legislative proce ss starts with bills, proposed by members  of congress, which after the discussion on both chambers,  may become a law. Webcitizen, a software company, offers  to the Brazilian public an internet based software system  with which people can vote on bills. This software interface  is a site named Votenaweb, which gathers votes and  publicize the results. As such, it aims to show how citizens  value the proposed bills. Our study identifies bills related to  transparency by using non -functional requirements   (hereafter, NFRs) of the T ransparency Softgoal  Interdependency Graph  (hereafter, SIG). Based on these  results, we argue that there is a real future demand for  software transparency, as advocated by previous work on  software transparency.    Categories and Subject Descriptors  Software and its engineering ~Software notations and  tools • Domain specific languages • Software and its  engineering~Software libraries and repositories.  General Terms  Design, Legal Aspects.  Keywords  Transparency, Brazilian bills, information r etrieval, text  mining, non-functional requirements, target transparency.  1. INTRODUCTION  The movement towards transparency is seen as a natural  evolution of democratic societies  [1], where c itizen  participation in decisions is based on the right to know.   Different work in different areas are tackling the concept of  transparency, as for instance in business [2][3], in culture  [4][5], or in regulations [6]. Also, there exists some  research conference in the field, i.e. The Global Conference  on Transparency Research, an international  multidisciplinary aca demic conference. This movement is  also supported by different Nongovernmental  Organizations (NGOs), from which th e Transparency  International [7] is the one of the most well -known ones,  despite its narrow focus.  In Software En gineering different authors pointed out  the role of transparency [8][9][10][11][12][13]. The shift to  a more connected society had an impact on software  production, as seen by the increasing offer of open  software. On the other hand, organizations of different  kinds are requested to provide more transparency to  citizens about the information and processes they handle.  Despite of the transparency movement, which is  more  related to politics, laws and information, the connection of  software to transparency, may not be easily perceived.  Notwithstanding, as citizens deal more and more with  software on a daily basis, this connection will be more  evident.  The Votenaweb service [14] started on 2010, and by  2016 published 5011 bills. Its interface does not provide  much information about their process, but there is an e ntry  on the Wikipedia that gives a brief explanation, claiming  that the service already registered more than 10 million  votes. Access to Votenaweb is open for browsing, but it  requires login identification for voting or commenting on  the bills. The selection of Votenaweb for our research was  based on its popularity, as we do not know of any other  similar initiative in Brasil.   To the best of our knowledge, data from Votenaweb has  only been studied from the perspective of deliberation in  on-line forums [15]. Our Transparency Gr oup at PUC -Rio  began using  Votenaweb [16] to perform  a manual  classification of bills related to targeted transparency  policies [6] in 2013. A subsequent work [17], incorporates', 'began using  Votenaweb [16] to perform  a manual  classification of bills related to targeted transparency  policies [6] in 2013. A subsequent work [17], incorporates  previous experience in repository mining  [18][19] to cover  ACM Reference Format:  R. L. Q. Portugal, P. Engiel, H. Roque, J. C. S. P. Leite. 2017. Is There a  Demand for Software Transparency? In Proceedings of 31st Brazilian  Symposium on Software Engineering, Fortaleza, Ceará, Brazil,  September 2017 (XXXI SBES), 10 pages.  https://doi.org/10.1145/3131151.3131155  Permission to make digital or hard copies of all or part of this work for personal  or classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice  and the full citation on the first page. Copyrights for components of this work  owned by others than the author(s) must be honored. Abstra cting with credit is  permitted. To copy otherwise, or republish, to post on servers or to redistribute  to lists, requires prior specific permission and/or a fee . Request permissions  from Permissions@acm.org.  SBES’17, September 20-22, 2017, Fortaleza, CE, Brazil.  © 2017 Copyright is held by the owner/auth or(s). Publication rights  licensed to ACM.  ACM ISBN 978-1-4503-5326-7/17/09…$15.00.  https://doi.org/10.1145/3131151.3131155  204', 'all existing bills in  Votenaweb. Further, we started to  explore the citizens´ comments on the bills [20].  Our goal with this paper is to verify if a demand for  software transparency is latent. We tackle the challenge by  reducing the problem to a more abstract question, which is,  to verify if there is a dema nd for transparency in a broader  sense. Therefore, based on previous work  [16][17][20], we  evolved early results on text mining bills and their votes to  better profit from background knowledge on transparency.   As such, we produced a process that uses information  retrieval (IR) techniques as well as text mining (TM). It  performs context elicitation, by means of text mining, with   the reuse of NFRs [21], specifically the NFR transparency  SIG [22]. Our process uses the large set of bills in  Votenaweb, 5011 bills to October of 2016 . We aim to find  bills that are transparency related and how they fare with   contributors (citizens). The results of this mining process is  then analyzed as to  evaluate our argument that the demand  for transparency in general will be , at some point in the  future, demanded from software artifacts, as a product and  as a process.    Section 2 reviews our understanding of software  transparency. Section 3 describes the process we have used  for mining. Section 4 provides  an analysis of bills mined  and how it can be a response to our question.  Section 5   briefly reviews literature about the use of NFRs as a drive  for mining texts . We conclude by summarizing the  contribution and arguing of why the data we produce may  be a response to our question as in the title, as well as  pointing to future work on software transparency.  2. SOFTWARE TRANSPARENCY  Motivated b y the fact that transparency is an important  factor in achieving democracy, the Requirements  Engineering (RE) group at PUC -Rio has been working on  eliciting the concept and modeling it as a n NFRs, using the  ideas of the NFR Framework [21].   While Cappelli [23][24] studied ways of applying  Transparency towards organizations, t he main focus of the  PUC-Rio RE Group is software t ransparency  [10][11][25][26]. The group understands software  transparency as a quality, and perceive s its impact on  software in manners similar to security, safety, privacy and  others qualities, which are generally understood as NFRs.  NFRs are characteristics that crosscut the functions of any  given software. Having transparency or not having  transparency will  not affect what the software does, but  how it does it. In addition, transparency is general, and as  such, spreads to different parts of a given software system.  Other research has used the concept of transparency with  respect to the openness of software as they became  available in detail (code) [9]. They a lso focus on the  disclosure of target information about its capabilities and  limitations, as for its potential users/clients to make an  informed decision [10], or stress the social relations within  the software production organization [12]. We, on the other  hand, focus on a broader sense of software transparency.   Figure 1 adapts a figure used in [10], whereas different  levels of transparency are shown.  We aim our work on the three levels: i) software  ecology transparency , when citizens are more concerned  with information transparency. ii) software interaction  transparency, when users or clients are concerned with the  mediator of information transparency  (the  product/software). iii) software organization  transparency, when stakeholders are concerned about  internal transparency of a product (the process of the  product). Concerning software production, Herbsleb  [27]  uses a definition of transparenc y inherited from Bernstein   [28] who states that transparency is “accurate observability  of an organization’s low level activities, routines,  behaviors, output, and performance”. This operational', '[28] who states that transparency is “accurate observability  of an organization’s low level activities, routines,  behaviors, output, and performance”. This operational  definition helps to focus on tra nsparency issues within the  software production organization.    Figure 1.  Levels of Software Transparency adapted from [10]  Those levels may allow to citizens to be informed about  software, witho ut restraining the transparency to software  production workers or clien ts/users. Therefore, we want  that actors belonging to the  different levels have adequate  access to what the software does and why, so the focus is  beyond information that the software processes but on the  software itself, stressing the perspective of proce ss  transparency.  Our group has gone  through an effort of eliciting and  modeling transparency as an NFR using the semantics of a  SIG [21], by eliciting a network of 33 softgoals  qualities  that are related to the concept of  transparency [22]. Figure  2 illustrates the SIG model , which shows the relation of  qualities by using the operator Help (+) of the NFR  framework [21].  205', ""We cannot say that something is or is not transparen t.  We will need to use a less objective judgment, like almost  transparent, or transparent enough, and so on . In that sense,  transparency is a kind of characteristic that defies the  notion of satisfaction in the traditional sense . The NFR  criteria for  satisfaction relies on  the shoulders of Herbert  Simon [29], who coined the term satisfice to denote an  outcome of a bounded rationality process, which is central  to his Administrative Behavior theory. We use the term  satisfice to name the degree of how a softgoal is fulfilled.      Figure 2.  Transparency Softgoal Interdependency Graph (SIG) As such, the use of help brings a semantic relation  among these qualities that are different from the usual  ontology's operators for mereology (part -of) or taxonomy  (is-a). So, more helps from the contributing qualities means  more t ransparency, keeping in mind that the help range  vary.  On the other hand, less helps means transparency to a  lesser degree, but still there is transparency.    The SIG of Figure 2 relates the qualities in a two -tier  graph.  The first tier is composed of five qualities, which  helps (+) transparency:   i. Accessibility: information about the software is  available to the external environment.   ii. Usability: available information can be easily   obtained and used.  iii. Informative: information is made available with  expected quality.  iv. Understandability: external users can understand  the available information  v. Auditability: external users can certify that the  available information is trustworthy.   A second tier of qualities helps (+) each of the qualities  in the first tier.  We further operationalized the transparency SIG as a  catalogue using the strategy of Goal, Questi on,  Operationalization (GQO) [11] for providing alternativ es  ways of satisficing these qualities by means of processes or  functions [22]. Figure 3 sho ws how we would  operationalize availability for one of the comp onents  (corpus of bills) of our proposed process for text mining  Votenaweb.      Figure 3.  An operationalization for availability quality  206"", 'The operationalization “Identify access resources”  is a  possible softgo al operationalization, which is further  detailed by possible ans wers to the questions of GQO [11]  for the transparency SIG. That is, in our process we make  (++) available the corpus of bills used to perform the  mining strategy at a public URL.  3. THE PROCESS TO DISCOVER BILLS  In order to attain our goal of text mining the bills of  Votenaweb repository, we en gineered a process for  automating the discovery of transparency related bills  as  depicted in Figure 4, in which it is  used the transparency  SIG as base to drive the automated search. We propose a  semi-automated approach, where, as a manual task, a  Requirement Engineer needs to  select the apparent  keywords for the qualities on the SIG, and taking into  account the context of the data . In our case, we are dealing  with data about Brazilian bills, which are in the Portuguese  language. As of now, we are still tun ing this process, but  the results do satisfice our goal of identifying the popularity  of transparency related bills, as per users of the Votenaweb.   Each activity of this process will be detailed below.    Figure 4.   Transparency bills mining  3.1 Building a Corpus  From Votenaweb [14], 5011 summary texts of bills were  retrieved with the aim to compose a proper corpus for text  mining. This retrieval activity  was based on a similar work  [18][19], which make use of an A pplication Program  Interface (API) for requesting data. However, as  Votenaweb does not have an API, a crawling script was  needed in order to retrieve the bills. The name of bills in  the corpus follows this pattern:  0516-pls-79-2010   [item in corpus - bill type - number - year]  The corpus is publicly available at https://git.io/vSdME for  further research.  3.2 Choose of keywords  The Transparency SIG describes 33 qualities, as depicted in  Figure 2 that help to achieve transparency. Using the  qualities as keywords may seem the first consideration;  however, we needed to consider other words in the context  of the Brazilian bills, which could be used as surrogate for  the quality characteristics. In this  activity, the authors  discussed possible keywords for each quality by testing the  keywords within the Votenaweb search box. This tuning  exercise produced a list of keywords available at  https://git.io/vSdMP.  An example o f this tuning is the list of keywords  displayed in Table 1 that we used to get bills for each of to  the levels of transparency of Figure 1.  Table 1. Keywords related to levels of software transparency  Levels Target Keywords  L3 Focus on citizens transparência,  informação,  informações, dados  L2 Focus on Sofware  Users/Clients  transparência,  software  L1 Focus on Software  Production Stakeholders  transparência,  processo  207', '3.3 Creating Queries  This activity addressed the problem we faced whe n  querying Votenaweb on its search box, as this website does  not allow phrase queries, e.g. ""transparência da informação "".  However, using text -mining packages for R  [30][31][32]  phrase queries can be performed. As such, we created a  script for querying  phrase queries, for example:  ""transparência da informação "", ""transparência das informações "",  ""transparência dos dados "", ""transparência de software "", and  ""transparência do p rocesso"". Finding exact phrases is more  precise than just single keywords, h owever, a s finding  phrase query  in bills  may be rare , the queries are split to  create a query string , e.g. transparência + dados , which  despite giving less accurate results, may pr esent hints about  transparency bills.  Below, Table 2 shows the results for th e mentioned  queries. A s expected, we noticed that phrase queries  got  fewer results. On the other hand, by using a query strings,  we have more  results but some of them can  be false  positives. We tackle this issue , by adding more weight to  bills that owns several words related to qualities of  transparency SIG, f or that, we created a weighing criterion  that is explained at the Ranking of bills sub-section.  Table 2. Phrase Queries X Query strings  Queries Results Bill  Phrase queries  “transparência das informações” 1 Obrigará aos  conselhos  profissionais a  se  submeterem  às regras da  Lei de Acesso  à Informação.  “transparência do processo” 1 Obrigará a  gravação e  manutenção  do áudio das  sessões  deliberativas  dos conselhos  Query strings “transparência” 82  https://git.io/vS FOf  “transparência” + “informação” 8  “transparência” + “informações” 24  “transparência” + “dados” 12  “transparência” + “processo” 13  “transparência” + “software” 0    3.4 Text Mining of Bills  As specified in 3.3, text-mining tasks were performed wi th  the statistic tool R [30], an open source project, with a  strong community on data analysis , as such providing  several packages that deal with text mining . R itself is a  programming language, with which we  were able to create  functions for our specific goal.  The bills were mined by using the R packages  [31][32].  Each bill was text -mined by using all keywords identified  in section 3.2. The results are formatted in Json for further  qualitative analysis and are available at https://git.io/vSdyJ.  We use the first bill in Table III  to exemplify the JSON  structure:  [      {      ""bill"": ""pls-79-2010"",      ""text"": "" ... As a udiências deverão ser realizadas na sede do Poder  Executivo, nos fins de semana ou outro dia que permita uma maior  participação dos interessados. O resultado da audiência deverá ser publicado  nos dois maiores jornais da  região atingida pela política ... ter [ acesso ] a  qualquer [ informação ] sobre a política a ser debatida, podendo pedir essas  informações a qualquer órgão; Não se tratando de uma política delimitada  geograficamente, deverá se consultar a população dos municípios envolvidos.  Os municípios co m mais de 100.000 habitantes deverão [ disponibilizar ]  mecanismos de consulta às audiências públicas pela internet. Segundo o  senador, as audiências garantem uma boa participação popular, mas pode ser  algo manipulado, pois não existe um marco legal que ga ranta os direitos do  cidadão participante. Assim, o projeto pretende normatizar as audiências  públicas, focando na publicidade e [ transparência ]... ”          ""query"": [""transparência AND informações"",  ""acesso AND informação"",  ""disponibilizar AND informação"", ""divulgação AND informação""]      ""date"": ""2017-03-07 16:19:23"",    }  ]   Figure 5 summarizes the quantity of bills obtained for  each quality in first -tier of the SIG, it is important to note  that some bills can be repeated in each quality. In total, 360  bills were text mined, however, there are bills that despite', 'each quality in first -tier of the SIG, it is important to note  that some bills can be repeated in each quality. In total, 360  bills were text mined, however, there are bills that despite  having several qualities in their texts, do not have the main  quality, i.e. transparência. In this regard, we filtered  projects that explicitly have the word transparência, and  with this, we were able to filter 44 bills.    Figure 5.  Bills mined by quality in first tier of transparency  SIG  Table 3  shows the top 20 bills of 44 https://git.io/v9EZ0,  which are ordered by weighing. Weighing criterion is  208', 'explained next . Furthermore, the column precision is  explained in Section 4.   3.5 Ranking of Bills  The transparency SIG allows us to explain that a bill is  about transparency if it has more qualities. For such, it was  created a weighing criterion according to the SIG  correlations and tiers depicted in Figure 2. This   organization is used to rank the bills, i.e. a bill gains a  higher score as it has more qualities.  Table 3. TOP20 bills text mined  Bills  Weighing  Precision  - Determinará obrigações para a realização de audiências públicas 11 tp  - Modificará o sistema de arquivamento dos votos em urnas  eletrônicas e os mecanismos de transparência dos votos   10 tp  - Regulamentará a licitação e a contratação de serviços de  publicidade por parte da administração pública  9 tp  - Instituirá o programa de Unidades de Polícia Pacificadora (UPP’s) 9 tp  - Obrigará as universidades públicas e as unidades de pesquisa a  criarem arquivos online, de acesso livre  9 tp  - Obrigará todos os senadores, deputados e vereadores a realizar  audiências públicas mensal com o objetivo de prestar informações   9 tp  - Excluirá da Presidência a Controladoria-Geral da União 8 fp  - Obrigará as empresas estatais federais a divulgarem, de 3 em 3  meses, as despesas realizadas com publicidade   7 tp  - Regulamentará a licitação chamada Pregão, para a aquisição de  bens e serviços  7 tp  - Disponibilizará as transações financeiras nas instâncias estadual e  municipal, na página da internet Portal da Transparência.  7 tp  - Determinará obrigações para a realização de audiências públicas  7 tp  - Modificará o sistema de arquivamento dos votos em urnas  eletrônicas e os mecanismos de transparência dos votos   7 tp  - Obrigará as empresas a prestarem contas às instituições públicas,  na época de campanhas eleitorais  7 tp  - Criará um banco de dados, único e centralizado, para registrar os  preços das licitações realizadas pela Administração Pública   7 tp  - Fará reformas na legislação eleitoral 7 tp  - Obrigará a OAB e os outros conselhos profissionais a se  submeterem às regras da Lei de Acesso à Informação   7 tp  - Impedirá o político de se candidatar novamente  7 tp  - Tornará obrigatória a divulgação, na internet, da folha de  pagamento dos funcionários da Administração Pública  6 tp  - Permitirá a realização de testes de integridade em agentes públicos  e policiais   6 tp  - Obrigará a divulgação na internet de dados sobre servidores público 6 tp  - Obrigará que as empresas de grande porte publiquem suas  demonstrações financeiras  6 tp  - Estabelecerá um prazo máximo de 30 dias para a aprovação ou  rejeição dos projetos culturais  6 tp  See detailed spreadsheet at https://git.io/v9EZ0  Therefore, bills with qualities that belong to the second - tier are assigned a weight of 1, and bills with qualities in  the first -tier a weight of 2.  On the other hand, bills  that  have the main quality, i.e. transparency, gain a weight of 5.  In Table 4 , we exemplify this weighing strategy by using  the top three bills mined belonging to the group of  accessibility qualities. Bill titles are shortened due to table  space.  Table 4. Ranking of bills in the accessibility group  Bills  Transparency  accessibility  portability  availability  publicity  Bill Weight  - Obrigará as universidades públicas e as  unidades de pesquisa a criarem  arquivos online, de acesso livre.  5 2 0 1 1 9  - Obrigará as empresas a disponibilizar  para os consumidores placas em locais  de boa visibilidade com informações de  acesso ao cadastro de reclamações   5 2 0 1 1 9  - Modificará o sistema de arquivamento  dos votos em urnas eletrônicas e os  mecanismos de transparência dos votos  5 2 0 1 0 8  4. ANALYSIS OF RESULTS  The analysis of results is performed in two phases: the first  to address the quality of bills mined (section 4.1), and the  second (section 4.2 ) address es the quantitative results of  our approach in comparison with previous work.', 'to address the quality of bills mined (section 4.1), and the  second (section 4.2 ) address es the quantitative results of  our approach in comparison with previous work.  4.1 Findings-phase 1:  From the top20 bills in Table 3 , we verified how accurate  the results were by reading each one. We assess the quality  of each bill by us ing the levels of software transparency  shown in Figure 1. Our approach got one false positives fp,  and by r eading this bill we found that it is related to the  name of a Brazilian government agency which has the  transparência word in its name:   A Controladoria-Geral da União tem como titular o Ministro de  Estado Chefe da Controladoria-Geral da União, que transforma o  cargo Ministro de Estado do Controle e da Transparência.  With the aim of verifying at which level of software  transparency the bills are related, we read two bills of Table  4 that, by their title, show a direct relation with software.  These are summarized below1:    Modificará o sistema de arquivamento dos votos em urnas  eletrônicas e os mecanismos de transparência dos votos  O projeto altera a lei 9.504, de 1 997, para que as urnas eletrônicas tenham um  sistema para gravar eletronicamente cada voto e identificar a urna em que ele foi  registrado. Assim que a votação acabar, os votos serão apurados  automaticamente, com a gravação de um arquivo de resultado e impressão de um  boletim de resultado apurado para todos os cargos e candidatos aprovados.  Depois de completarem os cargos, a Justiça Eleitoral vai disponibilizar aos  candidatos, partidos e coligações os arquivos do registro digital de votos da  totalidade das urnas, para conferência, auditoria e recontagem.  The votes will be cleared automatically, with the recording of a result file and  printing of a score sheet for all positions and approved candidates. The Electora l  Court will make available to the candidates,  parties and coalitions the files of the  digital register of votes of the totality of the polls, for conference, audit and recount.                                                                    1 The text of bills is presented in its original language as text - mining process was performed with keywords i n Portuguese;  however, we added a summary in English to allow explanation.  209', 'In this bill, we observed that  a demand for software  transparency exists in two levels. First, in the level 3 of  transparency, which is related to Software Ecology  Transparency focused on citizens . They are demanding  transparency in votes by making available the digital  register of votes. Second, the level 2 of transparency ,  Software Interaction Transparency is demanded as they are  mentioning the mediator of availability (electronic ballot  boxes).  In the other bill below, we identified that a demand for  software t ransparency exists in the 2nd level of software  transparency, as the Executive Branch ask Government  Units to provid e information  that are  accurate, complete  and easy to understand in order to provide support for  building a Transparency Portal (the mediator).    Disponibilizará as transações financeiras nas instâncias estadual  e municipal, com as mesmas regras que já são adotadas no  âmbito federal, na página da internet Portal da Transparência  Ficará sob responsabilidade do órgão federal, organizar as informações  fornecidas nas unidades federativas dentro do Portal da Transparência. Para que  a unidade federativa receba a t ransferência financeira federativa, ela deverá  encaminhar as informações necessárias dos últimos exercícios e do exercício  corrente com defasagem máxima de 2 meses. Essas informações deverão estar  corretas, completas e de fácil compreensão. Será considerad o infração caso a  unidade federativa omitir, falsear ou deturpar dados; interromper por 6 (seis)  meses o fornecimento de informações; atrasar em mais de 3 (três) meses o  fornecimento das informações de forma injustificada  The Executive Branch will carry ou t voluntary and constitutional financial transfer  to the federal units that provide sufficient information  An important observation is that d uring text mining,  most of the bills were obtained by using query strings , in  opposition to phrase queries . In Tab le V we show the  phrases queries and the number of bills that were found  with them. The ones with more occurrences (13 and 32) are  those belonging to the qualities  accessibility and  auditability respectively. Future work will focus in  assigning a different weighing criterion to bills found with  phrase queries, as they are more precise.    Table 5. Numbers of bills using phrase query  Phrase Queries Matches  Transparencia das informações 1  Transparência do processo 1  Acesso à informação 13  Dado detalhado 1  Dado dividido 1  Informação clara 1  Informação precisa 1  Forma única 1  Leitura fácil 1  Prestação de contas 32    4.2 Findings-phase 2:  We checked our results to compare with previous work to  verify how effective was our strategy in relation to manual  identification of bills [16].   In Figure 5, we report ed 360 bills mined and classified  according the SIG´s first-tier. From the 360 bills we  have  filtered the ones that besides the words related to qualities  possess the transparência word, thus resulting in 44 bills.  For this quantitative analysis  we used the 360 bills ,  because, in previous work bills were classified as  transparency bills by taking  into consideration the authors  background in transparency and the transparency SIG. For  such, some classified bills may not have the transparência  word as well as the qualities words.  Previous work identified 27 bills in 2013, and our  process mined 13 of them. In table 6 it is shown the ones  mined accompanied with the percentage of acceptance vo te  given by citi zen. The qualities column was added to bring  the characteristics given to bills in previous work [16].  Table 6. Matches with previous work  Bills Vote  yes  Qualities  1. Obrigará as operadoras de telefonia  celular a manter registros de telefonemas  recebidos e realizados por pessoas  desaparecidas.   93% transparência  2. Obrigará as financeiras a fornecerem ao  cliente, antes da contratação, informações  sobre os riscos do empréstimo  95% comparabilidad', 'desaparecidas.   93% transparência  2. Obrigará as financeiras a fornecerem ao  cliente, antes da contratação, informações  sobre os riscos do empréstimo  95% comparabilidad etransparência  3. Obrigará o comerciante a colocar cartaz  informando a data de validade de  produtos em promoção  88% acuracia  4. Obrigará o comerciante a colocar cartaz  informando a data de validade de  produtos em promoção  98%  accesibilidade  publicidade  clareza  5. Obrigará os órgãos de trânsito a informar,  em cada ponto de ônibus, as linhas que  passam por ele, bem como os horários e  as tarifas.   99% accesibilidade  6. Obrigará que os carros alugados pela  administração pública tenham um adesivo  informando para qual órgão público o  carro está a serviço  99% usabilidade  7. Obrigará as empresas que aplicam  película não refletiva nos vidros de  veículos  86% transparência  8. Obrigará a divulgação na internet dos  dados e justificativas sobre aumento nas  passagens de ônibus  97% transparência  9. Permitirá o acesso público a dados sobre  foragidos e procurados pela polícia 95% controlabilidade  informativo  10. Obrigará a OAB e os outros conselhos  profissionais a se submeterem às regras  da Lei de Acesso à Informação  97% auditabilidade  transparência  11. Obrigará a ANA a enviar semestralmente  a prestação de contas ao Congresso  Nacional  95% auditabilidade  12. Obrigará o DNPM a enviar  semestralmente a prestação de contas ao  Congresso Nacional  95% auditabilidade  13. Obrigará as duas agências nacionais de  transporte, a ANTT e a ANTAQ a enviar, a  cada 6 meses, a prestação de contas ao  Congresso Nacional.   95% auditabilidade    It is important to note that our process mined another 15  bills that were not found by previous work [16]. In Table 7,  210', '15 bills are listed accompanied of the qualities that were  used to match them.  Finally, table 8 reports 14 bills that were not mined by  our process. The column qualities list the characteristics of  each bill in [16]. Additionally, we read each one to find out  the possible keywords that could be a match. We  understand that these possible keywords could be used to  improve the set of initial keywords identified in sub -section  3.2.  Table 7. Bills not identified by previous work  Bills Qualities  4478-plc-6316-2013 transparência,auditabilidade  4473-plc-6207-2013 usabilidade, operabilidade  4472-plc-6170-2013 usabilidade,uniformidade  4452-plc-6003-2013 usabilidade,uniformidade  4401-plc-5912-2013 acessibilidade, informativo, clareza  4332-plc-5655-2013 entendimento  4288-plc-5555-2013 acesso e publicidade  4257-plc-5487-2013 acessibilidade e publicidade  4202-plc-5317-2013 usabilidade, intuitividade  4201-plc-5315-2013 auditabilidade, responsabilidade  4134-plc-5093-2013 auditabilidade  4102-plsc-5010-2013 acessibilidade, auditabilidade, controlabilidade  4101-plc-5009-2013 acessibilidade  4032-plp-245-2013 transparência  3965-pls-56-2013 acessibilidade, disponibilidade  Some issues about the proposed mined process is that  the bill PLS-32-2013 is not present in our corpus of bills due  to retrieval issues  (sub-section 3.1 ) which we will further  investigate. Furthermore, PLC-5508-2013 identified by  previous work is a false positive ; t his confirms that  processing hundreds of bills manually can result in  mistakes.  Table 8. Bills not identified by the mining process  Bills Qualities Possible keywords  PLC-6954-2013 informativo, entendimento,  acessibilidade  ensino, estudo  PLC-5986-2013 acessibilidade, usabilidade visiveis  PLC-5951-2013 acessibilidade, informativo divulgado, divulgação  PLC-5714-2013 acessibilidade, informativo informem, informações,  conhecimento  PLC 5896-2013 acessibilidade, usabilidade,  informativo  divulgar, internet,  informando  PLC-5660-2013 accesibilidade, informativo,  auditabilidade  divulgarem, internet  PLC-5198-2013 acessibilidade, usabilidade,  informativo  facil, visualização,  mostrar  PLC-5367-2013 acessibilidade e informativo informar, preciso, claro  PLC-5458-2013 acessibilidade, usabilidade,  informativo  divulgarem,  informações, internet  PLC-5476-2013 acessibilidade, usabilidade,  informativo  local visivel  PLC-5508-2013 not related to transparency, human error  PLS-57-2013 acessibilidade, informativo,  auditabilidade  prestar contas,  acompanhar, enviar  PLS-27-2013 accesibilidade sigilo, divulgar,  publicação  PLS-32-2013             not  identified by the corpus-builder   We understand from the data we have mined that our  previous finding [16] still holds, as it can be seen from  Table 6 that projects related to the transparency concept do  have a high popularity (vote ye s). We will treat this   confirmation as a possible answer to our question - title Is  There a Demand for Software Transparency?  in the  concluding Section.   Threats to validity : The proposed process is a semi - automated one. The first activity (sub-section 3.2) can vary  because keywords selection depends on people background   as well as  on knowledge about the topic, i.e. transparency.  On the other hand, the Votenaweb search is not reliable, as  the number of search results shown in the website does not  comply with the actual bills retrieved. e.g., query ing  transparency in Votenaweb shows 42 results, but actually,  there are only 40 bills.  5. RELATED WORK  To the best to our knowledge, few works use the qualities  of NFRs catalogs as a drive for text mining in the context  of Brazilian Laws. Existing work  [33][34] uses NFR s  catalogs for classifyi ng requirements specifications,  however they rely in supervised learning techniques; ou r  process is based in Information Retrieval (IR) techniques.  Other works [35][36][37] rely on keywords of NFR for', 'however they rely in supervised learning techniques; ou r  process is based in Information Retrieval (IR) techniques.  Other works [35][36][37] rely on keywords of NFR for  identifying/reuse components of software libraries  [35],  identify transparency qualities in requirements models [36],   as well as  to identify privacy and security req uirements  [37].   Our previous work using Votenaweb can be summarizes  as follows: a) Engiel et al . [16] performed a manual text  mining of bills related to target transparency in the year of  2013. b) Engiel et al. [17] presented an initial approach for  this work.  c) Portugal et al [20] is an initial approach to  mine the citizen comments existing in the transparency bills  identified by this work.  6. DISCUSSION  The results confirm a previous manual search on the  Votenaweb software that transparency related bills do have  high number of votes  [16].  Notwithstanding, we have to  consider that these bills are related to inform ation  transparency. Most of this information will be processed by  software, as for instance the tenth bill in table 6 that relates  to the Information Access Law. The Brazilian Access Law  explicitly states qualities for the presentation and the  validity of information.   Our question is directed to software transparency, that is  the disclosure of what, how and why the software does  what it does. Of course, that process transparency can  always be presented as information, but the difference is  the target of tr ansparency. So, will the demand for  information transparency migrate to process (software)  transparency?   211', 'We posit that the data we have confirms that this will  happen, for three main reasons. First, we have seen the  phenomena in the inner level of Figure 1, that is , software  developers are more and more demanding and using the  transparency given by open source environmen ts [12].     Second, other researchers [10] are foreseeing the need for  software providers to argue about the capabilities and  limitations of their software with users/clients, that is ,  dealing with the second level of Figure 1. Third, the  existence and usage (10 million votes) of the Votenaweb  software, since citizens are using it as a social network  [25], whereas the medium encourages the curiosity and  amplifies exposure, thus characterizing the situation of the  outer level of Figure 1.     Of course that, there are threads to the validity of t he  results we have shown here. One of them is how trustful are  the data we based our mining, the other is that the text - mining process we used is still maturing, although we  pointed out that some results were better than the previous  manual mining. We hope that a close  look at the bills  returned by the process could shed light not only in tuning  our keyword s, but also on the weighting criterion for  ranking the bills that we have used.     7. CONCLUSION  We departed from an open question, which address if, in  the future, society will demand transparency from software.   The work focuses on a broad view of software  transparency, that is one that reach all 3 levels of  transparency, as shown in Figure 1. We believe that this  will happen, and that we should work on this issue as t o  anticipate this possible future.  However, it is proper to question if this possibility does  have an anchor on real data. This paper addresses this  point, by using a semi -automated process to mine bills  proposed for the Brazilian Congress that embodies th e  transparency concept. The mining was performed in a  software repository that ranks these bills according to  citizens who vote on the Votenaweb portal, which aims to  rank bills by its popularity.   Future work should continue to monitor the bills to  check if society will demand more software quality by  means of transparency, that is , bills targeting concerns  about Level 2 and 3 of Figure 1. Bills that will require that  producers provide information about the quality of their  products, and/or bills that will  demand that some type of  software (election software, for instance) addresses the  questions of how, what and why they implem ent  themselves, may come along. It is interesting to observe  that in some specific contexts there are norms that require  software o rganizations to be certified by standards like  CMMI or ISO norms, that is there is a demand for  transparency at Level 2.    Regarding the mining strategy, one of the weakness of  the current strategy is to rely on exact pattern matching; we  are investigating  how to use partial matching strategies to  improve our results.  Further work is also needed to better tune the process as  well as to improve its automation by mining the votes as  well as composing the ranking of the bills automatically.  We may also improve  the mining, by exploring the  qualitative data provided by discussions or comments that  voters do provide in Votenaweb.  We keep believing that the software community should  treat transparency as other NFRs are being treated, that is  delving in better under standing of the consequences of the  demand for transparency and how software production will  deal with it.   8. ACKNOWLEDGMENTS  This research has been partially supported by Brazilian  Science without Borders, Capes, CNPQ and FAPERJ.  9. REFERENCES  [1] Burkat Holzner, and Leslie Holzner. 2006. Transparency in  Global Change: the Vanguard of the Open Society. 1 ed.  University of Pittsburgh Press.   [2] Adrian Henriques. 2007. Corporate Truth the Limits to  Transparency. Earthscan, UK.', 'Global Change: the Vanguard of the Open Society. 1 ed.  University of Pittsburgh Press.   [2] Adrian Henriques. 2007. Corporate Truth the Limits to  Transparency. Earthscan, UK.  [3] Don Tapscott, and David Ticoll. 2003. The Naked  Corporation: How the Age of Transparency Will  Revolutionize Business. Simon and Schuster.  [4] Kristin M. Lord. 2006. The Perils and Promise of Global  Transparency. State University of New York Press.  [5] Michael Schudson. 2015. The Rise of the Right to Know:  Politics and the Culture of Transparency, 1945-1975.   [6] Archon Fung, Mary Graham, and David Weil. 2007. Full  Disclosure: The Perils and Promise of Transparency. 2007.  Cambridge University Press, Londres, UK.   [7] Transparency International - The Global Anti-Corruption  Coalition. 2017. Available at: http://www.transparency.org.  Accessed 17 Jul. 2017.  [8] Daniel Jackson D, Martyn Thomas, and Lynette I. Millett.  2007. Software for Dependable Systems: Sufficient  Evidence? Committee on Certifiably Dependable Software  Systems, National Research Council. National Academy of  Sciences.  [9] Pascal Meunier. 2008. Software transparency and purity.  Communications of the ACM, 51(2), 104-104.  [10] Julio C. Sampaio do Prado Leite, and Claudia Cappelli.  2010. C. Software Transparency. Business & Information  Systems, Springer, 2: 127. doi:10.1007/s12599-010-0102-z.  [11] Mauricio Serrano, and Julio C. Sampaio do Prado Leite.  2011. Capturing transparency-related requirements patterns  through argumentation. In Requirements Patterns (RePa).  2011 First International Workshop on (pp. 32-41). IEEE.  [12] Laura Dabbish, Colleen Stuart, Jason Tsay, and Jim  Herbsleb. 2012. Social coding in GitHub: transparency and  collaboration in an open software repository. In Proceedings  212', ""of the ACM 2012 conference on Computer Supported  Cooperative Work (pp. 1277-1286). ACM.  [13] Mahmood Hosseini, Alimohammad Shahri, Keith Phalp, and  Raian Ali. 2015. Towards engineering transparency as a  requirement in socio-technical systems. In Requirements  Engineering Conference (RE), 2015 IEEE 23rd International  (pp. 268-273). IEEE.  [14] Webcitizen Technology and behavior for social  transformation. 2017. Online. Available at:  http://www.webcitizen.com.br. Accessed 17 Jul. 2017.  [15] Ricardo Fabrino Mendonça, and Marcus Abílio Pereira.  2011. Democracia digital e deliberação online: um estudo de  caso sobre o Votenaweb. In Congresso Latino Americano De  Opinião Pública-Wapor (Vol.4.)  [16] Priscila Engiel, Julio C. Sampaio do Prado Leite, and Claudia  Cappelli. 2014. Confirmando a Demanda por Transparência:  Um Estudo Inicial sobre um Sistema de Avaliação de  Projetos de Lei. In Anais do II Workshop de Transparência  em Sistemas. Londrina, Brasil.  [17] Priscila Engiel, Roxana L. Quintanilla Portugal, and Julio C.  Sampaio do Prado Leite. 2016. Descobrindo Projetos de Lei  relacionados a Transparência. In IV Workshop de  Transparência em Sistemas, Rio de Janeiro, Brasil.  [18] Roxana L. Quintanilla Portugal, Julio C. Sampaio do Prado  Leite, and Eduardo Kinder Almentero. 2015. Time- constrained requirements elicitation: reusing GitHub content.  In Just-In-Time Requirements Engineering (JITRE), 2015  IEEE Workshop on (pp. 5-8). IEEE.  [19] Roxana L. Quintanilla Portugal, Julio C. Sampaio do Prado  Leite, and Hugo Roque. 2016. Corpus Builder: Retrieving  Raw Data from GitHub for Knowledge Reuse in  Requirements Elicitation. In 3rd Annual International  Symposium on Information Management and Big Data  (SIMBIG), Cusco, Perú.  [20] Roxana L. Quintanilla Portugal, Priscila Engiel, and Julio C.  Sampaio do Prado Leite. 2017. Existe uma Demanda de  Transparência? Análise de comentários à Projetos de Lei. In  V Workshop de Transparência em Sistemas. São Paulo,  Brasil  [21] Lawrence Chung, Brian A. Nixon, Eric Yu, and John  Mylopoulos J. 2000. Non-Functional Requirements in  Software Engineering (Vol.5). Kluwer Academic Publishers   [22] RE - PUC Rio Transparency Catalog. Online. Available at:  http://transparencia.inf.puc- rio.br/wiki/index.php/Catálogo_Transparência. Last Access:  20-03-2017  [23] Claudia Cappelli. 2009. Uma Abordagem para  Transparência em Processos Organizacionais Utilizando  Aspectos. Tese de Doutorado - Departamento de Informática,  Pontifícia Universidade Católica do Rio de Janeiro  [24] Claudia Cappelli, Julio C. Sampaio do Prado Leite, and  Antonio de Padua Albuquerque Oliveira. 2007. Exploring  business process transparency concepts. In Requirements  Engineering Conference, 2007. RE'07. 15th IEEE  International (pp. 389-390). IEEE.   [25] Herbet Cunha, Julio C. Sampaio do Prado Leite, Leticia  Duboc, and Vera Werneck. 2013. The challenges of  representing transparency as patterns. In Requirements  Patterns (RePa), 2013 IEEE Third International Workshop  on (pp. 25-30). IEEE.  [26] Henrique Prado de Sá Sousa, André Luiz de Castro Leal, and  Julio C. Sampaio do Prado Leite. 2015. Alinhamento de  operacionalizações entre Transparência e MpsBr, iSys- Revista Brasileira de Sistemas de Informação, 8(4), 109-141.  [27] James Herbsleb, Christian Kästner, and Christopher Bogart.  2016. Intelligently Transparent Software Ecosystems. In  IEEE Software, 33(1), 89-96.  [28] Ethan S. Bernstein. 2012. The transparency paradox: A role  for privacy in organizational learning and operational  control. Administrative Science Quarterly, 57(2), 181-216.  [29] Herbert A. Simon. 1969. The Sciences of the Artificial. First  Edition, MIT Press.  [30] Ross Ihaka, and Robert Gentleman.1997. The R project for  statistical computing. Department of Statistics, University of  Auckland, Auckland, New Zealand. Available at: http://www.  r-project. org/254.  [31] Ingo Feinerer, and Kurt Hornik. 2015. tm: Text Mining"", 'Auckland, Auckland, New Zealand. Available at: http://www.  r-project. org/254.  [31] Ingo Feinerer, and Kurt Hornik. 2015. tm: Text Mining  Package. R package version 0.6-2.  [32] Tyler W. Rinker. 2013. qdap: Quantitative Discourse  Analysis Package. R package version 1.3.1  [33] Jane Cleland-Huang, Raffaella Settimi, Xuchang Zou, and  Peter Solc. 2007. Automated classification of non-functional  requirements. In Requirements Engineering, 12(2), pp.103- 120.  [34] Jane Cleland-Huang, Raffaella Settimi, Oussama BenKhadra,  Eugenia Berezhanskaya, and Selvia Christina. 2005. Goal- centric traceability for managing non-functional  requirements. In Proceedings of the 27th international  conference on Software engineering (pp. 362-371). ACM.  [35] Xavier Franch, Josep Pinyol, and Joan Vancells. 1999.  Browsing a component library using non-functional  information. In International Conference on Reliable  Software Technologies (pp. 332-343). Springer, Berlin,  Heidelberg.  [36] Joás W. Baía, and José L. Braga. 2013. Uso de sinônimos na  identificação de atributos de transparência. In 16th WER- Workshop em Engenharia de Requisitos (15th CibSE- Congresso Ibero-Americano em Engenharia de Software)  (pp. 94-104).  [37] Ana I Anton, David Bolchini, and Qingfeng He. 2003. The  use of goals to extract privacy and security requirements  from policy statements. North Carolina State University.  Dept. of Computer Science.   213']","['IS THERE A DEMAND OF SOFTWARE TRANSPARENCY? This  briefin  reports  scieitfc  evideice oi  the  efectveiess  of  ai  approach  to fid  if  there  is  a  deaaid  of  sotware traispareicy.  The approach aiies bills proposed  to  the  Braziliai  Coinress  ii order  to  arnue  for  the  existeice  of  a deaaid for sotware traispareicy.   FINDINGS \uf0b7 Bills  are  related  to  softare transparency in three diferent levels:  \uf0b7 Most  of  the  bills  are  demanding accessibility of  informatonn  as  the citiens  are  demanding  mostly accouitability  from  government. (Outer circle). \uf0b7 There  are  bills  such  as  the  one demanding  disclosure of  informaton of  each  state  government  in  its Transparency  Web  Portals;  this  is  a requirement  to  receive  fnancial support  from  the  main  government. (Middle circle). \uf0b7 We  found  that  qualites  used  to describe  nonfunctonal  requirements are  efectve  as  a  vocabulary  for fnding Bills related transparency. \uf0b7 Qualites served as a startng point to create  Keytords  manually;  hotevern as  this  task  is  subjectven  te  found necessary  the  automaton  by extractng  keytords  from  the  bills texts. \uf0b7 Keytords can be singles tords as tell as  compound  tords.  For  instance prestação de contas . We named this kind  of  keytords  of  phrase  queries. \uf0b7 Phrase  queries  shoted  to  be  more precise; thusn  acesso à informação  is more precise than  acesso in some line of  the  bill  and  informação in  other line. \uf0b7 We  took  advantage  of  the  qualites from the transparency catalogn thich is  organiied  in  a  three-ter  nettork. We  used  the  levels  as  to  rank  the vocabulary matching.     \uf0b7 From  5011  Bills  took  from  the Votenateb.com.br  repositoryn  te found  44  Bills  tith  high  demand  of transparency.  This  approach  can  be useful to fnd this type of demand in other sources of informaton.  Keywords:         Transparency         Legal Aspects         Nonfunctonal Requirements         Text mining Who is this briefin  or? Publicn as to provide of an atareness  about Softare Transparency. Softare researchers/practtoners as to be  informed of possible future  requirementsn that is: transparency Where the fidiins come  rom? All fndings of this briefng tere  extracted from the research conducted  by Portugal et al.   What is iicluded ii this briefin? The main fndings of the research.  What is iot iicluded ii this briefin? Detailed descripton of the techniques  used for our fndings. To access other evideice briefins  oi sofware einiieeriin: The original texts and the process  performed in this research https://git.io/vdBbc For additoial ii ormatoi about  our Research Group, aid the aiiual  workshop oi Systems Traispareicy:   http://transparencia.inf.puc-rio.br http://ttrans.inf.puc-rio.br/ ORIGINAL RESEARCH REFERENCE Roxana Portugaln Priscila Engieln Hugo Roquen Julio Cesar Sampaio do Prado Leite Is there a demand of Softare Transparency.  Braiilian Symposium on Softare Engineering SBESn 2017 https://doi.org/10.1145/1111151.1111155']","**Title: Understanding the Demand for Software Transparency in Brazil**

**Introduction:**
This evidence briefing presents the findings of a study that investigates the demand for software transparency in Brazil, particularly in the context of legislative processes. The research utilizes data from the Votenaweb platform, which allows citizens to vote on proposed bills, to explore how transparency-related bills are perceived and valued by the public. The ultimate goal is to assess whether there is a latent demand for software transparency in software development and governance.

**Main Findings:**
The study identifies a significant demand for software transparency based on an analysis of 5,011 bills proposed in the Brazilian Congress, focusing on those related to transparency. Key findings include:

1. **Public Engagement**: The Votenaweb platform has recorded over 10 million votes, indicating a high level of citizen engagement with legislative proposals. This engagement suggests that citizens value transparency in government processes and decision-making.

2. **Identification of Transparency Bills**: Using a semi-automated text mining process, the researchers identified 360 bills that are related to transparency. Among these, 44 bills explicitly contained the term ""transparência"", demonstrating a clear interest in legislation aimed at improving transparency.

3. **Levels of Transparency**: The study categorizes transparency into three levels: 
   - **Software Ecology Transparency**: Focused on public access to information.
   - **Software Interaction Transparency**: Concerned with how users interact with software and information.
   - **Software Organization Transparency**: Related to the internal processes of software production.

4. **Quality of Bills**: The analysis highlighted that bills with transparency-related qualities received higher acceptance rates from citizens. This correlation suggests that transparency in legislation is positively associated with public support.

5. **Future Demand**: The findings imply that as software becomes increasingly integral to governance and public services, the demand for transparency in software processes will likely grow. This shift may lead to new legislation requiring software producers to disclose how their systems operate and the rationale behind their functionalities.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, policymakers, and academics interested in the intersection of software development, transparency, and public engagement. It is particularly relevant for those involved in creating or managing software systems that impact public governance.

**What is NOT included in this briefing?**
This briefing does not cover detailed statistical analyses or technical specifics regarding the data mining methods used. It also does not provide exhaustive discussions on the implications of transparency for all software systems, focusing instead on the legislative context in Brazil.

**For further information:**
For additional evidence briefings on software engineering, visit: [http://www.lia.ufc.br/ccbsof2017/](http://www.lia.ufc.br/ccbsof2017/).

**Original Research Reference:**
Roxana Lisette Quintanilla Portugal, Priscila Engiel, Hugo Roque, Julio Cesar Sampaio do Prado Leite. 2017. Is There a Demand for Software Transparency? In Proceedings of 31st Brazilian Symposium on Software Engineering (SBES 2017), Fortaleza, Ceará, Brazil. https://doi.org/10.1145/3131151.3131155"
"['Late Decomposition of Applications into Services through Model-Driven Engineering Vinicius Nordi Esperança Computing Department - Federal University of São Carlos São Carlos, SP, Brazil vinicius.esperanca@dc.ufscar.br Daniel Lucrédio Computing Department - Federal University of São Carlos São Carlos, SP, Brazil www.dc.ufscar.br/~daniel ABSTRACT Currently, with the increasing number of connected devices and users, there are forces that lead to the distribution of software systems. Behind these forces is the need for increased reliability and performance, privacy and security issues, among other factors. However a distributed architecture has inherent complexity and is diﬃcult to change. Thus, distributed-related design decisions are normally taken in the early stages of the software life cycle. Nonetheless, in a dynamic scenario, it may be diﬃcult to predict how technologies or users may change in the future. Therefore, the ability to rapidly change how a software system is distributed may provide extra competitive advantage to institutions, allow- ing developers to better respond to market changes, or to more easily experiment with real diﬀerent distribution con/f_igurations before delivering a /f_inal product or update. In this paper an ap- proach to ease this task is presented. Using source code analysis, model-driven engineering and code generation, applications can be semi-automatically decomposed into services that can be easily distributed across multiple servers, without changing the origi- nal functionality. As a proof-of-concept, we used the approach to successfully distribute Apache Tomcat in distinct ways, in con/f_igu- rations that involve up to ten diﬀerent servers. CCS CONCEPTS •Software and its engineering → Model-driven software en- gineering; •Computer systems organization → Distributed ar- chitectures; KEYWORDS Model-Driven Engineering, Late decomposition, Distributed Sys- tems ACM Reference format: Vinicius Nordi Esperança and Daniel Lucrédio. 2017. Late Decomposi- tion of Applications into Services through Model-Driven Engineering. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. DOI: 10.1145/3131151.3131165 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131165 1 INTRODUCTION In today’s scenario of highly connected users and devices, it is rare to /f_ind a software system that is not distributed across multiple devices and servers. There are many reasons why software is dis- tributed: increased reliability [2], since multiple copies of the same system can provide extra backup in case of failure; increased perfor- mance [5], as multiple parts of the system can process information simultaneously; resource optimization, where each particular de- vice can be explored to execute particular functions according to its capabilities, such as memory and processing power; privacy and security requirements [12], since sensitive or critical data and functions can be con/f_ined to particular devices or servers; among many others. In terms of technologies to support distribution, there are several options available, such as WebServices [11, 15], SOAP [11] and RMI [3, 10]. But the main challenges are not technological. Developing distributed systems requires a carefully planned architecture, once distribution is diﬃcult to change after it has been implemented and', 'distributed systems requires a carefully planned architecture, once distribution is diﬃcult to change after it has been implemented and deployed. For this reason, distribution is normally de/f_ined in the early stages of the software life cycle [1]. However, due to the common uncertainty in requirement elicita- tion, or because of the dynamic nature of the IT scenario, it is not always possible to correctly predict how the designed architecture will respond to real usage conditions. Late discoveries or unfore- seen requirement changes may cause the need for changes in how a system is distributed. For example, a particular function or module may turn out to be a performance bottleneck due to unforeseen high user demand or excessive memory consumption, and perhaps it would be better to separate it into a diﬀerent server. In this paper it is argued that the ability to rapidly change how a software system is distributed can help to mitigate these problems. An approach is proposed to ease this task, allowing monolithic systems to be decomposed into a set of services that can be more easily distributed. This approach uses source code static analysis to identify the system modules and how they communicate with each other. Then, based on model-driven engineering techniques, the developer can create models that specify which modules are to be made remotely accessible through services. Finally, code generation automatically produces all the necessary implementation details according to the chosen distribution. To change between diﬀerent distributions, the developer only needs to update the models and regenerate the code. The contributions of the approach are threefold: (1) During de- sign and implementation, developers can more easily experiment with diﬀerent distributions, not in theoretical or simulated scenar- ios, but with the real system, so that there is better evidence to support a particular design decision; (2) The approach also allows a 164', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Vinicius Nordi Esperança and Daniel Lucrédio more agile practice, with less upfront eﬀort to design the distributed architecture. Developers may start with a monolithic system, focus- ing on functional requirements only. Later, when more is known about the system and usage conditions, changes in how the sys- tem is distributed can be rapidly performed; (3) After deployment, should any new unforeseen situation appear, developers are able to easily change how the software is distributed, creating new services or removing old ones rapidly, without having to rewrite hundreds or thousands code lines by hand. The remainder of this paper presents more details regarding the approach’s conceptual background and supporting technologies (Section 2), related work (Section 3), the approach itself (Section 4) and a proof-of-concept experiment (Section 5). Section 6 concludes the paper discussing its main contributions and future work. 2 CONCEPTUAL BACKGROUND This section describes the main concepts used in this research. First, an explanation about Model-Driven Engineering (MDE). Next, Eclipse Java Development Tools (JDT) are presented. Finally, some details about process communications are explored. 2.1 Model-Driven Engineering (MDE) According to OMG [13], a model is a simpli/f_ied representation used to explain the workings of a real world system or event. A model can be made for humans, who can use it to better understand the object being modeled, or it can be made for automatic machine processing, which is the idea behind Model-Driven Engineering (MDE). By using the information from models, software transformation techniques can be used to automatically generate other models or parts of an application’s source code, thus reducing development eﬀort. Transformations can also take care of many repetitive and error-prone implementation details, raising the abstraction level during development and making the creation of complex software systems simpler. Models can also be automatically checked for correctness, helping to detect and prevent errors early in the life cycle. For success in MDE, a model must be formal, unambiguous and rich enough to allow all these tasks to be performed automatically by a computer. It is also important to choose what is being modeled and for what purpose. A large, wide-scoped model that shows system modules and their connections can be used for checking architectural constraints and generating basic inter-communication code. However, it can not be used for checking or generating a particular module’s behavior. A state-based model, for example, would be more appropriate for this purpose. This is the reason why in MDE Domain-Speci/f_ic Languages (DSL) [7] are normally preferred over Generic-Purpose Languages, such as UML [14]. A DSL is a language focused on a speci/f_ic domain, thus making it easier create models that are closely related to a speci/f_ic MDE task. In this paper, a DSL was developed with the sole objective of de/f_ining how a system’s modules are distributed over multiple servers, thus making it simple to be used the automatic transformations. 2.2 Eclipse JDT The Eclipse JDT project[6] provides tools and APIs for developing applications that can read and manipulate Java programs. JDT allows access to the Java source code using the Java Model and the Abstract Syntax Tree (AST). 2.2.1 Java Model. The JDT Java Model is a lightweight and fault tolerant representation of all the information contained in a Java project. It is de/f_ined in theorg.eclipse.jdt.core pack- age, which contains metaclasses for manipulating Java elements. Its main classes are: each element in the Java Model is mapped to some IJavaElement; IJavaProject represents a Java Project; IPackageFragmentRoot represents the folders that hold the source, binary and external libraries of a project;IPackageFragment repre- sents Java packages; ICompilationUnit represents the .java /f_iles;', 'binary and external libraries of a project;IPackageFragment repre- sents Java packages; ICompilationUnit represents the .java /f_iles; and IType/IField/IMethod represents classes, interfaces, /f_ields and methods. 2.2.2 Abstract Syntax Tree (AST). JDT’s AST is a tree repre- sentation of Java source code. It de/f_ines an API for changing, creating, reading and deleting source code resources. Each Java /f_ile is represented as a subclass of theASTNode class. Each AST node provides information about a speci/f_ic object. For example, MethodDeclaration represents methods, SimpleName represents non-keyword identi/f_iers andVariableDeclarationFragment rep- resents /f_ield declarations. An AST instance is typically created from an ICompilationUnit instance, from the Java Model. JDT is built into the Eclipse platform, thus it also provides means for accessing and manipulating projects and its components in an Eclipse workspace. JDT can be used with diﬀerent purposes. For example, it is pos- sible to perform read-only tasks, such as static analysis, helping to extract source code metrics or understand how a particular project is structured. It can also be used to modify the source code. In fact, Eclipse implements refactorings with JDT, using its API to change names, move /f_ields, among other examples. JDT can also be used to generate code. By instantiating objects from the Java Model and AST, it is possible to automatically generate valid fragments or even complete Java code. These features make JDT a practical tool for performing some MDE tasks in Java-based projects. 2.3 Communication technologies - RMI RMI (Remote Method Invocation) applications often comprise two separate programs, a server and a client. A typical server program creates some remote objects, makes references to these objects ac- cessible, and waits for clients to invoke methods on these objects. A typical client program obtains a remote reference to one or more remote objects on a server and then invokes methods on them. RMI provides the mechanism by which the server and the client com- municate and pass information back and forth. Such an application is sometimes referred to as a distributed object application. RMI treats a remote object diﬀerently from a non-remote ob- ject when the object is passed from one Java virtual machine to another Java virtual machine. Rather than making a copy of the implementation object in the receiving Java virtual machine, RMI passes a remote stub for a remote object. The stub acts as the local 165', 'Late Decomposition of Applications into Services through Model-Driven Engineering SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil representative, or proxy, for the remote object and basically is, to the client, the remote reference. The client invokes a method on the local stub, which is responsible for carrying out the method invocation on the remote object. 3 RELATED WORK The problem of distributing monolithic applications into multiple distributed components was investigated by Sairaman [16], who developed a framework to help in this process. His idea is to au- tomatically partition a program into its primary tasks. Next, an analysis is carried out to determine which tasks are better suited for some de/f_ined platform. Finally, automatically reprogrammation is used to transform a sequential program into a distributed program. Sairaman’s framework has a process that starts with the appli- cation source code, which is a monolithic system. The following activities compose the process: • Task graph generation: First, the application source code is analyzed in search for its tasks. In Sairaman’s work, blocks of source code, such as function calls, simple calcu- lations and statements are identi/f_ied as tasks. Next, tasks are analyzed, measuring some attributes like CPU cycles used by the task, spent energy, amount of data shared with other tasks, among other information. A task graph is then generated. In this graph, nodes represent tasks and edges represent the amount of data shared between tasks. Figure 1 presents an example of a generated task graph. In the end of this step, the source code is annotated to identify which blocks of code correspond to the tasks in the graph. Figure 2 shows a sample annotated code generated in this step; • Code partitioning: Code partitioning can be de/f_ined as the process of forming partitions (task clusters) by com- bining individual tasks, represented as nodes on the task graph, and mapping each partition to a processing element in the heterogeneous distributed system; • Code generation: This step is responsible for: (i) analyz- ing individual code clusters in order to identify and isolate data dependencies between partitions; (ii) adding code to the original source code of each partition in order to con- vert them into nearly independent executable programs; and (iii) adding communication primitives to the source code to resolve inter partition data dependencies; and • Heterogeneous scheduling: The last step of the map- ping process is heterogeneous scheduling. In summary, Sairaman’s framework enables the decomposition of a monolithic system into a distributed system. However, the decision about the system distribution is performed mainly by the transformers, based on the clustering algorithm. It is possible for the software engineer to make choices such as the number of groups, but the actual /f_inal distribution falls oﬀ his/her control. Hunt and Scott [ 8], in their work, also showed an automatic distributed partitioning system called Coign [9]. Coign analyzes an application, chooses a distribution, and produces the desired distribution all without access to the application source code. In this paper it is argued that a diﬀerent approach would be better. Instead of relying on automatic clustering of tasks and distribution, Figure 1: Snap Shot of a Part of the Task Graph [16] Figure 2: Sample Annotated Code [16] the ability to explore diﬀerent distributions of the application source code, to test these distributions and to choose the best for each case would give the developer more power and control over the /f_inal results. This view was the same followed by Cornhill [4], who created in 1984 an approach for building distributed software where the entire application program is written for a single virtual machine. The goal of his project was to develop the methods, tools and runtime support to create application software consisting of one Ada pro-', 'application program is written for a single virtual machine. The goal of his project was to develop the methods, tools and runtime support to create application software consisting of one Ada pro- gram executing on a distributed system. The project was performed in order to think about a development approach in which there was a de/f_inition of notation for expressing application partitioning across distributed systems. The notation gave the software engineer control over how the system would be distributed. The project also included a compiler to identify partitions and generate the program and the Ada distributed system at runtime. Vahid e Gajski [17] de/f_ined closeness metrics for functionally partitioning a speci/f_ication among hardware or software compo- nents. Although their work do not actually perform the distribution, it encompasses an important system design task, which is the parti- tioning of system functionality for implementation among multiple system components, including partitions among hardware and soft- ware components [17]. These metrics can be used by a designer 166', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Vinicius Nordi Esperança and Daniel Lucrédio Figure 3: Approach for late decomposition of applications or by automated algorithms, to cluster together functional objects that should be implemented on the same component. Cloud computing is the future trend for enterprise software so- lutions, which means many legacy systems will need to be either adapted to /f_it the requirements of cloud computing or to be purged and redesigned from scratch [18]. However, enterprise software can be complex. Zhou, Yang and Hugill [18] proposed a novel ap- proach for reengineering enterprise software for cloud computing by building an ontology for enterprise software and then parti- tioning the enterprise software ontology to decompose enterprise software into potential service candidates. There are three steps in the approach: (i) to create the ontology for the source code, data and application; (ii) to integrate captured ontologies and to deploy the /f_inal produced ontology; and (iii) to decompose the system into service candidates, by analysing the concepts and relations in the ontology. 4 LATE DECOMPOSITION OF APPLICATIONS INTO SERVICES THROUGH MODEL-DRIVEN ENGINEERING Motivated by the need for distributing software systems into mul- tiple devices and servers, and considering previous studies, an approach is proposed to give developers the ability to decompose a system into services. Figure 3 illustrates the approach. The software engineer has two main tasks: system codi/f_ication and distribution modeling. System codi/f_ication corresponds to normal development, i.e. the implementation of business rules as source code without considering distribution. Distribution model- ing is where the software engineer speci/f_ies how the source code must be distributed across multiple servers. Using the source code and distribution model as input, a MDE partitioner mechanism performs the following tasks: • It partitions the source code according to the speci/f_ication contained in the distribution model. In this task, it analyses the source code searching for dependencies between the partitioned code, so that the original functionality remains the same; and • It generates the source code that implements communi- cation between the partitions, using Java RMI, and also independent deployment and execution. For the deploy- ment, a manager code is created, being responsible for managing the lifecycle of each remote object from the ser- vice. The manager can be used for any other service to request and access the remote objects. The main component of this approach is the MDE partitioner. The goal of the MDE partitioner is to partition a monolithic system into a set of services that can be independently accessed from a remote host. The MDE partitioner uses the following strategy: • Each public method from each class of the monolithic sys- tem is a potential services; • Each server will contain one or more classes grouped to- gether. The software engineer is responsible for establish- ing how many servers will exist, and which classes will be allocated to each server; • By default, all classes will be exposed as remotely accessi- ble, except for those explicitly speci/f_ied as local classes by the software engineer. There are also some constraints for remote classes, as discussed later. Since classes are the smaller units that can result from the par- titioning, if a single class needs to be separated into two diﬀerent servers it must be refactored before the approach is used. Next more details about the distribution model and the MDE partitioner are presented. 4.1 Distribution model The distribution model is composed of two XML /f_iles where the software engineer can choose which parts of the code will be par- titioned into which servers (referenced as “clouds” in the model), down to the level of individual classes. Next an example of the /f_irst /f_ile is shown.', 'titioned into which servers (referenced as “clouds” in the model), down to the level of individual classes. Next an example of the /f_irst /f_ile is shown. 1 <? xml v e r s i o n = "" 1 . 0 "" encoding = "" UTF −8 "" ?> 2 < c l o u d s > 3 < c l o u d name= "" S e r v i c e 1 "" u r l = "" 2 0 0 . 1 2 4 . 2 4 4 . 2 2 "" > 4 < c l a s s > J a r F a c t o r y < / c l a s s > 5 < c l a s s > L i b r a r y < / c l a s s > 6 < c l a s s > C o n s t a n t s < / c l a s s > 7 < / c l o u d > 8 < c l o u d name= "" S e r v i c e 2 "" u r l = "" 2 2 0 . 2 2 2 . 3 5 0 . 4 5 6 "" > 9 < c l a s s > MethodInfo < / c l a s s > 10 < c l a s s > A u t h e n t i c a t o r < / c l a s s > 11 < c l a s s > Confi g < / c l a s s > 12 < / c l o u d > 13 < / c l o u d s > In this example, cloud/server “Service1” will contain classes “JarFactory”, “Library” and “Constants”. The server location of “Servico1” will be 200.124.244.22. “Service2” will contain the classes “MethodInfo”, “Authenticator” and “Con/f_ig”, being located at the address 220.222.350.456. The second XML /f_ile identi/f_ies those classes that should not have their methods accessible as services. Not all methods need to be transformed into services, and some can not, as discussed later. 167', 'Late Decomposition of Applications into Services through Model-Driven Engineering SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Hence, this information helps the MDE partitioner during the iden- ti/f_ication of dependencies between classes and code generation. An example of this /f_ile is shown next, specifying that classes “JarFac- tory” and “MethodInfo” should not have their methods transformed into services. 1 <? xml v e r s i o n = "" 1 . 0 "" encoding = "" UTF −8 "" ?> 2 < c l a s s > J a r F a c t o r y < / c l a s s > 3 < c l a s s > MethodInfo < / c l a s s > Using these two /f_iles and the source code as input, the MDE partitioner is ready to start the process of creating the services. 4.2 MDE mechanism / MDE Partitioner With the help of JDT, the MDE partitioner analyzes the source code and the distribution model and decomposes the application into services that can be easily distributed across multiple servers, without changing the original functionality. The following steps are performed: (1) Read distribution model. This includes XML parsing of the two input /f_iles that compose the distribution model; (2) Determine which classes should not be remotely accessi- ble. Besides those classes de/f_ined in the second /f_ile of the distribution model (as discussed in previous section), some classes can not be made remote due to some restrictions, such as being a subclass of a non-remote class or using non-remote class as return functions. These restrictions are further discussed later; (3) Creation of class managers and process server starters. Every server has a manager for its objects’ life cycle, and a server starter, which is responsible for executing the server as a standalone program; (4) Creation of auxiliary connectors to enable communica- tion with the application services. The connection will be established by the service managers; (5) Creation of remote interfaces for the services. The role of these interfaces is to expose the signature of methods so that they can remotely accessible as services. This is a trivial task for non-static methods, as they merely point to the object instances created by the managers. For the static methods, however, a special treatment is necessary. Since Java RMI requires and instance to respond to a remote invocation, static methods can not be directly executed from a remote environment. To solve this problem, all static methods are moved into its containing class’ respective manager, so that there is always a single remote instance to respond to their invocation. The resulting behavior is the same as static method invocation in a single JVM. (6) After the remote interfaces are created, the MDE parti- tioner searches the code for object instantiations and static function calls. Object instantiations must be replaced by a correspondent call to a creation function in that object’s remote manager, so that the object is created in the re- mote server instead of locally. This process also involves looking into the distribution model in order to identify the server where the object’s class is contained. Similarly, static function calls are replaced by calls to their corre- spondent remote services created in the class’ manager, as described in the previous step; (7) Next, all references to classes that are made remote are replaced by their respective remote interfaces; and (8) The last step is to generate the project for each server. Now an example to illustrate the approach is presented. The system is composed of two classes A and B. The following listing presents the original monolithic code for the system. 1 p u b l i c c l a s s A { 2 p u b l i c A ( ) { / / c o n s t r u c t o r body 3 } 4 p u b l i c s t a t i c v o i d f u n c a o E s t a t i c a ( ) { 5 / / s t a t i c f u n c t i o n body 6 } 7 p u b l i c v o i d fu nca o ( ) { / / f u n c t i o n body 8 } 9 } 10 p u b l i c c l a s s B { 11 p u b l i c s t a t i c v o i d main ( S t r i n g [ ] a r g s ) {', '6 } 7 p u b l i c v o i d fu nca o ( ) { / / f u n c t i o n body 8 } 9 } 10 p u b l i c c l a s s B { 11 p u b l i c s t a t i c v o i d main ( S t r i n g [ ] a r g s ) { 12 A a = new A ( ) ; 13 a . fun cao ( ) ; 14 A . f u n c a o E s t a t i c a ( ) ; 15 } 16 } Inside method main() from class B, an object from class A is created (line 12). Then function funcao() from the new object is called (line 13). Finally, static function funcaoEstatica() from class A is called (line 14). In this example, Class A will be made remote, which means its methods will be transformed into services. Class B will remain non- remote. The following listing shows the /f_irst /f_ile of the distribution model. 1 <? xml v e r s i o n = "" 1 . 0 "" encoding = "" UTF −8 "" ?> 2 < c l o u d s > 3 < c l o u d name= "" Cloud1 "" u r l = "" 2 0 0 . 1 2 4 . 2 4 4 . 2 2 "" > 4 < c l a s s >A< / c l a s s > 5 < / c l o u d > 6 < c l o u d name= "" Cloud2 "" u r l = "" 2 2 0 . 2 2 2 . 3 5 0 . 4 5 6 "" > 7 < c l a s s >B< / c l a s s > 8 < / c l o u d > 9 < / c l o u d s > Since class B will not be made remote, the second /f_ile of the distribution model is needed, as shown next. 1 <? xml v e r s i o n = "" 1 . 0 "" encoding = "" UTF −8 "" ?> 2 < c l a s s >B< / c l a s s > After the partitioning, the following classes and interfaces are generated: “GerenciadorCloud1”, “GerenciadorCloud1Interface” and “ARemoteInterface”. “GerenciadorCloud1” and “GerenciadorCloud1Interface” corre- spond to the manager component associated with server “Cloud1”. Since “Cloud2” only contains non-remote classes, no manager is generated for it. The following listing shows the code for “GerenciadorCloud1”. It is marked as a remote interface through extension (line 2), which means this is a remotely accessible object via RMI. It contains method speci/f_ications for creating instances of this server’s classes. In this case, because there is only class A on this server, a single method (getA() in line 3) is created. The interface also contains 168', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Vinicius Nordi Esperança and Daniel Lucrédio a method speci/f_ication for the single static function of class A (AfuncaoEstatica() in line 4). 1 p u b l i c i n t e r f a c e G e r e n c i a d o r C l o u d 1 I n t e r f a c e 2 e x t e n d s Remote { 3 p u b l i c A R e m o t e I n t e r f a c e getA ( ) throws RemoteException ; 4 p u b l i c v o i d A f u n c a o E s t a t i c a ( ) throws RemoteException ; 5 } Next the code for class “GerenciadorCloud1” is presented. As expected, it implements “GerenciadorCloud1Interface” (line 3), but it also extends class “UnicastRemoteObject” (line 2), as part of Java RMI’s requirements for remote access. 1 p u b l i c c l a s s G e r e n c i a d o r C l o u d 1 2 e x t e n d s U n i c a s t R e m o t e O b j e c t 3 implements G e r e n c i a d o r C l o u d 1 I n t e r f a c e { 4 p u b l i c A R e m o t e I n t e r f a c e getA ( ) throws RemoteException { 5 r e t u r n ( A R e m o t e I n t e r f a c e ) 6 U n i c a s t R e m o t e O b j e c t . 7 e x p o r t O b j e c t ( new A ( ) , 1 0 9 9 ) ; 8 } 9 p u b l i c v o i d A f u n c a o E s t a t i c a ( ) throws RemoteException { 10 A . f u n c a o E s t a t i c a ( ) ; 11 } 12 } Class “GerenciadorCloud1” provides implementations for meth- ods getA() and AfuncaoEstatica() de/f_ined in its interface. The /f_irst (getA() in lines 5-7) creates an object from class A and exposes it as a remote object accessible through port 1099. The second ( Afun- caoEstatica() in line 10) simply performs a call to static function funcaoEstatica from class A. Since A is located in the same server as “GerenciadorCloud1”, this works without problems. Interface “ARemoteInterface” is also generated, to contain all public non-static methods from class A to be exposed as services, as shown in next listing. In this case, only method funcao() - line 3 is included. 1 p u b l i c i n t e r f a c e A R e m o t e I n t e r f a c e 2 e x t e n d s Remote , S e r i a l i z a b l e { 3 p u b l i c v o i d fu nca o ( ) throws RemoteException ; 4 } The /f_inal steps of the MDE partitioner involve the modi/f_ication of class B so that local calls are replaced by remote calls. The following listing shows the result of this process. 1 p u b l i c c l a s s B { 2 G e r e n c i a d o r C l o u d 1 I n t e r f a c e g e r e n c i a d o r ; 3 p u b l i c s t a t i c v o i d main ( S t r i n g [ ] a r g s ) { 4 A R e m o t e I n t e r f a c e a = g e r e n c i a d o r . getA ( ) ; 5 a . fun cao ( ) ; 6 g e r e n c i a d o r . A f u n c a o E s t a t i c a ( ) ; 7 } 8 / ∗ fu ncao que i n i c i a l i z a g e r e n c i a d o r ∗/ 9 p u b l i c v o i d s t a r t M a n a g e r s ( ) . . . 10 } Compare the two versions of class B. The old version creates an object from class A using the new statement. In this new ver- sion, the newly created remote method getA() from “Gerenciador- Cloud1Interface” is used instead (line 4). The result of this method is no longer a local reference, but a remote one. For this reason, the call to method funcao() (line 5) leads to a remote execution, thus achieving distribution as intended. Remote execution will also occur in the static call of method AfuncaoEstatica() (line 6), but here object “gerenciador” will be used instead, as explained earlier. Method startManagers() (line 9 - for which the implementa- tion is not shown) is responsible for creating an instance for object “gerenciador”. 4.3 Partitioning restrictions In this project, some restrictions in terms of partitioning had to be de/f_ined. Some are due to limitations in our implementation of the approach, while others are imposed by the use of Java RMI technology. These are explained next. R1: Two diﬀerent classes with the same name are not allowed, because the distribution model does not support fully qual- i/f_ied names. This is a simple limitation that can be easily /f_ixed in future work;', 'because the distribution model does not support fully qual- i/f_ied names. This is a simple limitation that can be easily /f_ixed in future work; R2: Each class must be de/f_ined in its own /f_ile. This is also a limitation of our implementation, intended to facilitate the work of the MDE partitioner and use of JDT. This can also be easily /f_ixed in the future; R3: Classes that extend some other class that lies outside the project are automatically considered as non-remote. This happens because all methods of a remote class must be made remote, including those inherited from superclasses. In Java RMI, this requires the modi/f_ication of the method’s signature to include a particular type of exception. The MDE partitioner automatically includes this exception in the methods that are being transformed into services. In fact, it can go up class hierarchy modifying superclasses too, but only if it has access to their source code. There- fore, it can not modify classes outside the project (such as classes from the Java base API, for example). This is a limitation caused by Java’s inheritance mechanism and the RMI requirement to include these exceptions in all remote methods; R4: Classes that extend some non-remote class can not be made remote. This is the same restriction as R3, but with a diﬀerence. Here the MDE partitioner has access to the superclass’ source code, but it can not modify it to make it remote, either because the software engineer explicitly speci/f_ied not to (in the second /f_ile of the distribution model) or due to some other restriction; R5: Java RMI requires all remote objects to be marked as Serializable. This is accomplished by making a class im- plement Java’s serializable interface. The MDE partitioner does this task automatically, but only where it has access to the source code. For this reason, classes that implement (directly or indirectly) some non-serializable interface that lies outside the project are automatically considered as non-remote, because the MDE partitioner can not mark these interfaces as Serializable too. This is also a limitation caused by Java’s inheritance mechanism and RMI; and R6: If a non-remote class is used as a method parameter, a method return type or a /f_ield type contained in some class, this class can not be made remote. Such situation could lead to a scenario where a non-remote object is sent over the network, as a result of a method call, for example. Being non-remote, only a copy of the object would be sent. As a 169', 'Late Decomposition of Applications into Services through Model-Driven Engineering SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil result, subsequent modi/f_ications made to this object’s state would only occur locally, leading to possibly inconsistent behavior in the system. We found no workaround for this problem yet. While these restrictions may present some problems to the ap- proach, in practice we observed that they normally represent situa- tions where partitioning is not desired, since they would cause the separation of classes that are tightly coupled through inheritance or strong dependency. It is also possible to overcome the limitations by refactoring the code using design patterns or other design solutions to remove inheritance. But such a discussion is left for future work. 5 PROJECT EVALUATION To evaluate the approach, some tests were conducted using the source code of Apache Tomcat, which is a stable software being extensively used for many years. The primary goal of the evaluation was to determine if the ap- proach can be used in practice in a real software. In particular, we were interested in determining if the distributed software would retain the same functions as the original software. As secondary goal, we tried to investigate the impact of distribu- tion in terms of performance and memory usage. This illustrates a real scenario where the approach could lead to interesting practical results. In terms of performance, however, we did not implement per- formance enhancement strategies for distributed systems, such as replication and load balancing. For this reason, we observed worse performance in the distributed system, caused by the inher- ent latency between multiple servers. Future work is needed to experiment with these techniques for improving performance. We then proceeded to evaluate memory consumption. In a single machine, Tomcat consumes an amount of memory. The idea was that with the distribution of the application into diﬀerent servers, the memory would be shared between the machines. The experiments were conducted with the help of Jmeter 1, a graphic tool for tests that allows developer to create tests scenarios for web applications. To analyse the memory, the Yourkit2 tool was used. The experiment was conducted in /f_ive steps, as described next: 5.1 Step 1: Creation of the distribution models The /f_irst step consisted of the creation of diﬀerent distribution models, which should specify the number of servers, number of classes in each server, and their location. For this experiment, four diﬀerent distribution models were created: DM1: this corresponds to the original Tomcat system, with all of its 1508 classes running on a single server; DM2: there are two servers, with the /f_irst server containing all 1302 classes that could not be made remote due to the re- strictions, and the second server containing all 206 classes elibigle for remote access; DM3: there are four servers. The /f_irst server contained all 1302 classes that could not be made remote due to the 1http://jmeter.apache.org/ 2https://www.yourkit.com/ restrictions. The second server contained the 40 classes that represent most of the memory consumption, as identi- /f_ied in a preliminary study. The third server contained 36 classes that de/f_ine constants. The fourth server contained the 129 classes that consume less memory; DM4: there are ten servers, using an arbitrary distribution of classes that follows no particular reasoning. 5.2 Step 2: Partitioning of Apache Tomcat according to the distribution models The approach described in this paper was applied almost entirely automatically, as planned. However, some modi/f_ications had to be done by hand in the generated code due to some limitations of our implementation: • For some classes that mixed inheritance and parametric polymorphism, our implementation failed to correctly re- place generic type declarations by their correspondent', 'implementation: • For some classes that mixed inheritance and parametric polymorphism, our implementation failed to correctly re- place generic type declarations by their correspondent remote interfaces. This happens because there are many diﬀerent situations involved, and not all of them were im- plemented. For these cases, the generated code had to be manually inspected to complete the processing; • We also observed problems when a remote class overrides methods from “java.lang.Object”, such as toString(). As described in the previous section (Restriction R3), due to the lack of access to the source code, the partitioner can not modify a superclass that does not belong to the project. Since “java.lang.Object” is always a superclass of any Java class, its methods can not be made remote. For this reason, we had to replace all calls for these overridden methods by a new hand-copied version of the method (toStringRemote(), for example) that could be made remote; • For this particular example ( toString()) there is an addi- tional problem, because in Java a call to this method can be made implicitly. Our current implementation fails to detect all these calls automatically. For this reason, we had to manually replace these implicit calls for explicit calls. These limitations were not identi/f_ied before the experiments because the initial tests did not present such situations. It was only during the experiments, and with a relatively large software (Apache has over a thousand classes), that we were able to detect these problems. Fixing them would not be trivial, as it would require large eﬀort to modify the partitioner and explore many diﬀerent options in JDT. Given the time constraints for this research project, we decided to continue with the experiment and leave these corrections for future work, because they would not aﬀect the conclusions. 5.3 Step 3: Test case de/f_inition The following scenarios were devised in an attempt to simulate diﬀerent usage scenarios of Apache Tomcat: TC1: this test case consisted of three diﬀerent HTTP requests submitted to Tomcat’s default management applications. Two of them were simple page requests and one was a login action. These requests would be submitted by 10 simultaneous clients, three times, with a delay of 1 second between each time; 170', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Vinicius Nordi Esperança and Daniel Lucrédio TC2: this test case consisted of the same requests as TC1, but with 100 simultaneous clients; TC3: this test case consisted of the deployment of a web ap- plication into Apache Tomcat. An e-commerce application called Hipergate3 was chosen for this test case; TC4: this test case consisted of HTTP requests made to the Hipergate application. Three diﬀerent requests were de- /f_ined: one of them was a login action, one was the con/f_ig- uration of conections with database and one was simple page request. These requests would be submitted by 10 si- multaneous clients, three times, with a delay of XX seconds between each time; and TC5: this test case consisted of the same requests as TC4, but with 100 simultaneous clients. 5.4 Step 4: Environment setup The experiment was conducted in a laboratory at the Computing Department of Federal University of São Carlos - SP - Brazil. A total of 10 machines were used. All machines were connected to the same local network through Ethernet, and had the same con/f_iguration: 8GB RAM, Intel(R) Core(TM) i5 CPU 660 @ 3.33GHz 64 bits processors. The operational system was Ubuntu 14.04 and the Java version 1.7.0.79 OpenJDK 64-Bit Server VM (build 24.79- b02, mixed mode). Apache JMeter was installed in a single machine. We decided to use the same machine that hosted the server containing those classes responsible for listening to requests in port 8080. By doing this, we ruled out any interference caused by network latency be- tween the client and the server endpoint. To simulate simultaneous clients, JMeter was con/f_igured to start simultaneous threads, up to 100 as described in previous section. Yourkit was installed in all machines that would be used as servers. 5.5 Step 5: Execution and data collection The following process was conducted for each distribution model: /f_irst the partitions were deployed in their respective servers, and the servers were started; next Yourkit was initialized in each server, starting memory monitoring; then JMeter was started, and ex- ecution of the test cases began; during execution, the researcher monitored all activity in JMeter; after execution, JMeter and Yourkit were stopped, and their data collected. 5.6 Results and discussion The experiment had a total duration of 9 hours (Step 1: 1 hour, Step 2: 4 hours, Step 3: 30 minutes, Step 4: 30 minutes and Step 5: 3 hours). As discussed before, we had two goals for this experiment: to determine if the approach can be used in practice in a real software and to investigate the impact of distribution in terms of performance and memory usage. Regarding the /f_irst goal, the experiment was successful. All four distribution models resulted in the same behavior, observed both in manual tests and in the results from JMeter. 3http://www.hipergate.org Figure 4: Candlestick chart for memory consumption for diﬀerent diﬀerent distribution models in TC1 Regarding the second goal, we observed a signi/f_icant decrease in performance for the distributed versions of Tomcat (DM2, DM3 and DM4). As discussed earlier, this happened because we did not adopt performance enhancement strategies such as replication and load balancing. For this reason, we do not present quantitative data, because it is not relevant. Regarding memory consumption, we observed some interesting results. Figure 4 presents the data from all distribution models for test case TC1 in a candlestick chart: each candlestick groups read- ings for memory consumption throughout the test case execution time. The highest point has the maximum value of a set, the lowest point the minimum value, and the thickest part of the are the values that occur the most in the unit between the /f_irst and third quartiles. Figure 4 presents some interesting observations. By comparing DM1 with DM2, we observe a real memory separation occurring', 'that occur the most in the unit between the /f_irst and third quartiles. Figure 4 presents some interesting observations. By comparing DM1 with DM2, we observe a real memory separation occurring between the servers. In DM2, although a big part of memory con- sumption still remains in the main Tomcat classes (DM2 - Server 1), a piece of memory has been migrated to the second server (DM2 - Server 2), thus reducing memory consumption in individual servers during most part of the execution. The same happens for DM4, with overall memory consumption being reduced for most part of execution. In DM3 - Server 1, we noticed a higher dispersion of memory usage. We also observed that for DM2, DM3 and DM4 (Server 1) there are higher peaks, which means that although in most of the time there is less memory consumtion, there are moments where more memory is necessary, probably because of the higher requirements caused by remote object invocations. Figure 5 shows the results for TC2. Here there are 100 simultane- ous clients being simulated, therefore the results are a little diﬀerent from TC1. In TC2, the distribution of memory consumption is not as clear as in TC1, probably because each individual server had to deal with many more simultaneous requests and remote method calls, and also instantiate more remote objects, which consume more memory than local objects. In fact, the diﬀerence between DM1 and DM3 in terms of memory consumption for Server 1 seems to be higher for most of the time than in TC1. 171', 'Late Decomposition of Applications into Services through Model-Driven Engineering SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figure 5: Candlestick chart for memory consumption for diﬀerent diﬀerent distribution models in TC2 Figure 6: Candlestick chart for memory consumption for diﬀerent diﬀerent distribution models in TC3 TC3 (Figure 6) corresponds to the deployment of Hipergate into Tomcat. As it can be seen, more memory is used here, when com- pared with TC1 and TC2. Although we can see memory sharing, again demonstrating that distribution is occurring correctly, here the distributed versions (DM2, DM3 and DM4) seem to use more memory for most of the execution time than the single server ver- sion. For test case TC4 (Figure 7) we observed that in distribution models DM2, DM3 and DM4 there is less memory consumption in individual servers than DM1. This is similar than TC1, although here there is not a higher dispersion in DM3. For test case TC5 (Figure 8) we observed the same behavior of TC2. Because there are more simultaneous clients, each individ- ual server has more memory consumption because of the higher number of remote objects being used. The results of the experiment indicate that the primary goal has been achieved. Although Apache Tomcat was not designed to be executed in a distributed environment, it was successfully partitioned. The results regarding memory consumption also help to con/f_irm this goal. Figure 7: Candlestick chart for memory consumption for diﬀerent diﬀerent distribution models in TC4 However, despite the potential that this kind of approach can bring to system development, we were unable to demonstrate solid improvements for the Apache Tomcat. Although we observed some reduction in memory consumption in individual servers (TC1 and TC4), when there are more simultaneous clients (TC2 and TC5) the results were not positive. Perhaps the choice of the distribution models was not ideal and need to be revised, but we believe the main problem was that we did not implement strategies for per- formance enhancement in distributed systems, like load balancing and replication. 6 CONCLUDING REMARKS The approach for late distribution of applications into services pre- sented in this paper allows software engineers to automatically partition monolithic systems into smaller, independent programs composed of services that can be executed in a distributed environ- ment without changing original functionality. Experiments show that the approach is viable and the results indicate some potential for improvements in execution. Next we plan to extend the distribution model to include strate- gies for performance enhancement, such as replication and load balancing. With additional copies of individual servers we expect to observe more solid improvements in terms of execution. More experiments will be conducted to further test these conditions. The approach has some limitations, mostly in terms of the im- plementation of the MDE partitioner. We intend to solve most of the practical problems with additional implementation eﬀort to be performed in the future. However, even with the limitations and the need for human intervention during the process, the approach ac- celerates the distribution of the application, making it easier for the software engineer to rapidly experiment with diﬀerent distribution options without having to rewrite many lines of code. REFERENCES [1] P. Bhateja. 2015. Designing Distributed Systems w.r.t. Conformance. In 2015 Asia-Paci/f_ic Software Engineering Conference (APSEC). 104–110. DOI:http://dx. doi.org/10.1109/APSEC.2015.11 [2] Y. Brun, J. y. Bang, G. Edwards, and N. Medvidovic. 2015. Self-Adapting Reliability in Distributed Software Systems. IEEE Transactions on Software Engineering 41, 8 (Aug 2015), 764–780. DOI:http://dx.doi.org/10.1109/TSE.2015.2412134 172', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Vinicius Nordi Esperança and Daniel Lucrédio Figure 8: Candlestick chart for memory consumption for diﬀerent diﬀerent distribution models in TC5 [3] Xuejun Chen. 2002. Extending RMI to support dynamic recon/f_iguration of distributed systems. In Distributed Computing Systems, 2002. Proceedings. 22nd International Conference on . 401–408. DOI:http://dx.doi.org/10.1109/ICDCS.2002. 1022278 [4] Dennis Cornhill. 1984. Partitioning Ada* programs for execution on distributed systems. In Data Engineering, 1984 IEEE First International Conference on . [5] S. Diwan and D. Cannon. 1998. Adaptive utilization of communication and computational resources in high-performance distributed systems: the EMOP approach. In High Performance Distributed Computing, 1998. Proceedings. The Seventh International Symposium on . 2–9. DOI:http://dx.doi.org/10.1109/HPDC. 1998.709942 [6] Eclipse. 2017. JDT. https://eclipse.org/jdt/. (2017). Accessed April, 2017. [7] Martin Fowler. 2010. Domain Speci/f_ic Languages(1st ed.). Addison-Wesley Professional. [8] G.C. Hunt and M.L. Scott. 1998. A guided tour of the Coign automatic distributed partitioning system. In Enterprise Distributed Object Computing Workshop, 1998. EDOC ’98. Proceedings. Second International . 252–262. DOI:http://dx.doi.org/10. 1109/EDOC.1998.723260 [9] Galen C. Hunt and Michael L. Scott. 1999. The Coign Automatic Distributed Partitioning System. In Proceedings of the Third Symposium on Operating Systems Design and Implementation (OSDI ’99) . USENIX Association, Berkeley, CA, USA, 187–200. http://dl.acm.org/citation.cfm?id=296806.296826 [10] D. Kurzyniec, T. Wrzosek, V. Sunderam, and A. Slominski. 2003. RMIX: a multipro- tocol RMI framework for Java. In Parallel and Distributed Processing Symposium, 2003. Proceedings. International . 6 pp.–. DOI:http://dx.doi.org/10.1109/IPDPS. 2003.1213269 [11] F. Liu, G. Wang, Li Li, and W. Chou. 2006. Web Service for Distributed Commu- nication Systems. In 2006 IEEE International Conference on Service Operations and Logistics, and Informatics . 1030–1035. DOI:http://dx.doi.org/10.1109/SOLI.2006. 328893 [12] M. Maﬀei, K. Pecina, and M. Reinert. 2013. Security and Privacy by Declarative Design. In 2013 IEEE 26th Computer Security Foundations Symposium . 81–96. DOI: http://dx.doi.org/10.1109/CSF.2013.13 [13] OMG. 2003. MDA Guide Version 1.0.1. http://www.omg.org/mda. (2003). Acessed May, 2016. [14] OMG. 2015. OMG Uni/f_ied Modeling Language (OMG UML), Superstructure, Version 2.5.0. (2015). http://www.omg.org/spec/UML/2.5.0 Acessed April, 2017. [15] A. K. Pandey, A. Kumar, and F. R. Zade. 2014. A novel robust and fault tolerance framework for Webservices using WS-I* speci/f_ication. InGreen Computing Com- munication and Electrical Engineering (ICGCCEE), 2014 International Conference on. 1–5. DOI:http://dx.doi.org/10.1109/ICGCCEE.2014.6921378 [16] Viswanath Sairaman. 2010. A Generalized Framework for Automatic Code Parti- tioning and Generation in Distributed Systems . Ph.D. Dissertation. University of South Florida. 111 páginas. [17] F. Vahid and D.D. Gajski. 1995. Closeness metrics for system-level functional partitioning. In Design Automation Conference, 1995, with EURO-VHDL, Proceed- ings EURO-DAC ’95., European. 328–333. DOI:http://dx.doi.org/10.1109/EURDAC. 1995.527425 [18] Hong Zhou, H. Yang, and A. Hugill. 2010. An Ontology-Based Approach to Reengineering Enterprise Software for Cloud Computing. In Computer Software and Applications Conference (COMPSAC), 2010 IEEE 34th Annual . 383–388. DOI: http://dx.doi.org/10.1109/COMPSAC.2010.46 173']",['LATE DECOMPOSITION OF APPLICATIONS This  briefin  reports  scieitfc  evideice oi  how  decompose  quickly  ai applicatoi iito services usiin MDE. FINDINGS • The  fidiigs  preseieed  ii  ehis  briefig coisider  success  as  ehe  applicatoi  memory diseributoi  amoig  ehe  softare  eigiieer tieh  a  little  or  io iieerfereice  of  softare  eigiieer  ii  ehe diseributoi process. • The projece preseies ehe successfully Apache Tomcae applicatoi aid memory diseributoi alehough haviig  ai  iicrease  of  memory  usage  thei more simuleaieous accesses tere made ii ehe decomposed applicatoi.  • We  had  eto  goals  for  ehis  experimeie:  eo deeermiie  if  ehe  approach  cai  be  used  ii practce ii a real softare aid eo iivestgaee ehe  impace  of  diseributoi  ii  eerms  of performaice aid memory usage. • There tere fve eese cases: TC1: HTTP requeses submitted  eo  Tomcae’ss  defaule  maiagemeie bue tieh  100  simuleaieous  clieiess  TC3: deploymeie  of  a  teb  applicatoi  called Hipergaee  iieo  Apache  Tomcae.  TC4:  HTTP requeses  made  eo  ehe Hipergaee applicatoi. These  requeses  tould  be  submitted  by  10 tieh a delay of  1  secoid  beeteei  each  tmes  aid  TC5: bue  tieh  100 simuleaieous clieies. • There tere four models used eo diseribuee ehe applicatoi  iieo  services.  DM1:  ehis correspoids eo ehe origiial Tomcae syseem ii a siigle servers DM2: Tomcae tas diseribueed beeteei eto servers tieh eto servicess DM3: Tomcae  ii  four  servers  tieh  four  services. DM4: Tomcae ii eei servers tieh eei services. • All ehe four models tere eeseed usiig all ehe fve eese cases.  • TC1  preseies  some  iieerestig  observatois. te observe a real  memory  separatoi  occurriig  beeteei a  big  pare  of  memory coisumptoi stll remaiis ii ehe maii Tomcae ehus reduciig  memory  coisumptoi  ii  iidividual servers duriig mose pare of ehe executoi. • ehe  diseributoi  of  memory ڗ probably because each iidividual server had eo  deal  tieh  maiy  more  simuleaieous aid also thich coisume more memory ehai local objeces. • thei compared tieh  TC1  aid  TC2.  Alehough  te  cai  see agaii  demoiseratig  ehae diseributoi is occurriig correcely. • Ii TC4 te observed ehae ii diseributoi models DM3  aid  DM4  ehere  is  less  memory coisumptoi ii iidividual servers ehai DM1. alehough here ehere is ioe a higher dispersioi ii DM3. • Ii TC5 te observed ehe same behavior of TC2. ڗ each  iidividual  server  has  more  memory coisumptoi because of ehe higher iumber of remoee objeces beiig used. Keywords: MDE Distributed architectures Who is this briefin  or? Softare eigiieeriig practtoiers tho taie eo make decisiois aboue  applicatoi archieeceures based oi  scieitfc evideice. Where the fidiins come  rom? All fidiigs of ehis briefig tere  exeraceed from ehe laee decompositoi  projece coiduceed by Esperaiça ee al.   To access other evideice briefins  oi sofware einiieeriin: http://ttt.lia.ufc.br/ccbsof2017//ei/'],"**Title:** Simplifying Application Distribution through Model-Driven Engineering

**Introduction:**  
This Evidence Briefing summarizes an innovative approach to decompose monolithic applications into distributed services using Model-Driven Engineering (MDE). The goal is to help software engineers adapt to the dynamic demands of distributed architectures, allowing for easier modifications and improved responsiveness to changing requirements.

**Main Findings:**  
1. **Late Decomposition Advantage:** The proposed approach facilitates late decomposition of applications, allowing developers to start with a monolithic design and later distribute the application into services as needed. This flexibility enables rapid adaptation to unforeseen changes in user demand or technological advancements.

2. **Source Code Analysis and MDE:** By leveraging source code analysis, the method identifies system modules and their interactions. Developers can then create distribution models that specify how these modules should be distributed across multiple servers. The MDE partitioner automates the decomposition process, ensuring that the original functionality remains intact.

3. **Proof of Concept with Apache Tomcat:** The approach was successfully applied to distribute Apache Tomcat across different server configurations. The experiments demonstrated that while the distributed systems retained the original software's functionality, performance may be impacted due to inherent latency in distributed environments.

4. **Memory Management Insights:** The distribution of memory consumption across multiple servers was observed, indicating effective memory sharing. However, performance enhancements such as load balancing and replication were not implemented, resulting in decreased performance in distributed versions compared to the monolithic setup.

5. **Practical Implications:** This approach allows for experimentation with various distribution configurations, enabling developers to make informed decisions based on real system behavior rather than theoretical models. It reduces the upfront effort required to design a distributed architecture, making it easier to respond to evolving requirements.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, system architects, and developers who are involved in designing and implementing distributed software systems. It is especially relevant for those seeking to enhance their applications' adaptability and performance through model-driven approaches.

**Where the findings come from?**  
The findings presented in this briefing are derived from the research conducted by Vinicius Nordi Esperança and Daniel Lucrédio, as detailed in their paper ""Late Decomposition of Applications into Services through Model-Driven Engineering,"" presented at the SBES 2017 conference.

**What is included in this briefing?**  
The briefing includes a summary of the proposed approach, its core findings, practical implications for software engineering, and insights from the application of the method to a real-world software system.

**What is NOT included in this briefing?**  
This briefing does not cover detailed statistical analyses or performance metrics from the experiments. It also does not include exhaustive discussions on limitations or the technical intricacies of the MDE partitioner.

To access other evidence briefings on software engineering:  
[http://ease2017.bth.se/](http://ease2017.bth.se/)  
For additional information about the research:  
[http://www.dc.ufscar.br/~daniel](http://www.dc.ufscar.br/~daniel)

**Original Research Reference:**  
Vinicius Nordi Esperança and Daniel Lucrédio. 2017. Late Decomposition of Applications into Services through Model-Driven Engineering. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017. DOI: [10.1145/3131151.3131165](https://doi.org/10.1145/3131151.3131165)"
"[""PBL Integration into a Software Engineering Undergraduate  Degree Program Curriculum: An Analysis of the Students’  Perceptions  G. T. A. Guedes  Federal University of Pampa  Campus Alegrete  Brazil  gilleanesguedes@unipampa.edu.br  A. S. Bordin  Federal University of Pampa  Campus Alegrete  Brazil  andreabordin@unipampa.edu.br  A. V. Mello  Federal University of Pampa  Campus Alegrete  Brazil  alinemello@unipampa.edu.br   A. M. Melo  Federal University of Pampa  Campus Alegrete  Brazil  amandamelo@unipampa.edu.br    ABSTRACT1  Problem-Based Learning (PBL) has been adopted by  undergraduate degree programs in different knowledge areas . In  Brazil, although there are reports about the use of this approach in  different formats  on Computing Programs, we are not aware of  many works available in the literature regarding its  integration into the curriculum. On Software Engineering  undergraduate degree programs, the only experience  that mentions the PBL integration we know until the current date  is that being applied at Federal University of Pampa (Unipampa).  This paper shows how the PBL approach is integrated  into the  curriculum of this  program by means of six problem -solving  courses distributed along the curriculum and organized within  thematic axes.  Moreover, this work presents and discusses the  students’ perception  regarding PBL adoption. These perceptions  were obtained through a research in strument applied to both  undergraduate students and bachelors in the program. Based on  the results, we concluded that most students agree that the  objectives of the PBL adoption in the curriculum have been  achieved. However, current results suggest that co llaborative  work still is a challenge to be addressed.                                                                        1Permission to make digital or hard copies of all or part of this work for personal or  classroom use is granted without fee provided that copies are not made or distributed  for profit or commercial advantage and that copies bear this notice and the full  citation on the first page. Copyrights for components of this work owned by others  than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,  or republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from Permissions@acm.org.    SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil   © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503-5326-7/17/09… $15.00   https://doi.org/10.1145/3131151.3131178  CCS CONCEPTS  • CCS →  Social and professional topics  →  Professional  topics →  Computing education  →  Computing education  programs →  Software engineering education  KEYWORDS  Problem-Based Learning,  Software Engineeri ng Undergraduate  Degree Program, PBL integrated into the Curriculum.    ACM Reference format:  G. T. A. Guedes, A. S. Bordin, A. V. Mello, A. M. Melo. 2017.  2017. PBL Integration into a Software Engineering Undergraduate  Degree Program Curriculum: An Analysis of the Students  Perceptions. In Proceedings of 31st Brazilian Symposium on  Software Enginee ring, Fortaleza, Ceará, Brazil, September 2017  (XXXI SBES), 10 pages.  https://doi.org/10.1145/3131151.3131178  1 INTRODUCTION  Problem-Based Lear ning (PBL) is an active learning approach  that has been increasingly adopted in degree programs in different  areas of knowledge [ 1]. Since its conception, it underwent many  different adaptations, so there is not just one single model  available for its adoption [2].  In PBL, students are organized  into small groups with the goal  of solving problems  that have some similarity with  real world  problems. In this process, the students act as protagonists of their  own learning, integrating and acquiring new  knowledge. The  professors, by their turn, take the role of facilitators, proposing  problems and driving the solving process [2]."", 'own learning, integrating and acquiring new  knowledge. The  professors, by their turn, take the role of facilitators, proposing  problems and driving the solving process [2].  PBL promotes the development of responsibility for self - learning, interpersonal abilities and team spirit,  self-motivation,  308', 'fostering o f collaboration among students,  and interdisciplinary  approach, among other benefits. Its greatest efficacy occurs when  it is incorporated  into the program curriculum magnifying the  interdisciplinary characteristics in problem-solving [3].      According to  the Curriculum Guidelines for Undergraduate  Degree Programs in Software Engineering (SE) [4], a key  objective of any engineering degree program is to provide  graduates with the tools necessary to begin professional  engineering practice. Besides that, software engineering education  should include student experiences with the professional practice,  allowing graduates of software engineering degree programs to  arrive in the workplace equipped to meet the challenges of  society’s critical dependence on software.  Prompted by the need of training professionals with practical  experience and skills to solve real market problems, the use of  PBL approach has been providing satisfactory results over the last  decades, in particular in the area of Computing [5]. In the  education of software engineers it has been perceived that such  approach can help students’ development, especially their creative  abilities [6, 7].  In spite of the benefits of PBL adoption there are few  experiences that describe its use in Software Enginee ring  undergraduate degree programs in Brazil [8 -10]. The SE  undergraduate degree program of Federal University of Pampa  (Unipampa) adopts PBL integrated into its curriculum since 2010  [10]. This integration is done through six courses, called Problem - Solving (PS), distributed in the first six semesters. It is intended,  with the application of PBL,  to promote a  significant relation  between theory and practice, to contribute to the development of  a proactive posture in the students and to  foster the collabora tion  among them.  This paper  describes how PBL has been adopted in the  SE  undergraduate degree program curriculum at Unipampa. It also  demonstrates the actual perception of the students enrolled and  bachelors in the program concerning the achievement of the  PBL  application goals by means of Problem-Solving courses.  The remaining of this paper is structured as follows.  Section 2  presents the results of literature  review about the PBL use in  undergraduate degree programs of the Computing area in Brazil.  Section 3 describes how PBL is integrated  into SE undergraduate  degree program curriculum at Unipampa.   Next, the students’  perception results with relation to the achievement of the PS  courses goals are presented and  analyzed in Section 4. Finally,  conclusions and future works are presented in Section 5.  2 BACKGROUND  In Brazil PBL has been adopted in Computing undergraduate  degree programs in different ways. Some papers describe the use  of PBL in one or more courses, but not integrated into the  curriculum. Other s report the use of PBL integrated into the  curriculum through one or more courses.  PBL experiences not integrated into the curriculum were found  in single courses, such as Operating Systems [11], Computer  Architecture and Organization [9], Human -Computer Interaction  [12], CS1 course [13,14], Database I [15], and Patterns and  Frameworks [16].   In the Operating Systems course of the Computing Licentiate  program at Caxias do Sul University [11], the use of PBL resulted  in a smaller number of professor’s interventions during the  learning process comparing to a classroom taught using the  traditional approach. The students called attention for their  autonomy when looking for solutions, winning obstacles, self - organization, motivation, and commitment.  The Computer Architecture and Organization course of the SE  undergraduate degree program at Brasilia University applied a  learning method that is a joint of Problem -Based Learning with  Learning by Teaching idea in different classrooms [9]. It is  reported that the new learning environment engaged and', ""learning method that is a joint of Problem -Based Learning with  Learning by Teaching idea in different classrooms [9]. It is  reported that the new learning environment engaged and  empowered the students’ learning. The students were more  motivated and attracted to the class development, acting as real  active learners. Moreover, there was a clear impact on the  students’ final grades.   The PBL adoption in the Human -Computer Interaction course  of the Computer Science and Information System undergraduate  degree programs at Mackenzie Presbyterian University [12]  caused a student's behavior changing from a passive role to a  more active one. Students became more responsible for their  learning and felt more prepared to apply their knowledge at the  job market.  The use of Problem-Based Learning, mobile, pen -based, and  computing technology in an introductory programming course  (CS1) at Federal Uni versity of Goias is reported in [13]. The  authors point out that PBL approach promoted lower dropout  rates, and fewer grade failures compared to previous years. They  also conclude that the use of PBL promotes students’ proactivity  and the group interaction  helps to develop communication and  collaboration skills.   An experience of the implementation of the Flipped Classroom  and Problem -Based Lea rning approaches into a CS1 course at  Federal Institute of Southern Minas Gerais is described in [14].   The present ed results suggest that PBL may be an adequate  approach to enhance the face -to-face time in the inverted  classroom.   A work about the use of PBL supported by a Virtual Learning  Environment (VLE) in the Database I course of the Computer  Science undergraduat e degree program at Cruzeiro do Sul  University concluded that the project about the development of a  database for a real world problem allowed the students to develop  some essential skills to the professional development [15].  309"", 'Another work studied the PBL experience in the Patterns and  Frameworks course of the Computer Engineering undergraduate  degree program at State University of Feira de Santana [16]. The  results suggest that the approach enhanced students’ technical  skills, but also essential non -technical skills, such as, teamwork,  listening, and writing communication, self-taught.  PBL experiences involving multiple courses, but not integrated  into the curriculum, were found in three papers. An academic  experience involving the Brazilian Aeronautics and Space  Institute (ITA) and the Brazilian National Institute for Space  Research (INPE) was conducted on the Real Time Embedded  Software and Software Quality, Reliability, and Safety courses  [17]. It involved students from both u ndergraduate and graduate  degree programs in a joint effort to develop three software  projects regarding real time embedded software. The authors  stated that the application of the PBL methodology resulted into a  more dynamic teaching with the students bec oming more  proactive, improving their work group skills and enhancing their  individual stimulus for study. During this experience, professors  realized they had to change their habits, once they were not  anymore the only knowledge transmitters.  Another PBL application performed at ITA describes an  academic project involving three graduate courses: Database  Systems Design, Information Technologies, and Software Testing  [18]. In this PBL experience, graduate and undergraduate students  formed teams and had to i nteract and collaborate in order to  produce a prototype of a Smart Grids System, simulating a real  project development. The authors considered it as a valid  experience, since it motivated the students to create artifacts that  demanded knowledge taught in a ll the involved courses. The  students’ perception was also positive, as 83% of them considered  the experience favorable and 73% stated that their knowledge was  improved.   In the Software Engineering undergraduate degree program at  Brasilia University, PBL was adopted in order to deal with some  problems faced during the teaching of Requirements Engineering  and Process Modelling courses [8].  The students were organized  in two different teams. The Requirements Engineering team was  formed only by students who were attending the Requirements  Engineering course. The Process Modelling team was comprised  of students from Process Modelling course. This method was  applied during two semesters, with 95 students and its analysis  was based on student grades comparison a nd students’ feedback  on final technical report. The results showed clearly the  Requirement Engineering team was able to achieve the learning  outcomes, but these were inconclusive about Process Modelling  team. About students’ feedback, items related to coo perative and  teamwork category were the most mentioned, specially discipline  and organization, commitment and respect, and communication.  Finally, PBL has been integrated into Computer Engineering  [19-22], Computer Science [23], and Software Engineering  undergraduate curricula [24].  The strategy adopted in the Computer Engineering  undergraduate degree program at State University of Feira de  Santana proposes a course named Integrated Study [22]. It takes  between 90 and 180 class hours and integrates differen t subjects,  from nine integrated courses [19 -22]. For instance, the Integrated  Study of Software Engineering joins Software Engineering,  System Design, and Database Systems [19]. According to the  authors, PBL has brought various benefits: “students mature both  academic and professionally, they gain experience in managing  teams, develop abilities of self -criticism and learn writing, and  speaking skills”. The authors also point out some challenges  regarding this learning approach, for instance, difficulties w ith  interpersonal relations between students, difficulties to ensure full', ""regarding this learning approach, for instance, difficulties w ith  interpersonal relations between students, difficulties to ensure full  participation of each tutor in problem design, and difficulties with  administrative support to the PBL approach and curriculum,  among others.  Cruzeiro do Sul University, in 2009, int roduced the Problem - Solving course at 1st semester in its Computer Science curricula  [23]. The course involves studying and classifying problems (e.g.  logical-math problems, decidable and undecidable problems,  treatable and intractable problems) as well as  studying the main  problem-solving strategies (e.g. logical argument, divide and  conquer, simulation). It dialogues with contents from other  courses like Computing Programming, Fundamentals of Logic,  Internet Applications and Topics of Calculation, serving  as basis  to courses from 2nd semester (e.g. Algorithms Development  Techniques, Algorithms Development Laboratory and Web  Programming). According to the author the inclusion of the course  into the curriculum has been bringing initial results quite  satisfactory, although not necessarily conclusive, since this  implantation recently occurred.  At Federal University of Pampa, PBL is strongly integrated  into SE undergraduate degree program curriculum since its  conception [24]. PBL has been applied through six cou rses named  Problem-Solving (PS I to VI) that are offered from the 1st  semester to the 6th semester. In those courses students work in  problems related to software development (PS I and II), modeling  and software design (PS III), software analysis and valid ation (PS  IV) and software process, evolution, quality and management (PS  V and VI). The authors present experiences in PS I and PS III  courses in which they report that it was possible to perceive that  there was improvement in students' performance, espec ially in  relation to the initiative and the team work. Also, it may be noted  that students work with more motivation.   These experiences highlight PBL qualities that are very well  known, such as: improving communication skills, fostering  collaborative work , promoting autonomy, enhancing motivation.  310"", 'Moreover, they reinforce the approach benefits to computing job  market training. When integrated into the curriculum in multiple  courses, these qualities and benefits could be reinforced, but there  are still few experiences reported in Computing education in  Brazil that allow to claim it.  Even though the PBL integration on SE undergraduate degree  program at Unipampa has been already reported in [24], it just  presents the PBL approach applied on two courses. Howeve r, our  paper went beyond that: it presents the PBL integration in details,  as well as, it highlights the problem -solving process adopted in  the program. Moreover, it discusses the perceptions of all enrolled  students and bachelors regarding PBL learning objectives reported  in the curriculum.   3 PROBLEM-BASED LEARNING  INTEGRATED INTO THE CURRICULUM  Unipampa undergraduate degree program on Software  Engineering proposes the adoption of pedagogical strategies that  emphasize the search and the building of knowle dge instead of its  simple transmission and information acquiring. In this way, the  program adopts, besides demonstrative methodologies like  lecturing classes, the Problem-Based Learning approach.  PBL is incorporated into the Political -Pedagogical Project o f  the Software Engineering undergraduate degree program. Such  strategy integrates different courses contents in an  interdisciplinary way, through problem situations that are close to  the professional reality the graduates will come across [25].  Thus the cu rricular program proposes six mandatory courses  called Problem -Solving I to VI, each one with 120 hours. The  PBL approach makes up about 34% of the total workload of the  mandatory curricular components.   These Problem -Solving (PS) courses are offered durin g the  first six semesters. On the last two semesters, the program focuses  on the production of a final project and the accomplishment of a  mandatory internship.   Each PS course, supported by other traditional courses, aims to  solve the software problems within a thematic axis. Four thematic  axes are adopted by the program: Construction; Modeling and  Design; Analysis and Verification; Process, Evolution, Quality  and Management.  Fig. 1 highlights the main courses that support  the PS courses in each semester.  Often, professors who teach PS  and supporting courses are the same  in order to favor the  integration and synchrony between these courses. When  professors are not the same, PS professors establish dialogue with  their colleagues for alignments.   It should b e pointed out that no prerequisites are enforced in  this curriculum. Thus, it is possible to have students at different   levels enrolled in a given PS. However, this heterogeneity is not  limited by the lack of prerequisites, since students already join the   program with different knowledge backgrounds.  In the next sections are presented details about the PS courses.  For this, first the thematic axes adopted by the program are  discussed in Section 3.1. In Section 3.2, the goals for each PS  course are presente d. Next, the rules that guide the execution of  PS courses are discussed in Section 3.3. Finally, in Section 3.4,  the process used in the development of a PS course is detailed.  3.1 Thematic Axes of the Curricular Structure  Thematic axes organize the courses in great themes of the  Software Engineering area. They guide what shall be taught in  depth at each time frame (e.g. a semester or a year), but they not  exclude other important topics. These axes were conceived taking  as reference the Curriculum Guidelines for Undergraduate Degree  Programs in Software Engineering proposed by ACM and IEEE  in 2004 [26], which was the main source available at the time the  Unipampa undergraduate degree program on Software  Engineering was proposed.      Figure 1: Problem -Solving C ourses grouped into Thematic  Axes.  The thematic axis Construction suffers influence from the', 'Engineering was proposed.      Figure 1: Problem -Solving C ourses grouped into Thematic  Axes.  The thematic axis Construction suffers influence from the  knowledge area Computing Essentials. This area includes the  Computer Science foundation as well as construction technologies  and tools, which support the design and construction of software  products. The goal of this axis is to emphasize the logical thinking  and abstraction, introduce Software Engineering area, problem - solving techniques as well as the programming practice by the  students.  311', 'The thematic axis Modeling and Design is, by its turn, mainly  influenced by the knowledge area Software Design, which  concentrates on issues, techniques, strategies, representations, and  patterns used to orient how to implement a component or a  system. Therefore, the goal of this axis is to allow the students to  develop knowledge and competencies regarding the software  design, in consonance with the stakeholders’ requirements.  The thematic axis Analysis and Verification is influenced by  both the Sof tware Modeling & Analysis area and the Software  Verification & Validation area. The Modeling & Analysis area  concentrates in the analysis, specification, and validation of  requirements, which represent the stakeholders’ needs. The  Software Verification & V alidation area, on the other hand, is  concerned with ensuring the artifacts, produced during the  development process, satisfy the requirements specification and  the stakeholders’ expectation. Thus, the axis proposes that the  students develop  knowledge and experiences in the processes of  Requirement Engineering, and Verification and Validation.  The thematic axis Process, Evolution, Quality and  Management is influenced by several knowledge areas: Software  Process, Software Evolution, Software Quality and Soft ware  Management  Software Process area is concerned with the definition,  implementation, measurement, management, change, and  improvement of software process. Software Evolution concerns  itself with the occurrence of changes and the evolution support.  Software Quality covers the quality of the product itself and the  quality of the process used to produce it. Finally, Software  Management is concerned with the planning, organization and  monitoring of all phases of the software life cycle.  It can be perceived, therefore, that this thematic axis has  multiple goals. In short, it aims to the students get to know how to  use the most suitable software process for a given situation; to  know how to manage changes in such way as to allow the  software evolution; to know how to ensure the quality of the  produced software; and to acquire the knowledge about how to  manage the development of a software project.  3.2 Goals for each Problem-Solving Course  As can be seen in Fig. 1, the first two PS courses are inserted into  the thematic axis Construction. A specific development process is  not adopted in this axis to organize the student activities, since the  emphasis lies on the code production itself. Therefore, there is no  deep concern in these courses in relation to development   processes, documentation production, or user interface design.  PS I course aims to provide the students a Software  Engineering introduction by means of the exploration of concepts,  techniques, and tools as well as their applicability on problem - solving in  the software development. This course is mainly  organized on concepts and practices from Algorithms and  Programming course. Programming problems are proposed, which  use to be easier, considering that the students’ knowledge and  experience on software development are incipient.    On its turn, PS II intends to offer the students a complementary  training in terms of programming through the exploration of  technologies oriented to collaboration and productivity as based  upon practices of collaborative problem -solving. This course is  supported by the Object-Oriented Programming and Abstract Data  Types courses. The problems presented in this PS course have a  higher difficulty level, involving the application of object-oriented  and data-structure concepts.   PS III course is located in the thematic axis of Software  Modeling and Design, therefore it focuses deeply on the systems  modeling and design. An iterative and incremental software  development methodology is applied, commonly being the  Unified Process. PS III is also supported by other courses taught', 'modeling and design. An iterative and incremental software  development methodology is applied, commonly being the  Unified Process. PS III is also supported by other courses taught  on parallel, as Software Modeling and Design, Database Modeling  and Design, and Human -Computer Interaction. During this  course, students can choose a specification language, exercise the  design of end-user interfaces, and enhance their knowledge in data  persistence.   PS IV course is inserted into the thematic axis Software  Analysis and Validation, which implies a strong application of  knowledge about requirements engineering, and verification and  validation techniques.  It is mainly supported by the courses of  Software Analysis and Software Verification and Validation,  which are taught in the same semester. Usually, the Model on V is  used to guide the students activities. Such model is a variation of  the classic or casca de life cycle, which places a strong emphasis  on verification and validation, besides preserving the traditional  focus on the requirement representation as endorsed by this  model. Thus, it is intended that the students who conclude this  course may know how to describe software requirements within a  context and be able to verify and validate such artifacts that are  produced along the development cycle.  According to the Fig. 1, the PS V and PS VI courses belong to  the thematic axis Software Process, Evolution , Quality and  Management. This axis stresses the application of agile methods  and project management.  PS V course allows the students to experience the application  of a development process in a software project, taking in  consideration its adequacy to the project to be developed in terms  of maximizing quality and productivity during the  implementation. In this course agile methods are applied,  particularly Scrum. This PS is mainly supported by Software  Process and Software Quality courses, which are taught on  parallel.  Finally, PS VI aims to contribute on training on software  project management. In this last PS the students are encouraged to  312', 'act as project managers, verifying issues as deadlines, budgets,  software quality, and risk management. PS VI intends the students  to acquire experience on project planning and management, as  well as monitoring and controlling software projects according to  its planning, fully handling related tools, techniques and methods.  It is supported by Software Evolution and Analys is and  Measurement courses. As usual, these courses are taught in the  same semester as PS VI.  3.3 Problem-Solving Courses Rules  All the Problem -Solving courses follow a set of rules defined in  the Political -Pedagogical Project of the Software Engineering  undergraduate degree program [34]. This set of rules was created  with the aim of orienting the execution of the PS courses. It  intends to explicit the roles and responsibilities, presenting  minimal requirements for the PS courses to comply with the PBL  approach while it also confers autonomy to the professors who  plan and execute them.  The PS learning objectives are three:  1. Objective 1 (O1): Establishing a relationship  between Software Engineering theory and practice,  producing perceptible results that demons trate the  skills developed by the students.  2. Objective 2 (O2): Developing the ability to work  collaboratively in order to solve the proposed  problem.  3. Objective 3 (O3): Developing a proactive attitude in  the search for knowledge to solve problems.  The profes sors, also named tutors, should specify the  evaluation criteria and report the grades after each evaluation.  They should also monitor the learning process and interact with  the students to help them to solve conceptual or practical  deadlocks, furthering the collaborative solution of the problem.  On their side  the students must be proactive, independent and  responsible for their own knowledge building. They must be  capable to understand the solution as a whole and to have the  ability to defend it, making themselves responsible for a part of  the work and its int egration. They should also be active members  in their workgroup, expressing their opinion in an argumentative  way and respecting the different viewpoints.  The problems on each PS course should be planned  to develop  the knowledge and skills related to its specific goals. They should  also allow the application of the Software Engineering skills  previously acquired and to awake the interest of the students in  order to further their involvement [34].  3.4 The PBL Development in the Program  Each semester, PS courses are distributed into groups formed by  two to four professors, who must plan and develop the courses  following the PS rules and the main topics of each course.   The course planning includes determinin g the problems to be  presented, organizing the students groups, and establishing the  evaluation process. Regarding the problems, whenever possible,  they seek to address demands from local organizations, for  instance, municipal administration, enterprises and university.   The teaching plans of these courses must register, in an  explicit way, the strategies that are going to be adopted with the  students, including individual and group assessments. While  individual assessments allow the professors to observe t he  progress of each student, dealing with students ’ heterogeneity,  group assessments allow observing aspects as communication,  negotiation, organization, and final product quality.  The professor, based on his/her knowledge and experience,  should assume the position of a facilitator to the students’ learning  process. The students, on the other hand, should gradually develop  autonomy in their learning process.  Different experiences have been published since the beginning  of the program [10,24,27]. In common to the process used in those  experiences is the organization of a PS course in, at least, two  iterations, in which are proposed one or more problems [25].', 'of the program [10,24,27]. In common to the process used in those  experiences is the organization of a PS course in, at least, two  iterations, in which are proposed one or more problems [25].   Using B usiness Process Model Notation (BPMN), Fig. 2  represents the development of several iterations of a PS course,  based on the perspective of the authors of this paper. Each  iteration comprises the activities of Iteration Planning, Problem  Scope Presentation, Group Organizing and Distribution, Tutoring,  Planning, Doing, Checking, Acting, Presenting, and Final  Iteration Assessment.  An iteration starts with the Iteration Planning activity, in which  professors plan the problem scope, defining the learning  objectives, the problem statement, the requirements that should be  observed by the students, the number of individual assessments,  the work deadline, the assessment criteria, and the general  recommendations. The problem scope is presented to the students  in the Problem Scope Presentation activity.   In the Group Organizing and Distribution activity, the groups  are formed by four to six students. In the Planning activity, the  students plan their work, identifying the group effort and  delivering the individual tasks. Then, they start the Doing activity,  in which the students build artifacts that are part of the solution.  In the Checking activity these artifacts are assessed by  professors (tutors) who provide feedback, pointing out corrections  and enhancements. In the Acting activity, the students fix and  improve their artifacts based on their evaluation.  The Tutoring activity is executed in parallel to the Planning,  Doing and Acting activities. In this activity professors guide the  students in the search for the problem solutions.  313', ""Figure 2: An Iteration Process of Problem-Solving.  The Planning, Doing, Checking and Acting activities can be  repeated, in this sequence, in a same iteration. The number of  iterations is defined by the number of individual evaluations,  established in the Iteration Planning activity.  When concl uding the work, the groups deliver the final  artifacts and present the achieved results to the colleagues and to  the professors, in the Presenting activity. Next, in the Final  Iteration Assessment, the professors perform the iteration final  evaluation cons idering the individual and group performance,  building an Assessment report.  After the Final Iteration Assessment, a new iteration can be  performed according to the course initial planning.   4 RESULTS OF PBL APPLICATION  In order to determine whether the learning objectives of the PS  courses, described in [25], have been achieved, a research  instrument was developed. This instrument was applied to the  students enrolled in the Software Engineering undergraduate  degree program at Unipampa and also to the students who have  already obtained their bachelor’s degree. Three statements  compose the research instrument, each seeking to evaluate one of  the objectives, as shown in Table 1.  Each research participant, based on their live d experiences in  PS courses offered throughout the Software Engineering program,  expressed agreement in relation to the statements using the  following Likert Scale: 1 - Strongly Disagree 2 - Disagree, 3 -  Indifferent, 4 - Agree, 5 - Strongly Agree. Additio nally, each  participant selected which PS courses (I to VI) they have  concluded or which courses are being attended by them. Thus, this  information seeks to determine whether there is difference  between the students' perception at different levels of the Software  Engineering program.  Table 1: Association between the Learning Objectives of the  PS Courses and the Research Instrument Statements  Objective Statement  O1. Establishing a  relationship between  Software Engineering  theory and practice,  producing perc eivable  results that demonstrate  the skills developed by  the students.  S1. In PS courses, I can  apply and acquire  theoretical knowledge  in the practice of  problem-solving.    O2. Developing the  ability to work  collaboratively in order  to solve the proposed  problem.  S2. Working as a team  in PS courses makes  me able to solve the  problems presented in  a better way.  O3. Developing a  proactive attitude in the  search for knowledge  to solve problems.  S3. In PS courses, I  feel encouraged to  search solutions for the   proposed problems.    The research instrument was applied on September 2016. On  this time, the Software Engineering program had 141 students  enrolled and 26 bachelors. 122 participants responded the  instrument, which correspond to 73% of the target public,   reaching 95% of confidence level in the responses, according to  [28].    314"", ""Figure 3: Number of responses per category.  Responses to the research instrument were divided into four  categories: Initial, Intermediate, Final, or Bachelor. The initial  category is composed by the responses from students who ar e at  the beginning of the program and, therefore, only attended the PS  I and/or II. The Intermediate category consists of the responses  from students who attended to the PS III and/or IV. Responses  from students who have attended the PS V and/or VI were  grouped in the Final category. The Bachelor category is composed  by the responses from participants who have obtained their  Bachelor’s degree. As noted in the graph in Fig. 3, there is a good  distribution of respondents in each of the categories.  The responses to the three statements without distinction  between categories are shown on the graph of Fig. 4, where the x - axis shows the percentage of responses on a scale of 1 to 100 and  the y-axis shows the statements.      Figure 4: Responses to each statement without distinguishing  between categories of respondents.  By analyzing the responses to the statement 1 (S1), 86% of  respondents agreed that they can apply and acquire theoretical  knowledge in the practice of problem-solving, 9% are indifferent  and 5% disagree. This statement has greater consensus among  respondents, indicating that they realize that the problem -solving  contributes to the learning of theory through practice.  About the second statement, working a s a team in PS courses  makes the student able to solve the problems presented in a better  way (S2), 64% of respondents agreed, 21% are indifferent and  15% disagree. This statement had the lowest percentage of  respondents who agree, thus signaling that team work is still, for  some respondents, a challenge in PS courses.  The responses to the statement 3 (S3) indicate that 79% of  respondents agree that in the PS courses they are encouraged to  search solutions for the proposed problems, 14% are indifferent  and 7% disagree. Therefore, the majority of respondents perceive  themselves acting actively in the search for solutions to the  problems presented.   Analyzing the three statements together, 86% of respondents  agreed that they can apply and acquire theoretical kn owledge in  the practice of problem-solving (S1); 64% agree that working as a  team in PS courses makes them able to solve the problems  presented in a better way (S2); and 79% agree that in the PS  courses they are encouraged to seek solutions to the proposed   problems. Consequently, most of the respondents realize that the  use of problem -solving assists in reaching the objectives of  establishing a relationship between theory and practice;  developing skills to work collaboratively and developing a  proactive attitude, meeting what is proposed in the Pedagogical - Political Project of the SE undergraduate degree program.  In order to check whether there are differences among the  respondents' perception at different levels of the Software  Engineering program, the resp onses to each statement were  analyzed considering its category.  The graphs of Fig. 5, 6 and 7 present the degree of reliability  for statements S1, S2 and S3 separated according to the category  of the respondent, where the x -axis shows the percentage of  responses on a scale of 1 to 100 and the y -axis shows each  category: Initial, Intermediate, Final and Bachelor.  The statement 1 has a percentage above 78% of agreement in  all categories. Especially, the Initial and Bachelor categories  where 91% and 100% of re spondents, respectively, agreed that  they can apply and acquire theoretical knowledge in the practice  of problem -solving (S1). One explanation for this difference is  that the students from Initial PS courses, being recently graduated  from high schools, mostly with traditional and not active teaching  methods, perceive more clearly the impact of the approach in the"", 'from high schools, mostly with traditional and not active teaching  methods, perceive more clearly the impact of the approach in the  process of construction of their knowledge. The bachelors value  the challenge of acquiring and applying theoretical knowledge in  the practice of p roblem-solving because they can perceive how  much the PS courses prepared them for the job market reality.    315', 'Figure 5: Responses to the statement 1 of each one of the  categories.  It is observed that respondents from Final and Bachelor  categories have higher percentage of agreement with the statement  2 than the Initial and Inte rmediate categories. A hypothesis for  this result is that bachelors or students who have attended the final  PS courses have gone through experiences during their  undergraduate that prepared them to handle the challenges of  teamwork, allowing them to apprec iate the benefits over the  difficulties imposed by the collaborative work.      Figure 6: Responses to the statement 2 of each one of the  categories.  The percentage of respondents who agree that they are  encouraged to seek solutions to the proposed problems (S3) is  100% in the Bachelor category and 82% in the Initial category.  Although the percentage of respondents who  disagree with the  statement 3 is smaller in the Final category than it is in the Initial  or Intermediate categories, the Final category has a percentage of  33% of indifferent responses. A hypothesis for this result is that  respondents from Final category realize other alternatives to PS  courses such as participation in projects, junior enterprises,  internships and other courses to improve their active attitude. On  the other hand, the Bachelor category presents no doubts that the  PBL encourages the search f or problem solutions. It may be  related to the fact that the bachelors are already inserted in the  work market.      Figure 7: Responses to the statemen t 3 of each one of the  categories.  Based on the perception of the respondents, the objectives of  the adoption of PBL in the curriculum of the Software  Engineering program have been achieved. However, there is room  for improvement, especially in relation to collaborative work.  Some hypotheses can be raised to explain why the statement  about collaborative work have the lowest level of agreement  among students, (1) conditioning to wor k individually; (2) the  complexity of the group work, which involves dealing with  communication, negotiation, organization, and people with  different knowledge backgrounds and experiences; (3) little  maturity to resolve conflicts; (4) discontinuance of gro up  members during the process of solving a problem; (5) strategies  for setting up working groups.  5 CONCLUSIONS  According to our review, the only Software Engineering  undergraduate degree program in Brazil adopting the PBL  integrated into the curriculum is that offered at Federal University  of Pampa. Thus, besides presenting a detailed view about PBL  integrated into a SE undergraduate degree program, this paper  presents the perceptions from both the students enrolled and the  bachelors about the learning obj ectives of the problem -solving  courses.  The results of the research showed up most of the respondents  realize that the use of problem -solving assists them into  establishing a relationship between theory and practice,  316', 'developing skills to work collaborative ly, and developing a  proactive attitude. However, there is room for improvement,  especially in relation to collaborative work. Though most of  students agree that problem -solving helps them to learn how to  work in teams, this was the statement which fewer students agreed  with.  Although we could confirm that integrating PBL into the  curriculum is reaching out its objectives from the undergraduate  students and the bachelors viewpoints, the research conducted  among undergraduate students was purely quantitative . A  qualitative approach could bring more insights on their feelings  and gather some contributions from their own perspective. A  longitudinal study could also allow the observing of changes in  the students perceptions overtime.  In this sense, the following  future works are pointed out, (1)  investigating the reasons why some students respond to the  statements in a negative or indifferent way; (2) proposing and  implementing alternatives to improve the students’ teamwork; (3)  applying the research instrument a s part of a longitudinal study;  (4) investigating also the perception of the tutors about the  reaching of PS objectives and how it could be improved.  REFERENCES  [1] U. F. Araújo and G. Sastre. 2009. Aprendizagem baseada em problemas no  ensino superior. São Paulo: Summus Editorial.  [2] D. S. Melo-Solarte and M. C. C. Baranauskas.  2009. Resolução de Problemas  e Colaboração a Distância.  Revista Brasileira de Informática na Educação 17,  2, 21-35.  [3] A. C. Gil. Didática do ensino superior. 2011. São Paulo: Atlas.  [4] ACM. Software Engineering 2014: Curriculum Guidelines for Undergraduated  Degree Programs in Software Engineering. A Volume of the Curricula  Computing Serie. https://www.acm.org/education/se2014.pdf. Accessed in: 01 - 04-2017.  [5] A. M. Olivei ra, S. C. Santos, and V. C. Garcia 2013. PBL in Teaching  Computing: An overview of the Last 15 Years. In Proc IEEE Frontiers in  Education Conference (FIE), Oklahoma, EUA. DOI:  http://dx.doi.org/10.1109/FIE.2013.6684830.  [6] J. Armarego. Educating Agents of Change. 2005. In Proceedings of 18th  Conference on Software Engineering Education and Training, IEEE Computer  Society Press, 181-194.  [7] J. Amarego. Beyond PBL: Preparing Graduates for Professional Practice. 2007.  In Proceedings of 20th Conference on Software Engineering Education and  Training, IEEE Computer Society Press, 175- 183.  [8] G. Marsicano, F. F. Mendes, M. V. Fernandes, and S. A. A. de Freitas. An  integrated approach to the R equirements Engineering and Process Modelling  teaching. 2016. In Proc. IEEE 29th International Conference on Software  Engineering Education and Training, 2016, DOI 10.1109/CSEET.2016.23.   [9] S. A. A. de Freitas, W. C. M. P. Silva, and G. Marsicano. 2016. Using an  Active Learning Environment to Increase Students’ Engagement. In Proc.  IEEE 29th International Conference on Software Engineering Education and  Training, DOI: 10.1109/CSEET.2016.23.  [10] M. C. Cera, M. H. Dal Forno, and  V. G. Vieira. 2012. Uma proposta para o  Ensino de Engenharia de Software a partir da Resolução de Problemas.   Revista Brasileira de Informática na Educação 29, 3, 116 -132. Available in  <http://www.br-ie.org/pub/index.php/rbie/article/view/1391>. Accessed in: 10- 10-2016.  [11] D. L. Notari, J. Bohn and, Elisa Boff. 2009. Uma abordagem baseada em  problemas para aprendizagem colaborativa de sistemas operacionais. XV  Workshop sobre Ed ucação em Computação (WEI). In Proc of the XXIX  Congresso da SBC, Bento Gonçalves, RS, 451-454.  [12] A. G. D. Corrêa  and V. F. Martins. 2014. Methodology applied problem -based  learning in teaching HCI: A case study in usability evaluation of an online  course. In Proc 9th Iberian Conference on Information Systems and  Technologies (CISTI). DOI: http://dx.doi.org/10.1109/cisti.2014.6877009.  [13] A. P. Ambrósio, F. M. Costa, L. Almeida, A. Franco, and J. Macedo. 2011.', 'Technologies (CISTI). DOI: http://dx.doi.org/10.1109/cisti.2014.6877009.  [13] A. P. Ambrósio, F. M. Costa, L. Almeida, A. Franco, and J. Macedo. 2011.  Identifying Cognitive Abilities to Improve CS1 Outcome. In Proc. 41st  ASEE/IEEE Frontiers in Educ ation Conference (FIE). DOI:  http://dx.doi.org/10.1109/FIE.2011.6142824.  [14] A. G. O. Fassbinder, T. G. Botelho, R. J. Martin s, and E. F. Barbosa . 2015.  Applying Flipped Classroom and Problem -Based Learning in a CS1 Co urse. In  Proc Frontiers in Education Conference (FIE). DOI :  http://dx.doi.org/10.1109/FIE.2015.7344223.   [15] R. A. Segura, C. F. Araújo Júnior, and I. F. Silveira. 2009. Apren dizagem  Baseada em Problemas Apoiada por Ambientes Virtuais: um Estudo de Caso  em Banco de Dados. XV Workshop sobre Educação em Computação (WEI). In  Proc of the XXIX Congresso da SBC, Bento Gonçalves, RS, 447 -450.  [16] D. M. B. Santos. 2007. Uma abordagem baseada em problemas para o ensino  de Padrões GRASP. XV Workshop sobre Educação em Computação (WEI). In  Proc. of the XXVII Congresso da SBC, Rio de Janeiro, RJ, 66 -75.  [17] D. S. Loubach, J. C. S. Nobre, A. M. da Cunha , L. A. V. Dias, and M. R.  Nascimento. 2006. Testing Critical Software: A Case Study for an Aerospace  Application. In Proc. AIAA/IEEE 25th Digital Avionics Systems Conference,  2006, 1-9.  [18] M. P. Ramos et al. 2013. Applying Interdisciplinarity and Agile Methods in the  Development of a Smart Grids System. 2013. In Proc . Tenth International  Conference on  Information Technology: New Generations (ITNG), IEEE  Press, 103-110. DOI: http://dx.doi.org/ITNG.2013.22.  [19] D. M. B. Santos, H. Saba, J. Rocha Junior, and V. Sarinho. 2007. Integrando as  Disciplinas de Engenharia de Software, Análise e Projeto de Sistemas e Banco  de Dados utilizando PBL. XV Workshop sobre Educação em Computação  (WEI). In Proc. of the XXVII Congresso da SBC, Rio de Janeiro, RJ, 66 -75.  [20] J. A. M. Santos, A. Loula, and M. F. Angelo. 2008. Experiências em um  Estudo Integrado de Programação usando PBL. Workshop sobre Educação em  Computação (WEI). In Proc. of the XXVIII Congresso da SBC, Belém, PA,  250-253.  [21] R. A. Bittencourt, C. A. Rodrigues, and D. S. S. Cruz. 2013.  Uma Experiência  Integrada de Programação Orientada à Objet os, Estruturas de Dados e Projeto  de Sistemas com PBL. XXI Workshop sobre Educação em Computação  (WEI). In Proceeding of the XXXIII Congresso da SBC, Maceio, AL, 591 -600.  [22] C. S. Cintra, and R. A. Bittencourt. 2015. Being a PBL teacher in computer  engineering: An interpretative phenomenological analysis. In Proc Frontiers in  Education Conference, FIE, 2015. DOI: 10.1109/FIE.2015.7344223.   [23] C. C. Hernandez,  and M. F. P. Ledón. 2010. A Disciplina de Resolução de  Problemas na Matriz Curricular de Ciência da Computação. XVI Workshop  sobre Educação em Computação (WEI). In Proceeding of the XXX Congresso  da SBC, Belo Horizonte, MG, 941-950.  [24] C. Z. Billa, and  M. C. Cera. 2012. Utilizando Resolução de Problemas para  aproximar Teoria e Prática na Engenharia de Software. In Proc. XII Fórum de  Educação em Engenharia de Software (FEES).  [25] UNIPAMPA - Universidade Federal do Pampa - Campus alegrete. 2015.  Projeto Político-Pedagógico do Curso de Engenharia de Software, Alegrete -  RS – Brasil. Available in:  http://cursos.unipampa.edu.br/cursos/engenhariadesoftware/?pagina_f ixa=ppc.  Accessed in: 10-20-2016.  [26] ACM. 2004. Software Engineering 2004: curriculum guidelines for  undergraduated degree programs in software engineering. a volume of the  curricula computing series. Available in:  http://sites.computer.org/ccse/SE2004Volume.pdf. Accessed in: 10-20-2016.  [27] S. Mergen, F. Kepler, J. da Silva, and M. Cera. 2014. Using PDCA as a  General Framework for Teaching and Evaluating the Learning of Software  Engineering Disciplines. Sys - Revista Brasileira de Sistemas de Informação   (CESI/SBCV) 7, 2, 451-462.', 'General Framework for Teaching and Evaluating the Learning of Software  Engineering Disciplines. Sys - Revista Brasileira de Sistemas de Informação   (CESI/SBCV) 7, 2, 451-462.  [28] M. F. Triola. 1999. Introdução à Estatística. 7a. Ed. Rio de Janeiro: LTC,1999.     317']","[', , by Guedes et al.  STUDENTS’ PERCEPTIONS ON PBL INTEGRATION IN A SE UNDERGRADUATE PROGRAM This  briefin  reports  the  studeits’ perceptois renardiin  PBL  iitenratoi iito  a  Softare  Einiieeriin Uidernraduate  Denree  Pronram  at Federal Uiiversity of Pampa - UiiPampa. This  iitenratoi  is  doie  throunh  six courses,  called  Problem-Solviin  (PS), distributed ii the frst six semesters. FINDINGS The  students´  perceptins  were  cillected  and analyzed  accirding  ti  the  general  learning ibjecties if the curriculum PS ciurses: 1. Establishing a relatinship between Sifware Engineering  theiry  and  practce,  priducing perceiiable  results  that  deminstrate  the skills deieliped by the students; 2. Deieliping the ability ti wirk cillabiratiely in irder ti silie the pripised priblem; 3. Deieliping a priactie attude in the search fir kniwledge ti silie priblems. The research iniilied 122 partcipants  (students and bachelirs). The partcipants were diiided inti fiur  categiries:  Inital,  Intermediate,  Final,  ir Bachelir.  The Inital categiry is cimpised by the respinses frim  students  whi  are  at  the  beginning  if  the prigram  and,  therefire,  inly  atended  the  PS  I and/ir II.  The  Intermediate  categiry  cinsists  if  the respinses frim students whi atended ti the PS III and/ir IV.  Respinses frim students whi haie atended the PS V and/ir VI were griuped in the Final categiry.  The  Bachelir  categiry  is  cimpised  by  the respinses  frim  partcipants  whi  haie  ibtained their Bachelir’s degree. Leariiin Objectve 1: \uf06c 86%  if  respindents  agreed  they  can  apply and  acquire  theiretcal  kniwledge  in  the practce  if  priblem-siliing,  9%  are indiferent and 5% disagree;  \uf06c This statement has greater cinsensus aming respindents, indicatng they realize that the priblem-siliing  cintributes  ti  the  learning if theiry thriugh practce.  Leariiin Objectve 2: \uf06c 64%  if  respindents  agreed,  21%  are indiferent and 15% disagree;  \uf06c This statement had the liwest percentage if respindents whi agreed, thus signaling that teamwirk  is  stll,  fir  sime  respindents,  a challenge in PS ciurses. Leariiin Objectve  : \uf06c 79% if respindents agree in the PS ciurses they are enciuraged ti search silutins fir the pripised priblems, 14% are indiferent and 7% disagree;  \uf06c Therefire,  the  majirity  if  respindents perceiie  themselies  actng  actiely  in  the search  fir  silutins  ti  the  presented priblems. Difereices amoin the respoideits of difereit levels: \uf06c The ibjectie 1 has a percentage abiie 78% if agreement in all categiries;  \uf06c The  Inital  and  Bachelir  categiries  where 91% and 100% if respindents, respectiely, agreed  that  they  can  apply  and  acquire theiretcal  kniwledge  in  the  practce  if priblem-siliing; \uf06c Respindents  frim  Final  and  Bachelir categiries  haie  higher  percentage  if agreement  with  the  ibjectie  2  than  the Inital and Intermediate categiries; \uf06c The percentage if respindents whi agreed they are enciuraged ti seek silutins ti the pripised  priblems  (S3)  is  100%  in  the Bachelir  categiry  and  82%  in  the  Inital categiry. Based  in  the  respindents’  perceptin,  we cincluded the ibjecties if PBL adiptin in the Sifware Engineering prigram curriculum haie been achieied. Hiweier, there is riim fir  impriiement,  especially  in  relatin  ti cillabiratie wirk. Sime  hypitheses  can  be  raised  ti  explain why the statement abiut cillabiratie wirk haie  the  liwest  leiel  if  agreement  aming students: (1) cinditining ti wirk indiiidually;  (2) the cimplexity if the griup wirk, which iniilies  dealing  with  cimmunicatin, negitatin,  irganizatin,  and  peiple  with diferent  kniwledge  backgriunds  and experiences; (3) litle maturity ti resilie cinficts;  (4) discintnuance if griup members during the pricess if siliing a priblem;  (5) strategies fir setng up wirking griups.       Keywords: Priblem-Based Learning, Sifware Engineering Undergraduate', '(4) discintnuance if griup members during the pricess if siliing a priblem;  (5) strategies fir setng up wirking griups.       Keywords: Priblem-Based Learning, Sifware Engineering Undergraduate  Degree Prigram, PBL integrated inti  the Curriculum. Who is this briefin  or? Sifware engineering academics whi  want ti make decisiins abiut PBL  adiptin inti the curriculum. Where the fidiins come  rom? All fndings if this briefng were  extracted frim a research instrument  applied by Guedes et al. ti bith  undergraduate students and bachelirs  in the prigram.   What is iicluded ii this briefin? Main fndings if the research  instrument applied ti the partcipants.  ORIGINAL RESEARCH REFERENCE  Gilleanes Thirwald Arauji Guedes, Andréa Saabedra Birdin, Aline Vieira de Melli, Amanda Miencke Meli. PBL Iitenratoi iito a Softare Einiieeriin  Uidernraduate Denree Pronram Curriculum: Ai Aialysis of the Studeits’ Perceptois. Priceedings if SBES’17, Firtaleza, CE, Brazil, September 20–22, 2017. DOI:  htps://dii.irg/10.1145/3131151.3131178.']","**Title: Enhancing Software Engineering Education through Problem-Based Learning**

**Introduction:**
This briefing presents findings from a study that investigates the integration of Problem-Based Learning (PBL) into the Software Engineering undergraduate curriculum at the Federal University of Pampa (Unipampa) in Brazil. The goal is to understand students’ perceptions of this educational approach and its effectiveness in achieving the intended learning outcomes.

**Core Findings:**
The research involved a survey of students enrolled in the Software Engineering program, focusing on their experiences with six Problem-Solving courses integrated into the curriculum. Key findings include:

1. **Achievement of Learning Objectives:** A significant majority of students (86%) reported that they could apply theoretical knowledge in practical problem-solving contexts, indicating that PBL effectively bridges theory and practice.

2. **Collaboration Challenges:** While 64% of students agreed that teamwork in PBL courses improved problem-solving abilities, this was the lowest agreement among the survey statements. This suggests that collaborative work remains a challenge, potentially due to factors such as varying levels of student preparedness and difficulties in group dynamics.

3. **Encouragement for Proactive Learning:** A strong majority (79%) felt encouraged to seek solutions actively, reflecting PBL's success in fostering a proactive attitude towards learning.

4. **Perceptions by Student Level:** Differences in perceptions were noted across different student levels (initial, intermediate, final, and bachelor). Notably, final-year students and graduates expressed higher confidence in their collaborative abilities, likely due to accumulated experiences throughout their studies.

5. **Room for Improvement:** Despite positive feedback, the study identified areas for enhancement, especially in facilitating effective teamwork. Suggestions include exploring the reasons behind negative or indifferent responses and implementing strategies to enhance collaborative skills.

**Who is this briefing for?**
This briefing is intended for educators, curriculum designers, and academic administrators involved in Software Engineering education, as well as stakeholders interested in innovative teaching methodologies.

**Where the findings come from?**
All findings are derived from a study conducted by G. T. A. Guedes and colleagues, which analyzed students' perceptions of PBL integration in the Software Engineering curriculum at Unipampa.

**What is included in this briefing?**
This briefing outlines the integration of PBL in the curriculum, students' perceptions of its effectiveness, and highlights both successes and areas for improvement.

**What is NOT included in this briefing?**
Detailed statistical analyses and qualitative insights from individual student experiences are not covered; further qualitative research could provide deeper understanding.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the Software Engineering program at Unipampa:**
[http://cursos.unipampa.edu.br/cursos/engenhariadesoftware/?pagina_fixa=ppc](http://cursos.unipampa.edu.br/cursos/engenhariadesoftware/?pagina_fixa=ppc)

**Original Research Reference:**
Guedes, G. T. A., Bordin, A. S., Mello, A. V., & Melo, A. M. (2017). PBL Integration into a Software Engineering Undergraduate Degree Program Curriculum: An Analysis of the Students’ Perceptions. In Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES), Fortaleza, CE, Brazil. https://doi.org/10.1145/3131151.3131178"
"['Programming Language Adoption as an Epidemiological Phenomenon Emanoel Barreiros University of Pernambuco Garanhuns, Brazil emanoel.barreiros@upe.br Jones Albuquerque Deinfo/UFRPE, LIKA/UFPE, Epitrack Recife, Brazil jones.albuquerque@pq.cnpq.br João F. L. de Oliveira University of Pernambuco Garanhuns, Brazil fausto.lorenzato@upe.br Helaine Lins Informatics Center/UFPE Recife, Brazil hsl@cin.ufpe.br Sergio Soares ∗ Informatics Center/UFPE Recife, Pernambuco scbs@cin.ufpe.br ABSTRACT In Software Engineering, technology transfer has been faced as a peer to peer problem, concerning only the adoption and innov a- tion agents. This approach works well when one is just seekin g the adoption of a technology by a speci/f_ic client, but it can no t solve a common problem that is the adoption of new technolo- gies by a large mass of potential new users. In wider context,it no longer makes sense to focus on peer to peer transfer. A new way of looking at the problem is necessary. The diﬀusion of in no- vations is more natural when it is treated as a spread of infor ma- tion in a community, similar to that observed in epidemics. T his work proposes a paradigm shift to show that the adoption of pr o- gramming languages can be formally treated as an epidemic. T his shift of focus allows the dynamics of adoption of programmin g lan- guages to be mathematically modeled as such, de/f_ining models that explain the communities’ behavior when adopting programmi ng languages, and allows some forecast to be made. CCS CONCEPTS •Mathematics of computing → Mathematical analysis ; • So- cial and professional topics → Industry statistics ; KEYWORDS Software Engineering, Computational Epidemiology, Genetics, Pro- gramming Languages ACM Reference format: Emanoel Barreiros, Jones Albuquerque, João F. L. de Oliveira, Hel aine Lins, and Sergio Soares. 2017. Programming Language Adoption as an Epidem i- ological Phenomenon. In Proceedings of SBES’17, Fortaleza, CE, Brazil, Sep- tember 20–22, 2017, 6 pages. https://doi.org/10.1145/3131151.3131188 ∗Also aﬃliated with SENAI Innovation Institute for ICT Permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided that copies ar e not made or distributed for pro/f_it or commercial advantage and that copies bear this n otice and the full cita- tion on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. T o copy otherwise, or re- publish, to post on servers or to redistribute to lists, requ ires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.or g. SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5326-7/17/09. . . $15.00 https://doi.org/10.1145/3131151.3131188 1 INTRODUCTION An innovation, as de/f_ined by Rogers [26], is “an idea, practic e, or object that is perceived as new by an individual or other unit of adoption”. In his career, ideas of diﬀusion of innovation ha ve been studied on the domain of agricultural innovations. They ste m from technological advances, such as novel machinery, to new pla nta- tion techniques. The process of technology transfer is very complex and a tech - nology may take a long time to be fully adopted in practice [23 ]. The diﬀusion of innovation process is one in which a speci/f_ic i nno- vation is communicated through certain channels over time a mong the members of a social system [26]. The innovation must be co m- municated through channels considered relevant to the comm u- nity it is targeted to, otherwise, that target community is v ery unlikely to “/f_ind” the new technology. A study by Jedlitschka et al. [14] discuss some relevant information sources of infor mation for technology transfer. The study shows that articles in sc ienti/f_ic journals and other kinds of literature are only the 10 th and 11 th', 'for technology transfer. The study shows that articles in sc ienti/f_ic journals and other kinds of literature are only the 10 th and 11 th most important information sources in a list of 11 possible s ources of information. This de/f_ines a problem we (technology develo pers) need to address to make our creations fully adoptable to the w ider public. Currently, the practice of Software Engineering (SE) is mos tly concerned with the models for eﬃcient and eﬀective technolo gy transfer from Academia (universities and research labs) to industry. The existing models are usually focused on a very restricted envi- ronment, where research is carried out by specialized resea rch labs (the innovation agent) keeping close contact with the organ ization (the adoption agent). To this end, some models of technology trans- fer have been proposed [10, 22, 23, 26]. The model proposed by Rogers [26] is a general one, even though most of his researchhas been done under the agricultural domain. The ones proposed b y Redwine and Riddle [23], P/f_leeger [22] and Gorscheck et al. [1 0] are more related to the domain of SE and describe their approa ch to the problem of technology transfer. Until now, the problem has only been addressed with technica l issues in mind. Technology transfer models have been propos ed as well as processes, support tools, and so forth. We propose to look at technology transfer from a diﬀerent point of view. Philosop hically, looking at a problem with a diﬀerent perspective might in/f_lue nce the way solutions are designed. SE researchers are starting to argue that the /f_ield carries many similarities with social science s [19]. In 255', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil E. Barreiros et al. this light, technology transfer may also be viewed as a socia l phe- nomenon, in which several rules may be derived from empirica l observation. The problem now shifts from technology transf er to technology (information) diﬀusion. This shift is possible because in diﬀusion of information social systems, the word of mouth is the key channel for communication, which closely resembles the dynamics of a disease spread in a community, often requiring per- sonal contact to spread the disease [14, 19]. Hence, we propose describing technology diﬀusion in SE, mor e speci/f_ically the adoption of programming languages, as a soc ial epidemic. The concept of epidemiology emerged in medical sc i- ences, where the dynamics of infectious diseases is studied within a population of susceptible individuals. Apart from the medi cal con- text, the dynamics of infectious diseases may be mathematic ally modeled [12], which gives us the ability to analyze and simul ate a wide range of scenarios. With the current computing tools, s imula- tions can be ran, and even some kinds of forecasting might be p er- formed [29]. To show that the dynamics of epidemic disease sp read and programming language adoption share similarities, Fig ure 1 (a) depicts the Ebola outbreak in 2014 and Figure 1 (b) shows the c re- ation of new projects using Matlab on Sourceforge from 2000 t o 2009. The main features include the slow start of the spread, but quickly builds up because of its initial exponential nature , and the progression to an asymptote after the in/f_lection point (peak of in- fection). Considering the social aspects of programming language ado p- tion and the mathematical modeling of the epidemic disease s pread dynamics, we de/f_ine our hypothesis: It it is possible to use bio- logical growth models to describe the dynamics of programmi ng language adoption by users of open source software hosting s ites. With a model satisfactorily /f_itted to the data, it is possible not only to describe the dynamics of programming language adoption, but, to some extent, forecast the dynamics of programming langua ge adoption. 1.1 Motivation The main question that motivated this work was: is there a way to understand the dynamics of programming language adoption b y a group of software developers? Understanding a system is the pri- mary step when one wants to start manipulating it. After read ing Meyerovich et al. ’s work [18–20] it became clear that the ado ption of programming languages was a social phenomenon. The auhto rs approached the problem from a social perspective only, thou gh. Malcolm Gladwell’s “The Tipping Point” [9], introduced the notion that simple ideas could go “viral”, with the dynamics of the p he- nomenon in/f_luenced mostly by social factors. The ideas discu ssed by Gladwell include the roles of speci/f_ic kinds of individual s, the relation of these individuals to the community, the kind of i nforma- tion being communicated and the channels used to communicat e the information. These concepts greatly resemble a biologi cal sys- tem when looked through the epidemics theories optics. In this context, diseases are very similar to the informatio n that is being transmitted throughout the community. More import antly, the information transfer requires the social interaction a mong indi- viduals. We already knew that this feature of innovation diﬀ usion Table 1: Epidemiological concepts and their mapping to this work. Original Concept Concept Mapping Incidence Number of individuals that use a given programming language per potential adopter Incidence Rate Incidence per unit of time (month or week) Prevalence The number or individuals (cases) that use a given programming language at one time Prevalence Rate The number of cases per potential adopter Infectious Agent Programming language Susceptible A potential adopter of a given programming language Infected', 'at one time Prevalence Rate The number of cases per potential adopter Infectious Agent Programming language Susceptible A potential adopter of a given programming language Infected Individual that has been identi/f_ied as user of a given programming language Transmission Route Word of mouth Symptom To participate in open source projects that use a given programming language could also exist in Software Engineering due to the work of Je dl- itschka et al. [14]. They discovered that, for a software dev eloper, the most important source of information for adopting a tech nol- ogy was a good experience and recommendation by a colleague. Nowadays, more than ever, software developers work collectively and collaboratively. The suggestion that the adoption of pr ogram- ming languages could be modelled as an epidemic seemed to mak e sense. The main idea behind this is that open source projects and software developers could be mapped as susceptible individ uals, while programming languages could be viewed as infective ag ents. Hence, the programming languages adoption can be character ized as a spreading disease in a community. Table 1 summarizes thi s concept mapping. It is clear, though, that diﬀerent infecti ous agents have diﬀerent infectivities, and the same should be observe d when studying the adoption of programming languages using epide mi- ological concepts. Similarly, diﬀerent factors in/f_luence h ow infec- tive is an infectious agent, and how “adoptable” is a program ming language. That’s why Ebola’s infectivity is higher than man y other diseases, and why some languages are adopted faster than oth ers, if at all. It is not possible for us to track the incidence or the inciden ce rate because, with the current approach, we are not able to es ti- mate the number of potential adopters beforehand. Since we c an- not estimate the susceptible population we had to use cumula tive biological growth models instead of the classical compartm ental models, which are only concerned with the prevalence. We then had to assess if the idea indeed made sense and we could continue the research. Meyerovich et al. made availab le a large dataset regarding the adoption of programming langua ges [20]. The data includes, among other things, information ab out projects on Sourceforge (https://sourceforge.net/). We d ecided to use this dataset to run a preliminary study as a proof of conce pt. 256', 'Programming Language Adoption as an Epidemiological Pheno menon SBES’17, September 20–22, 2017, Fortaleza, CE, Brazi l (a) Ebola outbreak in 2014 (b) new projects using Matlab, 2000 to 2009 Figure 1: Ebola vs Matlab. 1.2 Why Is This Important? It is not possible, with the current way of thinking of SE researchers, to address the problem of technology adoption by a large mass of potential users. Until now, SE researchers have only faced t he adop- tion of technologies in a peer-to-peer fashion. The current work relates three distinct areas of knowledge in order to reason about the diﬀusion of innovations in SE. By using concepts of epide mi- ology, powered by mathematical models of epidemics, we are a ble to understand, describe and forecast the adoption of progra mming languages in SE. To the best of our knowledge, there is no othe r work that relates these concepts in SE. To demonstrate that it is possible to use such concepts to stu dy the diﬀusion of innovations in SE, we focus on the adoption of pro- gramming languages by users of open source software reposit ories. With this approach, it is possible to: (1) Perform real time analysis of programming language adop - tion; (2) Forecast the adoption of programming languages; (3) In/f_luence decision markers regarding the adoption of promis- ing programming languages; 2 RELATED WORK Meyerovich and Rabkin have performed a series of studies to un- derstand the social and technical factors that in/f_luence the adop- tion of programming languages [18–20]. By mining open sourc e repositories and surveying developers the authors discove red that the use of programming languages follow a power law. This mea ns that a small number of languages account for the most users wh ile less used languages are used in niche domains. They also con/f_i rmed what has been discovered in previous research by Karus et al. [15], which stated that developers tend to stick with a set of langu ages over time. Adoption decisions for domains beyond programming languag es are studied by social scientists. Some models have been prop osed over the years, such as the Technology Acceptance Model (TAM ) [5] and the Uni/f_ied Theory of Acceptance and Use of Technology (UTAUT) [28]. These models relate factors, for example, per ceived ease of use to adoption decisions. Models that have been twea ked for software development have been able to explain 63% of the variance in developer intention to use object-oriented des ign tech- niques [11]. Whereas that work aims to understand the genera l factors behind technology adoption, we investigate the ado ption of programming languages in a given population. In our work, ev en though we do not identify the factors that are responsible fo r the speed of the dynamics, their eﬀect is perceived and represen ted as diﬀerent values for the /f_it parameters for each model and lang uage. A study shows that the Rogers’ model of diﬀusion [26] accu- rately described the process by which COBOL programmers at a large /f_inancial-services company learned the C language [1]. A later single-organization study investigated the decisio n of whether to adopt a formal development methodology [25]. In both case s, the researchers ignored the intrinsic technical attribute s of the lan- guage or methodology in question, and exclusively consider ed so- cial factors. In our work we are not concerned with diﬀerent k inds of factors. We simply assume that in/f_luencing factors exist a nd are embodied into the model. Understanding which and how much they in/f_luence are to be investigated in future studies. 2.1 Diﬀusion of Innovations and Epidemic Models for Non-Biological Phenomena In parallel to what has been written by Rogers [26], diﬀusion of innovation models have been studied for over 40 years follow ing the seminal work from Bass [3] on product growth models. It in - corporates concepts like product potential, the social net work in', 'innovation models have been studied for over 40 years follow ing the seminal work from Bass [3] on product growth models. It in - corporates concepts like product potential, the social net work in which the innovation diﬀuses, internal and external in/f_luen ces. A good review on product growth models has been written by Pere s et al. [21]. It is also a research /f_ield in Economics, which tries to model i t using diﬀerent techniques, factors, assumptions, and cont exts [17]. Some authors, though, try to use diﬀerent concepts when tryi ng to understand the phenomenon. Some make use of the mathematica l epidemiology theories, introduced by Daniel Bernoulli in 1 766 [7] and later followed by Kermaack and McKendrick [16]. Mathema t- ical epidemiology is now a very mature /f_ield of study, with ext en- sive literature [4]. By borrowing concepts from that /f_ield, researchers are able t o describe diﬀusion of innovations phenomena in an interesti ng way [8]. Even though several studies address the problems of diﬀ usion 257', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil E. Barreiros et al. of innovations and technology transfer in software enginee ring and computer science [2, 6, 14, 22, 23, 30], to the best of our k nowl- edge, there is no initiative that investigates the diﬀusion of inno- vations using the concepts of epidemiology in SE. 3 MATHEMATICAL EPIDEMIOLOGY Epidemiology is the science that studies patterns, causes and ef- fects of health and disease condition in de/f_ined populations . It is both studied by the medical sciences, which focuses on the pu b- lic health side of the problem, and by mathematics, which tri es to develop models to describe epidemiological phenomena allo wing the scienti/f_ic community to draw a number of relevant analyse s on them. In the present work, programming languages play the ro le of diseases, which act as infecting agents in a given populat ion. The initial mathematical epidemiology history dates 1760, when Daniel Bernoulli developed a model to describe the smallpox epi- demics. The basic theory, however, has been established bet ween 1900 and 1935 [4]. Compartmental models have played an impor - tant role since then [16]. 3.1 Models for Biological Growth and Their Applications on Epidemics Biological growth models, which were originally proposed t o model the growth of animal populations and forestry, have also bee n used to model epidemics [13]. Growth models in general describe o nly one curve, the accumulated number of individuals. When used to model epidemic scenarios, they model the accumulated numbe r of infected individuals. Diﬀerent from the traditional compa rtmental models[12], which de/f_ine the number of individuals in a given com- partment at time t, growth models describe the sum of individuals that have been infected up until time t. In this work, we have used the Richards growth function to model the adoption of progra m- ming languages. The Richards function [24] is one of the most well known func- tions for biological growth, and is expressed as Equation 1: I (t) = K [1 + e−r (t−tm) ] 1 a , (1) where K is the carrying capacity, or the total case number of the outbreak, r is the per capita growth of the infected population, and a is the exponent of deviation from the standard logistic curv e. This particular model assumes that there must be a single peak of h igh incidence of the disease, resulting in an S-shaped curve wit h one in/f_lection point. The in/f_lection point can be de/f_ined as the point in which the growth changes from increasing to decreasing, or v ice versa (change in the curve’s concavity). In Equation 1, tm = ti + (ln a)/r, where ti is the time of the in/f_lection point of the curve. The in/f_lection point of the curve can also be found by taking it s second derivative and /f_inding the solution for t when it equals 0. 4 MATERIALS AND METHODS In this section the nature of the data will be discussed and how we reason about it. The data is composed of metadata about ope n source projects, including their date of creation, associa ted users and programming languages. In order to map the dataset to the epidemic theory, we consider each project as susceptible in divid- ual, and the date of creation of the project accounts as the da te of its infection by the language it used. For example, if a pro ject named ProjectA was created on the fourth of October of 2005, and it used the Java language, we account it as an individual infe cted with the “disease” Java on October of 2005. In our context, pr o- gramming languages are treated as diseases that spread thro ugh a community, and the epidemic models described the dynamics of this spread. Also, the data is grouped by month, hence, each d ata point in the data set corresponds to a month. The data was extracted from Sourceforge 1, and comprises meta- data from 213,471 projects. The metadata includes, for each project, the set of languages used, primary project category, date of cre-', 'The data was extracted from Sourceforge 1, and comprises meta- data from 213,471 projects. The metadata includes, for each project, the set of languages used, primary project category, date of cre- ation and project’s owners. The years examined range from 20 00 to 2009. The dataset was constructed and distributed by Leo Mey ero- vich [20] and can be freely downloaded from his website 2. 4.1 Model Fitting and Evaluation Model /f_itting is performed using the Wolfram Mathematica soft- ware3. Mathematica is a based on the Wolfram Language 4, which uni/f_ies a broad range of programming paradigms and uses its co n- cept of symbolic programming to allow a higher level of abstr ac- tion. It includes many useful functions for mathematical pr ogram- ming, which added to the symbolic programming paradigm en- ables a series of very complex computation with minimum eﬀort. To verify goodness of /f_it and forecasting capabilities of the mod- els, we ran the Székely Energy test [27]. Models were /f_itted to only 80% of the data and the Székely Energy test was executed to tes t the /f_it of the model to the full dataset. The goal with this test is to evaluate goodness of /f_it and understand how much into the futu re the model can be used for forecasting. Identifying that a model /f_itted with only 80% of the data, stil l can describe the full data set with satisfactorily precisio n means that under real conditions, when analyzing real time data, w e can safely predict 25% of the size of the data set into the future. Of course, the more distant into the future, the lower the preci sion. 5 PRELIMINARY RESULTS The data from Sourceforge includes the languages: Java, C, P HP and C++, the four most used languages in the world from the TIO BE Index5 in the analyzed period, and also in a calculated program- ming language ranking with the most used languages on Source - forge from 2000 to 2009. The rank was calculated using the num ber of projects that use a given language in each year. 5.1 Fit Results The /f_itting algorithm from Mathematica should /f_ind values for the parameters discussed in Section ?? (K, r, tm and a) so that the curve best /f_its the data. The plots for the /f_it results can be seen from Figures 2 (a) to Figure 2 (d). In those plots, green points represent 80% dataset, the gray points are the remaining points to the f ull dataset. The red curve represents the /f_itted model to the full dataset 1https://sourceforge.net/ 2https://lmeyerov.github.io/ 3https://www.wolfram.com/mathematica/ 4http://bit.ly/1qD3GJw 5http://www.tiobe.com/tiobe_index 258', 'Programming Language Adoption as an Epidemiological Pheno menon SBES’17, September 20–22, 2017, Fortaleza, CE, Brazi l (green and gray), while the black curve is the /f_itted model to s liced dataset (green only). The /f_it parameters can be found on Table 2. Table 2: Fit parameters found by the algorithm of nonlinear model /f_itting using the Richards model. Language K r tm a C (100%) 38244 0.017 -748.71 7.56 × 10−7 C (80%) 29085 0.022 -592.45 8.96 × 10−7 C++ (100%) 78057 0.0133 -905.46 1.33 × 10−6 C++ (80%) 61705 0.015 -816.75 1.21 × 10−6 Java (100%) 86338 0.02 32.14 0.225 Java (80%) 179248 0.011 -1129.89 1.43 × 10−6 PHP (100%) 63607 0.015 -157.17 0.017 PHP (80%) 89569 0.013 -934.20 1.25 × 10−6 As can be seen, the /f_it functions seem to describe correctly th e “behavior” of the data. However, some measures of relative q uality for the data can be calculated. The results for the Székely En ergy test for goodness of /f_it is presented as well the R2 measure in Ta- ble 3. The test result shows that for all languages, it is not p ossible to reject the null hypothesis that the samples come from the s ame distribution. It is safe to say, though, given the size of the samples, that they indeed come from the same distribution. In other wo rds, the model /f_itted to only 80% of the data is still valid to descri be the full dataset, i.e., we can use the models to forecast the adop tion of programming languages. Table 3: Results for the goodness of /f_it test for the Richards model. Lang. R2 Székely statistic p-value Rejects H0? C (100%) 0.9988 1297.63 0.987 No C (80%) 0.9980 3967.76 0.706 No C++ (100%) 0.9989 1433.59 0.996 No C++ (80%) 0.9977 2194.39 0.961 No Java (100%) 0.9988 1793.37 0.993 No Java (80%) 0.9979 2040.49 0.987 No PHP (100%) 0.9990 942.95 0.996 No PHP (80%) 0.9981 1286.83 0.994 No 6 CONCLUDING REMARKS Researchers have already shown that the phenomenon of diﬀusion of innovations clearly resembles the spread of a disease in a pop- ulation. Based on that premise, we have provided evidence th at indicates that the adoption of programming languages can al so be considered an epidemiological phenomenon. In order to test our hypothesis we have obtained metadata of projects from Sourceforge. By using a well known model of bio - logical growth, frequently used to model the dynamics of epi dmic disease spread, we have shown that the dynamics of programmi ng language adoption is consistent with that model. Not only th at, we have also shown that it is possible to even forecast, even tho ugh with a limited time frame, the adoption of programming langu ages. There is evidence that shows that it is possible to predict fu ture datapoints that is equivalent in size to approximately 25% o f the dataset used to build the model. We envision that in order to continue this research, new func - tions should be added to the pool of possible models. Diﬀeren t functions provide diﬀerent features to the curve, what migh t be interesting because diﬀerent data might have diﬀerent stru cture and be better modeled by one or another. The analysis of resid uals might also provide relevant information. Preliminary resu lts indi- cate that the residuals have a repeating pattern that can be u sed to improve the model. New sources of data can also be explored. For example, Stack- over/f_low (http://stackover/f_low.com/) and GitHub (https://github. com/) are good candidates because they provide acces to thei r data- bases of questions and projects, respectively. Github is al so inter- esting because each project has followers, which are users t hat de- clare the intent to be noti/f_ied about changes in a project (wat ch feature) and follow users. This forms a graph of connected us ers and projects, on which models of diﬀusion can also be built an d formally analyzed, allowing researchers to understand, fo r exam- ple, the path of the diﬀusion or which users are the hubs in the network. Information systems may be implemented, providing real', 'formally analyzed, allowing researchers to understand, fo r exam- ple, the path of the diﬀusion or which users are the hubs in the network. Information systems may be implemented, providing real time analysis of the data, similar to what is found on GitHut ( http:// githut.info/), producing the same analysis shown on this pa per, but in an automated way, enabling simple and convenient deci sion making regarding the adoption and development of programmi ng languages. 7 ACKNOWLEDGMENTS This work is partially supported by INES, grants CNPq/46561 4/2014- 0, FACEPE/APQ/0388-1.03/14. Sergio Soares is partially su pported by CNPq, grant 304499/2016-1. Emanoel Barreiros and Fausto Loren- zato are partially supported by University of Pernambuco. REFERENCES [1] Ritu Agarwal and Jayesh Prasad. 2000. A /f_ield study of the a doption of software process innovations by information systems professionals . Engineering Manage- ment, IEEE Transactions on 47, 3 (2000), 295–308. [2] Maria Teresa Baldassarre, Danilo Caivano, and Giuseppe Visaggio. 2013. Em- pirical studies for innovation dissemination: ten years of experience. In Pro- ceedings of the 17th International Conference on Evaluatio n and Assessment in Software Engineering - EASE ’13 . ACM Press, New York, New York, USA, 144. https://doi.org/10.1145/2460999.2461020 [3] FM Bass. 1969. A New Product Growth Model for Consumer Dur ables. Manage- ment Sciences. Institute for Operations Research and the Management Scien ces. Evanston, XV (5) (1969). [4] Fred Brauer, Pauline van den Driessche, and Jianhong Wu. 2008. Mathematical Epidemiology (/f_irst ed.). Springer. [5] Fred D Davis, Richard P Bagozzi, and Paul R Warshaw. 1989. User acceptance of computer technology: a comparison of two theoretical mod els. Management science 35, 8 (1989), 982–1003. [6] Philipp Diebold, Antonio Vetro, and Daniel Mendez Ferna ndez. 2015. An Ex- ploratory Study on Technology Transfer in Software Enginee ring. In Empiri- cal Software Engineering and Measurement (ESEM), 2015 ACM/ IEEE International Symposium on . IEEE, 1–10. [7] Klaus Dietz and JAP Heesterbeek. 2002. Daniel Bernoulli epidemiological model revisited. Mathematical biosciences 180, 1 (2002), 1–21. [8] Paul A Geroski. 2000. Models of technology diﬀusion. Research policy 29, 4 (2000), 603–625. [9] Malcolm Gladwell. 2002. The Tipping Point: How Little Things Can Make a Big Diﬀerence (/f_irst ed.). Back Bay Books. [10] Tony Gorschek, Per Garre, Stig Larsson, and Claes Wohli n. 2006. A Model for Technology Transfer in Practice. IEEE Softw. 23, 6 (Nov. 2006), 88–95. https://doi.org/10.1109/MS.2006.147 259', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil E. Barreiros et al. (a) C (b) C++ (c) Java (d) PHP Figure 2: Fit plots. [11] Bill C Hardgrave and Richard A Johnson. 2003. Toward an i nformation sys- tems development acceptance model: the case of object-orie nted systems devel- opment. Engineering Management, IEEE Transactions on 50, 3 (2003), 322–336. [12] Herbert W Hethcote. 2008. The Mathematics of Infectiou s Diseases. Society for Industrial and Applied Mathematics 42, 4 (2008), 599–653. [13] Ying-Hen Hsieh. 2008. Richards model: a simple procedu re for real-time predic- tion of outbreak severity. Modeling and Dynamics of Infectious Diseases. Volume 11, APRIL 2009 (2008). https://doi.org/10.1142/97898142 61265 [14] Andreas Jedlitschka, Marcus Ciolkowski, Christian De nger, Bernd Freimut, and Andreas Schlichting. 2007. Relevant Information Sources f or Successful Tech- nology Transfer: A Survey Using Inspections as an Example. I n ESEM ’07: Pro- ceedings of the First International Symposium on Empirical Software Engineering and Measurement . IEEE Computer Society, Washington, DC, USA, 31–40. [15] Siim Karus and Harald Gall. 2011. A study of language usa ge evolution in open source software. In Proceedings of the 8th Working Conference on Mining Software Repositories. ACM, 13–22. [16] William O Kermack and Anderson G McKendrick. 1927. A con tribution to the mathematical theory of epidemics. In Proceedings of the Royal Society of London A: mathematical, physical and engineering sciences , Vol. 115. The Royal Society, 700–721. [17] Nigel Meade and Towhidul Islam. 2006. Modelling and for ecasting the diﬀusion of innovation - A 25 year review. International Journal of Forecasting 22 (2006), 519–545. https://doi.org/10.1016/j.ijforecast.2006.0 1.005 [18] Leo A Meyerovich and Ariel Rabkin. 2012. How not to surve y developers and repositories: experiences analyzing language adoptio n. In Proceedings of the ACM 4th annual workshop on Evaluation and usability of progr amming languages and tools . ACM, 7–16. [19] Leo A. Meyerovich and Ariel S. Rabkin. 2012. Socio-PLT: Principles for Programming Language Adoption. In Proceedings of the ACM Interna- tional Symposium on New Ideas, New Paradigms, and Re/f_lections on Pro- gramming and Software (Onward! ’12) . ACM, New York, NY, USA, 39–54. https://doi.org/10.1145/2384592.2384597 [20] Leo A. Meyerovich and Ariel S. Rabkin. 2013. Empirical A nalysis of Programming Language Adoption. In Proceedings of the 2013 ACM SIG- PLAN International Conference on Object Oriented Programmin g Systems Lan- guages &#38; Applications (OOPSLA ’13) . ACM, New York, NY, USA, 1–18. https://doi.org/10.1145/2509136.2509515 [21] Renana Peres, Eitan Muller, and Vijay Mahajan. 2010. In novation diﬀusion and new product growth models: A critical review and research di rections. Interna- tional Journal of Research in Marketing 27, 2 (2010), 91–106. [22] Shari L. P/f_leeger. 1999. Understanding and improving te chnology transfer in software engineering. Journal of Systems and Software 47, 2–3 (1999), 111–124. [23] Samuel T. Redwine, Jr. and William E. Riddle. 1985. Soft ware technology mat- uration. In ICSE ’85: Proceedings of the 8th international conference on Software engineering. IEEE Computer Society Press, Los Alamitos, CA, USA, 189–20 0. [24] FJ Richards. 1959. A /f_lexible growth function for empiri cal use. Journal of exper- imental Botany 10, 2 (1959), 290–301. [25] Cynthia K Riemenschneider, Bill C Hardgrave, and Fred D Davis. 2002. Explain- ing software developer acceptance of methodologies: a comp arison of /f_ive the- oretical models. Software Engineering, IEEE Transactions on 28, 12 (2002), 1135– 1145. [26] Everett M. Rogers. 2003. Diﬀusion of Innovations (/f_ifth ed.). Free Press. [27] GJ Szekely. 2000. Technical Report 03-05: E-statistics: energy of statistic al samples . Technical Report. [28] Viswanath Venkatesh, Michael G Morris, Gordon B Davis, and Fred D Davis.', '[27] GJ Szekely. 2000. Technical Report 03-05: E-statistics: energy of statistic al samples . Technical Report. [28] Viswanath Venkatesh, Michael G Morris, Gordon B Davis, and Fred D Davis. 2003. User acceptance of information technology: Toward a u ni/f_ied view.MIS quarterly (2003), 425–478. [29] H Hsieh Ying-Hen, N Fisman David, and Wu Jianhong. 2010. On epi- demic modeling in real time: An application to the 2009 Novel A (H1N1) in/f_luenza outbreak in Canada. BMC Research Notes 3, 1 (2010), 283. https://doi.org/10.1186/1756-0500-3-283 [30] Marvin V. Zelkowitz, Dolores R. Wallace, and David W. Bi nkley. 1998. Culture Con/f_licts in Software Engineering Technology Transfer. In NASA Goddard Soft- ware Engineering Workshop . 260']","[""PROGRAMMING LANGUAGE ADOPTION AS AN EPIDEMIOLOGICAL PHENOMENON This  briefin reports scieitfc evideice oi the  epidemic  iature  of  pronrammiin lainuane  adoptoi  by  developers  of  opei source softaree FINDINGS \uf0b7 By using concepts of epidemiology,  powered by mathematcal models of  epidemics, we are able to understand,  describe and forecast the adopton of  programming languages in SE. \uf0b7 The Richards functon was used to model  the phenomenon: I (t )= K ❑ tm=ti+ ln a r K : total case number of the infection r : per capita growthof the infecte population a : exponent of deviation the standard logistic curve ti :time of the curve ' sinflection point \uf0b7 Fit parameters found: Language K r tm a C (100%) 38244 0.017 -748.71 7.56 × 10-7 C (80%) 29085 0.022 -592.45 8.96 × 10-7 C++ (100%) 78057 0.0133 -905.46 1.33 × 10−6 C++ (80%) 61705 0.015 -816.75 1.21 × 10−6 Java (100%) 86338 0.02 32.14 0.225 Java (80%) 179248 0.011 -1129.89 1.43 × 10−6 PHP (100%) 63607 0.015 -157.17 0.017 PHP (80%) 89569 0.013 -934.20 1.25 × 10−6 \uf0b7 Fit was considered satsfactory according  to the goodness of ft analysis: Language R2 Székely p-value C (100%) 0.9988 1297.63 0.987 C (80%) 0.9980 3967.76 0.706 C++ (100%) 0.9989 1433.59 0.996 C++ (80%) 0.9977 2194.39 0.961 Java (100%) 0.9988 1793.37 0.993 Java (80%) 0.9979 2040.49 0.987 PHP (100%) 0.9990 942.95 0.996 PHP (80%) 0.9981 1286.83 0.994 \uf0b7 Fit graph for Java: \uf0b7 Fit graph for PHP: \uf0b7 Fit graph for C: \uf0b7 Fit graph for C++: \uf0b7 Even after using only 80% of the dataset, the resultng fied  model is stll able to describe  100% of the data. \uf0b7 It is possible, from the current  fndings, to predict satsfactorily 25% of the size of the used  dataset into the future. \uf0b7 Who is this briefin  or? Practtoiers interested in industry  trends/standards related to the adopton  of programming languages. Researchers  interested in the dynamics of adopton of  programming languages. Studeits seeking  for suggestons of which programming  language to study. Where the fidiins come  rom? All fndings of this briefng were extracted  from Sourceforge data, made available by  Meyerovich et al. (Empirical Analysis of  Programming Language Adopton,  OOPSLA’13)   What is iicluded ii this briefin? Data from the adopton of C, C++, Java and  PHP from 2000 to 2009. For additoial ii ormatoi about ESEG: hips://sites.google.com/site/eseportal ORIGINAL RESEARCH REFERENCE Emanoel Barreiros et al. Programming Language Adopton aa an  pidemiological Phenomenon. Proceedings of the 31st Brazilian Symposium on Software  Engineering (SBES’17), pages 255-260, ISBN: 978-1-4503-5326-7, 2017. hips://doi.org/10.1145/3131151.3131188""]","**Title: Understanding Programming Language Adoption Through an Epidemic Lens**

**Introduction:**
This Evidence Briefing summarizes findings from a study that proposes a novel perspective on the adoption of programming languages by treating it as an epidemiological phenomenon. The goal is to provide insights into how programming languages spread within developer communities, which can inform better strategies for technology adoption in software engineering.

**Core Findings:**
1. **Epidemiological Model of Adoption:** The study presents a paradigm shift by modeling the adoption of programming languages similarly to the spread of infectious diseases. This approach allows for a better understanding of how languages gain popularity within communities, where developers act as ""susceptible"" individuals and programming languages as ""infectious agents.""

2. **Mathematical Modeling:** By utilizing biological growth models, specifically the Richards growth function, the researchers demonstrated that the adoption patterns of programming languages can be mathematically described. This model captures the initial slow uptake followed by rapid growth and eventual saturation, mirroring the dynamics of disease spread.

3. **Forecasting Capabilities:** The research indicates that it is possible to forecast the adoption of programming languages based on historical data. The models developed can predict future adoption trends, providing valuable insights for decision-makers in software development.

4. **Empirical Evidence:** The study analyzed a dataset from Sourceforge, which included metadata from over 213,000 projects. The findings suggest that the dynamics of language adoption are consistent with the proposed epidemiological models, enabling real-time analysis and forecasting.

5. **Social Dynamics:** The research highlights the importance of social interactions and word-of-mouth communication in the adoption process. Just as diseases spread through personal contact, programming languages gain traction through recommendations and shared experiences among developers.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, technology decision-makers, and researchers interested in understanding the dynamics of programming language adoption and improving technology transfer strategies.

**Where the findings come from?**
The findings are derived from the study ""Programming Language Adoption as an Epidemiological Phenomenon"" conducted by Emanoel Barreiros et al., presented at the SBES 2017 conference. The research utilized a comprehensive dataset from Sourceforge to analyze the adoption trends of programming languages.

**What is included in this briefing?**
This briefing includes an overview of the study's core findings, the proposed epidemiological model for programming language adoption, and the implications for technology transfer in software engineering.

**What is NOT included in this briefing?**
This briefing does not include detailed statistical analyses or raw data metrics from the original study. It focuses on summarizing key insights and practical implications for practitioners.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research:**
Emanoel Barreiros (emanoel.barreiros@upe.br)  
Jones Albuquerque (jones.albuquerque@pq.cnpq.br)  
João F. L. de Oliveira (fausto.lorenzato@upe.br)  
Helaine Lins (hsl@cin.ufpe.br)  
Sergio Soares (scbs@cin.ufpe.br)  

**ORIGINAL RESEARCH REFERENCE:**
Emanoel Barreiros, Jones Albuquerque, João F. L. de Oliveira, Helaine Lins, Sergio Soares. ""Programming Language Adoption as an Epidemiological Phenomenon."" In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017. https://doi.org/10.1145/3131151.3131188"
"['Reuse of model-based tests in mobile apps Guilherme de Cleva Farto TOTVS Agroindustria Assis, SP, Brazil guilherme.farto@gmail.com Andre Takeshi Endo Universidade Tecnologica Federal do Parana (UTFPR) Cornelio Procopio, PR, Brazil andreendo@utfpr.edu.br ABSTRACT Mobile apps have been introduced in our lives and as a spe- ciﬁc class of software, developers and testers have to deal with new challenges. For instance, testing all conﬁgurations and characteristics of apps might be an expensive activity. It would be desirable to maximize the return of investment for systematic and automated test cases. This paper pro- poses an approach for mobile apps in which test models are reused to (i) reduce the eﬀort for concretization and (ii) test other characteristics of apps, such as device-speciﬁc events, unpredictable users’ interaction, telephony events for GSM/SMS, and sensors and hardware events. We assume that a model-based testing approach was applied to GUI test- ing of Android apps. Speciﬁcally, test models are designed using Event Sequence Graphs (ESGs) and concrete adapters developed using Java and automated testing frameworks. We evaluated the approach and its supporting tool with three Android apps in an industrial setting. CCS CONCEPTS •Software and its engineering→Software testing and debugging; KEYWORDS Model-Based Testing, Mobile Testing, Mobile Applications, Automated Tests, Event Sequence Graph, Android, Reuse ACM Reference format: Guilherme de Cleva Farto and Andre Takeshi Endo. 2017. Reuse of model-based tests in mobile apps. InProceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017,10 pages. DOI: 10.1145/3131151.3131160 1 INTRODUCTION Currently, the popularity of mobile devices, such as smart- phones, tablets, and e-readers has grown exponentially. The Android [4] platform had an increase of 127% in 2012, with approximately 121 million units sold [ 17]. Thus, Android is leader of the current mobile environments with 62% ahead of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131160 Apple iOS and Microsoft Windows Phone. Such platforms foster the rise of a new breed of software, so-called mobile applications (mobile apps for short). Due to the rapid growth in the number and diversity of mobile device users, the study of new testing approaches is essential to reduce the occurrence of faults and thereby ensure better quality of mobile apps. According to Muc- cini et al. [ 26], the context of mobility has characteristics that directly impact the testing activity, such as connectivity, limited resources, autonomy, user interface, context aware- ness, adaptation, new operating systems (OSs), diversity of settings, and touch screen. A special attention should be given in the mobile testing process in order to improve the de- sign and generation of test cases, as well as evaluate methods and tools available for veriﬁcation and validation. It must always be considered points of attention such as cost, fault detection eﬀectiveness, and ability to automate test cases. One of the techniques that can be applied to ensure soft- ware quality is Model-Based Testing (MBT). According to Utting and Legeard [33], MBT supports the automatic gener- ation of test cases through a model designed to describe the expected events and behavior of software under test (SUT).', 'Utting and Legeard [33], MBT supports the automatic gener- ation of test cases through a model designed to describe the expected events and behavior of software under test (SUT). MBT is an approach that has several beneﬁts reported in the literature, such as automated test case generation, fault detection eﬀectiveness, and reduction in time and cost for testing [9, 18, 33]. Previous research has explored speciﬁc characteristics of mobile apps and the application of MBT [ 1, 2, 14, 19, 23, 30, 35], but there is a lack of papers to support an MBT approach focused on mobile apps. Testers could beneﬁt from a formal and systematic MBT approach, while speciﬁc characteristics would be considered and veriﬁed in SUT. This paper introduces a testing approach for mobile apps in which test models are reused to reduce the eﬀort on concretiza- tion and verify other characteristics that are often ignored in conventional and manual testing, such as device-speciﬁc events, unpredictable users’ interaction, telephony events for GSM/SMS, and sensors and hardware events. Speciﬁcally, the approach was instantiated in the context of the Android platform, using Event Sequence Graphs (ESGs) [ 7, 8, 36] to describe the expected behavior of mobile apps. We developed a tool named MBTS4MA to support the conduction of an experimental evaluation with three real-world Android apps and six professionals from a multinational IT company. This paper is organized as follows: Section 2 presents the background on MBT and ESGs along with an example. Sec- tion 3 brings the approach proposed to reuse test models in the context of mobile apps. Section 4 describes the sup- porting tool. Section 5 presents the experimental evaluation. 184', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Guilherme de Cleva Farto and Andre Takeshi Endo Section 6 discusses the related work. Finally, Section 7 makes the concluding remarks and sketches future work. 2 BACKGROUND The software testing area has a rich and varied set of tech- niques that can be applied to reveal the presence of faults [ 6, 27, 29]. Approaches, strategies, and mechanisms have been increasingly proposed for test case generation using a behav- ioral or structural model, also called test model, of the SUT. This approach is known as Model-Based Testing (MBT) [ 28]. MBT can be divided into four main steps: (i) modeling: the tester uses her understanding of the SUT to create a test model; (ii) test generation: the test model and the test selec- tion criterion are submitted as input to a supporting tool that generates a set of abstract test cases; (iii) concretization: this step transforms the abstract test cases into ones executable in the SUT. This paper assumes that this step is performed by implementing adapters, so abstract test cases are interpreted and executed in the SUT by using them [ 31]; and (iv) test execution: this step is the application of concrete test cases in the SUT. Following the execution, results are analyzed and corrective actions taken. This section presents an example of using MBT and ESG in the context of mobile apps. The RouteTracker app, extracted from Deitel et al. [11], enables users to track their movements by collecting latitude and longitude coordinates. The GPS data is stored and then displayed as a line on a Google Map. The user can enable or disable the tracking feature by pressing a button. Figure 1 shows some screenshots of RouteTracker. Figure 1: Screenshots of the RouteTracker app [11]. In this paper, ESGs model the expected behavior of the SUT. We adopted the ESG technique because of its easiness to represent interactions between events and the simplicity to understand requirements and features of the SUT [ 8]. An ESG is a directed graph used to model interactions between the software events and consists of nodes that represent events while edges (event pairs) are valid sequences of these events [8, 36]. Figure 2 shows an ESG for the RouteTracker app; the model is a partial test-focused representation of the SUT. Elements “[” and “]” of the graph represent the start and the end of event sequences, respectively. Such elements add two restrictions to a valid model: (i) each node must be reachable, through a path, from “[”, and (ii) “]” must be reachable from all nodes. Figure 2: An ESG model for the RouteTracker app. From an ESG, test cases can be generated as Complete Event Sequences (CESs). A CES is a linear sequence, starting in [ and ending in ], generated as a path in the ESG model to help the developers and testers to obtain the ﬂow of events expected in the SUT [ 22]. So, CESs are test cases (or test sequences) generated from the model. From the ESG in Figure 2, a CES is listed as follows: [, Display tracking screen, Press start tracking, Listen to GPS, Read updated location, Update screen/map, Listen to GPS, Press stop tracking, Show distance and avg speed, Press OK, Display tracking screen, Press back, ] To concretize this CES, the tester can adopt a testing framework that simulates events and human interactions in Android apps. Each event in the ESG can be mapped to a method that implements the interaction to be performed on an Android device. In this work, the Robotium frame- work [32] was adopted to support the concretization and execution of test cases. Although the example and the literature [ 14, 19] provide evidence that MBT is feasible in mobile apps, three goals need to be handled for a cost-eﬀective testing: First, the testers need to have tools so that valid test models can be designed. For instance, ESGs require the deﬁnition of special nodes (“[” and “]”), concise and well-formed labels, and', 'testers need to have tools so that valid test models can be designed. For instance, ESGs require the deﬁnition of special nodes (“[” and “]”), concise and well-formed labels, and reachable events. Second, the concretization step demands time and skill concerning the SUT, as well as the test platform. 185', 'Reuse of model-based tests in mobile apps SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Strategies to automate and/or reduce the concretization eﬀort are needed for MBT of mobile apps. Third, mobile computing introduces particularities to the software testing activity [ 16, 20, 26, 34]. As the test model is a formal representation of SUT events, speciﬁc characteristics of the mobile apps could be inserted into it to support testing from other points of view. Hence, a test model would be used to verify not only the functionalities mapped by the tester, but also to test the app in diﬀerent scenarios. 3 APPROACH TO REUSE MODEL-BASED TESTS This section introduces the approach proposed to reuse test models for mobile apps. The approach is based on an MBT process with ESG models representing the features of a mobile app under test. Speciﬁcally, the models are focused on system testing, mainly user’s and GUI’s events (as illustrated in Section 2). The approach aims to reuse test models so that (i) the eﬀort for concretization could be reduced, and (ii) speciﬁc characteristics of mobile apps would be tested. To do so, we adopt the concept of stereotype, a marking associated with metadata that can be assigned to elements of the test model. The stereotype contains pieces of information that can be used in diﬀerent steps of the MBT process, namely, test generation and concretization. Stereotypes can be previously deﬁned by researchers and practitioners, or even be extracted on-the-ﬂy by a supporting tool that analyzes the mobile app project. Therefore, the tester can specify personalized metadata in the test model while modeling the SUT. From a technical perspective, each stereotype is related to a given snippet, a fragment of source code developed with resources provided by the supporting tool’s API and/or test platform. We divided the approach into two strategies: Method Template and Edge Template, described in the following sections. 3.1 Method Template The Method Template strategy focuses on associating stereo- types to events (nodes in the ESG). This strategy contains a set of stereotypes related to GUI events, such as press a physical/virtual button, select a menu item, and ﬁll an edit text. Figure 3 illustrates how an ESG model looks like when the Method Template strategy is applied, taking into account a snippet of the model shown in Figure 2. Grey nodes represent events associated with a stereotype, so-called stereotyped events. For events Press Menu and Press Back, stereotypes related to Press Physical/Virtual Button are applied. For events Select Map and Select Satellite, stereotype Press Menu Item was assigned. Snippets for stereotypes Press Back Button and Press Physical/Virtual Button using the Robotium framework are shown in Figure 4. As the test model is supposed to be manip- ulated by a tool, the stereotyped events can be used to reduce the eﬀort of the concretization step. To do this, the tool can Figure 3: Example of the Method Template strategy. generate test code connecting the event to be executed with code snippets like the ones presented in Figure 4. The con- cretization eﬀort can be zero (as the snippet for Press Back Button) or reduced as the snippet for Press Physical/Virtual Button, in which some coding is required. In general, both cases are better than fully manual concretization. // Snippet f o r the ’ ’ p r e s s Back button ’ ’ s t e r e o t y p e s o l o . goBack ( ) ; // Snippet f o r the ’ ’ p r e s s p h y s i c a l or // v i r t u a l button ’ ’ s t e r e o t y p e s o l o . clickOnButton ( s o l o . g e t S t r i n g ({{ idbutton }} ) ) ; Figure 4: Code snippets for the Method Template stereotypes. 3.2 Edge Template The Edge Template strategy focuses on associating stereo- types to event pairs (edges in the ESG). This strategy con- tains a set of stereotypes related to speciﬁc events that can happen between two events (an edge) sequentially. We con-', 'types to event pairs (edges in the ESG). This strategy con- tains a set of stereotypes related to speciﬁc events that can happen between two events (an edge) sequentially. We con- sider characteristics that might inﬂuence the behavior of mobile apps. They also have related code snippets, making it possible to simulate1, e.g., a GSM call (receive-accept-cancel) or the unavailability of the GPS sensor. The stereotypes we proposed are divided into: Device-speciﬁc events: Mobile devices and the Android OS execute background processes that might result in native events. For instance, Android has a battery manager that checks when the level is too low. Other native action managed by the device is the loss of Wi-Fi connection followed by a 3G signal recover. Unpredictable users’ interaction: The interaction of a real user with a mobile app is carried out in a very diﬀerent way from what is usually veriﬁed during software testing. Examples of actions are (i) lock and unlock the device, (ii) go to Home Screen and return to the app, (iii) stay for a long period on the same screen without interaction, or even (iv) keep going backward and forward through screens. Telephony events for GSM/SMS: It is possible that tele- phony events such as calls and text messaging interfere the 1Such events can be simulated using existing tools like ADB. 186', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Guilherme de Cleva Farto and Andre Takeshi Endo operation of a mobile app. A mobile device is susceptible to features and events that can be prioritized by the OS, placing the SUT as secondary importance. Thus, the behavior of the app and its proper operation when this kind of event occurs needs to be veriﬁed. Sensors and hardware events: Mobile devices increasingly incorporate new features like sensors and hardware events. We can make use of GPS, accelerometer, gyroscope, tempera- ture sensor, barometer, and much more. The mobile app may evaluate or obtain data from these sensors if necessary or may try to use them when they are not available or enabled. Figure 5a illustrates two edges (bold gray lines) when the Edge Template strategy is applied, namely edges ⟨Display tracking screen, Press Back⟩ and ⟨Display tracking screen, Press start tracking⟩. Stereotypes Change Wi-Fi data and Receive call are applied to the two edges, so-called stereotyped edges. Figure 5b shows two grey nodes (namely Change WiFi data and Receive call) and four bold grey edges, representing the events generated by the strategy. (a) (b) Figure 5: Examples of the Edge Template strategy. Figure 6 shows how the original ESG model (in Figure 2) may look like after the application of the Method Template and Edge Template strategies by a tester. As previously illustrated, grey nodes represent the use of Method Template and grey nodes connected by bold grey edges represent the adoption of Edge Template. 3.3 Test Case Generation The test case generation from ESGs can be achieved by diﬀerent model coverage criteria. In this paper, we adopted a greedy algorithm based on an event tree [ 13] to derive a suite of CESs that covers, at least once, each edge in the test model (i.e., the all-edges criterion). As for Method Template, Figure 6: An example of the RouteTracker’s ESG with the two strategies applied. the set of CESs remains the same since the strategy does not introduce new nodes or edges in the model. Concerning Edge Template, a new test suite is produced in which CESs with stereotyped edges are replicated. In particular, the CESs are modiﬁed with speciﬁc events (e.g., Receive Call) and connecting edges in the points marked by the tester. 4 THE MBTS4MA TOOL We have implemented the proposed testing approach in a tool called MBTS4MA ( Model-Based Test Suite For Mobile Apps). MBTS4MA was developed using the Java technology, integrates with the Robotium platform, and aims to test native Android apps. The developed tool lets developers and testers (i) extract useful pieces of information from Android SUT artifacts (such as XML ﬁles for labels, layouts, and other conﬁgurations), (ii) model ESGs through an interactive GUI, (iii) inject events on the original ESG if necessary, (iv) derive CESs (test sequences), and (v) generate test templates and adapters for Android apps. The tool extends the JGraphX API 2 to provide a GUI for modeling; JGraphX contains several facilities like zoom in/out, auto-scroll, drag-and-drop, and so on. In particular, MBTS4MA supports the design of ESG models integrated with the mobile app data like labels, activity names, and general conﬁgurations. These metadata can be used as arguments to stereotypes of the Method Template and Edge Template strategies. Figure 7 illustrates the main GUI of MBTS4MA, showing an ESG for the RouteTracker app. MBTS4MA is organized in a component architecture that eases the inclusion of new stereotypes and code snippets. The tool implements a limited set of stereotypes; the full list can be found in a website [ 15]. New stereotypes and snippets can be included; they are going to be dynamically 2https://github.com/jgraph/jgraphx 187', 'Reuse of model-based tests in mobile apps SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figure 7: Screenshot of theMBTS4MA tool. identiﬁed and made available to the tester. Figure 8 shows how to assign the “Press Back” stereotype to an event and the look of a stereotyped event with Method Template (light grey node). Figure 9 shows how to assign the “Change Wi-Fi Data” stereotype to an edge and the look of a stereotyped edge with Edge Template (labeled grey line). Figure 8: Adding a stereotype of the Method Tem- plate strategy. The tool produces as output a set of Java classes. This set of classes contains all the needed components, including CESs, adapters, and auxiliary classes. However, the adapter class may require some coding to fully support the execution in the SUT. The classes can be imported into an Android app project and be executed in devices or emulators (so-called AVDs – Android Virtual Devices). The source code for the MBTS4MA tool, an example, and the list of stereotypes are available in a website [15]. Figure 9: Adding a stereotype of the Edge Template strategy. 5 EXPERIMENTAL EVALUATION We conducted an experimental study in an industrial setting to evaluate the proposed approach and its supporting tool MBTS4MA. This study aims to answer the following research questions: RQ1 How much eﬀort is spent for the modeling and con- cretization steps considering the Method Template and Edge Template strategies? RQ2 What are the fault detection capabilities of the pro- posed approach? To answer RQ1, we measure the time elapsed for each step in minutes and the number of lines of code (LoC). As for RQ2, we analyze the faults detected during the study. The evaluation was conducted in cooperation with a multi- national IT company leader in solutions for agribusiness, mainly sugarcane to produce sugar and ethanol, and crops (like coﬀee, soybean, corn, and cotton). The study involved six participants: three are developers with expertise in Java and Google Android technologies, and three are testers with business knowledge in the mobile apps selected. The com- pany provided access to its apps, source code, documentation and other artifacts. Moreover, we had technical support as well as a conﬁgured environment to carry out the study. Initially, we surveyed with the participants, aiming to proﬁle the knowledge and experience of the participants, and understand the test practices and process adopted by the company. Then, we had a one-hour training session to introduce the concepts of MBT, ESGs, and Robotium. The proposed approach and its supporting tool were introduced along with an example. After the training, we divided the six participants into three pairs, each one with a developer and a tester. Each pair selected a mobile app project based on its relevance, users’ importance and critical functionalities. Table 1 shows pieces of information about the three selected projects 3. The ﬁrst columns indicate speciﬁc features of each app. It also shows the number of LoC, number of activities (single screens in the Android terminology), and average cyclomatic complexity. MobApp1 aims to support phytosanitary inspection; an employee can visit the ﬁeld and evaluate the health of the 3For the sake of conﬁdentiality, we omit the actual name of the apps used. 188', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Guilherme de Cleva Farto and Andre Takeshi Endo Table 1: Details of the mobile apps. crop. MobApp2 is for transferring notes of agricultural ac- tivities and operations; it helps ﬁeld team leaders to record activities and operations performed. Finally, MobApp3 sup- ports conﬁguration and visualization of indicators like reports and dashboards; it enables the analysis of data that can be extracted from the company’s ERP. The test projects conducted were executed in three de- vices4: ( D1) AVD with Android 2.3.3/API 10, screen 3.2” (mdpi), 512 MB RAM, and memory card with 50 MB; ( D2) real device with Android 3.2/API 13, screen 7.0” (hdpi), 1 GB RAM, CPU @ 1.2GHz DualCore, and memory card with 2 GB; and ( D3) real device with Android 4.0.3/API 15, screen 10.1” (mdpi), 1 GB RAM, CPU @ 1GHz DualCore, and memory card with 2 GB. At the end, we surveyed the participants with respect to MBT, ESGs, and the proposed approach, as well as got feedback from the participants concerning the MBTS4MA tool. 5.1 Analysis of Results The three developers claimed experience on Java and Google Android, classifying their skill as high. On the other hand, the testers claimed low knowledge on such technologies. While the knowledge on software testing concepts was classiﬁed as medium for developers and high for testers, no one had previous experience with MBT. The testing process of the IT company is based on manual testing. Developers perform manual testing for functionalities they implement and ﬁx bugs. Testers use requirement speci- ﬁcations and experience to design and execute diﬀerent tests, which are documented to generate test evidence artifacts. However, as reported by the participants, these artifacts do not make the tests repeatable, trackable and scalable. More- over, information about the settings and mobile devices used is not documented as well. In summary, there is no formal and systematic process to test mobile apps. The participants considered the manual testing labor-intensive and ineﬃcient. This is particularly mentioned by the participants when new versions are released and the tests have to be reexecuted (i.e., regression testing). 4Conﬁgurations were based on the company clients’ devices. Test modeling and concretization. Each pair (devel- oper and tester) was asked to design an ESG model to test a functionality of the app selected. To do so, the partici- pants used the MBTS4MA tool and followed an MBT process. Initially, the proposed approach to reuse test models with Method Template and Edge Template was not taken into account. Table 2 shows pieces of data related to the test model (#events and #edges), time spent in the modeling and concretization steps, LoC of the test project, and number of veriﬁed activities. Table 2: Data about test modeling and concretiza- tion. After the ﬁrst iteration, the participants were asked to, besides using ESGs and MBT, employ the approach along with the Method Template strategy. Thus, the test models in the ﬁrst iteration were evolved (reused). Table 3 shows the updated data for this iteration with Method Template. Columns “Modelling”, “Concretization” and “LoC” present the values using the proposed approach with the Method Template strategy; between parentheses, it compares with the ﬁrst iteration summarized in Table 2. Column “stereo- typed nodes” shows how many events were associated with stereotypes. Notice that the modeling time was 52 min, 65 min, and 50 min for MobApp1, MobApp2, and MobApp3, re- spectively. Using the ﬁrst iteration as reference (Table 2), the increase of modeling time varies from 10 to 15 min (around 25-30%). Table 3: Data about the Method Template strategy. On the other hand, the approach with Method Template had a positive impact on concretization. The concretization step had a reduction varying from 18 to 24 min (around 38-48%). A reduction on the LoC is also observed (from 13', 'had a positive impact on concretization. The concretization step had a reduction varying from 18 to 24 min (around 38-48%). A reduction on the LoC is also observed (from 13 to 30 LoC); this results from code snippets inserted by the Method Template strategy. 189', 'Reuse of model-based tests in mobile apps SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil After the second iteration, the participants were asked to, besides using ESGs and MBT, employ the approach along with the Edge Template strategy. Thus, the test models in the second iteration were evolved (reused). Table 4 shows the updated data for this iteration with Edge Template. In this case, Columns “nodes”, “stereotyped nodes”, and “edges” also have values between parentheses as they compare with the second iteration summarized in Table 3. Notice that there are more model elements (events and edges) and the increase of modeling varies from 6 to 10 min in comparison with the second iteration (12-19%). Table 4: Data about the Edge Template strategy. As expected, the LoC in the test projects increased with the addition of code snippets for the Edge Template strat- egy (from 24 to 42 LoC). However, such LoC are generated automatically by the MBTS4MA tool. Thus, the concretization time had similar results (varying from -2 to +3 min). Test execution and faults detected . The tests for MobApp1 and MobApp3 were run in the AVD (D1) and real devices (D2 and D3). For MobApp2, only the real de- vices (D2 and D3) were used since this project requires a Bluetooth connection. The test execution was conducted us- ing the test projects generated in two of the three iterations: (i) based on the adoption of method template, and (ii) based on the adoption of both method template and edge template. Table 5 shows pieces of information about the test exe- cution; for each app, we summarize the execution time and number of faults for Stages 1 and 2, as well as the comparison between execution time in both stages. Using Method Tem- plate, the execution time took from 16min20s to 18min43s (around 52min for the three apps). Moreover, two faults were detected: one in MobApp2 and one in MobApp3. Us- ing Method Template and Edge Template, the execution time took from 21min09s to 25min33s (around 68min for the three apps). Moreover, three faults were detected 5: two in MobApp1 and one in MobApp2. The Edge Template strategy increased the execution time, varying from 22.2% to 36.5%; for the three apps, the increase was of 15min38s (29.9%). As for execution time, the generated test scripts are based on Robotium. As this tool interacts with the mobile app’s 5The two previously detected faults were not taken into account. Table 5: Data about test execution and faults found. GUI simulating the user’s actions, the test execution is auto- mated, yet slow. Concerning the faults, the two faults detected using the approach with Method template are described as follows: • Fault#1 – Physical back button enabled in MobApp2: In this SUT, the physical back button was replaced by a virtual “cancel” button and the physical but- ton was disabled. The participants veriﬁed that the physical back button was enabled in a speciﬁc activ- ity and, as a result, the form remained ﬁlled after ﬁnishing and coming back to the same screen. • Fault#2 – Incorrect/incomplete data form in MobApp3: The form was not ﬁlled with all the re- quired data and an uncaught Java NullPointerEx- ception was launched. The three faults detected using the approach with Edge Template are described as follows: • Fault#3 – Incoming call when displaying oﬄine map Activity in MobApp1: The SUT adopts an oﬄine map’s API that crashes when a call is received, ac- cepted, and ﬁnished in front of the oﬄine map Ac- tivity. • Fault#4 – GPS coordinates are sent with reduced precision (few decimal points) in MobApp1: An utility class named TestLocationProvider, we im- plemented in MBTS4MA, sends coordinates with six- decimal point precision. The SUT throws an In- dexOutOfBoundsException fault since it expects a latitude and longitude data with 10-decimal points. • Fault#5 –Sending data via Bluetooth when Bluetooth hardware is not available in MobApp2: After es- tablishing a Bluetooth connection, the hardware is', '• Fault#5 –Sending data via Bluetooth when Bluetooth hardware is not available in MobApp2: After es- tablishing a Bluetooth connection, the hardware is turned oﬀ. Then, an uncaught NullPointerException is thrown because InputStream and OutputStream objects assume null values. Overall, the participants found the approach and the MBTS4MA tool easy to use. As they had initially no knowledge in MBT, this feedback is promising. 190', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Guilherme de Cleva Farto and Andre Takeshi Endo 5.2 Discussion of Results Based on the results of the experimental study presented, we revisit the research questions and provide some discussion as follows. RQ1 How much eﬀort is spent for the modeling and con- cretization steps considering the Method Template and Edge Template strategies? The answer herein provided is based on the results of the experimental study (in particular, Tables 2, 3, and 4). Con- cerning modeling, we noticed an increase on the eﬀort when the Method Template strategy was employed. In comparison with MBT (ﬁrst iteration), the increase of modeling time varied from 25% to 30%. The eﬀort also increased with the Edge Template strategy, though inferior to Method Template. The increase of modeling time varied from 12% to 19% when using Edge Template. As for concretization, the Method Template strategy helped to reduce the eﬀort. The results showed a reduction varying from 38% to 48% in the concretization step, in comparison with MBT (ﬁrst iteration). The concretization eﬀort with the Edge Template strategy remained almost constant. This behavior was expected once the new events and edges added to the model already have concrete code associated. Thus, we could say that the Edge Template strategy helps to test other characteristics of mobile apps, while the cost of manual concretization is close to null. By testing speciﬁc characteris- tics of mobile apps, we are covering cases beyond the goals initially deﬁned for the test model. Thus, we can increase the return on the eﬀort spent to design the model. Notice that there is a tradeoﬀ between the manual eﬀort on modeling and concretization steps. In the proposed approach, the direction chosen was to increase the eﬀort on modeling and reducing the eﬀort on concretization. Concretization is mainly a conﬁguration and coding task, which requires from testers knowledge on programming and very speciﬁc testing frameworks. Such frameworks also vary between development platforms and OSs. Therefore, it would be helpful to eliminate or reduce the eﬀort on it. Observe that the sum of modeling and concretization times in Table 2 (ﬁrst iteration) and Table 4 (third iteration) have similar values. We believe that the impact on the concretization step was lower since the pairs had a developer. As concretization involves coding scripts and dealing with speciﬁc frameworks (like Robotium), the increase on a more general modeling eﬀort is justiﬁed. Besides, the proposed approach and tool were able to test diﬀerent characteristics of mobile apps, increasing the return on investment over the eﬀort spent. RQ2 What are the fault detection capabilities of the pro- posed approach? In total, ﬁve faults were found in the SUTs (Table 5). It is important to emphasize that the mobile apps’ versions were already in a production environment. Therefore, the apps have been previously tested by developers and quality assurance teams, as well as validated by their end users. Therefore, such apps had fewer faults to be detected. Yet, the fact that faults were identiﬁed with the proposed approach and the testing tool provides some evidence of fault detection capabilities. Faults #1 and #2 were essentially revealed by employing an MBT process. Faults #3, #4 and #5 are associated with the Edge Template strategy and were revealed by test- ing speciﬁc scenarios like incoming calls, sensor data (GPS coordinates), and loss of Bluetooth connection. Our experimental study has also experienced some of the mobile app challenges reported in the literature. However, no speciﬁc fault was found. First, we dealt with multiple conﬁgurations: three devices with diﬀerent hardware settings (CPU, RAM, screen sizes, and so on). The participants did not detect any failure concerning this issue; the faults detected showed up in all devices. The simulation of device-', '(CPU, RAM, screen sizes, and so on). The participants did not detect any failure concerning this issue; the faults detected showed up in all devices. The simulation of device- speciﬁc events was also ineﬀective; the following were tested: changes in battery status, loss of connection and change of networks. The tests also simulated unpredictable user’s interaction. However, no fault was found with this scenario. While such cases are intuitively worth testing, the maturity of the selected SUTs might be a factor preventing the detection of more faults. 5.3 Limitations and Threats to Validity The current approach and its supporting tool have some limitations. The adoption of the approach with the Method and Edge Template strategies requires an extra eﬀort on the modeling step. As the test model grows in size, the eﬀort also increases. Mechanisms to reduce this eﬀort should be investigated. Other issue is that few app developers and testers employ models in practice. We tried to overcome such limitation by providing an open source prototype tool that supports a simple yet formal modeling technique, namely ESGs. In general, no resistance (on using models) from participants has been observed during the experimental study. The MBTS4MA tool could be extended to handle diﬀerent testing frameworks (e.g., Appium, UIAutomator, Espresso) and mobile apps developed with other technologies (e.g., iOS, hybrid apps). The adoption of templates would be particu- larly helpful in this task. Currently, the tool is not capable of applying two or more stereotypes in the same element of the model. We implemented a limited set of events for the Edge Template strategy. In future work, we plan to include other mobile events found in literature. We opted by an event- driven model, but diﬀerent types of models (like state-based) might be integrated in the approach and supporting tool. Moreover, diﬀerent model coverage criteria (like edge-pair and prime path coverage [3]) could be investigated. More experimental studies are needed to clarify the beneﬁts and drawbacks of the proposed approach. We conducted a study with just one company and its mobile apps. Thus, the results herein presented cannot be generalized. For example, we did not compare with a competing approach like manual testing. As testers and end users have previously tested the selected apps, the new faults found indicate promising results 191', 'Reuse of model-based tests in mobile apps SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil when comparing with manual testing. The participants had a good knowledge of mobile development and testing. The inclusion of participants with diversity of skills and levels of technology proﬁciency might produce diﬀerent results. The selected SUTs are mature and our results on fault detection were limited. Moreover, there is the threat that more faults were revealed because of more tests. Other scenario would be how the approach performs with evolving apps. Overall, we plan to carry out formal experiments involving comparison with other techniques, more participants, a large sample, and as a consequence, more data to perform statistical analysis. At the end of the experimental study, the researcher had an open session with the participants in order to collect sug- gestions. An idea was that the tool introduced automatically the stereotypes, when possible. Such activity could be ap- plied with Edge Template, though an exhaustive approach would increase the number of tests. Other suggestion was versioning the test models. As the mobile app is evolving constantly, testing and its related artifacts should also be under version control. 6 RELATED WORK The rise of technologies and platforms fosters new research topics in software testing. As a result, the testing of mobile apps has introduced challenges that must be overcome; such topic has been extensively investigated in the literature (see the systematic mapping of Zein et al. [37]). Testing speciﬁc characteristics. Pathak et al. [30] investi- gate software faults related to excessive energy consumption on Android mobile devices. The authors provide an auto- matic solution to detect these problems by using a data ﬂow analysis algorithm. Yang et al. [ 35] propose a testing technique to identify and quantify faults related to excessive waiting time for certain events in Android apps. Their ap- proach relies on artiﬁcial insertion of delay instructions in problematic operations. Liu et al. [ 23] characterize a series of performance faults commonly identiﬁed in Android apps. The authors also present a static analysis tool to detect fault patterns. Adamsen et al. [ 1] propose and describe an ap- proach to test mobile apps by leveraging existing test suites to adverse conditions. A tool, so-called Thor, was imple- mented and can be used to inject events such as (i) changes in Activity state (Pause-Resume, Pause-Stop-Restart, and Pause-Stop-Destroy-Create) and (ii) manipulation of the audio manager. These papers have focused on speciﬁc char- acteristics of mobile apps. However, they did not explore how such kind of specialized tests can be applied in an MBT context. Model-based testing. Gudmundsson et al. [ 19] report the application of MBT to a mobile trivia Game named QuizUp. Their case study took three months and employed extended ﬁnite state machines to model the SUT and Appium to automate the test execution. State and transition coverage criteria have been adopted to support the test case generation. In a previous work [ 14], we evaluated how MBT can be used to support automated testing of mobile apps. The study involved 15 undergraduate students and developers, which were trained to apply MBT and ESGs to an address book app. These papers provide evidence that MBT is a promising approach to verify mobile apps. However, they did not adapt the approach and supporting tools for mobile apps, employing MBT as it is. Automated test generation. Automated test input gen- eration for Android apps has been particularly successful. Tools from industry (e.g., Monkey [ 5]) and from academy (e.g., Sapienz [25], Dynodroid [24], MobiGUITAR [2]) have shown eﬀectiveness to detect faults and increase code cover- age [10, 25]. These tools aim to explore automatically the GUI of mobile apps, using black, white, or grey-box analysis techniques. Such tools do not rely on the domain knowledge', 'age [10, 25]. These tools aim to explore automatically the GUI of mobile apps, using black, white, or grey-box analysis techniques. Such tools do not rely on the domain knowledge and tester’s expertise (embedded in a model as in MBT) and usually employ a null test oracle strategy (NOS). NOS is a weak oracle strategy and bases on abnormal termination (app crashes) and uncaught runtime exceptions. While cheap to implement, empirical evidence has shown that NOS might not be eﬀective to detect faults in some contexts [ 21]. We believe that automated test input generation and MBT are complementary; therefore, mobile app testers can beneﬁt from the best of both worlds. 7 CONCLUDING REMARKS This paper presented an approach to evaluate the use of Model-Based Testing (MBT) in the context of mobility. The main objective was to investigate the reuse of test models to verify other characteristics of mobile apps. We adopted the ESG technique to design the test model expressing the app’s features under test. The research was focused on apps developed in the Google Android platform and therefore test cases were implemented using the Robotium framework. The approach was evaluated in cooperation with a multina- tional IT company that provided access to its apps, technical support, and documentation. The results evince that our approach is able to uncover new faults by reusing designed models to test other characteristics of mobile apps. There was also signiﬁcant interest in the company to adopt the proposed approach as a mechanism for automated mobile testing. Suggestions of improvements have been identiﬁed to provide new features for the MBTS4MA tool. Our initiative helped to simplify and result in a partial generation of concrete test cases. Other studies could be performed to deepen the fault detection capabilities, as well as time and cost when compared with manual testing. Specif- ically, new experiments could be conducted to evaluate the number of detected faults (or mutants [ 12]), the time to de- sign and run tests, the eﬀort to learn a supporting tool (like Robotium), and so on. As future work, diﬀerent forms of model reuse could be investigated, e.g., by testing a diﬀerent app. We also plan to investigate how to integrate automated test input generation tools in an MBT process. 192', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Guilherme de Cleva Farto and Andre Takeshi Endo ACKNOWLEDGMENTS A.T. Endo is partially ﬁnancially supported by CNPq/Brazil (grant number 445958/2014-6). The authors are grateful to the anonymous reviewers for their useful comments and suggestions. REFERENCES [1] C. Q. Adamsen, G. Mezzetti, and A. Møller. 2015. Systematic Execution of Android Test Suites in Adverse Conditions. In Proc. 24th International Symposium on Software Testing and Analysis (ISSTA). [2] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, B. D. Ta, and A. M. Memon. 2015. MobiGUITAR: Automated Model-Based Testing of Mobile Apps. IEEE Software 32, 5 (Sept 2015), 53–59. DOI:https://doi.org/10.1109/MS.2014.55 [3] Paul Ammann and Jeﬀ Oﬀutt. 2008. Introduction to Software Testing (1 ed.). Cambridge University Press, New York, NY, USA. [4] Android.com. 2017. Android Developers. (2017). http://developer. android.com [5] Android.com. 2017. The Monkey UI android testing tool. (2017). http://developer.android.com/tools/help/monkey.html [6] Boris Beizer. 1990. Software Testing Techniques (2nd ed.). Van Nostrand Reinhold Co., New York, NY, USA. [7] F. Belli, M. Beyazit, and A. Memon. 2012. Testing is an Event- Centric Activity. In 2012 IEEE Sixth International Conference on Software Security and Reliability Companion . 198–206. DOI: https://doi.org/10.1109/SERE-C.2012.24 [8] F. Belli, C. J. Budnik, and L. White. 2006. Event-based modelling, analysis and testing of user interactions: approach and case study. Software Testing, Veriﬁcation and Reliability (STVR) 16, 1 (2006), 3–32. DOI:https://doi.org/10.1002/stvr.335 [9] M. Blackburn, R. Busser, and A. Nauman. 2004. Why model- based test automation is diﬀerent and what you should know to get started. In International Conference on Practical Software Quality and Testing, Software Productivity Consortium . 212– 232. [10] S. R. Choudhary, A. Gorla, and A. Orso. 2015. Automated Test Input Generation for Android: Are We There Yet? (E). In Pro- ceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE) . Washington, DC, USA, 429–440. DOI:https://doi.org/10.1109/ASE.2015.89 [11] P. J. Deitel, H. M. Deitel, A. Deitel, and M. Morgano. 2012. Android for Programmers: An App-Driven Approach (1st ed.). Prentice Hall Press, Upper Saddle River, NJ, USA. [12] L. Deng, J. Oﬀutt, P. Ammann, and N. Mirzaei. 2017. Mutation operators for testing Android apps. Information and Software Technology 81 (2017), 154 – 168. DOI:https://doi.org/10.1016/j. infsof.2016.04.012 [13] A. T. Endo and A. Simao. 2017. Event tree algorithms to generate test sequences for composite Web services. Software Testing, Veriﬁcation and Reliability (2017). DOI:https://doi.org/10.1002/ stvr.1637 [14] G. C. Farto and A. T. Endo. 2015. Evaluating the Model-Based Testing Approach in the Context of Mobile Applications. Elec- tronic Notes in Theoretical Computer Science 314, 0 (2015), 3 – 21. DOI:https://doi.org/10.1016/j.entcs.2015.05.002 the XL Latin American Conference in Informatic (CLEI 2014). [15] G. C. Farto and A. T. Endo. 2016. MBTS4MA – Model-Based Test Suite For Mobile Applications. (2016). https://github.com/ guilhermefarto/MBTS4MA [16] J. Gao, X. Bai, W. T. Tsai, and T. Uehara. 2014. Mobile Appli- cation Testing: A Tutorial. Computer 47, 2 (Feb 2014), 46–55. DOI:https://doi.org/10.1109/MC.2013.445 [17] Inc. Gartner. 2014. Gartner Says Worldwide Tablet Sales Grew 68 Percent in 2013, With Android Capturing 62 Percent of the Market. (2014). http://www.gartner.com/newsroom/id/2674215 [18] W. Grieskamp, N. Kicillof, K. Stobie, and V. Braberman. 2011. Model-based Quality Assurance of Protocol Documen- tation: Tools and Methodology. Software Testing, Veriﬁca- tion and Reliability (STVR) 21, 1 (Mar 2011), 55–71. DOI: https://doi.org/10.1002/stvr.427 [19] V. Gudmundsson, M. Lindvall, L. Aceto, J. Bergthorsson, and D. Ganesan. 2016. Model-based Testing of Mobile Systems –', 'https://doi.org/10.1002/stvr.427 [19] V. Gudmundsson, M. Lindvall, L. Aceto, J. Bergthorsson, and D. Ganesan. 2016. Model-based Testing of Mobile Systems – An Empirical Study on QuizUp Android App. In Proceedings First Workshop on Pre- and Post-Deployment Veriﬁcation Tech- niques (Electronic Proceedings in Theoretical Computer Sci- ence), L. Aceto, A. Francalanza, and A. Ingolfsdottir (Eds.), Vol. 208. 16–30. DOI:https://doi.org/10.4204/EPTCS.208.2 [20] M. E. Joorabchi, A. Mesbah, and P. Kruchten. 2013. Real Chal- lenges in Mobile App Development. In 2013 ACM / IEEE In- ternational Symposium on Empirical Software Engineering and Measurement. 15–24. DOI:https://doi.org/10.1109/ESEM.2013.9 [21] N. Li and J. Oﬀutt. 2017. Test Oracle Strategies for Model-Based Testing. IEEE Transactions on Software Engineering 43, 4 (2017), 372–395. DOI:https://doi.org/10.1109/TSE.2016.2597136 [22] M. Linschulte. 2013. On the Role of Test Sequence Length, Model Reﬁnement, and Test Coverage for Reliability . Ph.D. Dissertation. Universitat Paderborn, Germany. [23] Y. Liu, C. Xu, and S. C. Cheung. 2014. Characterizing and Detecting Performance Bugs for Smartphone Applications. In Proceedings of the 36th International Conference on Software Engineering (ICSE) . ACM, New York, NY, USA, 1013–1024. DOI:https://doi.org/10.1145/2568225.2568229 [24] A. Machiry, R. Tahiliani, and M. Naik. 2013. Dynodroid: An Input Generation System for Android Apps. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2013). ACM, New York, NY, USA, 224–234. DOI: https://doi.org/10.1145/2491411.2491450 [25] K. Mao, M. Harman, and Y. Jia. 2016. Sapienz: Multi-objective Automated Testing for Android Applications. In Proceedings of the 25th International Symposium on Software Testing and Analysis (ISSTA). ACM, New York, NY, USA, 94–105. DOI: https://doi.org/10.1145/2931037.2931054 [26] H. Muccini, A Di Francesco, and P. Esposito. 2012. Software testing of mobile applications: Challenges and future research directions. In Proceedings of the 7th International Workshop on Automation of Software Test (AST) . 29–35. DOI:https://doi. org/10.1109/IWAST.2012.6228987 [27] G. J. Myers, C. Sandler, and T. Badgett. 2011. The Art of Software Testing (3rd ed.). Wiley Publishing. [28] J. Oﬀutt and A. Abdurazik. 1999. Generating Tests from UML Speciﬁcations. In Proceedings of the 2Nd International Con- ference on The Uniﬁed Modeling Language: Beyond the Stan- dard (UML’99). Springer-Verlag, Berlin, Heidelberg, 416–429. http://dl.acm.org/citation.cfm?id=1767297.1767341 [29] A. Orso and G. Rothermel. 2014. Software Testing: A Research Travelogue (2000–2014). In Proceedings of the on Future of Software Engineering (FOSE). ACM, 117–132. [30] A. Pathak, A. Jindal, Y. C. Hu, and S. P. Midkiﬀ. 2012. What is Keeping My Phone Awake?: Characterizing and Detecting No-sleep Energy Bugs in Smartphone Apps. In Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services (MobiSys) (MobiSys ’12) . ACM, New York, NY, USA, 267–280. DOI:https://doi.org/10.1145/2307636.2307661 [31] A. Pretschner and J. Philipps. 2005. Methodological Issues in Model-Based Testing. In Model-Based Testing of Reac- tive Systems , M. Broy, B. Jonsson, J. P. Katoen, M. Leucker, and A. Pretschner (Eds.). Lecture Notes in Computer Sci- ence, Vol. 3472. Springer Berlin Heidelberg, 281–291. DOI: https://doi.org/10.1007/11498490 13 [32] Robotium. 2017. Robotium - The worlds leading Android test automation framework. (2017). https://code.google.com/p/ robotium [33] Mark Utting and Bruno Legeard. 2007. Practical Model-Based Testing: A Tools Approach. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. [34] A. I. Wasserman. 2010. Software Engineering Issues for Mobile Application Development. In Proceedings of the FSE/SDP Work- shop on Future of Software Engineering Research (FoSER ’10) . ACM, 397–400. DOI:https://doi.org/10.1145/1882362.1882443', 'Application Development. In Proceedings of the FSE/SDP Work- shop on Future of Software Engineering Research (FoSER ’10) . ACM, 397–400. DOI:https://doi.org/10.1145/1882362.1882443 [35] S. Yang, D. Yan, and A. Rountev. 2013. Testing for poor respon- siveness in android applications. In 1st International Workshop on the Engineering of Mobile-Enabled Systems (MOBS) . 1–6. DOI:https://doi.org/10.1109/MOBS.2013.6614215 [36] Xun Yuan, M.B. Cohen, and AM. Memon. 2011. GUI Interaction Testing: Incorporating Event Context. IEEE Transactions on Software Engineering 37, 4 (July 2011), 559–574. DOI:https: //doi.org/10.1109/TSE.2010.50 [37] S. Zein, N. Salleh, and J. Grundy. 2016. A systematic map- ping study of mobile application testing techniques. Jour- nal of Systems and Software 117 (2016), 334–356. DOI:https: //doi.org/10.1016/j.jss.2016.03.065 193']","['REUSE OF MODEL-BASED TESTS IN MOBILE APPS     This briefing reports scientific evidence on  effectiveness of an approach for mobile  apps in which test models are reused to (i)  reduce effort for concretization and (ii)  test other characteristics of apps, such as  device-specific events, unpredictable  users’ interaction, telephony events, and  sensors and hardware.      FINDINGS  •  The findings presented in this briefing consider  the analysis of the adoption of an proposed  approach to reuse test cases in the context of  mobile applications based on the use of Model- Based Test (MBT) and Event-Sequence Graph  (ESG);  •  The experimental evaluation of the proposed  approach was conducted collaboratively with a  multinational IT company leader and from the  use of three real mobile applications;  •  The proposed approach aims to reuse test  models so that (i) the effort of concretization  could be reduced and (ii) specific characteristics  of mobile apps would be tested;  •  The approach is divided in strategies:  - Method Template: focuses on associating  stereotypes (templates) to reduce the effort  for automation (test concretization);    - Edge Template: focuses on associating  stereotypes to event pairs to consider  characteristics that might influence the app  behavior;    •  A tool named MBTS4MA (Model-Based Test  Suite For Mobile Apps) was developed using the  Java technology, integrates to the Robotium  framework, and aims to test native Android  apps;    •  Main features of MBTS4MA tool are:  - Extract useful metainformation from  Android SUT artifacts (such as XML files for  labels, layouts, and other configurations);  - Model ESGs with an interactive GUI;  - Inject events on the original ESG;  - Derive CESs (test sequences);  - Generate test templates and adapters for  Android apps;  •  Concerning modeling, we noticed an increase  on the effort when the Method Template  strategy was employed – the increase of  modeling time varied from 25% to 30%, in  comparison with MBT (first iteration);  •  The modeling effort also increased with Edge  Template strategy by varying from 12% to 19%;  •  As for concretization, the Method Template  strategy helped to reduce effort and had a  positive impact – a reduce varying from 38% to  48%, in comparison with MBT (first iteration);  •  The concretization effort with the Edge  Template strategy remained almost constant –  thus, Edge Template strategy helps to test other  characteristics of mobile apps, while the cost of  manual concretization is close to null;  •  By testing specific characteristics of mobile  apps, we covered test cases beyond the goals  initially defined for the test model – thus, we  can increase the return on the effort spent to  design the model;  •  The proposed approach and tool were able to  test different characteristics of mobile apps,  increasing the return on investment over the  effort spent – the tested scenarios were:  - Incoming call when displaying offline map;  - GPS coordinates are sent with reduced  precision (few decimal points);  - Sending data via Bluetooth when Bluetooth  is not available;  •  Five faults were found in the SUTs and it is  important to emphasize that the mobile apps’  versions were already in a production env.;  - Two faults were detected using the  approach with Method Template strategy;  - Three faults were detected using the  approach with Edge Template strategy by  testing specific scenarios;  •  The fact that faults were identified with  proposed approach and the testing tool  provides some evidence of fault detection  capabilities;  •  Our initiative helped to simplify and result in a  partial generation of concrete test cases and the  results also evince that our approach is able to  uncover new faults by reusing designed models  to test other characteristics of mobile apps;    Who is this briefing for?    Software engineering practitioners who  want to make decisions about an', 'uncover new faults by reusing designed models  to test other characteristics of mobile apps;    Who is this briefing for?    Software engineering practitioners who  want to make decisions about an  approach for mobile apps in which test  models are reused to (i) reduce effort  for concretization and (ii) test other  characteristics of apps  based on  scientific evidence.        Where the findings come from?    All findings of this briefing were  extracted from the experimental  evaluation with three real-world  mobile apps  conducted by Guilherme  de Cleva Farto et al.       What is included in this briefing?    The main findings of the original  experimental evaluation conducted in  cooperation with a multinational IT  company leader .    Evidence characteristics through  a brief  description about the original  experimental evaluation and the  studies it analyzed .      What is not included in this briefing?    Background information on Model- Based Testing (MBT) and its related  concepts presented in the original  paper.    Detailed discussions about the studies  analyzed in the original experimental  evaluation .      To access other evidence briefings  on software engineering:    https://dl.acm.org/citation.cfm?doid=3  131151.3131160     http://www.lia.ufc.br/~cbsoft2017/xxxi  -sbes/sbes-cfp/     https://github.com/guilhermefarto/MB  TS4MA   ORIGINAL RESEARCH REFERENCE   Guilherme de C. Farto, André T. Endo, Reuse of model-based tests in mobile apps, Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES 2017),  Brazil - September 2017, Pages 184-193, ISBN 978-1-4503-5326-7, https://doi.org/10.1145/3131151.3131160']","**Title: Enhancing Mobile App Testing Through Model Reuse**

**Introduction:**
This Evidence Briefing summarizes findings from a study on the reuse of model-based tests in mobile applications. The goal is to present a systematic approach to testing that leverages existing test models to reduce effort and enhance the testing of various characteristics of mobile apps. The insights provided here aim to assist software engineering practitioners in understanding how to effectively implement this approach in their testing processes.

**Main Findings:**
1. **Model-Based Testing (MBT) Approach**: The study introduces a model-based testing approach using Event Sequence Graphs (ESGs) to represent the expected behavior of mobile apps. This method allows for the systematic generation of test cases based on the app's functionalities and user interactions.

2. **Reuse of Test Models**: By reusing test models, the approach significantly reduces the effort required for the concretization step in testing. Two strategies were identified:
   - **Method Template**: This strategy associates stereotypes with events in the ESG, enabling automatic generation of test code snippets. It resulted in a reduction of 38% to 48% in the time needed for concretization.
   - **Edge Template**: This strategy focuses on event pairs and allows for testing of device-specific events and unpredictable user interactions. While it slightly increased modeling effort, it provided coverage for additional scenarios that conventional testing might overlook.

3. **Practical Implications**: The experimental evaluation conducted in collaboration with a multinational IT company demonstrated that the proposed approach could effectively uncover faults in mobile applications, even those already in production. Five faults were detected across three apps tested, highlighting the approach's capability to enhance fault detection.

4. **Trade-offs**: While the Method Template strategy increased the modeling time by approximately 25-30%, it simultaneously reduced the effort in concretization. This trade-off suggests that while initial modeling may require more resources, it ultimately leads to more efficient testing processes.

5. **User Feedback**: Participants in the study found the approach and the supporting tool (MBTS4MA) user-friendly, despite having no prior experience with model-based testing. This indicates the approach's potential for wider adoption among practitioners.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, including developers and testers, who are involved in mobile app development and quality assurance. It is particularly relevant for those seeking to improve their testing processes through systematic and automated methods.

**Where the findings come from?**
All findings in this briefing are derived from the experimental evaluation of the model-based testing approach conducted by Guilherme de Cleva Farto and Andre Takeshi Endo, as reported in their paper presented at the 31st Brazilian Symposium on Software Engineering (SBES 2017).

**What is included in this briefing?**
This briefing includes a summary of the proposed model-based testing approach, the strategies for reusing test models, practical implications for fault detection, and insights from the experimental evaluation.

**To access other evidence briefings on software engineering:**
[Evidence Briefings on Software Engineering](http://ease2017.bth.se/)

**For additional information about the research:**
Guilherme de C. Farto, Andre T. Endo, ""Reuse of model-based tests in mobile apps,"" Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES 2017), Brazil - September 2017, Pages 184-193, DOI: [10.1145/3131151.3131160](https://doi.org/10.1145/3131151.3131160)."
"['Revealing Design Problems in Stinky Code A Mixed-Method Study Willian Oizumi1, Leonardo Sousa1, Alessandro Garcia1, Roberto Oliveira1, Anderson Oliveira1, O.I. Anne Benedicte Agbachi1, Carlos Lucena1 1Opus Research Group, Informatics Department, PUC-Rio, Rio de Janeiro, Brazil {woizumi, lsousa, afgarcia, rfelicio, aoliveira, oagbachi, lucena}@inf.puc-rio.br ABSTRACT Developers often have to locate design problems in the source code. Several types of design problem may manifest as code smells in the program. A code smell is a source code structure that may reveal a partial hint about the manifestation of a design problem. Recent studies suggest that developers should ignore smells occur- ring in isolation in a program location. Instead, they should focus on analyzing stinkier code, i.e. program locations – e.g., a class or a hierarchy – aﬀected by multiple smells. The stinkier a program location is, more likely it contains a design problem. However, there is limited understanding if developers can eﬀectively identify a design problem in stinkier code. Developers may struggle to make a meaning out of inter-related smells aﬀecting the same program location. To address this matter, we applied a mixed-method ap- proach to analyze if and how developers can eﬀectively /f_ind design problems when re/f_lecting upon stinky code – i.e., a program loca- tion aﬀected by multiple smells. We performed an experiment and a survey with 11 professionals. Surprisingly, our analysis revealed that only 36.36% of the developers found more design problems when explicitly reasoning about multiple smells as compared to single smells. On the other hand, 63.63% of the developers reported much lesser false positives. Developers reported that analyses of stinky code scattered in class hierarchies or packages is often dif- /f_icult, time consuming, and requires proper visualization support. Moreover, it remains time-consuming to discard stinky program locations that do not represent design problems. KEYWORDS design problem, software design, code smell, agglomeration 1 INTRODUCTION The identi/f_ication of design problems in the source code is not a trivial task [6, 31]. A code smell is a structure in the source code that may provide developers with a partial hint about the manifestation of a design problem [10]. Examples of code smells are God Class , Feature Envy and Brain Method. However, the occurrence of a single smell in isolation in a program often does not represent a design problem [18, 25, 26]. Recent studies reveal that design problems are much more often located in stinkier program locations, i.e., a class, a hierarchy or a package aﬀected by multiple smells [1, 18, 25, 26, 38, 39]. For instance, a Fat Interface [19] is a design problem that often manifests as multiple smells in a program, aﬀecting various classes that implement, extend and use the interface in a program [26]. The stinkier a program location is, more likely it contains a design problem [25, 26]. In fact, developers tend to focus on refac- toring program locations with a high density of code smells, and ignore those locations aﬀected by a single smell [ 4, 5]. However, there is limited understanding if developers can eﬀectively identify design problems in stinkier code, i.e. program locations – e.g., a class or a hierarchy – aﬀected by multiple smells. Indeed, existing techniques tend to focus on the detection and visualization of each single smell [9, 23, 27, 35]. They do not oﬀer a summarized view of inter-related smells aﬀecting a program location [26]. Moreover, previous studies focus on simply analyzing the correlation between design problems and code smells [ 17, 26]. They have not inves- tigated if and how developers are indeed eﬀective in the task of /f_inding design problems in stinkier code. Therefore, we do not know whether the analysis of multiple smells actually provides better precision for the identi/f_ication of', '/f_inding design problems in stinkier code. Therefore, we do not know whether the analysis of multiple smells actually provides better precision for the identi/f_ication of design problems. Developers may struggle to make a meaning out of inter-related smells aﬀecting the same program location. To ad- dress this matter, we designed and executed a multi-method study with 11 professional developers. Our goal was to analyze if devel- opers can eﬀectively /f_ind design problems when re/f_lecting upon multiple smells aﬀecting program locations. Developers were asked to identify design problems in this context. Our study comprised both quantitative and qualitative analyses. For the quantitative analysis, we compared the precision of the developers with a baseline, i.e. situations where only single smells were given to them. As we want to assess if multiple smells can help developers to reveal more design problems than single smells, we divided the developers into two groups. In the /f_irst group, we asked them to identify design problems through the analysis of stinky program locations. In the second group, we asked them to identify design problems with the analysis of single smells. After that, we inverted the groups, and we asked them to repeat the identi/f_ication of design problems in a second system. In each identi/f_ication task, we used the group that identi/f_ied design problems with single smells as the control group. Thus, we could use the control group to measure if the analysis of stinkier program locations can improve the precision of design problem identi/f_ication. In the qualitative analysis, we performed a detailed and system- atic evaluation through: (1) the careful observation of participants during the experiment execution, and (2) the conduction of a post- experiment survey. The objective of this analysis was to identify the main advantages and barriers of re/f_lecting upon multiple smells along the task of identifying design problems. The outcomes of this analysis helped us to better understand ways to improve support for the identi/f_ication of design problems.', 'XI SBCARS, September 2017, Fortaleza, Ceará, Brazil W. Oizumi et al. By triangulating the results of both analyses, we noticed that 36.36% of the developers found more design problems when ex- plicitly reasoning about multiple smells. We found that the un- derstanding of complex stinky structures helped to con/f_irm the occurrence of non-trivial design problems, such as Scattered Con- cern [12]. Furthermore, we found that 63.63% of the developers reported much less false positives when analyzing multiple smells than when single smells. Thus, developers that consider stinky program locations, instead of isolated smelly code, could identify design problems with higher precision. However, this study also showed that developers need better support to analyze stinky pro- gram locations for revealing design problems. We observed that the analysis of stinky code may be diﬃcult and time consuming. For instance, a prioritization algorithm is required so that developers do not waste time analyzing stinky program locations not related to design problems. In addition, developers need better visualiza- tion support to analyze complex stinky code scattered across class hierarchies or packages. The remainder of this paper is organized as follow. Section 2 introduces basic concepts and presents an illustrative example. Sec- tion 3 describes the settings of our study. Section 4 summarizes the main results. Sections 5 and 6 present the related work and the threats to validity, respectively. Finally, Section 7 concludes the paper. 2 CONTEXTUALIZATION This section is organized into two subsections. Section 2.1 presents basic concepts. Section 2.2 brings up an illustrative example of analyzing stinky code to identify design problems. 2.1 Basic Concepts Design Problem. A design problem is a design characteristic that negatively impacts maintainability [ 31]. Design problems aﬀect program locations like packages, interfaces, hierarchies, classes and other structures that are relevant for the design of the system [3]. Examples of design problems are Scattered Concern [12] and Fat Interface [19]. The description of the 8 types of design problems considered in our study is available in our complementary material [7]. We opted by selecting these design problems since: (i) they are often considered as critical in the systems [26] chosen in our experiment, and (ii) other studies haven shown the relation between such design problems and code smells [15–18, 26]. Smelly Code. Code smell is a recurring micro structure in the source code that may indicate the manifestation of a design problem [10]. A design problem can manifest itself in a program by aﬀecting multiple source code locations. Each of these locations are called here smelly code . Thus, the developers can analyze the smelly code to identify a design problem. There are several types of code smell, which may aﬀect a method, a class or a hierarchy. In this paper, we used nine types of code smell, as described in Table 1. These types of smell were considered in this study as they occur in the systems of our experiment (Section 3.3). Stinky Program Location. Developers can rely on the analysis of code smells to identify design problems [14, 17, 30]. In fact, recent studies [1, 18, 26, 39] suggest that the stinkier a program location is, the more likely it is to be aﬀected by a design problem. Stinky code Table 1: Types of code smell Type Description God Class Long and complex class that centralizes the intelligence of the system Brain Method Long and complex method that centralizes the intelligence of a class Data Class Class that contains data but not behavior related to the data Disperse Coupling The case of an operation which is excessively tied to many other operations in the system, and additionally these provider methods that are dispersed among many classes Feature Envy Method that calls more methods of a single external class than the internal methods of its own inner class Intensive Coupling', 'dispersed among many classes Feature Envy Method that calls more methods of a single external class than the internal methods of its own inner class Intensive Coupling When a method is tied to many other operations in the system, whereby these provider operations are dispersed only into one or a few classes Refused Parent BequestSubclass that does not use the protected methods of its superclass Shotgun Surgery This anomaly is evident when you must change lots of pieces of code in diﬀerent places simply to add a new or extended piece of behavior Tradition Breaker Subclass that provides a large set of services that are unrelated to services provided by the superclass is the manifestation of multiple code smells in a program location. In this paper, we are especially interested in stinky code indicated by smell agglomerations [26]. A smell agglomeration is a group of inter-related code smells aﬀecting the same program location, such as a method, a class, a hierarchy or a package [26]. Thus, the agglomeration is determined in the program by the co-occurrence of two or more code smells in the same method, class, hierarchy or package (or component). For code smells that co-occur in the three last cases, we only consider they are part of a (class-, hierarchy- or package-level) agglomeration if they are syntactically related [26]. For instance, two classes can be related through structural relationships in the program, such as method calls and inheritance relationships. In this paper, we considered /f_ive categories of ag- glomeration, namely intra-method, intra-class, hierarchical, and intra-component [26]. For instance, a method that contains several code smells represents an intra-method agglomeration. A full de- scription of the categories of agglomeration considered in our study is available in our complementary material [7]. The agglomerations used in this study were detected with the Organic tool [24]. The Organic tool is a plug-in developed for the Eclipse IDE. 2.2 Identifying Design Problem in Stinky Code As explained in previous section, the identi/f_ication of design prob- lems can be based on code smells. For instance, let us consider the example illustrated in Figure 1. This /f_igure presents some classes that belong to the Work/f_low Managersubstystem – a subsystem of the Apache OODT (Object Oriented Data Technology) system [20]. It is responsible for description, execution, and monitoring of work/f_lows. Supposing that a developer is in charge of identifying design problems in the Work/f_low Manager. She can rely on the analysis of code smells to spot program locations that may contain a design problem. If she is analyzing the repository package, she', 'Revealing Design Problems in Stinky Code XI SBCARS, September 2017, Fortaleza, Ceará, Brazil Figure 1: Example of agglomeration in the Work/f_low system will notice that this package contains several code smells as indi- cated by a smell agglomeration. This agglomeration is formed by 4 instances of the Feature Envy smell. As illustrated by Figure 1, each of the Feature Envy occurrences aﬀects a diﬀerent class. In this case, 3 classes implement the Work/f_lowRepositoryinterface. When the developer analyze these classes based on the Feature Envy smell, she will realize that these classes contain the smell because one of their methods is more interested in other classes than in its own hosting class. This happens because these methods are forced to implement a method that was de/f_ined in theWork/f_lowRepository interface. That is, the smells in the agglomeration are indicating that (the corresponding method in) the interface may contain a design problem. In fact, this “forced implementation” becomes a problem because these methods are implementing a concern that should not have been implemented in their hosting classes. That happens because of the fact that the Work/f_lowRepositoryinterface processes multiple services; thus, any class that implements this interface needs to handle more services than it actually should have to. In this example, the developer knows that the code smells in the agglomeration have the same type ( Feature Envy ). Also, she knows that 3 classes aﬀected by the code smells implement the same interface, as rei/f_ied in ahierarchical agglomeration. This interface, in its turn, seems to provide non-cohesive services. Thus, the developer can infer that a design problem, called Ambiguous Interface, is aﬀecting theWork/f_lowRepostioryinterface. On the other hand, if she did not re/f_lect upon the code smell agglomeration, it would be harder to her to identify the same design problem. One of the reasons is the number of code smells spread over the 6 classes and 2 interfaces within the package. Although the package contains only 8 classes (Figure 1 only shows some of them), it has more than 50 code smells. Thus, she has to analyze many smelly code snippets in order to discard, postpone or further consider them in the identi/f_ication of design problems. Let us assume that the developer only reasons about each code smell in isolation to identify the design problem, i.e., without taking into consideration smell relationships in an agglomeration. Thus, she can choose to analyze the DataSourceWork/f_lowRepositoryclass /f_irst because the class contains the highest number of smells in the package. Analyzing the 21 instances of code smells in the class, the developer will notice that the class has smells related to high coupling with other classes (Intensive Coupling and Dispersed Cou- pling), low cohesion (Feature Envy ), and overload of responsibilities (God Class ). However, all these smells may indicate diﬀerent prob- lems. Thus, she has to extend the analysis to other classes in order to gather more information that can potentially indicate a design problem. Unfortunately, the other classes also have diﬀerent in- stances of code smells, and these instances may not be related to any design problem. Therefore, the developer can face diﬃculties to /f_ind the relevant code smells that can help him to identify a design problem. Thus, the analysis of stinky program locations, as revealed by agglomerations, seems to be a better strategy. However, there is limited empirical understanding about this phenomenon. 3 STUDY PLANNING This section contains the settings of this study. Here, we present the research questions, empirical procedures and other details about our quantitative and qualitative analysis. 3.1 Research Questions Previous studies suggest code smell agglomerations are consistent indicators of design problems [ 26]. However, there is a need to investigate whether developers can indeed identify design prob-', 'Previous studies suggest code smell agglomerations are consistent indicators of design problems [ 26]. However, there is a need to investigate whether developers can indeed identify design prob- lems when exploring smell agglomerations. In order to address this matter, we de/f_ined two research questions. The /f_irst one is presented as follow: RQ1. Does the use of agglomerations improve the precision of developers in identifying design problems? Research question RQ1 allows us to analyze whether code smell agglomerations help developers to identify design problems with high precision. To answer this question, we conducted a controlled experiment with 11 professional developers. In this context, preci- sion is measured based on the percentage of true positives indicated by the developers – i.e., the percentage of correctly identi/f_ied design problems. Precision is an important aspect of the identi/f_ication task. Through the correct identi/f_ication of design problems, developers are able to optimize their work by solving problems that really impact design. On the other hand, the lack of precision would lead software development teams to spend time and budget with irrelevant tasks. For example, in companies adept to code review practices [21], the lack of precision can lead developers to waste time on refactoring tasks that do not contribute to system main- tainability. The precise identi/f_ication of design problems is also important in open source projects. For instance, the contributions of eventual collaborators are often rejected by core developers due to the presence of design problems [ 29]. Therefore, in this case, a lack of precision could lead core developers to reject relevant contributions due to “false design problems”. In this study, we did not measure recall because of the high num- ber of design problems in the analyzed systems. Together with the system’s original developers, we created a ground truth of design problems (Section 3.5) with more than 150 instances of design prob- lems. Hence, it would be impracticable for participants to /f_ind all the design problems in the system due to the time constraints in the study (45 minutes). Consequently, they were expected to reach a low recall value. Therefore, we focused on the precision.', 'XI SBCARS, September 2017, Fortaleza, Ceará, Brazil W. Oizumi et al. In order to measure if there was an improvement or not in the precision, we are comparing the participants using agglomerations with a control group. The control group comprises of participants identifying design problems without agglomerations, but only with (non-agglomerated) code smells. Thus, we analyzed the list of de- sign problems identi/f_ied by the participants /f_irstly. In this analysis, we use a ground truth to con/f_irm or refute each design problem indicated by participants. Then, we compared the number of false positives and true positives produced with the code smell agglom- erations against the number of false and true positives produced by the control group. Someone could assume that developers would often bene/f_it from the use of agglomerations in their quest for /f_inding design problems. However, it is through the analysis of RQ1 that we will be able to verify if developers can correctly identify more design problems us- ing smell agglomerations. Regardless of the result, another question that should be investigated concerns how to better support devel- opers in exploring smell agglomerations. Even though a previous study [26] has shown the strong relation between design problems and code smells within an agglomeration, we do not know whether and how the identi/f_ication of design problems with agglomerations can be improved. The following question address this matter. RQ2. How can the identi/f_ication of design problems with code smell agglomerations be improved? This question was addressed by conducting a qualitative analysis. This analysis was based on the observation of participants during the experiment and based on a post-experiment survey (Section 3.6). This analysis is necessary because it provides a complementary perspective on the identi/f_ication of design problems with agglom- erations. We could reveal advantages and barriers on the use of smell agglomerations. As reported in Section 4, the combination of quantitative and qualitative analysis helped us to draw more well grounded conclusions. 3.2 Experiment Procedures We applied a quasi-experiment [28] in order to perform our study. A quasi-experiment is an experiment in which the units or groups are not assigned to conditions randomly. In our study, we could not select participants randomly because we need to ensure that they meet the requirements described in Section 3.3. The experiment was conducted individually with each participant. They had to perform the experiment in two steps with four tasks in each one. Both steps comprise the same set of tasks; the only diﬀerence between the steps was regarding the usage of agglomerations. As explained before, we need to compare developers using ag- glomeration with a control group. This comparison allowed us to verify if there was an improvement in the precision when develop- ers use agglomerations. Hence, the control group comprises the developers that had to identify design problems with a list of non- agglomerated code smells. Thus, we divided the participants into two groups. The /f_irst group would identify design problems using agglomerations in the /f_irst step. After that, they would identify design problems using a list of non-agglomerated code smells in the second step. The second group of participants would make the identi/f_ication inversely: using the non-agglomerated code smells in the /f_irst step and, then, using the agglomerations in the second step. Table 2: Combinations of groups, projects and steps Step 1 Step 2 Arrange Group Project Group Project 1 Agglomeration Project 1 Control Project 2 2 Agglomeration Project 2 Control Project 1 3 Control Project 1 Agglomeration Project 2 4 Control Project 2 Agglomeration Project 1 Thus, in each step, we have two groups of participants: a group using agglomerations and a control group. As each participant identi/f_ies design problems twice (/f_irst and', 'Thus, in each step, we have two groups of participants: a group using agglomerations and a control group. As each participant identi/f_ies design problems twice (/f_irst and second step), we had to select two software projects. Thus, each participant could identify design problems using a diﬀerent project in both steps. Another reason for providing two software projects is to avoid bias with the learning curve. For example, supposing that the participant uses the same project in both steps. She could /f_ind more problems in the second step than in the /f_irst step. That could happen because she can identify in the second step the same problems that she identi/f_ied in the /f_irst step, plus other design problems identi/f_ied only in the second step. This increase in the number of design problems found in the second step would not be due to the use of agglomerations, but rather due to the knowledge acquired by the participant. There are four possible combinations with the participants based on the distribution between steps and software projects. Therefore, all participants were divided into four groups equally to promote a fair comparison. Table 2 presents the cross design for the four arranges. The agglomeration group represents the group of partic- ipants that identi/f_ied design problems using the agglomerations, and the control group comprises the participants that identi/f_ied design problems using the list of non-agglomerated code smells. The study was composed by a set of six activities distributed into three phases, as represented in Figure 2 described as follows. Activity 1: Apply the questionnaire for subjects’ charac- terization. The subjects’ characterization questionnaire is com- posed of questions to characterize each participant, including aca- demic degree, professional experience with Java programming, background on code smells and Eclipse IDE. Activity 2: Training Session. After de/f_ining the order of exe- cution of each steps, the next step was to provide a training session for the participants. The main objective of the training session was to level the participant at the same background required to understand and properly execute the experimental tasks. Thus, they received training about basic concepts and terminologies. This training was given only once for each participant before the /f_irst steps of the experiment. The training consisted of a 15-minute pre- sentation that covered the following topics: software design, code smells, and design problems. The training session took approxi- mately 15 minutes, and the participants could make any question throughout it. After the training, subjects received some artifacts that could be used during the experiment. They received a list with a brief description of the types of design problems presented in the train- ing session. They also received a list with the description of basic', 'Revealing Design Problems in Stinky Code XI SBCARS, September 2017, Fortaleza, Ceará, Brazil Figure 2: The experimental design principles of object-oriented programming and design. They re- ceived a document containing: (i) a brief description of both project systems (Section 3.3), and (ii) a very high-level description of their design blueprint. We gave these documents because when they have to conduct perfective maintenance tasks, they need to have some minimal information about the systems to be maintained. The design blueprint represented the high-level design in the view of the project managers, but it was not detailed enough to support the identi/f_ication of design problems. As it often occurs in practice, the analysis of the source code is inevitably required to identify a design problem. Activity 3: System Introduction. We asked the participants to read the document containing the description of the project in which they would identify design problems. They had 20 minutes to read the description and the design blueprint of the system. Thus, they could start the identi/f_ication with a certain level of familiarity with the software project. Activity 4: Understanding the Task. In this activity, we ex- plained how the participant could use the Organic tool (Section 2.1) to collect either the agglomerations or the list of (non-agglomerated) code smells. As the Organic tool was developed as an Eclipse plug- in, we explained each one of the sections displayed in the Eclipse IDE and that was related to the Organic tool. This activity lasted approximately 10 minutes. Activity 5: Identi/f_ication of Design Problems. In this ac- tivity, the participant had 45 minutes to identify design problems in the project. We emphasized to the participant the importance of achieving the key goal of /f_inding design problems. For each identi/f_ied design problem, the participant was asked to provide the following information: (i) short description of the problem, (ii) possible consequences caused by the problem, (iii) classes, methods or packages realizing the design problem in the source code, and (iv) the category(s) of agglomerations (Section 2.1) that helped him to identify the design problems. If the participant was identifying design problems as part of the control group, she needed to provide almost the same information; the diﬀerence was that instead of pro- viding the agglomeration (and its category), she needed to provide the code smells that she used to identify the design problem. Activity 6: Post-experiment Survey. In this activity, the par- ticipant received a feedback form. This form provides a list of questions, which enables the participant to expose her opinion on the identi/f_ication of design problems. More details about this activity are provided in Section 3.6. After the sixth activity had been completed, we asked the same participant to repeat all tasks in the second phase. 3.3 Software Projects and Participant Selection In order to conduct the experiment as explained in the previous sec- tion, we selected two software systems in which developers had to identify design problems. We selected two programs that represent components of the Apache OODT project [20]: Push Pull and Work- /f_low Manager. We selected subsystems of the OODT project since it is a large heterogeneous system; then, we could choose subsystems based on their diversity. Also, the Apache OODT project has a well-de/f_ined set of design problems previously identi/f_ied by OODT developers who actually implemented the systems (Section 3.5) [26]; thus, avoiding the introduction of false positive design problems in the ground truth. In addition, the OODT project was developed for NASA, used in other studies [15–18, 26] and with a global commu- nity involved in its development. A brief description of the project systems are presented as follow: • Push Pull: it is the OODT component responsible for down-', 'nity involved in its development. A brief description of the project systems are presented as follow: • Push Pull: it is the OODT component responsible for down- loading remote content (pull) or accepting the delivery of remote content (push) to a local staging area. • Work/f_low Manager: it is a component that is part of the OODT client-server system. It is responsible for describing, executing, and monitoring work/f_lows. After choosing the projects, our next step was to recruit develop- ers for the experiment. Thus, we sent a characterization question- naire for a group of developers of our network. Their answers were analyzed to determine which of them were eligible to participate in the study based on the following requirements: R1. Four years or more of experience with software develop- ment and maintenance. We have chosen four years because this is the average time used by companies such as Yahoo [37] and Twitter [32] to classify a developer as experienced. R2. No previous knowledge about Push Pull and Work/f_low Manager. R3. At least basic knowledge about code smells. R4. At least intermediary knowledge on Java programming and Eclipse IDE. We de/f_ined the knowledge in each topic based on a scale com- posed of /f_ive levels:none, minimum, basic, intermediary, advanced and expert. We included in the questionnaire a description of each level, allowing the subjects to have a similar interpretation of the answers. The description of such classi/f_ication can be found in the complementary material [7]. Table 3 summarizes the characteristics of each developer selected for the experiment.', 'XI SBCARS, September 2017, Fortaleza, Ceará, Brazil W. Oizumi et al. Table 3: Characterization of the Participants ID Experience in years Education Level Knowledge Java Code Smells Eclipse P1 5 PhD Advanced Advanced Advanced P2 6 Graduate Advanced Basic Advanced P3 8 Master Advanced Intermediary Advanced P4 4 Graduate Intermediary Basic Basic P5 5 Master Advanced IntermediaryIntermediary P6 5 Graduate IntermediaryIntermediaryIntermediary P7 12 Graduate Expert Advanced Expert P8 5 Graduate Advanced Advanced Advanced P9 10 Graduate IntermediaryIntermediaryIntermediary P10 4 PhD Advanced Intermediary Advanced P11 5 PhD Advanced Intermediary Advanced 3.4 Quantitative Analysis Procedures In order to answer research question RQ1, we asked the experiment participants to analyze two systems with the aim of identifying design problems as described above. For each system, we analyzed the precision of participants regarding the identi/f_ication of design problems. The precision of participants was measured based ontrue positives (TP) and false positives (FP). In this context, a true positive is a candidate of design problem, as indicated by the participant, that was con/f_irmed by a ground truth analysis. On the other hand, a false positive is a candidate of design problem that was not con/f_irmed in the ground truth analysis. Thus, the precision is calculated using the following formula: Precision = T P T P+ FP (1) 3.5 Ground Truth Analysis We had to validate the identi/f_ied design problems as true positives or false positive for each one of the analyzed systems. However, we could not argue that a design problem was correct or not since we were not involved with the design of each system. Thus, we relied on the knowledge of the systems’ original designers and developers to help us in validating the design problems. We certi/f_ied they were the people who had the deepest knowledge of the design of the investigated projects. We highlight that designers and devel- opers used to validate the ground truth were not subjects of the experiment. We performed two steps to incrementally develop the ground truth. First, we asked original OODT designers and developers to provide us a list of design problems aﬀecting the systems. They listed the problems and explained the relevance of each one through a questionnaire [7]. They also described which code elements were contributing to the realization of each design problem. Second, we identi/f_ied some design problems using a suite of design recovery tools [11]. We asked developers of the systems to validate and com- bine our additional design problems with their list. The procedure for the additional identi/f_ication was the following: (i) an initial list of design problems was identi/f_ied using a method presented in [16], (ii) the developers had to con/f_irm, refute or expand the list, (iii) the developers provided a brief explanation of the relevance of each design problem, and (iv) when we suspected there was still inaccuracies in the list of design problems, we discussed with them. In the end, we had the ground truth of design problems validated by the original designers and developers. 3.6 Qualitative Analysis Procedures The experiment with professional developers helped us to assess the precision of developers in the identi/f_ication of design prob- lems with agglomerations. The results observed in the experiment revealed that agglomerations can, in fact, help to improve the pre- cision of some developers in the identi/f_ication of design problems (Section 4.1). Nevertheless, we also observed that there is room for improvements. Therefore, we conducted a qualitative analy- sis to investigate what should be improved from the perspective of professional software developers. Besides identifying possible improvements, this analysis also helped us to understand what are the main strengths of exploring agglomerations for design problem identi/f_ication.', 'improvements, this analysis also helped us to understand what are the main strengths of exploring agglomerations for design problem identi/f_ication. As described in Section 3.2, we asked the participants to provide us feedback about the identi/f_ication of design problems. They answered a post-experiment survey, and we use their answers to conduct a qualitative analysis. The objective of the survey was to gather participant’s opinion regarding (i) the (dis)advantages of using the agglomerations or code smells to identify design problems, (ii) whether the provided information could be easily understood, (iii) which types of information were fundamental to identify design problems, (iv) what she believes that should be done to improve the identi/f_ication of design problems, (v) what she thought about the use of the code smells for the identi/f_ication of design problems, (vi) how the visualization mechanism provided by the Organic tool aﬀected her performance, and (vii) which types of code smell and categories of agglomeration were the most useful for identifying design problems. The results of this survey helped us to answer research question RQ2. By conducting the survey, we were able to gather the opinion of developers regarding the use of code smell agglomerations. How- ever, as reported by [8], what is reported in the survey may not be what actually happens in practice. Therefore, to obtain more reliable results, we also observed the participants of our experiment during the identi/f_ication of design problems. This observation was performed during the experiment and also in analyzes after the ex- periment, through video and audio recorded during the experiment. This analysis allowed us to look at code smell agglomerations from the standpoint of professional software developers. It is important to note that the observation of participants during the experiment does not replace nor invalidate the survey. In fact, the combination of observations and survey helped us to obtain a deeper understand- ing and interpretation on the results observed in the experiment. 4 RESULTS AND ANALYSIS The results of this study are organized in two sub-sections. Sec- tion 4.1 presents the results of our quantitative analysis regarding research question RQ1. Section 4.2 provides the results of our qual- itative analysis to answer research question RQ2. 4.1 Do Agglomerations Improve Precision? As described in Section 3.4, we conducted a quantitative analysis to answer our /f_irst research question:Does the use of agglomerations', 'Revealing Design Problems in Stinky Code XI SBCARS, September 2017, Fortaleza, Ceará, Brazil Table 4: Precision Agglomeration Group Control GroupID TP FP Precision TP FP Precision 1 2 1 66.67% 1 1 50% 2 0 3 0% 1 4 20% 3 3 2 60% 1 4 20% 4 2 0 100% 1 3 25% 5 4 0 100% 3 1 75% 6 1 0 100% 1 0 100% 7 1 1 50% 1 1 50% 8 3 0 100% 3 0 100% 9 0 1 0% 0 6 0% 10 0 0 - 1 1 50% 11 0 1 0% 0 0 - All 16 9 64% 13 21 38.24% improve the precision of developers in identifying design problems? . Table 4 presents the precision results for each participant (rows). The /f_irst column (ID) shows the identi/f_ication number of each partic- ipant. The second column (Agglomeration Group ) presents the true positives (TP), false positives (FP) and precision for the participants when they were provided with agglomerations to identify design problems. Similarly, the third column (Control Group ) presents the true positives (TP), false positives (FP) and precision for the partic- ipants in the control group, i.e., when they were provided with a /f_lat list of single smells. Developers identi/f_ied a few more true positives using ag- glomerations. We can see in Table 4 that the developers identi/f_ied a few more design problems (TPs) when they were in the agglom- eration group (16 TP design problems) than when they were in the control group (13 TP design problems). As far as the per-subject analysis is concerned, 4 developers (light gray rows) identi/f_ied more true positives when they used agglomerations than when they used the list of code smells in the control group. The use of agglomera- tions clearly outperformed the use of smells in these 4 cases. On the other hand, 2 participants (2, 10) did not identify any true positive using the agglomerations, but they identi/f_ied a true positive each in the control group. The rest of the participants (6, 7, 8, 9 and 11) identi/f_ied the same number of true positives (5 TP design problems) regardless the group. Upon data analysis, we were able to reveal the main reason why the 4 developers in the light gray rows identi/f_ied more true positive design problems in the agglomeration group than in the control group. As illustrated in the example in the Figure 1 (Section 2.2), these 4 participants systematically used each agglomeration’s smell as an indicator of the presence of a design problem. They ana- lyzed each one of the code smells as a complementary symptom of the presence of a design problem, which gave them increasing con/f_idence to con/f_irm the occurrence of the design problem. Sur- prisingly, we noticed the same behavior for the participant 8 even when she was in the control group. She was capable of agglom- erating the code smells on her own, starting from the individual smells given in the /f_lat list. Then, she used such agglomerations to identify design problems in the control group. This is the reason why she reached a precision value of 100% in both groups. Agglomerations help developers to avoid false positives . In general, developers identi/f_ied less false positives when they used agglomerations (9 FP design problems) than when they used the list of code smells (21 FP design problems). With the exception of participant 11, all others identi/f_ied either less or equal number of false positives when they were in the agglomeration group than when they were in the control group. When we analyze the control group, we can notice that more than half of the identi/f_ied design problems are false positives (61,76%) while the agglomeration group identi/f_ied only 36% of false positives. After observing how developers identify design problems in the control group, we noticed that they did not go further with the anal- ysis of the elements. Usually, a developer needs to analyze other classes in order to gather more information that can potentially indicate a design problem as discussed in Section 2.2. When the par- ticipants used the agglomerations, they analyzed multiple elements', 'indicate a design problem as discussed in Section 2.2. When the par- ticipants used the agglomerations, they analyzed multiple elements because they analyzed each code smell within the agglomeration even when the smells were in diﬀerent elements. This behavior did not happen when participants were in the control group. In most of the cases, the participants in the control group analyzed only one code smell, which increased the likelihood of reporting false positives. Then, they reported a design problem in the class due to the presence of the code smell. However, some code smells are not related to any design problem; thus, the developer can report a false positive if she mistakenly considers a code smell that is not related to a design problem. That explains why developers in the control group found so many false positives. As developers tend to look at all agglomeration’ smells before reporting a design problem, the likelihood of reporting a false positive decreases, even when there is a code smell that is a false positive by itself. Agglomerations improve the precision . Even though we cannot claim a statistical signi/f_icance in our results due to the sam- ple size of this study, we can notice that developers achieve a higher precision (64%) when they use agglomerations than when they use code smells (38,24%). Therefore, this result suggests that agglom- erations may improve the precision of developers in identifying design problems, answering our /f_irst research question. However, someone could expect that all developers using agglomerations would signi/f_icantly outperform the control group. As a matter of fact, we noticed some factors that explain, at least partially, why de- velopers did not /f_ind much more design problems when they were in the agglomeration group than when they were in the control group. These factors are presented in the next subsection, and they are useful to discover improvements for the identi/f_ication of design problem with the analysis of stinky program locations. 4.2 How to Improve Design Problem Identi/f_ication? This section presents the answer for our second research question: How can the identi/f_ication of design problems with code smell ag- glomerations be improved? We conducted a qualitative analysis to answer this question. As described in Section 3.6, this analysis was based on the observation of participants during the identi/f_ication of design problems as well as the analysis of the post-experiment survey.', 'XI SBCARS, September 2017, Fortaleza, Ceará, Brazil W. Oizumi et al. Where to start from? As discussed in the previous section, the participants identi/f_ied few more true positives using agglomer- ations. Someone could expect that all developers using agglomera- tions would signi/f_icantly outperform the control group. However, we observed that participants spent much more time analyzing the agglomerations than analyzing the smells in the control group. That happened because they analyzed each code smell in the stinky program location as previously explained Section 4.1. Furthermore, sometimes the participants analyzed agglomerations that were not related to any design problem. That is another factor that explains the almost same number of true positives between both groups. Unfortunately, almost all the participants analyzed irrelevant agglomerations. Participants 6, 9, 10 and 11 were the ones that suﬀered the most from the analysis of irrelevant agglomerations. Since these four participants faced such issue, they suggested in our post-experiment survey that the Organic tool (Section 2.1) should provide means to prioritize relevant agglomerations. Hence, they would not spend time with the analysis of irrelevant stinky code. This issue helps us to explain why they fell short in identifying design problems through the analysis of agglomerations. Need for prioritizing agglomerations. The aforementioned need for prioritization shows that the time and eﬀort required to identify design problems is a key factor for developers; thus, prioritization should be taking into consideration. As a matter of fact, the prioritization of smelly code has been the focus of recent research [2, 33, 34]. In [33], for example, we proposed and assessed prioritization criteria for smell agglomerations. As we have observed, the prioritized list of agglomerations would help a developer to progressively analyze the agglomerations that have more chance to represent design problems, discarding the irrelevant ones. This would be especially useful in large legacy systems, in which thousands of agglomerations may be detected. Nevertheless, there is no prioritization criterion that is eﬀective for any system [33]. Based on our qualitative analysis, we noticed that existing crite- ria for prioritization should select agglomerations that are cohesive. A cohesive agglomeration in our context is an agglomeration in which all the code smells are related to the same design problem. If there is one code smell that is not related to the design problem, such smell may direct the developer away from the design problem in the worst case. In the best case, the developer will spend time analyzing a code smell useless to identify the design problem. This fact suggests that developers need accurate algorithms to /f_ind cohe- sive agglomerations and to discard the less cohesive ones. However, prioritization algorithms based on existing criteria are unable to do this as far we are concerned. Consequently, the prioritization of stinky program locations still poses as a challenging research topic. Stinky code analysis is challenging. Besides the prioritiza- tion issue, participants also suﬀered to analyze the smelly source code. As reported in Section 4.1, this problem was even worse for agglomerations aﬀecting larger program scopes, i.e., agglomera- tions crosscutting implementation packages or class hierarchies. We noticed that a large agglomeration requires that developers reason about a wide range of scattered code smells. As they tend to use each code smell as a symptom of design problem, they have diﬃculties to correlate the multiple symptoms of an agglomera- tion. This is a challenging task because the higher the number of code elements involved in an agglomeration, the greater is the volume of code that must be analyzed. Consequently, developers will have more code to analyze, which increases the complexity of the analysis.', 'volume of code that must be analyzed. Consequently, developers will have more code to analyze, which increases the complexity of the analysis. Need for proper visualization mechanisms. In order to alle- viate the analysis of stinky code, some participants suggested the adoption of visualization mechanisms. For instance, participant number 8 suggested the visualization of agglomerations through a graph-based representation [13]. She mentioned that such visualiza- tion would provide an abstract and general view of agglomerations. The main advantage of this form of visualization is that the more abstract a representation is, the less details will be displayed for analysis. Consequently, the developers would not be overloaded with details. At the same time, an abstract representation like the graph-based visualization would help developers to see the full extent of an agglomeration. After providing an abstract view, a visualization mechanism could allow developers to progressively explore the agglomeration details such as the types of smells, loca- tion of stinky code and relationships among smells. Such details could be displayed in the graph itself, in the source code, or in complementary views. Identi/f_ication of the design problem type.The diﬃculty in analyzing agglomerations also raised the need for recommendations on which types of design problem each smell agglomeration is more likely to indicate. These recommendations would reduce the eﬀort required to decide whether the elements are aﬀected by design problems or not. For example, the agglomeration of Figure 1 occurs in classes of the same hierarchy that are implementing the Work/f_lowRepositoryinterface. All smelly code of this stinky program location present the same type of smell, which is the Feature Envy. The occurrence of multiple Feature Envies in a unique hierarchy, suggests that there is a problem in the interface, which is spreading through all classes of the hierarchy. Therefore, to help developers to decide whether there is a problem or not, the Organic tool could suggest the analysis of this hierarchical agglomeration trying to identify problems like Ambiguous Interface [12] and Fat Interface [19], for example. Suggestions of design problem types can help developers to focus their attention is speci/f_ic characteristics of the suggested design problems. However, this kind of recommendation algorithm re- quires multiple case studies to understand how and when each form of agglomeration may represent speci/f_ic types of design prob- lem. As reported in our previous study [25], this is a challenging research topic. 5 RELATED WORK Previous studies have not investigated if developers can indeed /f_ind more design problems when they focus on inspecting stinky program locations. Thus, we do not know whether the analysis of multiple smells actually provides better precision for the identi/f_ica- tion of design problems. In fact, related works propose techniques for supporting the detection and visualization of both single smelly code and inter-related smells. There are several studies that investi- gated detection and visualization of single smelly code [9, 23, 27, 35]. However, we found few studies that investigated the detection of inter-related smells aﬀecting a program location [ 24, 34]. In this', 'Revealing Design Problems in Stinky Code XI SBCARS, September 2017, Fortaleza, Ceará, Brazil context, Vidal et. al. [34] present a tool for detecting code smells and agglomerations of a (Java-based) system and ranking them ac- cording to diﬀerent criteria [34]. The main bene/f_it of using this tool is that developers can con/f_igure and extend the tool by providing diﬀerent strategies to identify and rank the smells and groups of smells (i.e., agglomerations). However, a disadvantage of this tool is that it represents agglomeration without show the relation that could exist between the code smells. Regarding detection and visualization of single smelly code, Van Emden and Moonen [9] present a tool that detects and visualizes code smells in source code, displays the code structure as a graph and maps code smells onto the attributes of that graph. This tool can be problematic for several reasons. The visualization is built assuming that code smells are concentrated in a particular region of the code and that metrics will point reviewers there. This assump- tion does not always hold; many code smells require understanding the relationships between many interacting components and thus are spread throughout the program. These relationships cannot be represented by a simple mapping between code structure and color. Other studies [17, 39] have investigated the eﬀects of code smells on the software design. For instance, Yamashita et al. [39] studied collocated smells – code smells that interact in the same source code /f_ile –, and coupled smells – code smells that interact across dif- ferent source code /f_iles. Regarding software design, they observed that limiting the analysis to collocated smells would reduce their capability to reveal design problems, as coupled smells may reveal critical design problems. We also found studies that have investigated the use of informa- tion other than code smells to identify design problems [22, 36]. In this case, Mo et al. [22] proposed and evaluated the combination of structural, history and design information to identify potential design problems. Xiao et al. [36] introduced an approach that uses a history coupling probability matrix to identify and quantify design problems. However, one disadvantage of such studies is they rely on design information, which may not exist for many software systems. In addition, they have not evaluated from the perspective of software developers. Based on these related studies, we observed that they did not present whether developers can indeed /f_ind more design problems when they focus on inspecting stinky program location. Therefore, our research covers this gap by investigating whether the analysis of stinky program locations help developers in revealing more design problems than the analysis of single smells. 6 THREATS TO VALIDITY This section presents some threats that could limit the validity of our main /f_indings. For each threat, we present the actions taken to mitigate their impact on the research results. The /f_irst threat to validity is related to the number of participants in the study. We have selected a sample of 11 participants, which may not be enough to achieve conclusive results. However, instead of drawing conclusions based on the quantitative results, we con- ducted a qualitative analysis. In addition to conduct a qualitative analysis, we de/f_ined a set of requirements to selecting developers suitable for the study. Also, we conducted training sessions with all participants. Such sections aimed to resolve any gaps in the participants’ knowledge and any terminology con/f_licts, allowing us to increase our con/f_idence in the results. The second threat is related to possible misunderstandings dur- ing the study. As we asked developers to conduct a speci/f_ic software engineering task and to answer a survey, they could have conducted the study diﬀerent from what we asked. To mitigate this threat, we', 'engineering task and to answer a survey, they could have conducted the study diﬀerent from what we asked. To mitigate this threat, we assisted the participants during the entire study, and we make sure of helping them to understand the experiment tasks and survey questions. We highlighted that our help was limited to only clarify the study in order to avoid some bias on our results. Finally, there are two threats concerning the selected projects. The /f_irst one is about the diﬃculty of the participants in understand- ing the source code used in the experimental tasks. This diﬃculty appears due to the complexity of the source code and time con- straints to complete each task. The second threat is related to one software project could be easier to identify design problem than the other. We minimized the /f_irst threat by running a pilot-experiment to de/f_ine a experimental time reasonable to perform the tasks. To minimize the second threat, we selected projects with similar size, complexity, and number of known design problems. We also have trained all participants about each project. In addition, our results suggest no variation in diﬃculty for identifying design problems in the two projects. 7 CONCLUDING REMARKS In this paper, we assessed if developers are eﬀective in revealing design problems when they reason about multiple code smells. We conducted such investigation because recent studies have shown that design problems are likely to be located in elements aﬀected by two or more smells. However, these studies do not evaluate if devel- opers can reason about multiple smells to reveal design problems. Thus, we conducted a multi-method study with 11 developers. In the study, we asked them to identify design problems using code smell agglomerations. After that, we compared their results with the results of when they used the /f_lat list of code smells to identify design problems. The data analysis showed that developers /f_ind most design prob- lems when they use code smell agglomerations to identify design problems, i.e., when they reason about stinkier program elements. In addition, we noticed that agglomerations help developers to avoid false positives. Therefore, our results suggest that agglomera- tions may improve the precision of developers in identifying design problems. When we analyze the survey’s answers and how the developers identi/f_ied design problems, we noticed that developers tended to have higher con/f_idence to identify the occurrence of non- trivial design problems when using agglomerations. That happens because the developers tend to analyze each agglomeration’s smell before reporting a design problem. Consequently, the likelihood of reporting a false positive decreases. In addition, we noticed that agglomerations help them to understand complex stinky structures. Our results also indicate that developers need better tool sup- port to analyze stinky code. For instance, the developers need to prioritize agglomerations that are most likely to indicate a design problem. The prioritization algorithms are required because the analysis of stinky code is diﬃcult and time-consuming. Thus, de- velopers should focus on those agglomerations that more likely', 'XI SBCARS, September 2017, Fortaleza, Ceará, Brazil W. Oizumi et al. indicate design problems. In addition, we also noticed that develop- ers need proper visualization mechanisms to support the analyses of stinky code scattered across wider program locations, such as hierarchies or packages. Some agglomerations are widely spread in the source code; a single agglomeration may contain code smells located in multiple class hierarchies. Thus, the developers have a large program scope to analyze. They may have diﬃculty to visualize how the code smells are related in the agglomeration. A graph-based visualization can help developers to /f_igure out how the code smells are related in the agglomeration. The results of our study encourage the use of smell agglom- erations to identify design problems. However, there are some issues that should be addressed before developers can explore smell agglomerations in a time-eﬀective manner. As discussed above, there is a need to provide mechanisms for better prioritizing and visualizing smell agglomerations. In the future, we plan to imple- ment these mechanisms in Organic (Section 2.1) and evaluate their eﬀectiveness. REFERENCES [1] M Abbes, F Khomh, Y Gueheneuc, and G Antoniol. 2011. An Empirical Study of the Impact of Two Antipatterns, Blob and Spaghetti Code, on Program Com- prehension. In Proceedings of the 15th European Software Engineering Conference; Oldenburg, Germany . 181–190. [2] R. Arcoverde, E. Guimarães, I. Macía, A. Garcia, and Y. Cai. 2013. Prioritization of Code Anomalies Based on Architecture Sensitiveness. In 2013 27th Brazilian Symposium on Software Engineering . 69–78. https://doi.org/10.1109/SBES.2013.14 [3] L Bass, P Clements, and R Kazman. 2003. Software Architecture in Practice . Addison-Wesley Professional. [4] D. Cedrim, A. Garcia, M. Mongiovi, R. Gheyi, L. Sousa, R. Mello, B. Fonseca, M. Ribeiro, and A. Chávez. 2017. Understanding the Impact of Refactoring on Smells. In 11th Joint Meeting of the European Software Engineering Conference and the ACM Sigsoft Symposium on the Foundations of Software (ESEC/FSE’17) . Paderborn, Germany. [5] Diego Cedrim, Leonardo Sousa, Alessandro Garcia, and Rohit Gheyi. 2016. Does Refactoring Improve Software Structural Quality? A Longitudinal Study of 25 Projects. In Proceedings of the 30th Brazilian Symposium on Software Engineering (SBES ’16). ACM, New York, NY, USA, 73–82. [6] O. Ciupke. 1999. Automatic detection of design problems in object-oriented reengineering. In Proceedings of Technology of Object-Oriented Languages and Systems - TOOLS 30 (Cat. No.PR00278) . 18–32. [7] Online Companion. 2017. https://wnoizumi.github.io/SBCARS2017/. (2017). [8] Steve Easterbrook, Janice Singer, Margaret-Anne Storey, and Daniela Damian. 2008. Selecting Empirical Methods for Software Engineering Research . Springer London, London, 285–311. https://doi.org/10.1007/978-1-84800-044-5_11 [9] E Emden and L Moonen. 2002. Java quality assurance by detecting code smells. In Proceedings of the 9th Working Conference on Reverse Engineering; Richmond, USA. 97. [10] M Fowler. 1999. Refactoring: Improving the Design of Existing Code . Addison- Wesley Professional, Boston. [11] J Garcia, I Ivkovic, and N Medvidovic. 2013. A Comparative Analysis of Soft- ware Architecture Recovery Techniques. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering; Palo Alto, USA . [12] J Garcia, D Popescu, G Edwards, and N Medvidovic. 2009. Identifying Architec- tural Bad Smells. In CSMR09; Kaiserslautern, Germany . IEEE. [13] I. Herman, G. Melancon, and M. S. Marshall. 2000. Graph visualization and navi- gation in information visualization: A survey. IEEE Transactions on Visualization and Computer Graphics 6, 1 (Jan 2000), 24–43. [14] M Lanza and R Marinescu. 2006. Object-Oriented Metrics in Practice . Springer, Heidelberg. [15] I Macia. 2013. On the Detection of Architecturally-Relevant Code Anomalies in', '[14] M Lanza and R Marinescu. 2006. Object-Oriented Metrics in Practice . Springer, Heidelberg. [15] I Macia. 2013. On the Detection of Architecturally-Relevant Code Anomalies in Software Systems . Ph.D. Dissertation. Ponti/f_ical Catholic University of Rio de Janeiro, Informatics Department. [16] I. Macia, R. Arcoverde, E. Cirilo, A. Garcia, and A. von Staa. 2012. Supporting the identi/f_ication of architecturally-relevant code anomalies. InICSM12. 662–665. [17] I. Macia, R. Arcoverde, A. Garcia, C. Chavez, and A. von Staa. 2012. On the Rele- vance of Code Anomalies for Identifying Architecture Degradation Symptoms. In CSMR12. 277–286. [18] Isela Macia, Joshua Garcia, Daniel Popescu, Alessandro Garcia, Nenad Medvi- dovic, and Arndt von Staa. 2012. Are Automatically-detected Code Anomalies Relevant to Architectural Modularity?: An Exploratory Analysis of Evolving Systems. In AOSD ’12. ACM, New York, NY, USA, 167–178. [19] Robert C. Martin and Micah Martin. 2006. Agile Principles, Patterns, and Practices in C# (Robert C. Martin) . Prentice Hall PTR, Upper Saddle River, NJ, USA. [20] C Mattmann, D Crichton, N Medvidovic, and S Hughes. 2006. A Software Architecture-Based Framework for Highly Distributed and Data Intensive Scien- ti/f_ic Applications. InProceedings of the 28th International Conference on Software Engineering: Software Engineering Achievements Track; Shanghai, China . 721– 730. [21] Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E. Hassan. 2014. The Impact of Code Review Coverage and Code Review Participation on Software Quality: A Case Study of the Qt, VTK, and ITK Projects. InProceedings of the 11th Working Conference on Mining Software Repositories . Hyderabad, India, 192–201. [22] Ran Mo, Yuanfang Cai, R. Kazman, and Lu Xiao. 2015. Hotspot Patterns: The Formal De/f_inition and Automatic Detection of Architecture Smells. InSoftware Architecture (WICSA), 2015 12th Working IEEE/IFIP Conference on . 51–60. [23] Emerson Murphy-Hill and Andrew P Black. 2010. An interactive ambient visu- alization for code smells. In Proceedings of the 5th international symposium on Software visualization; Salt Lake City, USA . ACM, 5–14. [24] W Oizumi and A Garcia. 2015. Organic: A Prototype Tool for the Synthesis of Code Anomalies. (2015). http://wnoizumi.github.io/organic/ [25] W Oizumi, A Garcia, T Colanzi, A Staa, and M Ferreira. 2015. On the Relation- ship of Code-Anomaly Agglomerations and Architectural Problems. Journal of Software Engineering Research and Development 3, 1 (2015), 1–22. [26] W Oizumi, A Garcia, L Sousa, B Cafeo, and Y Zhao. 2016. Code Anomalies Flock Together: Exploring Code Anomaly Agglomerations for Locating Design Problems. In The 38th International Conference on Software Engineering; USA . [27] Jacek Ratzinger, Michael Fischer, and Harald Gall. 2005. Improving evolvability through refactoring. Vol. 30. ACM. [28] W. R. Shadish, T. D. Cook, and Donald T. Campbell. 2001. Experimental and Quasi-Experimental Designs for Generalized Causal Inference (2 ed.). Houghton Miﬄin. [29] Marcelino Campos Oliveira Silva, Marco Tulio Valente, and Ricardo Terra. 2016. Does Technical Debt Lead to the Rejection of Pull Requests?. In Proceedings of the 12th Brazilian Symposium on Information Systems (SBSI ’16) . 248–254. [30] G Suryanarayana, G Samarthyam, and T Sharmar. 2014. Refactoring for Software Design Smells: Managing Technical Debt . Morgan Kaufmann. [31] A. Trifu and R. Marinescu. 2005. Diagnosing design problems in object oriented systems. In WCRE’05. 10 pp. [32] Twitter. 2017. Working at Twitter. (April 2017). Available at https://about.twitter. com/careers. [33] S. Vidal, E. Guimaraes, W. Oizumi, A. Garcia, A. D. Pace, and C. Marcos. 2016. Identifying Architectural Problems through Prioritization of Code Smells. In 2016 X Brazilian Symposium on Software Components, Architectures and Reuse (SBCARS). 41–50. https://doi.org/10.1109/SBCARS.2016.11', '2016 X Brazilian Symposium on Software Components, Architectures and Reuse (SBCARS). 41–50. https://doi.org/10.1109/SBCARS.2016.11 [34] Santiago A. Vidal, Claudia Marcos, and J. Andrés Díaz-Pace. 2016. An Approach to Prioritize Code Smells for Refactoring. Automated Software Engg. 23, 3 (Sept. 2016), 501–532. https://doi.org/10.1007/s10515-014-0175-x [35] Richard Wettel and Michele Lanza. 2008. Visually localizing design problems with disharmony maps. In Proceedings of the 4th ACM symposium on Software visualization. ACM, 155–164. [36] Lu Xiao, Yuanfang Cai, Rick Kazman, Ran Mo, and Qiong Feng. 2016. Identifying and Quantifying Architectural Debt. In Proceedings of the 38th International Conference on Software Engineering (ICSE ’16) . ACM, New York, NY, USA, 11. [37] Yahoo! 2017. Explore Career Opportunities. (April 2017). Available at https: //careers.yahoo.com/us/buildyourcareer. [38] A Yamashita and L Moonen. 2013. Exploring the impact of inter-smell rela- tions on software maintainability: an empirical study. In Proceedings of the 35th International Conference on Software Engineering; San Francisco, USA . 682–691. [39] A. Yamashita, M. Zanoni, F. A. Fontana, and B. Walter. 2015. Inter-smell relations in industrial and open source systems: A replication and comparative analysis. In Software Maintenance and Evolution (ICSME), 2015 IEEE International Conference on. 121–130.']","['REVEALING DESIGN PROBLEMS IN STINKY CODE This  briefin  reports  scieitfc  evideice oi how to help developers to use code smells  for  revealiin  desini  problems  ii the source code. FINDINGS \uf0b7 The fidiigsî of thisî briefig regard the usîe of code sîmellsî for revealiig desîigi problemsî ii the sîource code.    \uf0b7 A  desîigi  problem  isî  a  desîigi  characterisîtc that  iegatvell  impactsî  qualitl  atributesî. Each  desîigi  problem  isî  ofei  located  ii program  locatoisîs  sîuch  asî  packagesî  aid hierarchiesî.  The  prevaleice  of  desîigi problemsî  ii  a  program  mal  lead  to  sîevere coisîequeicesî oi sîoftare qualitl. \uf0b7 Exisîtig sîtudiesî proposîe the aiallsîisî of code sîmellsî  for  asîsîisîtig  developersî  oi  revealiig desîigi problemsî. The aiallsîisî of code sîmellsî cai  be  performed  tith  the  help  of  sîmell detectoi aid visîualizatoi toolsî. \uf0b7 Previousî  sîtudiesî  sîtate  that  the  sîtikier  a program locatoi isîs the more likell it coitaiisî a  desîigi  problem.  Hotevers  there  isî  litle kiotledge  if  developersî  cai  efectvell ideitfl desîigi problemsî ii sîtikier code.  \uf0b7 Through  a  mixed-method  sîtudls  te  asîked profesîsîioial  developersî  to  ideitfl  desîigi problemsî  through  the  aiallsîisî  of agglomerated  aid  ioi-agglomerated  code sîmellsî. \uf0b7 Ai agglomeratoi of code sîmellsî isî a group of iiter-related  code  sîmellsî  occurriig  ii  the sîame program locatoi.  \uf0b7 For examples ai agglomeratoi of code sîmellsî mal occur ii a hierarchl of clasîsîesî (Figure 1). Multple  clasîsîesî  of  sîuch  a  hierarchl  are afected bl code sîmellsî. To ideitfl a desîigi problems  developersî  mal  aiallze  aid correlate  the  iiformatoi  provided  bl  each code sîmell agglomerated ii the hierarchl. \uf0b7 The  overall  precisîioi  of  developersî  usîiig agglomeratoisî  tasî  29%  higher  thai  the precisîioi  of  developersî  usîiig  ioi- agglomerated  code  sîmellsî.  Hotevers  the aiallsîisî  of  multple  code  sîmellsî  aid  their relatoisîhipsî  ii  the  code  isî  sîtll  challeigiig aid tme-coisîumiig to mosît developersî. \uf0b7 Agglomerated code sîmellsî helped developersî to  reveal a fet more  desîigi  problemsî  thai ioi-agglomerated  code  sîmellsî.  Thisî  sîmall difereice  occurred  becausîe  developersî  sîtll sîpeit coisîiderable efort tith the aiallsîisî of irrelevait code sîmellsî ii ai agglomeratoi. \uf0b7 To help developersî ii revealiig more desîigi problemsîs the prioritzatoi of agglomeratoisî isî  required.  Developersî  sîhould  focusî  oi  the mosît relevait agglomeratoisî of code sîmellsîs disîcardiig the irrelevait oiesî. Hotevers there isî io prioritzatoi criteria that isî efectve for ail sîlsîtem.  \uf0b7 Exisîtig raikiig criteria sîhould prioritze thosîe agglomeratoisî that are  cohesive. A cohesîive agglomeratoi isî ai agglomeratoi ii thich all the code sîmellsî are related to the sîames sîiigle desîigi problem. Mail agglomeratoisî are iot cohesîive. Figure 2 sîhotsî ai example of ioi- cohesîive agglomeratoi. \uf0b7 Agglomeratoisî  helped  developersî  to  avoid falsîe  posîitvesî  thei  ideitfliig  desîigi problemsî. Agglomerated code sîmellsî asîsîisîted developersî to iidicate lesîsî falsîe posîitvesî thai ioi-agglomerated code sîmellsî. \uf0b7 For both agglomerated aid ioi-agglomerated code sîmellsîs falsîe posîitvesî occurred becausîe developersî  tere  uiable  to  perform  deeper aiallsîisî of the sîtikl program locatoisî.  \uf0b7 Usîuallls a developer ieedsî to aiallze multple clasîsîesî aid methodsî to sîpot the full exteisîioi of a desîigi problem. Agglomeratoisî helped developersî  to  perform  sîuch  aiallsîisî. Hotevers  thisî  aiallsîisî  tasî  much  more difcult  tith  the  ioi-agglomerated  code sîmellsî. \uf0b7 To  reduce  falsîe  posîitvesîs  developersî  ieed beter sîupportr (1) ai improved visîualizatoi tailored  for  agglomeratoisîs  (2)  oi  demaid examplesî of desîigi problemsîs aid (3) tpsî oi thich  desîigi  problem  each  agglomeratoi mal iidicate.', 'beter sîupportr (1) ai improved visîualizatoi tailored  for  agglomeratoisîs  (2)  oi  demaid examplesî of desîigi problemsîs aid (3) tpsî oi thich  desîigi  problem  each  agglomeratoi mal iidicate. \uf0b7 The usîe of agglomeratoisî for revealiig desîigi problemsî isî promisîiig. Hotevers asî presîeited ii  thisî  briefigs  there  are  mail  opei challeigesî  that  sîhould  be  addresîsîed  before agglomeratoisî cai be usîed ii a tme-efectve maiier.  Keywordsr Desîigi problem Softare Desîigi Code Smell Agglomeratoi Who is this briefin  or? Resîearchersî  aid  practtoiersî  tho tait to  uidersîtaid hot to usîe  code sîmellsî for revealiig desîigi problemsî. Where the fidiins come  rom? A mixed-method sîtudl iivolviig elevei profesîsîioial  sîoftare  developersî  aid tto opei sîource sîlsîtemsî.  What is a mixed-method study? A  sîtudl performed  tith tto  or  more empirical methodsî. What is iicluded ii this briefin? The maii fidiigsî of the origiial mixed- method  sîtudls  aid  brief  iiformatoi about the coitext of the fidiigsî. What is iot iicluded ii this briefin? Desîcriptoisî  about  the  resîearch method  or  detailsî  about  the  resîultsî presîeited ii the origiial paper. For additoial ii ormatoi about  this research: tioizumi.github.io/SBCARS2017 OPUS Research Group: htpr//lesî.iif.puc-rio.br/opusî/ ORIGINAL RESEARCH REFERENCE Oizumi et al. Revealing Design Problems in Stnnk  ooee A Mixeo-Methoo Stuok. Braziliai Slmposîium oi Softare Compoieitsîs Architecturesîs aid Reusîe (SBCARS)s 2017. Finure 1: Example of Hierarchical Annlomeratoi Finure 2: Example of ioi-cohesive Annlomeratoi']","**Title:** Enhancing Design Problem Identification in Software through Code Smell Agglomerations

**Introduction:**  
This Evidence Briefing summarizes a study that investigates how developers can effectively identify design problems in software code by analyzing code smell agglomerations—areas in the code where multiple code smells co-occur. The findings aim to inform software engineering practitioners about the potential benefits and challenges of utilizing code smell agglomerations for identifying design problems.

**Main Findings:**  
1. **Mixed Results on Precision:** The study found that only 36.36% of developers identified more design problems when analyzing multiple code smells (agglomerations) compared to analyzing single smells. However, 63.63% of developers reported a significant reduction in false positives when working with agglomerations, suggesting that while not all developers found more design problems, those who did could do so with greater accuracy.

2. **Challenges in Analysis:** Developers reported that identifying design problems in stinky code (code with multiple smells) was often difficult and time-consuming. They expressed a need for better visualization tools to help them navigate complex code structures, particularly when smells were scattered across class hierarchies or packages.

3. **Need for Prioritization:** Developers indicated that a prioritization mechanism could help them focus on agglomerations that are more likely to indicate design problems, thereby saving time and effort. This is particularly important in large legacy systems where many agglomerations may exist.

4. **Visualization Support:** Participants suggested that improved visualization techniques, such as graph-based representations, could assist in understanding the relationships between different code smells within an agglomeration, making it easier to identify design problems.

5. **Recommendations for Tool Development:** The findings highlight the need for tools like Organic to incorporate features that prioritize relevant agglomerations and provide effective visualization support, which could enhance the identification of design problems in stinky code.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, including developers and project managers, who are involved in code maintenance and improvement. It is particularly relevant for those interested in understanding the implications of code smells and their agglomerations in the context of design problem identification.

**Where the findings come from?**  
The findings are based on a mixed-method study conducted with 11 professional developers, which included both quantitative and qualitative analyses of their effectiveness in identifying design problems using code smell agglomerations.

**What is included in this briefing?**  
This briefing includes key insights into the effectiveness of using code smell agglomerations for identifying design problems, the challenges faced by developers, and practical recommendations for improving tool support in this area.

**For additional information about the original research:**  
Oizumi, W., Sousa, L., Garcia, A., Oliveira, R., Oliveira, A., Agbachi, O.I.A.B., & Lucena, C. (2017). Revealing Design Problems in Stinky Code: A Mixed-Method Study. In XI SBCARS, September 2017, Fortaleza, Ceará, Brazil. [Link to paper](https://doi.org/10.1109/SBCARS.2017.11)."
"['So/f_tware Interoperability Analysis in Practice - A Survey Hadil Abukwaik University of Kaiserslautern Erwin-Schr¨odinger-Straße 1 Kaiserslautern 67663, Germany abukwaik@cs.uni-kl.de Dieter Rombach University of Kaiserslautern Erwin-Schr¨odinger-Straße 1 Kaiserslautern 67663, Germany rombach@cs.uni-kl.de ABSTRACT So/f_tware interoperability property plays a vital role in enabling interoperation in today/f_is system-of-systems, cyber-physical sys- tems, ecosystems, etc. Despite the critical role of interoperability analysis in enabling a successful and meaningful so/f_tware inter- operation, it is still facing challenges that impede performing it eﬀectively and eﬃciently. We performed an online survey of so/f_t- ware engineers with so/f_tware integration experiences to identify the main diﬃculties of performing interoperability analysis. /T_he results con/f_irm that the state of available practical support and current input artifacts used during the analysis are signi/f_icantly perceived as important diﬃculties. Respondents claim a lack of guidelines and best practices for applying interoperability analysis and claim insuﬃciency of shared information about interoperable so/f_tware units. /T_his indicates the need for providing directive and rigorous guidelines for practitioners to follow and to enrich the content of shared documents about interoperable so/f_tware units. CCS CONCEPTS •So/f_tware and its engineering→ Empirical so/f_tware validation; KEYWORDS Conceptual interoperability, interoperability analysis, survey ACM Reference format: Hadil Abukwaik and Dieter Rombach. 2017. So/f_tware Interoperability Analysis in Practice - A Survey. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017,9 pages. DOI: h/t_tp://dx.doi.org/10.1145/3084226.3084255 1 INTRODUCTION In the past, a so/f_tware system was developed by a single organi- zation to provide a tightly focused support for certain tasks and speci/f_ic purposes. However, today’s so/f_tware providers are urged to adopt integration solutions of independent so/f_tware systems built by diﬀerent organizations [3, 5]. Successfully integrating so/f_t- ware units and enabling their meaningful exchange of data and services requires a comprehensive analysis of both their technical constraints (which control the actual exchange of data and services like network protocol, programming languages, data types, etc.) Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. EASE’17, Karlskrona, Sweden © 2017 ACM. 978-1-4503-4804-1/17/06. . . $15.00 DOI: h/t_tp://dx.doi.org/10.1145/3084226.3084255 and their conceptual constraints (which control the meaningful- ness of interoperation results like usage contexts, architectural con- straints, semantics, qualities, etc.). /T_hus, interoperability analysis supports an early detection for so/f_tware mismatches and estimat- ing their impact on the desired interoperation. It supports decision making about the feasibility of completing an integration project within available budget and time. However, according to Krueger [13], organizations adopting in- tegration report success limitations due to the non-technical issues. For example, COTS-based integration projects o/f_ten suﬀer from signi/f_icant overruns in budget and schedule due to unexpected inter- operability mismatches [4]. /T_his indicates problems in performing interoperability analysis. In-depth investigation of these problems', 'operability mismatches [4]. /T_his indicates problems in performing interoperability analysis. In-depth investigation of these problems is important in order to provide be/t_ter support for practitioners to increase the quality of the interoperability analysis results. /T_his paper contributes the identi/f_ication of problems experienced when analyzing the interoperability between two so/f_tware units from the perspective of so/f_tware engineering practitioners with practical experience in integration. We focused on diﬃculties re- lated to the practices along with available methodological support and the input artifacts used in performing the interoperability anal- ysis. To reach a large number of practitioners, we designed and performed an online survey at the mid of 2016. /T_he sample consisted of 64 so/f_tware engineer with experience in so/f_tware integration. In Section 2, we brie/f_ly introduce the concept of so/f_tware interop- erability and its related analysis activity and input/output artifacts. In Section 3, we describe the design of the online survey, while in Section 4, we present the results of the survey along with our discussion. Finally, In Section 5, we summarize the work. 2 INTEROPERABILITY BACKGROUND So/f_tware integration is the process of bringing so/f_tware units to- gether into one so/f_tware system to enable a desired interoperation. Integration projects vary in the type of the external interoperable so/f_tware units that can be integrated within one so/f_tware system. For example, these external units can be commercial-oﬀ-the-shelf (COTS), open source so/f_tware units, web service APIS, platform APIs, etc. On a larger scale, integration plays a vital role in to- day’s evolving so/f_tware systems such as systems-of-systems, cyber- physical systems, and so/f_tware ecosystems. In practice, interoperability is considered as the base or the en- abler for so/f_tware integration [15] and it determines the readiness of a so/f_tware unit for integration. /T_hat is, interoperability is a key property of so/f_tware units to cope with current business demands for so/f_tware integration. Interoperability is de/f_ined by IEEE [12] as “the ability of two or more systems or components to exchange information and to use the information that has been exchanged”.', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Abukwaik and Rombach Beyond the technical exchange of information, our research em- phasizes the importance of having the integrated so/f_tware units conceptually and architecturally aligned to allow meaningful and proper exchange of data and services. Hence, we de/f_ine the concep- tual interoperability of so/f_tware units as “the ability of two or more seperately-developed so/f_tware units to communicate and exchange data and services seamlessly and in a meaningful way”. Having this been said, building successful integration and con- sequently achieving meaningful interoperation starts with an early assessment and reasoning about the properties of the two so/f_tware units that are intended to interoperate. /T_his assessment takes place before adapting, con/f_iguring, or glue-coding the units [6] and it is performed by so/f_tware architects and analysts. We call it “inter- operability analysis” and we de/f_ine it as the process of checking the constraints and assumptions of two so/f_tware units in order to /f_ind if they have any mismatches that impede their desired com- munication. A very critical part of interoperability analysis is the conceptual interoperability analysis, which focuses on checking the conceptual constraints of two so/f_tware units that are intended to interoperate in order to /f_ind if they have any conceptual mis- matches. We de/f_ined the conceptual interoperability constraints [1] as the non-technical characteristics of of a so/f_tware unit that, if mis-assumed, may lead to conceptually wrong, meaningless, or improper interoperation results (i.e., they cause the conceptual mis- matches). We classify the conceptual interoperability constraints into six categories [1] that are: (1) Syntax (i.e., speci/f_ies concept- packaging methods and terminologies), (2) Semantic (i.e., states meaning and intended goals of data and services), (3) Structure (i.e., depicts the unit’s elements, their relations and their arrangements), (4) Dynamic (i.e., restricts the elements behavior during the inter- operation), (5) Context (i.e., pertains to external aspects forming the interoperation se/t_tings), and (6) /Q_uality (i.e., captures the re- quired and provided quality characteritics of exchanged data and services). /T_hus, conceptual interoperability mismatches are caused by conceptual constraints (e.g., diﬀerent usage context, structural multiplicity, interaction order, response time, data resolution, etc.), while technical interoperability mismatches are caused by techni- cal constraints (e.g., con/f_licting network protocols, programming languages, data types, argument names, etc.). /T_he commonly available input artifact for the interoperability analysis process of external so/f_tware units in black-box integration context (where source code is not provided) is a shared documenta- tion about the so/f_tware that is prepared by the so/f_tware provider (e.g., API documentation of web services). /T_his input is used to retain and communicate information about the various aspects of the so/f_tware unit to its audience [10]. /T_herefore, a proper so/f_tware documentation is considered as a necessity for enabling a success- ful so/f_tware integration as it helps in assessing the so/f_tware unit eﬀectively with reasonable eﬀort [16]. With regard to the output of the interoperability analysis process for a so/f_tware unit, it is a list of all the constraints and assumptions of this analyzed unit. Such an output is used to compare it with the list of constraints and assumptions of the overall so/f_tware system (which the analyzed so/f_tware unit is intended to interoperate with) in order to /f_ind if they have any technical or conceptual mismatches. 3 RESEARCH METHODOLOGY 3.1 Research Goal and /Q_uestions It is important to collect the practitioners’ experience on the current state of interoperability analysis to get insights about its main', '3 RESEARCH METHODOLOGY 3.1 Research Goal and /Q_uestions It is important to collect the practitioners’ experience on the current state of interoperability analysis to get insights about its main diﬃculties. Hence, we decided to perform an exploratory study as an online survey. /T_his type of studies is appropriate for approaching a large number of participants and allows collecting a wide range of needed data (e.g., actual experiences or personal opinions) [9, 11]. Add to this, we performed the study in a systematic way, where we followed a prede/f_ined protocol and documented the resuts of each step. /T_his allows the traceability between our study goal, the research questions, the questionnaire design, and the collected data from the participants. It also serves future studies that may aim at replicating or extending our study with larger sample size, diﬀerent contexts, or additional questions. We formulate our survey goal in terms of the GQM goal tem- plate [18] as: to explore the state of practice of interoperability analysis for external so/f_tware unitsfor the purpose ofcharacter- izing its current state and identifying its diﬃculties with respect to its practices and input artifacts in the context ofa survey from the viewpoint of architects and analysts. /T_his forms a basis for developing practically applicable enhancements towards eﬃcient and eﬀective interoperability analysis. We translate this goal into the following research questions (RQs): RQ1: How is interoperability analysis currently being performed by practitioners? RQ2: What are the diﬃculties experienced when performing inter- operability analysis? RQ2.1: with respect to its current practices? RQ2.2: with respect to its input (i.e., available sources of infor- mation about external so/f_tware units)? 3.2 Survey Design and Execution /T_his survey is designed according to the guidelines proposed by [8], [9], [11], and [14]. Target Group. /T_he target group of our survey consists of ar- chitects and so/f_tware engineers with practical experience in in- teroperability analysis in so/f_tware integration projects (including projects in which integration is a part). Accordingly, we invited 115 practitioners known for their experience in so/f_tware integration directly via email and asked them to distribute the call to whoever they deemed appropriate. We also posted the call for experts’ par- ticipation on the websites of professional groups (e.g., LinkedIn special interest groups for architecture, so/f_tware interoperability, and so/f_tware integration). /Q_uestionnaire Structure. Based on our goal and research ques- tions, we designed a questionnaire supported with a 2-minute video1 that introduces the concepts and terminologies of inter- operability analysis that we use in order to ensure correct, mutual understanding. Our survey questionnaire consists of questions that explored the following aspects: - Current state of interoperability analysis practices and input ar- tifacts (RQ1), which covered the respondents’ practical feedback on performing the interoperability analysis task with regard to its 1h/t_tps://www.youtube.com/watch?v=-WUU8sQmn08', 'So/f_tware Interoperability Analysis in Practice - A Survey EASE’17, June 15-16, 2017, Karlskrona, Sweden engineering aspects such as the followed process, the responsible roles, the methods or tools used , and the typically available sources of information that they used. - Perceived diﬃculties (RQ2 including RQ2.1 and RQ2.2), which covered the main diﬃculties that practitioners experience when analyzing the interoperability of external so/f_tware units especially on the conceptual level. /T_his includes the cost of the analysis and of unexpected conceptual mismatches, the lack of knowledge and guidance, the quality and availability of sources of information, the need for automation and tooling support, standard templates for documenting analysis results, etc. - Demographic information, which covered the respondents’ expe- rience in performing interoperability analysis, the type of appli- cations they build and integrate, their position, organization type, sector, size, and location. Type of questions. /T_he questionnaire consists of /f_ixed-set an- swer questions some of single- and some of multi-selection. To avoid closing oﬀ the range of potential answers that respondents may provide, some questions have a choice to enter a free text answer if none of the provided selections applies. All questions related to degree of experience and agreement provide a scale rang- ing from 1 (lowest) to 5 (highest). Also, some questions have the “I don’t know” to avoid forcing answers with the lack of knowledge. /Q_uestionnaire length. /T_he designed questionnaire includes up to 26 questions wri/t_ten in English. We used /f_ilters to avoid overwhelming the participants with irrelevant questions to their actual experience in so/f_tware integration analysis. /T_hat is, detailed questions regarding the actual diﬃculties faced during the interop- erability analysis were not asked to the participants who indicated that they did not perform interoperability analysis in their integra- tion projects. /T_he time needed for answering the questions of the questionnaire was between 10 and 15 minutes. /T_he /f_inal version of the questionnaire is available under a sharing agreement. Pre-execution evaluation. To assess the quality of our de- signed survey, two senior so/f_tware engineers with expertise in interoperability and so/f_tware architecture peer-reviewed it. they checked the relevance and the clarity of the survey questionnaire and its supporting video. A/f_ter revising the survey according to the results of the peer review, one expert in empirical so/f_tware engineering assessed the questions with respect to the principles de/f_ined in [14], and the ethical criteria stated in [ 2]. As a result, some questions were re-categorized and shortened to improve the understandability of the questionnaire. Pilot study. We performed a pilot study with four so/f_tware engineering researchers (with a background in interoperability) and with two computer science researchers (with no background in interoperability) to assess the understandability of the questions and to estimate the time required to answer them. We encouraged the six researchers to take notes on any ambiguous words, uncertainties regarding the meaning of the questions or their answers, and to track the time they spent on /f_illing out the questionnaire. As a result, two questions were classi/f_ied as very complex, so we split them into logical subgroups. Implementation. We implemented the survey using Limesur- vey [17] and deactivated it a/f_ter six weeks of its activation date. In the survey invitation, we explained the survey goal, its scienti/f_ic value, and target group. We also stated the con/f_identiality and anonymity of responses along with the expiry date. According to the recommendations of Dillman et al. [9], we sent two reminder emails for increasing the response rate. /T_he /f_inal dataset collected from the participants is stored in a repository of the AGSE group', 'emails for increasing the response rate. /T_he /f_inal dataset collected from the participants is stored in a repository of the AGSE group of the University of Kaiserslautern. If the reader is interested in further anonymized analysis results, please contact the author. 3.3 Data analysis We analyzed the data using MS Excel and IBM SPSS Statistics 23 [7]. Our descriptive analysis includes the median and frequency. As some of the questions were presented to the respondents condi- tionally, we explicitly report the total number of subjects (N) who answered each question. We performed statistical analysis to ex- plore how signi/f_icantly diﬀerent the ordinal answers are from a speci/f_ic point using the one-sample Wilcoxon signed-rank test [19]. Also, we analyzed the statistical diﬀerence between two groups of respondents based on their experiences using Pearson’s chi-square (χ2) test for binary data and the Mann-Whitney (U) test for ordi- nal data. In addition, we ran a Spearman rho (ρ) test to check the correlation between ordinal variables. In all tests performed, the con/f_idence interval is stated at 95 4 RESULTS AND DISCUSSION In total, we got 73 complete responses for our survey from the targeted group. However, we excluded nine responses as their demographic information showed that they neither had any year of experience in integration nor worked on any integration projects. /T_his ensures credible /f_indings based on actual experiences rather than inexperienced opinions. Accordingly, the /f_inal number of included respondents size was N = 64. Next, we will /f_irst provide a brief overview of the demographic characterization of the respondents, and then answer the research questions by presenting the results we found based on the included 64 responses. 4.1 Background Overview So/f_tware integration experience. /T_he respondents’ experience in so/f_tware integration varied in terms of their years of experience and the number of projects they had participated in (see Fig. 1). /T_he largest shares (35.9% and 30.2%) of years of experience were for “2-5 years” and “> 8 years”, respectively. Most respondents (25.4%) had participated in more than /f_ive projects. Across the diﬀerent integration projects, the respondents reported playing diﬀerent roles. /T_hese roles were frequently reported to be programmers (71.4%), architects (54%), system analysts (39.7%), project managers (34.9%), testers (23.8%), requirements engineers (19%), and techni- cal writers (10%). Single cases reported playing other roles (e.g., DevOps engineer, technical project manager, system expert, and risk assessor). Nature of integration projects. /T_he respondents worked mainly (80.8%) in the domain of information Systems, Mobile Systems (30.1%), and Embedded Systems (26%). /T_hey also reported experi- ences in integrating open source so/f_tware (OSS), commercial-of- the-shelf (COTS) so/f_tware, web service APIs, and platform APIs all with almost equal shares (54.5% on average for each type). /T_he', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Abukwaik and Rombach Figure 1: Experience of the survey respondents with so/f_tware integration. projects’ size was mainly (40.6%) medium (i.e., 3 - 6 months, $250- 750K, 4 - 10 team members) and the remaining share was divided almost equally between large projects (i.e., > 6 months, > $750K, > 10 team members) and small projects (i.e., < 3 months, < $250K, 3 - 4 team members). Nature of work organizations. While, the respondents were mostly (37.1%) employed by enterprises (i.e., with > 1000 employ- ees), very few (only 8.1%) were employed by large organizations (i.e., 251 − 999 employees). /T_he others were equally distributed (27.4%) between small (i.e., with < 50 employees) and medium (i.e., 51/f_i/question_exclam250) organizations. Among the many reported sectors (e.g., agriculture, health, /f_inance, military, automotive, etc.), the so/f_tware development sector was dominant. /T_he data was collected interna- tionally from several locations (e.g., USA, UK, Switzerland, Belgium, China, etc.), but the majority (32.8%) was from Germany. 4.2 Interoperability Analysis: As-Is State in Practice (RQ1) In this subsection, we characterize the current state-of-the-practice with regard to the interoperability analysis actually performed by the respondents to our survey. Neglected interoperability analysis. To our surprise, we found that a large group (42.2%) of the respondents stated that interoper- ability analysis did not take place at all in their integration projects. /T_hese respondents (N = 27; 42.2%) a/t_tributed this behavior to dif- ferent reasons including: R1: /T_here is not enough knowledge and experience about it (37%). R2: Priority is given to other tasks, e.g., implementation and testing (37%). R3: Tight schedule and limited resources (29.6%). R4: It is hard to perform (18.5%). R5: Unit and integration testing are performed instead (18.5%). R6: It is not that necessary (14.8%). Furthermore, some respondents independently provided their reasons for not performing interoperability analysis. For example, one respondent stated that, in the context of integrating so/f_tware units into ecosystems, they could replace this interoperability analy- sis with some negotiation on business rules and so/f_tware interfaces. /T_his shows a business-oriented analysis regarding interoperability, which cannot reveal conceptual so/f_tware mismatches by itslef. An- other respondent, who also selected R6, reported that in his unit they had decided to proceed with integrating units only if they were implemented using the same implementation technologies. /T_his in- dicates a probable awareness problem regarding the consequences of ignoring the early interoperability analysis. Looking at the demographic characteristics of these 27 partic- ipants, we found that the majority were working in small orga- nizations with fewer than 50 employees (35%) on medium size integration projects (40%) with less than 5 years of integration experience (61.53%). Note that, the only statistically signi/f_icant factor correlated to this group of participants (i.e., Spearman’s rho ρ = 0.307, p-value = 0.015) was the reported estimation regarding the rather expensive cost of the analysis, which could be what discourages practitioners from performing it. /T_his indicates that it could be the cost of the analysis that discourages practitioners from performing it. Immature, unstandardized, unsystematic interoperability analysis. /T_he rest of the respondents (N = 37; 57.8%) stated that interoperability analysis actually took place in their integration projects. When? /T_he responses show that the majority (67.6%) performed it at the beginning of the integration projects and before starting the technical implementation. However, many other respondents (24.3%) stated that interoperability analysis happened during the technical implementation. Such an approach would obviously re-', 'the technical implementation. However, many other respondents (24.3%) stated that interoperability analysis happened during the technical implementation. Such an approach would obviously re- quire reworking the implemented parts of the integration if the analysis shows the existence of more mismatches. /T_hrough sta- tistical analysis, we found that the analysis time was signi/f_icantly correlated to integration experience in terms of number of years (i.e., Spearman’s rhoρ = - 0.608, p-value = 0.000) and number of projects (i.e., Spearman’s rhoρ = - 0.679, p-value = 0.000). /T_hat is, experienced analysts recognized the importance of early analysis. Very few respondents (only 2.7%) reported that analysis takes place a/f_ter the technical implementation, which is the worst time (in terms of rework consequences) to detect mismatches. Who? /T_he roles responsible for performing the interoperability analysis task varied. However, architects were most prominent (see Fig. 2) among the roles. We observed that unlike the other roles, testers were never reported to assume responsibility for the', 'So/f_tware Interoperability Analysis in Practice - A Survey EASE’17, June 15-16, 2017, Karlskrona, Sweden Figure 2: /T_he roles responsible for performing interoperability analysis. analysis on their own. Some of the respondents chose one role, while most (72.9%) selected at least two roles, which indicates a form of collaboration on the analysis task. In fact, some explicitly reported that collaboration happened between technical managers and engineering or DevOps team. Also, collaboration between domain experts was reported by one respondent, which would be of great value for analyzing the conceptual level of interoperability. Regarding the size of the analysis team, only three respondents stated that this depended on the project type and size. However, 62.2% said it was small (i.e., < 5 members) although the project size ranged from small to large. Unexpectedly, 66.6% of the respondents, who stated that it was performed by exactly one person worked on large projects. What? Digging deeper, we asked the practitioners to specify the types of information they targeted during the interoperability analysis task. /T_he answers showed that technical aspects were dom- inant. Speci/f_ically, 75.7% targeted the communication constraints (e.g., networking protocols, message formats, etc.) and 64.9% tar- geted the technical syntax (e.g., argument order, data types, etc.). Fewer respondents (58.1% on average) reported being interested in semantic constraints (e.g., terminologies, goals, rationale, etc.) and behavior constraints (e.g., pre-/post- conditions, invariants, interaction protocols, control /f_low, etc.). Minor shares of a/t_tention (48.6% on average) were given to context, structure, and quality. As expected, the covered information aspects were less for those who reported that interoperability analysis was performed by one per- son rather than by a team. For example, one respondent indicated that the analysis was performed by one developer and the only in- formation targeted was the technical communication. Statistically, the signi/f_icantly correlated factor to the information targeted dur- ing analysis was the responsible role (i.e., Spearman’s rhoρ = 0.493 and p-value = 0.002). For example, architects and developers were the main roles who targeted semantic and behavior information. Furthermore, we found a statistical signi/f_icance in the correlation between the years of experience in so/f_tware integration and the targeting of semantic information (i.e., Spearman’s rhoρ = 0.333 and p-value = 0.008). How? To be/t_ter assess the current situation, we asked about the input, process, and output documentation of the interoperability analysis. /T_he answers con/f_irmed our expectation that API doc- umentation was the main available input artifact and source of information as stated by 78.4% of the respondents. Other available sources of information included high-level architecture, require- ments speci/f_ication, and source code. However, these apply to white-box so/f_tware integration (e.g., open source so/f_tware projects) rather than black-box so/f_tware integration (e.g., commercial COTS). One respondent reported that contacting the team members of the external so/f_tware unit was his source of information due to lack of information in the shared artifacts. With respect to the support used for performing the interop- erability analysis, we found that about 30% of the practitioners with knowledge about this issue had no support. Two respondents declared that they did the analysis in an ad-hoc manner, focusing on identifying the gaps between the integration requirements and the capabilities oﬀered by the external so/f_tware units. On the other hand, 13.5% used analysis models and frameworks, 10.8% followed guidelines, 8.1% performed systematic analysis, 8.1% used a tem- plate, and 5.4% had tool support. However, the respondents did not give any further information, details, or references for the reported', 'plate, and 5.4% had tool support. However, the respondents did not give any further information, details, or references for the reported support, with two exceptions. One added that the guidelines fol- lowed had been developed internally and the other one added that they also had an internally de/f_ined process to follow. With regard to documenting the analysis results, we found that about 30% of the practitioners did not document the results of their interoperability analysis. /T_his documentation status had a statistically signi/f_icant correlation with integration experience in terms of the number of projects that the participants had worked on (i.e., Spearman’s rho ρ = -0.438, p-value = 0.009). Consequently, there was a loss of information that would support decision traceability within a project as well as a loss of knowledge that would allow learning from experiences and cases across integration projects. 4.3 Problems in Performing Conceptual Interoperability Analysis (RQ2) In this subsection, we present the evidence collected from practi- tioners on the relevance of the practical problems that are related to interoperability analysis practical support and input artifacts. /T_hen, we shed light speci/f_ically on those problems that are related to the conceptual level of interoperability analysis. We collected data about perceived problems from the entire sample size (N = 64). However, for the questions that depend on actual experience with interoperability analysis (i.e., RQ2.1 and RQ2.2), we diﬀerentiate between the results obtained from those who claimed to perform interoperability analysis (actual experiences) and those who did not (subjective opinions). High cost of interoperability analysis. In Fig. 3, we depict the cost of interoperability analysis reported by the respondents who had knowledge about it. On a scale from 1 (i.e., < 10% of the total cost of integration projects) to 5 (i.e., > 70% of the total cost of integration projects), the largest share of the responses was for the', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Abukwaik and Rombach answer 2, which shows that interoperability analysis accounts for 10% to 30% of the total cost of integration projects. Moreover, a por- tion of the respondents (N = 9) stated that the analysis would range between 30% and 50%. Obviously, such cost ranges are relatively high taking into account the other development activities included in an integration project (e.g., requirements analysis, design, im- plementation, testing, etc.). In one case, a respondent stated that it would even be worse and would reach 51% to 70% of the total cost. Remarkably, this respondent was one of those who reported performing the analysis during the implementation phase of the integration project. Figure 3: Cost for interoperability analysis reported by prac- titioners with knowledge Few responses (N = 12) indicated that the cost of interoperability analysis would be less than 10% of the total cost of integration projects. Most of these respondents had rather low integration ex- perience (i.e., they had worked on 2 - 5 projects). Some respondents (N = 21) did not provide an answer due to a lack of knowledge or traceability information about the cost of interoperability analysis. For example, one respondent stated in the comment box that they did not track the cost of the analysis independently, but rather accumulated it within the whole project cost. Another respondent reported that, in the context of so/f_tware projects that were not dedicated to integration only, the costs for interoperability analysis varied depending on the size of the integration requirements com- pared to the total project requirements. No signi/f_icant correlations were found between the cost of the analysis and the respondents/f_i demographic features. Frequently undetected conceptual interoperability mis- matches. On a response scale ranging from 1 (i.e., Never) to 5 (i.e., Always), the survey respondents, who had knowledge about the frequency of integration problems related to undetected conceptual mismatches (N = 59) stated that this problem was most likely to happen. /T_he majority (52.54%) stated it happened sometimes, while 30.51% said that it was a common issue. Only three respondents stated that unexpected conceptual mismatches were always hap- pening. Very few (N = 7) claimed that this problem was rare and none said that it was never a problem. Statistically, the responses show signi/f_icant agreement (i.e., one-sample Wilcoxon signed-rank Z = 470, p-value = 0.000, andH0: Median = 3: sometimes) regarding the frequency of undetected conceptual mismatches. Expensive resolution for unexpected conceptual interop- erability mismatches. On a scale from 1 (i.e., < 10% of the total cost of integration projects) to 5 (i.e., > 70% of the total cost of integration projects), respondents who had knowledge about the resolution cost (N = 47), reported rather high additional costs for resolving undetected mismatches. As seen in Fig. 4, the majority of the respondents (46.81%) agreed that the total integration project cost increased by 10% to 30%. Furthermore, a considerable portion of them (more than a third) agreed that the added cost would even be 31% to 50%. Note that, the few respondents (6.38%) who said that conceptual mismatches were a rare cause of problems, still agreed that they were expensive (e.g., one stated that it would cost > 70%). Figure 4: Additional project cost for resolving unexpected conceptual mismatches 4.3.1 Problems Related to Practices of Interoperability Analysis (RQ2.1). /T_his question reveals any signi/f_icant diﬃculties faced in perform- ing interoperability analysis with respect to current practices. Perceived need for better practical support for perform- ing interoperability analysis. We oﬀered the survey participants a list of practice-related diﬃculties (D) that would impede perform- ing interoperability analysis and asked them to select which ones', 'ing interoperability analysis. We oﬀered the survey participants a list of practice-related diﬃculties (D) that would impede perform- ing interoperability analysis and asked them to select which ones they considered to be the main ones. /T_his list included: D1: Lack of focus on detecting “conceptual” mismatches compared to “technical” mismaches. D2: Lack of support for traceability between interoperability analy- sis activities and results (i.e., within a project and among projects). D3: Lack of standard templates for consistent documentation of interoperability analysis results. D4: Lack of interoperability analysis guidelines and best practices for practitioners. D5: Undirected collection of information about external so/f_tware units (i.e., no plan or pre-de/f_ined data elements). D6: Posterior collection of information about the external so/f_tware unit (i.e., reactive collection based on rising problems along the project). D7: Manual eﬀort in analyzing the description of external so/f_tware units and in documenting the analysis results. According to the responses ( N = 64), D4, D7, and D1 had the highest agreement among practitioners as seen in Table 1. /T_his pro- vides evidence on the need for helping interoperability analysts and architects identify conceptual mismatches by providing practical', 'So/f_tware Interoperability Analysis in Practice - A Survey EASE’17, June 15-16, 2017, Karlskrona, Sweden Table 1: Perceived diﬃculties of interoperability analysis practices Diﬃculty (D) D1 D2 D3 D4 D5 D6 D7 Total agreements 25 24 23 26 17 9 26 Agreement percentage% 39.06 37.50 35.94 40.63 26.56 14.06 40.63 Test statistics a χ2 0.080 0.961 3.025 6.723 0.225 0.337 0.249 p 0.777 0.327 0.082 0.010* 0.635 0.562 0.618 Test statistics b ρ -0.035 0.123 0.217 0.324 0.059 -0.073 -0.062 p 0.781 0.335 0.084 0.009** 0.641 0.569 0.624 a Pearson’s chi-square (χ2) test H0: agreement percentage (respondents who performed interoperability analysis) = agreement percentage (respondents who did not perform interoperability analysis) b Spearman’s rho (ρ) test H0: /T_here is a correlation between the agreement percentage and the respondents’ group (performed interoperability analysis or not); * p <0.05 ; ** p <0.01, *** p <0.001 guidelines and automation tools. Furthermore, also D2 and D3 had a considerable amount of agreement. Obviously, these two essential diﬃculties are related, as the ability to trace interoperability anal- ysis results requires documenting them. Accordingly supporting practitioners with standard documentation templates would serve the aforementioned traceability need along with other bene/f_its such as consistency and readability. Although D5 and D6 got the least shares of agreements, there were still practitioners who agreed on the importance to overcome them. Hence, directed analysis with proactive preparation can enhance the analysis experience and the results for some analysts and so/f_tware architects. A/f_ter a more thorough investigation, we found one statistically signi/f_icant diﬀerence between the agreement percentages on D4 of the two groups of surveyed practitioners (i.e., those who performed interoperability analysis in their so/f_tware integration projects and those who did not). In fact, this diﬀerence was also justi/f_ied by the statistical signi/f_icance of the correlation between the group type and the agreement on D4. Although, there were some other percentage diﬀerences between the answers per group; however, they were not statistically signi/f_icant. For example, D2 and D3 had more votes by practitioners inexperiened in performing the analysis task (21.12%, and 12.01%, respectively). /T_hus, we conclude that all reported diﬃculties are important for both groups, but overcoming D4 would be of higher value for inexperienced practitioners. 4.3.2 Problems related to the Input Artifacts of Interoperability Analysis (RQ2.2). /T_his question reveals any signi/f_icant diﬃculties faced in perform- ing interoperability analysis with respect to current input artifacts. /T_his includes the content and presentation of the input. Perceived insuﬃciency of shared information. According to the respondents with knowledge about the current input artifacts of interoperability analysis (N = 59), they mostly (37.50%) reported that these artifacts were “Not suﬃcient”. In other words, on the 5- point Likert scale, the main rating was 2: insuﬃcient. Statistically, the agreement on insuﬃciency was signi/f_icant (i.e., one-sample Wilcoxon signed-rank Z = -4.76, p-value = 0.000, and H0: Median = 4: suﬃcient) from both groups (i.e., those who performed interop- erability analysis and those who did not). Furthermore, we found a statistically signi/f_icant agreement from both groups of respondents on this issue (i.e., Mann-Whitney U = 452, p-value = 0.497, and H0: median (respondents who performed interoperability analy- sis) = median (respondents who did not perform interoperability analysis)). Perceived need for enhancing conceptual information con- tent. /T_he respondents (N = 64) voted for what they perceived as required enhancements for the content of input artifacts used in the interoperability analysis task. We oﬀered a list of interoperability- related content (C) and asked the respondents to select what they', 'interoperability analysis task. We oﬀered a list of interoperability- related content (C) and asked the respondents to select what they considered to be important for enhancing the content related to them. /T_his list included the following: C1: Communication constraints (e.g., networking protocols, mes- sage formats, etc.). C2: Syntax constraints (e.g., argument order, data types, etc.). C3: Semantic constraints (e.g., glossaries, goals, rationale, etc.). C4: High-level architecture view (e.g., architecture style, design pa/t_terns, etc.). C5: Low-level design decisions (e.g., inheritance, synchronicity, concurrency, etc.). C6: Behavior constraints (e.g., pre/post conditions, interaction pro- tocols, control /f_low, etc.). C7: Context constraints (e.g., stakeholders, environments, user ex- perience, use cases, etc.). C8: /Q_uality constraints (e.g., data precision, service reliability, re- sponse time, etc.). Based on the responses ( N = 64), C4 got the largest share of practitioners’ interest (see Table 2). /T_his evidently indicates the serious need to enrich shared documents about interoperable so/f_t- ware units with high-level architecture to improve the analysis. Next, C1 and C6 got substantial agreement, which gives them high priority too. Note that C1 is a technical type of content, while C6 is conceptual. A/f_terward, C3, C8, C2, and C5 got convergent large shares of respondents’ agreement. /T_his shows there is awareness of the need to improve the quality, semantics, syntax and low-level de- sign information of interoperable units. Although C7 got the lowest share of agreements, 24 practitioners still agreed that it is important. /T_hese results denote potential improvement for interoperability analysis results when the content issues are resolved. A more thorough investigation showed us that there was one sta- tistically signi/f_icant diﬀerence between the agreement percentages on C7 from the two respondent groups (i.e., those who performed interoperability analysis in their so/f_tware integration projects and', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Abukwaik and Rombach Table 2: Perceived need to enhance the content of input artifacts for interoperability analysis Content problem (C) C1 C2 C3 C4 C5 C6 C7 C7 Total agreements 37 29 31 39 27 36 24 31 Agreement percentage% 57.81 45.31 48.44 60.94 42.19 56.25 37.50 48.44 Test statistics a χ2 0.098 2.705 2.430 0.568 0.098 1.246 4.651 1.108 p 0.755 0.100 0.119 0.451 0.755 0.264 0.031* 0.293 Test statistics b ρ -0.039 -0.206 -0.195 -0.094 0.039 -0.140 -0.270 -0.132 p 0.759 0.103 0.123 0.459 0.759 0.272 0.031* 0.300 a Pearson’s chi-square (χ2) test H0: agreement percentage (respondents who performed interoperability analysis) = agreement percentage (respondents who did not perform interoperability analysis) b Spearman’s rho (ρ) test H0: /T_here is a correlation between agreement percentage and respondents’ group (performed interoperability analysis or not); * p <0.05 ; ** p <0.01, *** p <0.001 those who did not). In addition, there was a statistical signi/f_icance of the correlation between the group type and the agreement on C7. /T_he percentage diﬀerences between the two groups on the content problems not statistically signi/f_icant. For example, C2 and C3 had more votes by experienced practitioners in performing the analysis task (almost 20% each). We conclude that all content items are important for both groups, however, experts of interoperabil- ity analysis perceived them of higher importance (especially C7) compared to inexperienced respondents. Perceived need for enhancing conceptual information pre- sentation. According to the respondents (N = 64), some enhance- ments are required for the presentation of the input artifacts used in the interoperability analysis task. Out of the list of presentation enhancements (P) that we suggested in the survey, the respondents voted for those they perceived as being the most important ones. /T_his list included the following: P1: Mixing conceptual and technical constraints without clear bor- ders between them. P2: Unstructured verbose text. P3: Lack of easy-to-read process diagrams (e.g., /f_lowcharts). P4: Inconsistency in reporting constraints for the diﬀerent data items and services. P5: Level of formality too low, which prevent potential automation of the analysis. Based on the responses ( N = 64), P1 and P3 got the highest agreement in equal amounts of the practitioners’ interest (see Table 3). /T_his points out a critical need to improve the structure of the information shared about interoperable so/f_tware units in order to clearly diﬀerentiate between conceptual and technical information. Moreover, an abstract process view could enhance these documents. A/f_terwards, the respondents voiced considerable agreement on P4 and P2, which shows that structure and consistency in present- ing content among equal elements could improve the usefulness of shared artifacts. Although P5 got the lowest agreement by a large number of practitioners, 20 practitioners still agreed that it is an important presentation issue. Apparently, resolving all the presentation problems for interoperability analysis input artifacts on which the respondents agreed would add value to their users. /T_he statistical test results showed that there was one statistically signi/f_icant diﬀerence between the agreement percentages on P3 of the two respondent groups (i.e., those who performed interoperabil- ity analysis in their so/f_tware integration projects and those who did not). Moreover, a statistical signi/f_icance of the correlation between the group type and the agreement on P3 was found. Hence, we conclude that enhancing the presentation of the content of input artifacts for interoperability analysis would be of value for both experienced and inexperienced analysts. However, inexperienced ones will appreciate it more if the processes were abstracted in diagrams rather than in unstructured text. 5 THREATS TO VALIDITY 5.1 Internal Validity', 'ones will appreciate it more if the processes were abstracted in diagrams rather than in unstructured text. 5 THREATS TO VALIDITY 5.1 Internal Validity Content validity. As we described earlier in the survey design (see Section 3.2), we did multiple peer reviews with experts in so/f_tware architecture, so/f_tware engineering, and empirical research. Furthermore, we evaluated the survey in pilot studies to assess the understandability of the questionnaire. External factors: Environment, surrounding conditions, and interactions with other participants can in/f_luence the answers of a respondent. However, in our online survey it was less likely that the participants aﬀected the responses of each other as they were geographically distributed. 5.2 External Validity Representative sample. /T_he /f_inal number of included responses was (N = 64). /T_hese included so/f_tware architects and engineers with integration experience from diﬀerent organizations, industrial domains, and locations. Although we included LinkedIn (which has limitations on restricting the invited sample) in our survey invitation strategy, we designed our demographic questions in the questionnaire to help us in identifying and excluding any partici- pant with irrelevant background and experiences. /T_hus, we assume that our results are very likely representative for the state-of-the- practice of interoperability analysis as of June 2016. However, for be/t_ter generalization and observations over time, further surveys with a larger sample size are required. Completion rate. As peer reviews considered the questionnaire to be long, we shortened it to increase the questions’ completion rate. Also, to collect reliable responses based on genuine experi- ences, we had conditional appearance of questions and oﬀered the', 'So/f_tware Interoperability Analysis in Practice - A Survey EASE’17, June 15-16, 2017, Karlskrona, Sweden Table 3: Perceived need for enhancing the presentation of input artifacts for interoperability analysis Presentation problem (P) P1 P2 P3 P4 P5 Total agreements 30 25 30 27 20 Agreement percentage % 46.88 39.06 46.88 42.19 31.25 Test statistics a χ2 0.706 0.055 4.854 0.040 0.616 p 0.401 0.814 0.028* 0.841 0.432 Test statistics b ρ -0.105 0.029 0.275 -0.025 -0.098 p 0.409 0.818 0.028* 0.844 0.440 a Pearson’s chi-square (χ2) test H0: agreement percentage (respondents who performed interoperability analysis) = agreement percentage (respondents who did not perform interoperability analysis) b Spearman’s rho (ρ) test H0: /T_here is a correlation between agreement percentage and respondents’ group (performed interoperability analysis or not); * p <0.05 ; ** p <0.01, *** p <0.001 option to answer “I don’t know” to questions referring to knowl- edge rather than opinions. As a result, we got completely answered questionnaires from all respondents. 6 SUMMARY AND CONCLUSION In this paper, we have presented a consolidated description of the current state of practice of so/f_tware interoperability analysis. We started by describing the as-is situation, which revealed that 30% of practitioners do not perform interoperability analysis in their integration projects. /T_he main reasons behind this were found to be a lack of knowledge about how to perform it and the lack of awareness regarding its importance, which leads to prioritizing other tasks over it. On the other hand, practitioners who performed interoperability analysis showed us that current state of interop- erability analysis was immature. More speci/f_ically, there was no standard or systematic activities being followed (e.g., some per- formed interoperability analysis during the implementation rather than before it) and there was no comprehensive investigation to /f_ind interoperability information during the analysis (e.g., very few of the practitioners with li/t_tle experienced targeted conceptual in- formation). A/f_terwards, we put our hands on the exact diﬃculties related to both the practices and the input artifacts of the conceptual interoperability analysis task. In light of the diﬃculties identi/f_ied in this survey, we see that so/f_tware engineering researchers should (1) develop rigorous in- teroperability analysis approaches and automation tools with com- prehensive coverage for both conceptual and technical constraints, (2) identify criteria for deciding the cost of resolving the diﬀer- ent types of interoperability mismatches, (3) deriving guidelines for improving the content and presentation of shared information about interoperable so/f_tware unit, and (4) developing guidelines for applying interoperability analysis as well as for standard templates for results to allow reuse experiences and decisions. ACKNOWLEDGMENTS /T_his work was funded by the Ph.D. Program and the Nachwuchsring of the University of Kaiserslautern. We thank Liliana Guzm`an and Ma/t_thias Naab for reviewing the study design and the questionnaire. /T_he authors would also like to thank the anonymous reviewers for their valuable comments. REFERENCES [1] Hadil Abukwaik, Ma/t_thias Naab, and Dieter Rombach. 2015. A Proactive Support for Conceptual Interoperability Analysis in So/f_tware Systems. InWICSA’15. [2] British Sociological Association. 2013. Statement of ethical practice for the British Sociological Association. (2013). www.britsoc.co.uk [3] Victor Basili and Dieter Rombach. 1991. Support for comprehensive reuse. 6 (1991), 303–316. Issue 5. [4] Jesal Bhuta. 2007. A framework for intelligent assessment and resolution of commercial-oﬀ-the-shelf product incompatibilities. Pro/Q_uest. [5] Barry Boehm and Chris Abts. 1999. COTS integration: Plug and Pray? Computer 32, 1 (1999), 135–138. [6] Barry Boehm, Dan Port, Ye Yang, Jesal Bhuta, and Chris Abts. 2003. Compos-', '[5] Barry Boehm and Chris Abts. 1999. COTS integration: Plug and Pray? Computer 32, 1 (1999), 135–138. [6] Barry Boehm, Dan Port, Ye Yang, Jesal Bhuta, and Chris Abts. 2003. Compos- able process elements for developing COTS-based applications. International Symposium on Empirical So/f_tware Engineering. [7] IBM Corporation. 2010. IBM SPSS Statistics for Windows, Version 23.0. (2010). [8] Johnnie Daniel. 2011.Sampling essentials: Practical guidelines for making sampling choices. Sage. [9] Don A Dillman, Jolene D Smyth, and Leah Melani. 2011. Internet, mail, and mixed-mode surveys: the tailored design method. JSTOR. [10] Andrew Forward. 2002. So/f_tware documentation: Building and maintaining artefacts of communication.University of O/t_tawa (Canada). [11] Floyd J Fowler. 1995. Improving survey questions: Design and evaluation. Vol. 38. Sage. [12] Anne Geraci and others. 1991. IEEE standard computer dictionary: Compilation of IEEE standard computer glossaries. [13] Charles Krueger. 1992. So/f_tware reuse. 24, 2 (1992), 131–183. [14] Steinar Kvale. 2008. Doing interviews. Sage. [15] Steﬀen Olbrich, Balthasar Weitzel, Dominik Rost, Ma/t_thias Naab, and Gilb Kutepov. 2011. Decmposing Interoperability: A /Q_uality A/t_tribute in the Balance of System Usage, Operation and Development. Technical Report. [16] Johannes Sametinger. 1997. So/f_tware engineering with reusable components. Springer Science & Business Media. [17] LimeSurvey Project Team. 2015. LimeSurvey: An Open Source survey tool. (2015). h/t_tp://www.limesurvey.org [18] Rini Van Solingen, Vicor Basili, Gianluigi Caldiera, and H Dieter Rombach. 2002. Goal Question Metric (GQM) Approach. Encyclopedia of so/f_tware engineering (2002). [19] RF Woolson. 2008. Wilcoxon Signed-Rank Test. Wiley encyclopedia of clinical trials (2008).']","['SOFTWARE INTEROPERAB ILITY ANALYSIS IN  PRACTICE     This briefing reports evidence on the  current difficulties faced by practitioners  in performing effective and efficient  software interoperability analysis based  on scientific evidence from a survey study.      FINDINGS    The find ings presented in this briefing   define software interoperability analysis as  the process of checking the constraints and  assumptions of two software units in order  to find if they have any mismatches that  impede their desired communication.    The as -is state of practice for software  interoperability analysis shows that a large  group of practitioners ( 40.2%) neglect to  perform interoperability analysis mainly  due to t he lack of knowledge and  experience about it and due to task  prioritization reasons.    Analysis results, for data collected from   those who actually performed  interoperability analysis, indicate that the  current performed interoperability  analysis is immature, unstandardized, and  unsystematic.     The findings describe that performing  interoperability analysis is of high cost   (mostly reported to range from 10% to 30%  of the project cost ). However, not  performing it and facing unexpected  mismatched later in the project is even  more expensive (mostly reported to range  from 10% to 50% of the project cost).      The evidence shows a significantly  perceived need for better practical support  to overcome practical difficulties that  include the following:   \uf0b7 Lack of focus on detecting “conceptual”  mismatches compared  to “technical”  mismatches (39.1%)          \uf0b7 Lack of support for traceability between  interoperability analysis  activities and  results within a project and among  projects (37.5%)   \uf0b7 Lack of interoperability analysis  guidelines and best practices  for  practitioners (40.6%)  \uf0b7 Manual effort in analyzing the external  software units and in documenting the  analysis results (40.6%)    The evidence reveals a perceived need for  enhancing the content of the input artifact,  which is used in software interoperability  analysis, including:     \uf0b7 Communication constraints  like  network protocols, message  formats,  etc. (57.8%)  \uf0b7 Semantic constraints like glossaries,  goals, rationale, etc. (48.4%)  \uf0b7 High-level architecture view like  architecture style  and design patterns,  etc. (60.9%)  \uf0b7 Behavior constraints like pre/post  conditions, interaction  protocols,  control flow, etc. (66.2%)    The survey results also present a significant  perceived need to enhance the  presentation of information in the input  artifact used in interoperability analysis.  This includes overcoming the following  problems:  \uf0b7 Mixing conceptual and techn ical  constraints without clear borders  between them (46.9%)  \uf0b7 Unstructured verbose text (39.1%)  \uf0b7 Lack of easy -to-read pro cess diagrams  like flow charts (46.9%)  \uf0b7 Inconsistency in r eporting constraints  for the different data items and services  (42.2%)  Who is this briefing for?    Software engineering practitioners who  want to know about the problems  experienced when analyzing the  interoperability between software units  within integration projects based on  scientific evidence.      Where the findings come from?    All findings of this briefing were  extracted from the online survey  conducted by Hadil Abukwaik and  Dieter Rombach.        What is included in this briefing?    The main findings of the conducted  survey with software engineers with  software integration experiences in the  industry about the faced difficulties in  performing software interoperability  analysis.      What is not included in this briefing?    Additional information not presented in  the original survey paper.    Detailed descriptions about the original  survey design and execution.      To access other evidence briefings  on software engineering:    http://ease2017.bth.se/      For additional information about  AGSE (Software Engineering', ""survey design and execution.      To access other evidence briefings  on software engineering:    http://ease2017.bth.se/      For additional information about  AGSE (Software Engineering  Research Group: Process and  Measurement:    http://agse.cs.uni-kl.de/          ORIGINAL RESEARCH REFERENCE    Hadil Abukwaik and Dieter Rombach. Software Interoperability Analysis in Practice – a Survey. In Proceedings of the 21st International Conference on Evaluation and  Assessment in Software Engineering (EASE'17). ACM, New York, NY, USA, 12-20. DOI: https://doi.org/10.1145/3084226.3084255""]","**Title:** Enhancing Software Interoperability Analysis: Key Challenges and Recommendations

**Introduction:**
This evidence briefing aims to summarize the findings from a survey conducted by Hadil Abukwaik and Dieter Rombach on the current state of software interoperability analysis. The goal is to identify the main challenges faced by practitioners in performing interoperability analysis and to suggest actionable recommendations that can improve the effectiveness and efficiency of this critical process.

**Main Findings:**
1. **Prevalence of Neglected Analysis:** A significant portion of respondents (42.2%) reported that interoperability analysis was not performed in their integration projects. Reasons included a lack of knowledge, prioritization of other tasks, and tight schedules. This indicates a need for increased awareness of the importance of interoperability analysis.

2. **Immature Practices:** Among those who did perform interoperability analysis, the process was often unstandardized and inconsistent. Many practitioners conducted the analysis during the implementation phase rather than before, leading to potential rework and increased costs.

3. **Lack of Guidelines and Tools:** Respondents highlighted a critical shortage of guidelines and best practices for conducting interoperability analysis, particularly at the conceptual level. Most reported that they relied heavily on API documentation as their primary input artifact, which was often insufficient for comprehensive analysis.

4. **High Costs and Undetected Mismatches:** The cost of interoperability analysis was reported to account for 10% to 30% of total integration project costs, with some respondents indicating it could be as high as 70%. Additionally, many practitioners acknowledged frequent undetected conceptual mismatches, which could lead to costly resolutions later in the project.

5. **Need for Enhanced Documentation and Presentation:** There is a strong demand for improved content and presentation of input artifacts used in interoperability analysis. Respondents indicated the importance of structured, clear documentation that distinguishes between technical and conceptual information to facilitate better understanding and analysis.

6. **Recommendations for Improvement:** Based on the findings, it is recommended that:
   - Organizations develop comprehensive guidelines and standard templates for interoperability analysis to aid practitioners.
   - Enhance the quality and availability of shared information about interoperable software units, focusing on both technical and conceptual aspects.
   - Invest in tools and automation to assist with interoperability analysis and documentation processes.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and organizational leaders involved in software integration projects who seek to improve interoperability analysis practices.

**Where the findings come from?**
The findings are based on an online survey conducted with 64 software engineers who have practical experience in software integration, as detailed in the paper ""Software Interoperability Analysis in Practice - A Survey"" by Hadil Abukwaik and Dieter Rombach, presented at EASE’17.

**What is included in this briefing?**
This briefing includes insights into the current challenges faced in interoperability analysis, practitioners' experiences, and recommendations for enhancing practices and documentation.

**For additional information about the original research:**
Abukwaik, H., & Rombach, D. (2017). Software Interoperability Analysis in Practice - A Survey. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017. DOI: [10.1145/3084226.3084255](http://dx.doi.org/10.1145/3084226.3084255)"
"['Feasibility of using Source Code Changes on the  Selection of Text-based Regression Test Cases  Joelson Araújo  Informatics Center, Federal University of  Pernambuco  Recife, Brazil  jisa@cin.ufpe.br  Jaedson Araújo  Informatics Center, Federal University of  Pernambuco  Recife, Brazil  jboa@cin.ufpe.br  Cláudio Magalhães  Informatics Center, Federal University of  Pernambuco  Recife, Brazil  cjasm@cin.ufpe.br   João Andrade  Informatics Center, Federal University of  Pernambuco  Recife, Brazil  jlan@cin.ufpe.br  Alexandre Mota  Informatics Center, Federal University of  Pernambuco  Recife, Brazil  acm@cin.ufpe.br    ABSTRACT  This paper investigates the relationship between recently modified  source code and the selection of text-based regression test cases.  The main reason is that our industrial partner uses release notes  documents to perform such a selection, but these documents are  not so well-written as the text-based test cases. Therefore, we  intend to extract useful text from source code to see whether they  can serve as a source of keywords in the selection process. We  present an experiment that shows promising results about this  hypothesis.  CCS CONCEPTS  • Software defect analysis → Software testing and debugging;  • Information systems → Information Retrieval  KEYWORDS  Source code; Test case selection; Information retrieval;  Regression campaign.  1 INTRODUCTION  During software development, changes occur to fix bugs or  improve functionality. A change in the system source code  comprises [1]: 1) understanding how the change affects the  system; 2) implementing the proposed change and 3) testing the  new version of the system to see whether the change did not  introduce new bugs as well as preserved the previously accepted  behavior of the system.      \uf020Permission to make digital or hard copies of all or part of this work for personal or  classroom use is granted without fee provided that copies are not made or distributed  for profit or commercial advantage and that copies bear this notice and the full  citation on the first page. Copyrights for components of this work owned by others  than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,  or republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from permissions@acm.org.    SAST, September 18–19, 2017, Fortaleza, Brazil   © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503-5302-1/17/09\uf0bc$15.00   https://doi.org/10.1145/3128473.3128481  Regression testing is the kind of software testing responsible for  checking whether the new source code does not modify  previously accepted functional behavior, fix previously detected  bugs and does not introduce new bugs.  Each change requires an analysis to identify which tests need to  be run to check the impact of that change. Regression testing  suggests several manners to do that, where the hardest is to run all  the tests. As running all tests is infeasible in general, some  selection is performed in almost all cases. And when test cases are  not automated, this task requires a lot of manual effort. In general,  a test architect is responsible for making such a selection which  requires in-depth knowledge about the application so that tests are  known to exercise the applied changes.  This research is being done in an industrial environment. And as  such, we try to support testers and test architects. Previously, we  created a tool named Auto Test Plan (ATP) [2]. This tool  automates several tasks originally executed manually by test  architects, together with the use of Information Retrieval [14] and  Test Selection [15][16][17] techniques therefore we can offer the  best selection of test cases based on natural language artifacts  (release notes and test cases). This is particularly challenging due  to the freeness we can find in these artifacts.', 'best selection of test cases based on natural language artifacts  (release notes and test cases). This is particularly challenging due  to the freeness we can find in these artifacts.  In this paper, we propose to modify the selection mechanism  performed by ATP in the sense of considering source code as a  resource of keywords instead of the keywords found in the release  notes. The reason is that release notes are not so well-written as  test cases, both being written in natural language. Thus we present  ATPCode as a variant of the original ATP.  The main contributions of this work are:    \uf0b7 A process that generates a relevant keywords list from a  file containing source code about recent changes;  \uf0b7 Evidences that the source code can be used to perform  regression test selection;  \uf0b7 A tool called Auto Test Plan Code, where the process  proposed is implemented.    This work is organized as follows. In Section 2 we introduce the  main concepts used in this work, namely regression testing,  information retrieval and code coverage. Section 3 presents the  tool developed to consider source code in the selection of textual- based regression test cases. We perform an experiment with some', '2    phases to check our approach and improve our results in Section  4. Finally, our conclusions, related and future work are discussed  in Section 5.  2 BACKGROUND AND RELATED WORKS    This section presents basic concepts related to regression tests,  information retrieval and code coverage analysis. Then, we  present related works about test selection.  2.1 Regression Tests  The test activity can be understood as a dynamic product analysis,  essential for the identification and elimination of persistent errors.  The information generated by the test execution plays a  significant role in the activities of debugging, maintenance and  estimation of software reliability [4]. Although this activity is  considered to be costly, Myers affirms that knowledge about  software testing one of the aspects and/or activities of software  development [5].  In general, the software product test involves four steps: test  planning, test case design, execution and evaluation of the test  results.  The regression tests are a phase applied to a new software version  or the need to run a new test cycle during the development  process. It consists of applying, with each new version of the  software or cycle, all the tests that have already been applied in  previous versions or test cycles of the system. Therefore, such  testing is necessary to ensure that modifications to the program  have not caused any new errors [6].  Regression testing does not match a test level, but it is considered  an important strategy for reducing side effects that may arise  during software development. They can be applied at any test  level, such as unit, integration, system or acceptance.  Efficient regression testing is necessary, even crucial, for  organizations with a large share of their cost in software  development. In this context, a test selection technique for  regression testing tries to find a subset of TCs that satisfies certain  goals, relevant to a change in the system under test [2]. So, we  used a new approach to this, using the modified source code to  extract keywords and we consider the hypothesis there is a  relation between the code and the TCs. To validate this, we  developed an experiment to examine the relationship and check if  it works like expected. More details are present in the next  section.  2.2 Information Retrieval  The definition of information retrieval can be understood from its  name, but it can be quite broad. So to delimit its definition we will  use:  “IR is finding material (usually documents) of an  unstructured nature (usually text) that satisfies an  information need from within large collections (usually  stored on computers)” [7].  The IR usually is a specific activity of some people, such as  librarians and professional researchers, but as the world changes,  people began to use it in their day-to-day, even without realizing,  for example, the simple the act of doing searches in the email box.  Thus, IR is quickly becoming the predominant way to access to  information.  The IR also works with other data types and problems beyond the  one exemplified above, so that we can observe the data into two  broad groups: the structured and the unstructured. The term  unstructured data refers to data which does not have a clear,  semantically open, easy structure for a computer. On the other  hand, structured data can be, for example, a relational database  that companies typically use to maintain product inventories and  personnel records. In this context, Manning [7] affirms that in  fact, no data is completely unstructured, considering a data in text,  since in a known way there is a linguistic structure. Most texts  have structure, such a little, paragraphs, and footnotes, which is  commonly represented in documents by explicit marking (such as  web page encoding).  The IR employs three main tasks: text preprocessing, indexing,  and retrieval. Preprocessing involves text normalization, stop-', ""web page encoding).  The IR employs three main tasks: text preprocessing, indexing,  and retrieval. Preprocessing involves text normalization, stop- word removal, and stemming. A text normalizer removes  punctuation, performs case-folding, tokenized terms, etc. In the  stop-word removal phase, an IR application discards the  frequently used terms such as prepositions, articles, and so on, to  improve efficiency and reduce spurious matches. Finally,  stemming combines variants of the same term (for instance, see,  seeing, saw) to improve term matching between query and  document. Then, documents are indexed for fast retrieval. Once  indexed, queries are submitted to the search engine, which returns  a ranked list of documents in response. Finally, the search engine  is evaluated by measuring the quality of its output ranked-list  relative to each user's input query. In this paper, we use the text  processing framework Lucene [3].  2.3 Code Coverage  Code coverage is a common task in the testing process used for  marking the source code segments that were executed and those  that were not [12]. It can express like a measure that captures the  degree to which some region of the source code of an application  is executed after a test suite campaign. In this way, it can be used  by software developers and sometimes by vendors to indicate  their confidence in the readiness of their software [13].  The coverage is just calculated as the division (\u0bbc\u0be2ௗ\u0bd8\u0cd0ೣ\u0cd0\u0cce \u0bbc\u0be2ௗ\u0bd8ೝ\u0cd0\u0cd2\u0cd4\u0cda\u0cd9 ) of the  part of the source code that was executed (𝐶𝑜𝑑𝑒ୣ୶ୣୡ) by the full  amount of source code corresponding to the regions of the  application being analysed (𝐶𝑜𝑑𝑒୰ୣ\u0b65୧୭୬); this provides a  percentage.  The higher the code coverage, the bigger is the amount of source  code executed during a testing execution. This coverage suggests  a lower chance of escaping undetected bugs compared to an  application with low code coverage. Several different metrics can  be used to calculate code coverage. For object-oriented  applications, one can measure classes, methods, statements,  conditionals, etc., exercised. This list is in the fine-grained  direction. That is, class coverage is less accurate than conditionals  coverage.  2.4 Related works  About test selection, an interesting work is reported in [8]. Its test  selection is based on test suite reduction where they evaluate  several trade-offs in test suite reduction algorithms, evaluating  adequate test suite reduction with requirements defined by killed"", '3 mutants and evaluating inadequate reduction that conduct  reduction without satisfying all the requirements as the original  test suite. They introduced a novel evaluation metrics based on  software evolution to retain as most as possible of the original  properties in terms of source code and requirements coverage.  And they concluded that test suite reduction algorithms are robust  to software evolution, regardless of the requirements used. In our  approach we were interested in obtaining a better coverage within  a set of TCs selected using information extracted from source  code. But, in addition, we can also reduce the number of TCs with  a similar coverage when we compared with [2] in our final result  without use any specific algorithm for this. Also, using source  code, the work reported in [9] focuses on features extraction. The  authors aims at identifying what information they can extract from  the source code. They sought answers to two questions: what  should one extract from the source code? and what is important to  take into account to determine that the information that has been  extracted is a relevant information? They conducted an  experiment using an eclipse plugin known as FLAT3 (Feature  Location and Textual Tracing Tool). They used latent semantic  indexing to find the semantic similarity among source code and  test cases modifying steaming and preprocessing techniques and  demonstrated that the division identifiers in the process have a  higher impact on the semantics and, thus, it is possible to select  the most appropriate test cases. In our approach we did not use  any technique to analyze the similarity between the words  extracted from source code and TCs because the relationship was  identified during our study and our results were good and  promising to the partner’s context. But we know that it will be  important to be treated in the future.  Other works use mathematical models to select regression test  cases such as [10, 11]. While the work [10] reduces a test plan by  checking dependency in EFSM structures, the work [11] reduces  by applying a similarity algorithm on transition systems. Although  both use some kind of similarity algorithm, they use some formal  notation. Like the work [2], we do not use any mathematical  model.  3 THE PROPOSED SELECTION METHOD  This section presents the tool we have developed that uses source  code recent changes to extract keywords used to select the most  related test cases to cover these regions.   3.1 Test Case Selection Process  The proposed solution for the automatic TCs selection is to base  this process on the source code which was changed in current  regression test campaign. The aim is to automate the manual  process, which is very costly and not always effective. The  automated process receives as input a file preprocessed that  indicates where occurred the changes (like the completed line of  source code or the change path), and returns as output a list of  keywords which will be used to perform a selection in the TCs  database to create a new regression test plan.  Some versions of the source code (these comprehend the source  codes related to the current regression, that is, source codes from  an initial date to today) are analyzed by a program, developed in  the research project, but outside the scope of the current paper, to  compare them and collect all source code parts that suffered  changes. This program (named AutoTestCoverage) creates a file,  containing the identified changes, that is used by us to extract the  relevant keywords. The AutoTestCoverage (ATC) tool is a  research tool developed inside our industrial partner to gather and  analyze data from test plan executions to assist testing teams in  identifying covered and not covered regions as well as coverage  percentage. This percentage was our metric adopted to evaluate  our results.  The TCs are obtained from a comprehensive test repository', ""percentage. This percentage was our metric adopted to evaluate  our results.  The TCs are obtained from a comprehensive test repository  maintained by the company. As the comprehensive repository is  vast and frequently updated, all testing campaigns start by  selecting from this repository a (possibly high) number of TCs  related to the test goals (named as Master Plan or just MP).  This selection is traditionally executed manually, based on test  goal descriptions and the architect's experience. As our aim is to  automate the whole process of test plan creation, we must also  address the selection and creation of this initial TCs subset (from  the Master Plan). Finally, note that although the Master Plan is  already a subset of the general repository, it is usually still large  and preserves coverage of the code to be tested. Thus, the Master  Plan is later reduced by the test architect to manually create more  objective/focused test plans (which is the main aim of our  automated process).    3.1.1 Manual Process  All test campaigns start with the creation of a Master Plan (MP), a  large set of TCs related to the new product to be tested and the  selection process is manually executed, based on the test goals  and the architect's experience.   As new versions of the software/product are released, regression  tests are run. However, it is not always feasible to execute all TCs  in the MP for every new product release (build); particularly  because execution is mostly manually performed. It is not  necessary to test features which have not been changed in the  build under test.   It’s worth noting that the main bottleneck of this manual process  is the creation of Test Plans, for each new product demands a  single MP, but with several test plans (one for each new build).  And there may be several builds released until a product reaches  an acceptable stable to be released in the market, test plans may  be generated for each new product release.    3.1.2 Automated Process  The aim is to automate the manual process, which is very costly  and not always effective. Unlike the previously developed  proposal to solve this problem [2], we look at the changed source  code instead of looking at release notes documents.  This process consists of four main phases, detailed in what  follows and illustrated in Figure 3.    Phase 1 (Process the source code file): in this phase, a single  textual file, containing the changed regions from preprocessed  source code files, is received. We use a regular expression to  create a list of keywords from this file.  The regular expression we use is based on the CamelCase rule  [18], which breaks compound names that are written together  using upper case initial letters. Figure 1 has this rule encoded in  Python."", '4     Figure 1: Python code with the regular expression to  capture words.    Phase 2 (Extract the keywords): a lot of words are not  interesting for us because they have no significant meaning or  relationship to existing test cases. That way we have a dictionary  of stop words. The list of words from the previous phase is  compared to this dictionary, and the stopwords are removed,  remaining the relevant keywords. Besides that, we care to  associate a set of keywords to their respective set, on the other  hand, that is, we can grouped words with a software component  also identified from source code.  Phase 3 (Do a search on TC database): We noted that words  extracted from the text file have a relationship with TCs (This is  indeed our hypothesis). In Figure 2 we create a similar example of  the relationship between source code and a test case to illustrate  our approach.    Figure 2: Relationship between source code and test case.     In Figure 2, C1 and C2 are lines of preprocessed source codes  indicating changed locations. In C1, we can see that the word  ""calculator"" represents the component associated with the words  “sum” and “values” and in C2 the word ""student"" represents the  component of “school” and “average”. By using these keywords,  we can find test cases TC1 and TC2.  Thus, in this phase, the words of phase 2 are used to do an  automatic search on TC database. Like described in phase 2, we  also identified software components. Hence our search is  performed using the format “component” + “keywords”. With the  multiple searches, we obtain a set of individual results.    Phase 4 (Test plan creation): all results (individual results) of  phase 3 are combined and duplicated TCs are removed. This list  of tests makes up the regression plan.                    Figure 3: Tool’s architecture.  3.2 Auto Test Plan Code (ATPCode)  The automated process described previously was implemented in  a tool named Auto Test Plan Code (ATPCode). It is indeed a  modification in the implementation of the Auto Test Plan (ATP)  [2], reusing the same infrastructure.  4 EXPERIMENT AND THREATS TO  VALIDITY  In the following subsections we present our initial results from  performing experiments as well as the threats to validity we have  identified.  4.1 Results  The experiment built in this study was divided in three stages:  1) Understanding the scenario and initial design – at that time it  was necessary to analyze the scenario investigated to identify  the relationship between the modified source code and the  TCs. Figure 4 illustrates this stage. Manually looking at the  code and some TCs, it was possible to identify that there is a  relationship as well as there is a standard in the code that  allowed us to create a mechanism for extracting the relevant  information (keywords). The code was broken into words  using a regular pattern. These words give support to do a  search in TCs database. At this stage, we did a single search  with the combination these words using an “OR” operator.  2) Adaptation of changes to search and collection of test cases –  after the pilot result we can work on changes in the process.   Here, we identified that the words obtained in the first stage  were not enough to do a good search and we performed a  further analysis aiming at identifying the possibility of  capturing more than just keywords. We also started capturing  components associated with these keywords from source  code. Now, we have begun doing multiple searches instead  of a single one as we did at the beginning. These searches  were composed of a component and keywords associated  with it (component “AND” (keyword1 “OR” keyword2  “OR” …)). In the end, we merged all individual results to  generate a final result (regression test plan).  3) Improvement and implementation through integration – the  input for this experiment is a text file with source code  recently modified. This file must be imported into the tool,', '3) Improvement and implementation through integration – the  input for this experiment is a text file with source code  recently modified. This file must be imported into the tool,  which analyzes line by line and extracts keywords.  These  keywords pass through filters to basically remove stop  words. Multiple searches are done and the individual results  are combined to compose the final result. At this stage the  process has been tested and approved to your objective. This  stage is illustrated below in Figure 5.', ""5   Figure 4: Initial stage.  Figure 5: Last stage of experiment.  In our experiment we used the AutoTestCoverage (ATC) tool  previously defined in Section 3.1 and the percentage obtained was  our metric adopted to evaluate the experiment results.  To reduce the scope involved in this experiment we applied our  study in a specific partner’s application as long as we had access  just to this source code. The analysis and results were obtained  from differences between two software versions. We have applied  the ATC tool on the selected TCs to obtain the corresponding  code coverage. We compared our results with those of test  architects and the original ATP. This is indicated in Table 1.  Thus, from the percentage obtained from this analysis, we can  determine how we progressed. Recall from Section 2.3 that, the  higher is this percent the better is the selection method.  As discussed at the start of this section, we performed three  different experiments (indicated in Table 1 with the prefixes 1 -, 2  -, and 3 -) to see which of them yielded the best scenario. All of  them used the same code changes. They only differed on the way  the selection was made. Furthermore, we use three different test  selectors: architects (a human being), original ATP and ATPCode.  They created three different regression test plans.  In the first and second experiments the selections use a Master  Plan (MP), which filters the possible test cases to be selected  according to partner's decisions. In the last experiment we  disregard the MP to see the result and compare it with the first  experiment using MP.  In the first experiment, the architect has selected 120 test cases,  ATP selected 62 (an almost 50% decrease), and ATPCode selected  60. Unfortunately, due to the dynamics of the industrial context,  some test cases cannot be executed (They were presented inside  parentheses). Thus, in the first experiment, only 62 test cases were  executed, and 40 for ATP and ATPCode. In this scenario, ATP was  the best choice because it has selected less test cases (just 2 more  than ATPCode) and provided the same code coverage (51,74%) of  the architect (An experienced employee), against 39,53% of  ATPCode. Although ATPCode did not provide the best coverage, but  our hypothesis had a good indication that it is valid because  39,53% is somewhat close to 51,74%. This is not depicted in  Table 1, but the MP itself had a 51,74% because the architect  decided to use the MP instead of performing any selection (This  usually happens when we have a few test cases to execute). That  is, ATP got the optimal possible code coverage in a completely  automatic way and using fewer test cases. And consequently,  ATPCode gave a very interesting first code coverage using just  source code as source of changes to relate to tests cases selection.    Table 1: The experiment results.  Stage Selector TCs Coverage  1 - one search Architect (62)120 51,74%  ATP (40)62 51,74%  ATPCode (40)60 39,53%  2 - multiple  searches  Architect (62)120 51,74%  ATP (40)62 51,74%  ATPCode (48)84 44,18%  3 - without  master plan  Architect - -  ATP (166)211 58,57%  ATPCode (132)167 58,18%    In the second experiment, instead of doing only one search with a  combination of the keywords, we used multiple searches in the  attempt to improve the selection method. Now ATPCode performed a  little better. It got a 44,18%. But was selected more test cases than  ATP: 84 TCs were chosen and 48 were indeed executed. That is,  ATPCode had an increase of 8 TCs (48-40=8) to get an increase of  4,65% (44,18%-39,53%=4,65%) with respect to the first  experiment.  In our last experiment we made a simple change. We simply do  not consider the MP. This reason was simple. As the MP was  chosen by human beings, it could not be the best one. We could  not have the participation of our architect due to daily activities  and that the test database had 5500 test cases instead of just the"", 'not have the participation of our architect due to daily activities  and that the test database had 5500 test cases instead of just the  120 test cases to perform some selection. Even considering the  full test database, both ATP and ATPCode took about 4 minutes to  perform the search, prioritize the tests and create the test plan.  ATPCode brought 167 TCs and ATP 211 TCs. Concerning coverage,  ATPCode got a 58,18% with 132 TCs really executed and ATP a  58,57% with 166 TCs executed. Besides that, only 8 TCs selected  by ATPCode are not present in the 166 TCs selected by ATP. This  was a fantastic result for ATPCode because we got almost the same  coverage using 25,75% less test cases.  As the third experiment increased the coverage of the second  experiment, we have a simple proof that the MP is not the best  choice as a filter of further selections and nowadays our industrial  partner does not use MP anymore when ATP is used in practice.  Another aspect that deserves further investigation is that the test  cases executed by ATP were not the same as those selected by  ATPCode in the last experiment but the code coverage was similar.', '6    Thus, a question emerges: Is that the case of the combination of  ATP and ATPCode can bring better results than those used  separately? The current answer to this question is YES! But we  need further investigation to see whether this was just a  coincidence or an evidence.  4.2 Threats to validity  A first threat we identified is related to TCs not captured by ATC.  With the coverage of such tests, our result could be different  because our analysis could be more complete. But it is not a  problem because our parameters to compare the results were the  same used in the ATP tool.   The second threat is associated with the sample size as long as we  use only a single partner’s application and the generalization to  the utilization of our approach to other software features is a  hypothesis derived from preliminary experimental results. But we  think that the other software features have the same or similar  code pattern as the studied one. Thus, this will not be a problem  for future work. It is worth stressing that ATP also used the same  sample to develop its research and it is now successfully used by  our industrial partner.  Another threat is related to static analysis. ATPCode works with a  preprocessed file of source code that indicates where the changes  occurred because many code lines can be useless for TCs  selection and the search could bring wrong results. But this does  not happen because these irrelevant words simply do not bring  any TCs at all. They only increase search and processing time.  Finally, the last threat identified is related to the keywords  signification to TCs (relationship). But in this study, it is not a  problem because the words extracted from source code have a  similarity to those present in TCs. In the future work, we intend to  use a thesaurus (looking for synonyms) to keep the relationship  between code and TCs as close as possible. This because the  textual information on TCs despite following a standard and being  reviewed, they can use different but equivalent names.  5 CONCLUSIONS  In this paper we have shown that we can use source code to  perform selection of the text-based test cases. This is indeed a  standard when one has automated test cases (such as, JUnit tests)  and source code [15]. But when text is involved instead of source  code, the result is not so standard. Obviously, we benefited from  the fact that our industrial partner uses the rigorous guideline for  codification based on verbs, nouns, etc., well-defined.  The results presented so far were promising. It is the fact that we  presented few experiments, but this was a limitation imposed by  our industrial partner. But besides that the experiments presented  here showed several interesting things. First, select the text-based  test cases from keywords extracted from source-code is a valid  attempt. Second, the selection can be similar to traditional  information retrieval of only text-based documents as performed  by ATP. Third, due to different teams, write release notes and  source-code, combining both selection methods can bring even  better results in our context.  The current research is indeed an attempt to get further progress  on previous work developed by part of our research team [2]. The  reason to do that was simple: the quality of text-based release  notes was not so good as the quality of text-based test cases. So,  would it be a good idea to use an alternative source of information  beyond release notes? Furthermore, could we even replace release  notes by just source code? This questions inspired us to develop  this work. Thus, here we answered that YES, we can use source  code instead of release notes. But it is too premature to disregard  release notes absolutely. Particularly because the best scenario we  got was provided by a combination between source code and  release notes. Obviously that, we need more time to provide  strong answers about what we can do. Currently, our industrial', 'got was provided by a combination between source code and  release notes. Obviously that, we need more time to provide  strong answers about what we can do. Currently, our industrial  partner is already using ATP but we intend to put ATP + ATPCode  in operation as soon as possible.  In the future we intend to perform more experiments using our  approach (considering a thesaurus to look for synonyms) as well  as the combination between ATPCode+ATP.  ACKNOWLEDGMENTS  We would like to thank Alice Arashiro, Eliot Maia, and  Guilherme Almeida from Motorola Mobility. And to Virgínia  Viana from CIn-UFPE. This work was supported by the research  cooperation project between Motorola Mobility (a Lenovo  Company) and CIn-UFPE.  REFERENCES  [1] Bohner, S. A. Software change impacts – an evolving perspective. In:  Proceedings of the International Conference on Software Maintenance  (ICSM’02). Washington, DC, USA: IEEE Computer Society, 2002. P. 263-2.  ISBN 0-7695-1819-2.  [2] Magalhães, C. et al. Automatic Selection of Test Cases for Regression Testing.  In: Proceedings of the 1st Brazilian Symposium on Systematic and Automated  Software Testing. ACM, 2016. p. 8.  [3] Michael McCandless, Erik Hatcher, and Otis Gospodnetic. Lucene in Action:  Covers Apache Lucene 3.0. Manning Publications Co., 2010.  [4]  Pressman, R. S. Software Engineering – A Practitioner’s Approach. McGraw- Hill, 4º edition, 1997.  [5] Myers, G. J. The Art of Software Testing. Wiley, New York, 1979.  [6] Araújo, Marco A. P., Spínola, E. Introduação a testes de software. Revista  Engenharia de Software, DevMedia, 2007.  [7] Manning, Christopher D., Regnavan, P. Schütze, H. An Introduction to  Information Retrieval. Cambridge University Press – Cambridge, England, 2009.  [8] August Shi, Alex Gyori, Milos Gligoric, Andrey Zaytsev, and Darko Marinov.  Balancing trade-o s in test-suite reduction. In Proceedings of the 22Nd ACM  SIGSOFT International Symposium on Foundations of Software Engineering,  FSE 2014, pages 246-256. ACM, 2014.  [9] Alazzam, I; Alsmadi, I.;Akour, M. Test Cases Selection Based on Source Code  Features Extraction. International Journal of Software Engineering and Its  Applications, v.8, n.1, p.203-214, 2014.  [10] Paolo Tonella Cu D. Nguyen, Alessandro Marchetto. Model based regression  test reduction using dependence analysis. In In Proceedings of the International  IEEE Conference on Software Maintenance, pages 214-223. IEEE, 2002.  [11] Patrícia Duarte de Lima Machado Francisco Gomes de Oliveira Neto. Seleção  automática de casos de teste de regressão baseada em similaridade e valores. In  Revista de Informática Teórica e Aplicada:RITA, v20(2), pages 139-154, 2013.  [12] Adler, Yoram et al. Advanced code coverage analysis using substring holes. In:  Proceedings of the eighteenth international symposium on Software testing and  analysis. ACM, 2009. p. 37-46.  [13] Yang, Qian; LI, J. Jenny; WEISS, David M. A survey of coverage-based testing  tools. The Computer Journal, v. 52, n. 5, p. 589-597, 2009.  [14] Michael Unterkalmsteiner, Tony Gorschek, Robert Feldt, and Niklas Lavesson.  Large-scale information retrieval in software engineering - an experience report  from industrial application. Empirical Software Engineering, pages 1–42, 2015.  [15] Rothermel, Gregg; Harrold, Mary Jean. Analyzing regression test selection  techniques. IEEE Transactions on software engineering, v. 22, n. 8, p. 529-551,  1996.  [16] Engström, Emelie; Runeson, Per; Skoglund, Mats. A systematic review on  regression test selection techniques. Information and Software Technology, v.  52, n. 1, p. 14-30, 2010.  [17] Ripon K. Saha, Lingming Zhang, Sarfraz Khurshid, and Dewayne E. Perry. An  information retrieval approach for regression test prioritization based on  program changes. In Proceedings of the 37th International Conference on  Software Engineering - Volume 1, ICSE ’15, pages 268–279. IEEE Press, 2015.  [18] Rouse, M. CamelCase. TechTarget, WhatIs.com. Avaliable in:', 'Software Engineering - Volume 1, ICSE ’15, pages 268–279. IEEE Press, 2015.  [18] Rouse, M. CamelCase. TechTarget, WhatIs.com. Avaliable in:  http://searchmicroservices.techtarget.com/definition/CamelCase.']","['SOURCE CODE TO REGRESSION CAMPAIGNS This  briefin  reports  scieitfc  evideice based oi experimeits about the use of iiformatoi extracted from source code as  iiput  to  the  selectoi  process  of creatin renressioi test campainisn FINDINGS The  findigs  preseiten  di  thds  brdefig  coisdner results  obtadien  from  three  step-by-step experdmeits.  Thds  stuny  was  nevelopen dicremeitally  ain  the  fial  result  ds  a  tool  to automatize  the  process  of  test  cases  selectioi basen oi receit source cone chaiges. The maiual tests selectioi process ds very costly ain  iot  always  efective.  Iinustry  nemains automatic  tools  to  perform  such  processes  di  a nadly basds. Our hypothesds was that dt ds possdble to dneitify a relatioishdp betweei source cone ain text-basen test cases. If thds ds dineen valdn, thei we cai use the  diformatioi  extracten  from  source  cone  to select  the  most  relaten  test  cases  to  create  a regressdoi test plai. The  diformatioi  obtadien  from  source  cone  cai reflect  a  more  precise  vdew  of  source  cone chaiges thei that preseiten di release-iotes. To use diformatioi retrdeval techidques oi source cone ain text-basen test cases, we ieen frst to extract relevait keyworns from source cone. Not all keyworns from source cone are relevait or relatioishdp to exdstiig test cases. So, a ndctioiary of stop worns ieens to be nefien such thai oily the relevait keyworns remadi. A  metrdc  to  valdnate  the  results  of  a  techidque shouln be anopten. Ii sceiardos where cone are usen, cone coverage ds a represeitative metrdc. Cone coverage cai express the negree to whdch some regdoi of the source cone of ai appldcatioi ds executen  afer  a  test  sudte  campadgi.  Wdth  thds metrdc,  we  cai  netermdie  how much a  chaigen regdoi ds exercdsen. The  metrdc  cai  also  be  usen  to  compare  the resultiig campadgi (automaten) wdth the sofware archdtect  result  (maiual)  to  dneitify  the  most approprdate  approach  (that  wdth  the  greatest coverage) to the orgaidzatioi. Ii  a  frst  experdmeit,  we  iote  that  keyworns together  usen  di  a  query oi  the  test  reposdtory noes  iot  exhdbdt  a  goon  result  whei  comparen wdth the test archdtect campadgi. Ii the frst experdmeit, test archdtect selecten 120 test case ain obtadien 51,74% of cone coverage, whdle our selectioi basen oi source cone chaige ain usdig all keyworns selecten 60 test cases ain obtadien 39,53% of cone coverage. Ii  a  secoin  experdmeit,  beyoin  coisdnerdig source  cone,  we  also  usen  the  compoieit assocdaten wdth the part of cone chaigen. The  combdiatioi  betweei  compoieits  ain keyworns  showen  a  sdgidfcait  dmprovemeit  oi cone  coverage.  Dodig  multiple  searches  oi  test reposdtory  the  cone  coverage  dicreasen  from 39,53% to 44,18%. Our  thdrn  ain  last  experdmeit  was  basen  ai exteisdoi  of  the  secoin  by  avodndig  some restrdctiois usen by our dinustrdal partier. That ds, we experdmeiten wdthout usdig dts Master Plai. Whei  we  ndsregarn  the  Master  Plai  di  our proposen process we obtadien a beter result thai test archdtects  selectioi. The cone covarage was 58,18%. Thus, we have some evdneice that the Master Plai ds  iot  the  best  chodce  as  a  flter  of  further selectiois. The  process  to  use  source  cone  di  regressdoi campadgis basen oi text-basen test cases cai be summardzen di fgure below: Keywords: Source cone Iiformatioi retrdeval Regressdoi campadgi Who is this briefin  or? Sofware eigdieerdig practitioiers who wait to make necdsdois about the use  diformatioi extracten from source  cone to geierate regressdoi tests  campadgis basen oi scdeitifc evdneice. Where the fidiins come  rom? All findigs of thds brdefig were  extracten from the experdmeit  coinucten by Araújo et al.   What is iicluded ii this briefin? The madi findigs of the ordgdial stuny. Evdneice characterdstics through a brdef  nescrdptioi about the experdmeit  coinucten.', 'coinucten by Araújo et al.   What is iicluded ii this briefin? The madi findigs of the ordgdial stuny. Evdneice characterdstics through a brdef  nescrdptioi about the experdmeit  coinucten. What is iot iicluded ii this briefin? Anndtioial diformatioi about the  experdmeits. Detadlen nescrdptiois about the tools  ain other diterial proprdetary  diformatioi. To access other evideice briefins  oi sofware einiieeriin: htp:////www.lda.ufc.br//ccbsof2017//dd- sast// ORIGINAL RESEARCH REFERENCE Joelsoi Araújo et al. Feasibility of using Source Code Changes on the Selecton of  eettbased  egression  est Cases. 2in Brazdldai Symposdum oi Systematic ain Automaten  Sofware Testiig, Fortaleza, Cearl-Brazdl, September 2017, ISBN 978-1-4503-5302-1//17//09, htps:////nod.org//10.1145//3128473.3128481.']","**Title: Enhancing Regression Test Case Selection Using Source Code Changes**

**Introduction:**
This evidence briefing summarizes research investigating the potential of using recently modified source code to improve the selection of text-based regression test cases. The motivation stems from the inadequacies of release notes used in traditional selection processes. By leveraging source code changes as a source of keywords, the study aims to provide a more effective and automated approach to regression testing.

**Main Findings:**
1. **Source Code as a Keyword Resource:** The study introduces a novel approach where keywords are extracted from modified source code instead of relying solely on less reliable release notes. This method shows promise in enhancing the selection of relevant test cases.

2. **Tool Development:** The research resulted in the creation of a tool called Auto Test Plan Code (ATPCode), which automates the selection process by analyzing the source code changes, extracting keywords, and identifying corresponding test cases from a database.

3. **Experimental Validation:** An experiment conducted in an industrial context revealed that ATPCode can effectively select test cases based on source code changes. Although the coverage achieved by ATPCode (39.53%) was slightly lower than that of the traditional method (51.74%), it demonstrated a significant reduction in the number of test cases selected (60 vs. 62). The tool's performance improved with multiple searches, achieving a coverage of 44.18% with 84 test cases selected.

4. **Comparison with Traditional Methods:** In subsequent tests, ATPCode's results were comparable to those of the original Auto Test Plan (ATP) tool, indicating that it can serve as a viable alternative or complement to existing methods. Notably, ATPCode's use of source code for keyword extraction has the potential to streamline the test selection process and reduce manual effort.

5. **Future Directions:** The study suggests that combining ATP and ATPCode could yield even better results than either method used independently. The researchers plan to explore this combination and incorporate a thesaurus for synonym recognition to enhance keyword matching.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, test architects, and quality assurance professionals involved in regression testing who seek to improve the efficiency and effectiveness of their test case selection processes.

**Where the findings come from?**
The findings presented in this briefing are derived from an empirical study conducted by Joelson Araújo et al. at the Informatics Center, Federal University of Pernambuco, focusing on the applicability of source code changes in regression test case selection.

**What is included in this briefing?**
The briefing includes an overview of the process for extracting keywords from source code, the development and implementation of the ATPCode tool, experimental results comparing ATPCode with traditional selection methods, and insights into future research directions.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about this research:**
Contact Joelson Araújo at jisa@cin.ufpe.br.

**Original Research Reference:**
Araújo, J., Araújo, J., Magalhães, C., Andrade, J., & Mota, A. (2017). Feasibility of using Source Code Changes on the Selection of Text-based Regression Test Cases. In Proceedings of the SAST, Fortaleza, Brazil, September 18-19, 2017. DOI: [https://doi.org/10.1145/3128473.3128481](https://doi.org/10.1145/3128473.3128481)"
"['Spotify Characterization as a Software Ecosystem Vinicius J. Schettino Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais vinicius.schettino@ice.ufjf.br Regina Braga Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais regina.braga@ufjf.edu.br José Maria N. David Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais jose.david@ufjf.edu.br Marco Antônio P. Araújo Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais marco.araujo@ufjf.edu.br ABSTRACT [Context] Software production methods are changing at high rate. Organizations and their former product lines are evolving to soft- ware ecosystems, seeking to take advantage of external resources that can aggregate value on their projects, such as seasoned engi- neers and open-source components. Meanwhile, external develop- ers are looking to bond products with established market share, aiming for niches that may provide innovative business opportunity. [Objective] The objective of this work is to describe and evaluate technical, organizational and social aspects of Spotify, called soft- ware ecosystem, regarding previous works. [Methods] Besides the literature review and research about Spotify characteristics, we de- veloped an application that extends Spotify native recommendation and presents information about the tracks listened, such as energy, danceability and popularity. These properties are gathered from the Spotify Web API, bringing forward deepen enlightenment about technical aspects of the platform. [Results] Through the knowledge acquired on this work, we present improvement suggestions for the ecosystem, regarding interoperability aspect. CCS CONCEPTS • Software and its engineering → Software design engineer- ing; Reusability; Software product lines; KEYWORDS Software ecosystems, software reuse, software engineering ACM Reference format: Vinicius J. Schettino, Regina Braga, José Maria N. David, and Marco An- tônio P. Araújo. 2017. Spotify Characterization as a Software Ecosystem. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017, 10 pages. DOI: 10.1145/3132498.3133836 Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national govern- ment. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SBCARS 2017, Fortaleza, CE, Brazil © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5325-0/17/09. . . $15.00 DOI: 10.1145/3132498.3133836 1 INTRODUCTION Software ecosystems (or SECOs) are growing as a research field [14]. Some definitions, encapsulating concepts of interaction and symbio- sis (e.g. business and digital ecosystems) are proposed. One of the most recent, proposed by Manikas [14] says that a software ecosys- tem is a “software and actor interaction in relation to a common technological infrastructure, that results in a set of contributions and influences directly or indirectly the ecosystem. ”. On software ecosystem, interactions between actors are moti- vated by the value creation. This value can be, for instance, mone- tary fee, skills, experience, or software development contributions. Companies can no longer develop an entire software product them- selves and still fulfill all customers’ demands [ 26], because that would require expertise in several different fields. This model extends the product line concept, evolving it to take advantage of external resources and keep competitive traits, while each organization can focus on its own specialty. These resources can be software, people, companies, and technical specifications, as protocols or standards. However, it is important to highlight that SECO emerging from SPL is one approach. Another way is to evolve software products or services towards ecosystem platforms [12].', 'as protocols or standards. However, it is important to highlight that SECO emerging from SPL is one approach. Another way is to evolve software products or services towards ecosystem platforms [12]. Spotify1 is an online audio streaming service, containing more than 30 millions of tracks and 75 millions of active users [24]. Soft- ware as a service model (SaaS) ensures that its content is available through mobile, desktop and web apps in exchange for a periodic fee (for premium users) or being exposed to advertising (for free users). The SaaS approach allows users to switch providers without long term contracts or infrastructure bounding costs, as well as smaller costs on distribution, maintenance, and deployment of products [7]. These aspects nourish interaction, both in terms of competition and partnership, between several elements that compose a software ecosystem [3]. As stated by Manikas [14], it is important to provide more in- depth studies about real-world context SECOs, focusing on studying the different aspects of this type of platform. Detailing these studies would allow the identification of similar cases for theory confirma- tion, improvement, and repeatability. Based on this argument, we proposed to investigate Spotify as an Ecosystem platform. Spotify is the biggest audio streaming service. Its fast growth over the past few years engaged us to understand how they are tackling the SECO challenges, that can be similar to other software companies. We 1http://spotify.com/', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Schettino et al. choose to look for a privately-owned product, to study its scenario and specific challenges. This work aims to investigate relations regarding the state-of- the-art software ecosystems descriptions and organizational, tech- nical, and social issues described in the literature [6] about SECO platforms. The disruptive nature of Spotify platform aggregates challenges from the organizational point of view. The streaming model breaks the content distribution paradigm that giants from the audiovisual industry have been keeping for decades. Furthermore, Spotify needs to deal with those big players to bargain authorial rights from famous artists. Regardless engaging the users to pay for the service and encour- aging the content producers to participate, the fees and royalties involved must find a balance. It gets even more challenging when the free users are taken into consideration, that contribute with advertising incomes. At the same time, they are important to the ecosystem, the advantages of the premium users, such as no adver- tising, offline availability and high audio quality must be attractive enough to boost new premium users. This balance is one of the social aspects briefly addressed in this work. Lastly, some technical challenges are considered here. The in- teroperability level between the several agents is critical for the ecosystem success [3]. Furthermore, the rapid growth of the service, as the user base went from 10 to 100 million active users in five years [28]. Deepening into the technical point of view, it is presented a small application, named KnowYourBeat, that integrates with Spotify Web API and extends the platform features, adding value to the users on a specific niche. We hypothesize that the use of KnowYourBeat does extend the Spotify native recommendation system in order to characterize Spo- tify as a software ecosystem platform. Based on this hypothesis, our research question is “How KnowYourBeat extends Spotify native recommendation system, in order to find new tracks to listen?” Therefore, as contribution, this work present Spotify characteri- zation as a software ecosystem, along with suggestions that may improve interoperability issues. This paper is organized as follows: section 2 presents related works and fundamental concepts and theories supporting this re- search. Section 3 reports the tools and methods that supported this research. Section 4 discuss this concepts on Spotify scenario, structuring its business model in the light of software ecosystems. Section 5 is about the developed application in order to better un- derstand Spotify’s ecosystem. There are presented architectural, functional and conceptual decisions made, and how they are related with ecosystem’s infrastructure. Whereas in section 6 we discuss the found the constraints and suggestions, in section 7 we pro- vide a closure for this work, also indicating future paths of related research. 2 RELATED WORK The ecosystem characterization is conducted mainly in adherence with the framework proposed by Santos [ 6], where three reflec- tion dimensions of a software ecosystem are described: technical, organizational and social. The technical perspective focuses on an internal view of the ecosystem, mainly on its platform. Software architectures, evolu- tion guidelines, code maintainability among other internal metrics can be discussed, regarding their influence over the ecosystem. Interoperability between the ecosystem components is a techni- cal challenge, mainly for the platform. It has to provide a stable and flexible interface, while avoiding opportunities for defective or malicious external code to affect the whole system [3]. The ecosystem needs to grow in features, components and inter- actions. Old features tend to be less differentiating, so new features are necessary to keep advantage over other ecosystems. These ar-', 'The ecosystem needs to grow in features, components and inter- actions. Old features tend to be less differentiating, so new features are necessary to keep advantage over other ecosystems. These ar- chitecture transactions need to be carefully designed to minimize the impact, especially if it involves interfaces with other compo- nents. New features and its interface changes need to be presented long before the actual release takes place [3]. The organizational (or transactional) perspective includes busi- ness level interaction challenges between ecosystem components. The ecosystem must offer advantages to new incomers, such as advanced infrastructure or competitive advantage among its com- petitors, or it will not be attractive. However, these needs must be equated with niches prospection and opportunities to innovative solutions, or it can be seen as a competitor that will not allow a mu- tual relationship of growth. In this point of view also are included challenges with external regulatory bodies and stability risks, that may hinder new incomers. On the social point of view, challenges as requirements gathering, extensibility, feedback capture and reputation are discussed [13]. How people see the ecosystem can influence how willing they are to actively participate, through use, feedback and promotion. This participant’s perspective goes beyond the product itself and can encompass branding, social actions and even environment positioning. Although these dimensions presented above are used to catego- rize the discussed topics, the challenges are not exclusive of each point of view. In software ecosystems, as it happens in nature, the impacts of decisions are felt in other dimensions, being unfeasible to study them in an isolated way. To support the terms used in this paper, two exploratory stud- ies are resorted. Jansen and Cusumano [12] suggest, among other proposals, some classification parameters for platforms, taking into consideration projects such as Spotify itself. Through a system- atic mapping, Manikas [14] suggests state-of-the-art definitions for the field. Concepts proposed by Christesen et al. [ 5] are consid- ered on this systematic mapping and are especially useful for the characterization and description aimed at this work. 3 METHODS AND TOOLS In order to characterize different aspects of Spotify’s SECO, we used some different approaches, each one was focused on specific traits. Although we understand that using assessment models proposed in previous works [6, 26] would be more suitable, there are problems with this approach. Since Spotify is a proprietary platform, we do not have access to all data necessary, especially when it comes from the development process statistics and information.', 'Spotify Characterization as a Software Ecosystem SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil We propose to use previous assessment models, limited to the data we could gather from public sources, such as official docu- mentation and specialized media. These sources were useful to understand technical, social and organizational traits of the SECO, to characterize it towards the literature already presented in this paper. To deepen our knowledge about technical aspects, we propose an application that extends features from Spotify native recom- mendation service, with its data and public API. For evaluating the application’s capacity of adding value through participation in Spotify’s SECO, we conducted a Case Study. 4 CHARACTERIZING SPOTIFY ECOSYSTEM As a matter of organizational structure, there are traits discussed in the literature [14] that support SECOs characterization, inspired by IT governance research field. It investigates software as a mean to support business in a corporation [27]. More specific, Manikas et al. [15] discuss ecosystem orchestration, or who make the decisions and strategic leading of the platform: Monarchy: All the orchestration is made by a single actor, typi- cally the one with great influence over the main technology of the ecosystem. Federal: There is a set of representative actors that conduct the orchestration together. Collective: All actors can participate the decision process, through practices as polls. Anarchy: Each actor acts on its own, and there is no general orchestration. Spotify can be seen as a monarchy ecosystem since the main decisions are held by the platform itself. It can lead to an easier decision-making process, but leaves out opinions and visions that may be helpful for the ecosystem health. Also, the monocratic model can increase the social challenges discussed in section 2. Over the perspective of business structure, the ecosystem may be analyzed by its means of creating value [14]. Thus, they can be characterized as: Proprietary: The main value creation is made by proprietary contributions. The components may have a common interface for exchange of data and features, but they are typically protected by intellectual property (IP) management processes. In this scenario, the values exchanged often involve monetary fees. Open Source: The contributions are generally open to the public. The main value types are knowledge, experience and satisfaction, whereas the monetary value decreases in participation. Hybrid: TThere are cases where the ecosystem supports both proprietary and open source contributions, with the correspondent compensations associated. Spotify can be seen as a Hybrid ecosystem, since the platform is proprietary but the partner services can be both proprietary or open source, as long as they implement a common interface. With future strategic decision, for instance, there can be fees for APIs access, open source modules or additional SDKs, taking the ecosystem to a different business structure. 4.1 Platform Classification The platform is the base technology behind an ecosystem. In this work we will consider mainly the interaction between its compo- nents. Considering Jansen and Cusumano [12] classification, Spotify is considered a service platform, privately owned and coordinated. The platform is the base technology behind an ecosystem. In this work, we will consider mainly the interaction between its compo- nents. Considering Jansen and Cusumano [12] classification, Spotify is considered a service platform, privately owned and coordinated. Extension Market is a component present in some ecosystems [12], and it aims to support niche players to publish and even sell their products. Some examples are Firefox2 and Slack3, where the exten- sion market is an appeal for external developers to interact with the ecosystem. Spotify had this service, called “App Finder”, but it was discontinued 4. To publish in the App Finder and even get', 'sion market is an appeal for external developers to interact with the ecosystem. Spotify had this service, called “App Finder”, but it was discontinued 4. To publish in the App Finder and even get access to the APIs, the application would undergo an evaluation by Spotify. This service was removed when new SDKs and APIs were developed and made available to the general public. An extension market can be seen by external developers as an attractive, since it supports the contact between several ecosystem’s actors, such as content producers and user. In this same space, it is possible to showcase successful applications and to stress unex- plored niches, increasing the ecosystem’s advantages visibility. The closer service to a extension market available now is the “Case of Success” section provided by Spotify, where selected appli- cations are displayed for the developer community for inspirational and motivational purposes [22]. 4.2 Niche Players Niche players are actors inside an ecosystem that interacts with the platform and among themselves, influencing its evolution. The niche players aim to distinguish their services and products from competitors, that can be in the same ecosystem. They are classified in three major groups[26]: Influencers. : They are responsible for interventions on the key organization and their strategies and main politics. Generally, are early adopters, developing its technology with a fundamental role played by other ecosystem components. Hedgers. : They present solutions for interacting with several platforms and ecosystem, not being strongly attached to a single component. Disciples. : They are compromised to only one platform and impute a fundamental role to it on their products and services. Usually, one does not have the same key role in the ecosystem strategies as seen on the influencers. To find and analyze Spotify’s niche players, official sources [19, 20], and specialized media [16, 25] were addressed. The main niche players founded, excluding the already discussed platform users, are: 4.2.1 Record Labels. The record labels are the main audio providers to the platform. Thus, they can influence the platform’s business 2https://addons.mozilla.org/pt-BR/firefox/ 3https://slack.com/apps 4https://developer.spotify.com/technologies/apps/', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Schettino et al. model and strategic decisions. In exchange for the authorial con- tent for streaming, they receive a financial compensation (royalties) based on the amount of streaming. In addition, the record labels di- versify the logistics of distribution and disclose of its material. The origin and values diversity of these compensations are discussed in section 4.3. The copyright protection has been a challenge in audio and video content in the internet era. The Spotify business proposal can be considered an alternative that, even possibly less lucrative, faces the charms of piracy, such as lower costs and distribution facilities. 4.2.2 Independent Artists. Independent Artists. Content providers without the support of major record labels are increasingly com- mon, thanks to the accessibility of technologies. Today, it is possible to produce high-quality audio content with a fraction of what was needed some years ago. However, the traditional distribution logis- tics and publicity still supported by the major record labels. In this scenario, the Spotify can be seen as an alternative platform of audio content host. Since the gains are bounded to the amount of streaming, small producers can finance undergoing projects and find a stable way of publicizing their work. They can be seen as disciples or hedgers, depending on how they use the platform. 4.2.3 Developers. Spotify holds more than 30 million of tracks and 75 million of active users [24], as well all the metadata asso- ciated with these records. This can add value in several forms for different services related to music. This data is available through APIs and SDKs maintained and documented by the platform. In counterpart, the eventual success of third-party’s applications in- creases the Spotify market share and its brand value. They can be seen as disciples or hedgers, depending on how they use the platform. Although the discontinuity of the central extension market, it is possible to find some third parties applications that claim to extend the Spotify services and features. For example, the Djay 5 and Waze 67 . On the unofficial application discover site AppCrawlr it is possible to list 18 Android8 and 24 iOS9 apps that claim some kind of integration with Spotify. 4.3 Monetization Model Spotify holds two main ways of raising funds: Premium users’ payment and advertisements circulation. The first is converted through a monthly fee related to a chosen subscription plan 10. The former is accessible from the free users, that are subjected to advertisers between tracks. Depending on the origin, the income associated to the streaming is different [21]. Even though a record label is not mandatory for keeping tracks on the Spotify, independent authors need an intermediary for it [ 21]. There are some options available, and the bargain over the royalties’ percentage must be discussed with the intermediary. 5https://www.algoriddim.com/spotify 6https://support.google.com/waze/answer/7341361?hl=pt-BR 7http://tecnologia.ig.com.br/2017-03-14/waze-spotify.html 8http://appcrawlr.com/app/search?q=spotify+integrationdevice=android 9http://appcrawlr.com/app/search?q=spotify+integrationdevice=iphone%2Cipad 10https://www.spotify.com/br/subscriptions/ Figure 1: Interaction Between Spotify’s ecosystem compo- nents 4.4 Ecosystem Interaction Figure 1 describes the interaction between Spotify1s main agents. The communication occurs primarily over the platform. The content providers can be either Independent Artists or Record Labels, but the first needs an intermediary to publish their content They receive, from Spotify Platform, royalties based on the amount of streaming. There are free and paid users. The last pay a monthly fee for the service, and the former pays through advertising fees. The developers, through the available APIs, offer extensions and niche appliances of the Spotify services, adding value to its', 'service, and the former pays through advertising fees. The developers, through the available APIs, offer extensions and niche appliances of the Spotify services, adding value to its products, improving the Spotify brand value and market share and increasing the amount of data available. 4.5 APIs and Niche Players Interface The communication between the applications and the ecosystem are mainly conducted over the APIs available by the Spotify11. There are SDKs for mobile (Android and iOS) and web applications, the Play Button (meant to be incorporated in blogs and social pages) and integration tools for Internet of Things (IoT) devices. The web API endpoints are mostly compliant with the REST ap- proach. According to Fielding and Taylor [8], REST is a “coordinated set of architectural constraints that attempts to minimize latency and network communication, while at the same time maximizing the independence and scalability of component implementations”. Web Services that implement these approaches are called popularly as RESTful services. 11https://developer.spotify.com/', 'Spotify Characterization as a Software Ecosystem SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil When a resource is fetched, the Spotify API also include hyper- text links to related resources. For instance, on a track, there are links to the album and the artist. However, this feature does not adhere with the “Hypermedia as the Engine of Application State” (HATEOAS) proposed by the REST approach [ 8]. This standard advertises that a RESTful API needs to be machine navigable. Thus, it must be possible to discover other actions and resources that may be reachable from the current resource and its current sta- tus. Furthermore, accordingly Richardson Maturity Model for APIs, HATEOAS is necessary to reach high levels of discoverability and interaction [9, 17]. To access this APIs, it is mandatory to have a previous authoriza- tion protocol [23]. Endpoints that do not provide personal data from the users only asks for the third parties applications credentials, that must be registered on the Spotify Developer’s area11. However, to access user data, such as streaming history, personal information and playlists, it is necessary to follow the OAuth 2.0 protocol, described in RFC-6749 [11]. Through this flow, presented in Figure 2, it is possible to be granted with access credentials limited to a well-defined scope. For instance, the user can allow access to his name and email, but not to its birth date. The authorization must be refresh under defined time boxes, but it does not require user direct interaction. 4.6 Discussions about Spotify’s Characterization To be characterized as a SECO, a set of interacting components of Spotify must overcome challenges from different points of view, in order to create benefits for those involved. A successful ecosys- tem provides an adequate environment that provides competitive advantages to all participants, especially in comparison with out- siders [4]. In this scenario, Spotify presents some strategies and interaction models that characterize it as an ecosystem, and they will be discussed further in this section. 4.6.1 Niche Creation: Spotify does not offer all predictable value inside the platform. It focuses on main features related to audio content streaming and discovery for mainstream public, but it let specific domains such as DJs, musicians and audiophiles need to be handled by external developers. 4.6.2 Platform Value Addition. a SECO that does not provide enough value on its keystone will not be seen as attractive as other ecosystems. This may commoditize the ecosystem and cause dis- ruption of the platform and the keystone firm over time [4]. 4.6.3 Functionality Evolution. Functionality Evolution. Spotify does add new features in order to avoid commoditized functionali- ties only on the platform. In the API changelog12 it is possible to see that new features are added, some of them based on community requests [18]. In an ecosystem, this innovation is necessary in order to keep competitive advantages over external agents and increase SECO’s attractiveness [3]. 4.6.4 Public APIs. The public APIs provided by Spotify follow designs and standards in order to increase interoperability, as dis- cussed in subsection 4.5. It also allows client inputs that tailor 12https://developer.spotify.com/web-api/change-log/ the outcomes, like pagination, sorting and filtering. This kind of functionality enables new client features without high amounts of computational resources, like network capacity or processing power, thus offering new niches and value addition. 4.6.5 Security Concerns. As discussed by Bosch [ 3], the plat- form is the main responsible to avoid malicious code to infect all the ecosystem and its data, since usually it is the main communica- tion channel on a SECO. The Spotify Web API shows concerns on security and privacy, for instance through HTTPS connections and OAuth 2.0 protocol, as discussed in subsection 4.5. 4.6.6 External Developers Feedback Channel. Feedback from', 'security and privacy, for instance through HTTPS connections and OAuth 2.0 protocol, as discussed in subsection 4.5. 4.6.6 External Developers Feedback Channel. Feedback from ecosystem’s components is a valuable input that the keystone or- ganization must gather and consider for decision making process [13]. Spotify uses Github as a public issue tracker13, encouraging enhancement and feature requests from external developers. Another way of analyze the SECO is to understand its keystone company and how balanced its strategy is with the challenges faced on a ecosystem business model. Bosch [4] says that there are five levels on how ecosystem engaged is an organization. We will briefly present each of these stages. In the beginning, the company is iinternally focused. It only participates on an ecosystem because it needs to, e.g. they need services or products that internal production is unfeasible. Then, it becomes anad hoc ecosystem engagement, where it finds out SECO’s advantages, primarily from cost perspective. It suspiciously starts to seek for external components, trying to forge partnerships for specific tasks, incapable to link demands between different relations as e ecosystem. In this level, the company avoids high- level dependency over external developers and lack governance techniques to manage the ecosystem. When the SECO’s advantages become more widespread, the company reaches the Tactical ecosystem engagement level. More contextual areas start to considering outsourcing due to a change on the organization’s culture. Then it becomes a Strategic single ecosystem management, where being on a ecosystem starts to being part of strategic decisions. It sees the ecosystem as a whole, broad seeking advantages of its structure. The last level an organization can reach consists to thinking itself as part of several ecosystems, as it usually happens to big players [4]. This stage is calledStrategic multi-ecosystem management. From this point, the company tries to leverage competitiveness by interacting in all its ecosystems at the same time with win/win perspective rather than a traditional win/lose lens. In our analysis, Spotify still figures as anad hoc ecosystem engage- ment. It already takes responsibilities as a platform and ecosystem keystone by facing the challenges discussed above. On the other hand, the presented aspects show that there are more than mone- tary contributions, but it still lacks ecosystem governance tactics to increase fidelity and nourish long term relationships. Some of the discussed topics need an insider perspective to be explained, especially those related to technical traits. Furthermore, a deepened point of view helps us to find challenges and weak spots against the theoretical content we faced. For these reasons, we developed an application inserted on the present SECO. By this 13https://github.com/spotify/web-api', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Schettino et al. approach, we expect to enhance our knowledge about the ecosystem and observe possible improvements that could be made. Bosch[4] says that traditional way of thinking puts a company on only one ecosystem. Figure 2: Authorization Code Flow [23] 5 KNOWYOURBEAT To better understand the interactions and relationships between niche players and the platform, KnowYourBeat was developed. It helps user to understand his musical taste and find songs he may like. It extends native recommendation subsystem on Spotify, let- ting the user seek audio properties such as energy, valence and popularity. This data is gathered through Spotify API and made ac- cessible directly for the user, feature which is not currently available the platform. This track metadata come from the recent streaming history of the user, by a new endpoint [18]. The main features are (i) display recent tracks and aggregated statistics such as popularity, energy and danceability, (ii) display top artists from the current user and their average popularity, (iii) display the most popular artist between the current user’s top artists, (iv) display recommen- dations based on user’s top tracks and artists and the mean audio features found over those tracks. Although the Spotify native recommendation system is straight- forward to use, since it requires no direct input, this feature can also be considered a weakness. Users have no way to understand the parameters that will be used, therefore cannot narrow the rec- ommendation to only new data, or to be based on specific aspects as energy or danceability. The importance of user input in recommen- dation system context has been already explored in the literature [2]. Thus, we improved Spotify’s recommendation system with the new (user input) feature. Figure 3: KnowYourBeat Component diagram This scenario can be seen as a niche for KnowYourBeat, since it fulfills a peculiar group of individuals needs who want tailored rec- ommendations, even it demands more interaction and user manual input. The Spotify API allows the client application to defined tailored seeds to be used on the recommendation system. It can be any combination of tracks, artists and genres, as long as it summarizes a set of five elements. It is also possible to set minimal and maximal intervals of audio features, as well as target values for these fields. In this last case, tracks with closest values will be returned. We propose a recommendation model where the user’s favorite artists and recently played tracks are used for seeds, as well as for target values. This might give a reasonable balance between ade- quate results (because of the top artists) and a tailored suggestion based on the present user’s mood. 5.1 Architecture KnowYourBeat have two main components. The first is a server application that consumes the Spotify Web API, transforms and filters it, keeping the authentication and authorization data. The second is a client application that interacts with the user, rendering data and collect user inputs. Figure 3 shows the relation between these components and the Spotify services.', 'Spotify Characterization as a Software Ecosystem SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil The server application is a RESTful service mostly written in PHP7, on the top of Yii2 Framework14. The communication with the API is supported by a PHP SDK for Spotify15. The choice for this technology, among others same purpose libraries is justified by the recent development activity (making available new endpoints integration) and higher PHP dependencies (indicative of taking advantage of new language features). The server code is available on Github16. To store the authorization data, Redis 3.2.9 is used. It is a key- value data store with optional persistence, that can be reached in different forms. It is provided by a Docker Container17, abstracting some installing and management details of the tool. The client is mostly written in Javascript, on the top of React18, a library for build user interfaces. React is a component based library, where the developer is encouraged to implement small and reusable components, from himself and from open source projects. For instance, all the interface components (tables, menus, drawers) were extended from the Material UI set of react components. They are based on Google’s Material Design, a guideline for interface design that are behind google products and serves as a foundation for many other design standards over the industry [10]. This architecture was design to allow changing the components when needed. It is possible, for instance, to change the client or the database for other components that offer the same interface. The REST approach also enables that each component implements its own languages or paradigms internally, but still communicating whereas the standard interfaces are provided. 6 EVALUATION This section presents an initial evaluation of KnowYourBeat and its capability of extend native recommendation subsystem on Spotify. It presents the method used to evaluate the hypothesis, and finally the results of this evaluation. KnowYourBeat was evaluated through a case study. This study was specified to demonstrate the technical feasibility of the KnowYourBeat service, and also, concepts and technologies involved in the solution. Therefore, it was designed to be a testbed to reveal improvements that could be done and flaws that must be addressed. 6.1 Study Definition We aimed to perform an initial evaluation of KnowYourBeat and its capability of extending native recommendation subsystem on Spotify platform. Based on the Goal/Question/Metric (GQM) [ 1] approach, the objective was defined as follows: “Analyze the use of KnowYourBeat for the purpose of verifying its capacity of extending native recommendation subsystem on Spotify from the point of view of a user in the context of finding new tracks to listen. ” From the scope definition, the research question is: How Know Your Beat extends Spotify native recommendation system, in order to find new tracks to listen? 14https://github.com/yiisoft/yii2/ 15https://github.com/jwilsson/spotify-web-api-php/ 16https://github.com/facebookincubator/create-react-app 17https://hub.docker.com//r edis/ 18https://facebook.github.io/react/ In order to collect evidences about the research question, direct observations were used as data sources. 6.1.1 Study Planning. 6.1.2 Context Selection. This study was carried out, considering a real-world context were a Spotify user wants to find new songs he may like. 6.1.3 Hypothesis Formulation. The evaluations were conducted bearing in mind the following hypothesis. (i) H0 (Null hypothe- sis): The use of KnowYourBeat does not extend the Spotify native recommendation system. (ii) H1 (Alternative hypothesis): The use of KnowYourBeat does extend the Spotify native recommendation system. 6.1.4 Goals. This case study aims to evaluate if KnowYourBeat helps Spotify to recommend new songs to listen, based on his current mood. This mood can be roughly translated at the audio', 'system. 6.1.4 Goals. This case study aims to evaluate if KnowYourBeat helps Spotify to recommend new songs to listen, based on his current mood. This mood can be roughly translated at the audio features of his recent streaming history, such energy, positiveness, or danceability. 6.1.5 Subject Characterization. The subject is a male, 23 years old, student, Spotify premium user that declares to use the platform, on average, between 2 and 3 hours every business day. He will be identified as A, from now on. He was selected based on his knowledge and familiarity with Spotify. The central criterion for participant selection was his knowledge on Spotify functioning and execution. 6.1.6 Scenario and Results. This subject A wants recommen- dations of new songs to listen, based on his current mood. This mood can be roughly translated at the audio features of his recent streaming history, such energy, positiveness or danceability. In order to use KnowYourBeat, subject A was required to log in with his Spotify account. This is achieved with the OAuth 2.0 protocol (as explained in subsection 5.1), so the client application does not have access to his access credentials. Instead, the request describes which data is needed, and it is granted by Spotify upon user confirmation. Once properly authenticated and conscious of his grant over his data, subject A navigates through visualizations regarding his musi- cal taste. In Figure 4, his recent streaming record is partially shown, along with average audio features of this history. This information reveals peculiar patterns such as relatively low popularity rate and energy of his recently played tracks. Artists have tender relations with genres and moods. So, we choose to display user top 20 artists, not based only in the recent history, as partially shown in Figure 5. This data is also gathered on Spotify Web API, and the most popular artist and the average popularity of the artists is found and displayed. Subject A can also have access to a list of 20 recommendations, based on the parameters explained at subsection 5.1. The partial list is represented on Figure 6. He declares that his recommendations are tailored for his actual mood, but they are still attached to known artists. This conclusion indicates that the provided application explores a niche created by Spotify SECO, where users can personalize recommendation with recent history input.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Schettino et al. Figure 4: User A’s recent streaming page Figure 5: User A’s top 20 artists page OAuth 2.0 protocol allowed proper authorization without client access to the user credentials. It decreases the withdrawal factor that involves placing user’s password in an unknown application, as well as drops security levels that client would need to have for manipulating and storing this kind of data. Sorting and pagination features provided by the web API were used considering user interaction with minimal effort by the client application. Subject A asked about genre information on tracks,', 'Spotify Characterization as a Software Ecosystem SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Figure 6: User A’s recommendations page both on recent history and recommendations page. This informa- tion is not provided by Spotify API, thus cannot be displayed for the end user. Subject A asked about how he could find similar applications, especially when visualizing relations between artists, tracks, and genres In the lack of a extension market, according to the Google Play model for instance, we presented the “Case of Success” section [22] for him. 6.2 Analysis of Results Considering this case study, we analyzed the results from the user’s point of view. This research evidenced that Spotify can be con- sidered a software ecosystem, regarding definitions and aspects presented and discussed in the related work [3–5, 12, 14]. As a SECO, the presented challenges must be addressed in constant evolution. Through this evaluation, we have shown how KnowYourBeat can find new tracks to listen, raising evidences to answer the research question: How KnowYourBeat extends Spotify native recommenda- tion system, in order to find new tracks to listen? However, they cannot be generalized through a single case study. Rather, we iden- tified situations in which similar results could be obtained. Thus, considering our hypothesis, there is evidence that KnowYourBeat is feasible and its use can improve Spotify SECO through the rec- ommendation service. Therefore, there is evidence that the null hypothesis (H0) could be rejected and the alternative hypothesis (H1) could be accepted. In addition, we can draw suggestions to enhance Spotify plat- form, considering SECO dimensions, which is our main goal. The first suggestion is about the lack of extension market, discussed at subsection 4.1. This kind of structure is attractive to newcomers, since it supports the contact between the product and the users. Also, this space can serve as showcase for successful applications and unexplored niches, that may increase the ecosystem visibil- ity and attractiveness. We recommend that the provided “Case of Success” section [22] grow to a wild access extension market. Second suggestion regards the lack of HATEOAS [8] standard that allows automated feature and data discover, as discussed on subsection 4.5. For instance, with suitable linkage and description between a track representation resource and the action of adding the said track on a playlist, a client application could discover this feature and present it on a user interface automaticaly. This is a pattern found in high-level maturity APIs [ 9, 17], and especially useful regards the interoperabilty aspect. That said, we suggest the adition of a HATEOAS standard over the provided endpoints of the Spotify Web API. Finally, there are additional audio content streaming services, such as TIDAL19 and Apple Music20 that also have public APIs for external developers. These APIs use data and services to generate value for their ecosystems with applications such as KnowYourBeat. However, there is no common standard over these APIs. This sce- nario is going against the interoperability issues and aggregated value, since it demands client application to implement different integration solutions to attend to different users. In this scenario, Spotify could leverage its ecosystem strategic engagement level, as discussed in subsection 4.6, looking for interaction support between competitors as a business opportunity. 6.3 Threats to Validity This evaluation is only valid within the specified context. Addi- tional evaluations are necessary, considering other SECOs contexts. 19http://tidal.com 20https://www.apple.com/br/music/', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Schettino et al. Therefore, the results cannot be generalized nor other problems faced in Spotify domain. It is necessary to prepare and conduct addi- tional experimental studies to extend the validity of the hypotheses previously stated. Another drawback is the number of case studies. Further studies could collect additional evidence that were not observed in the pre- sented case study. Additional experimental studies could also reveal certain aspects that were not explored such as health, robustness and attractiveness. It is also important to emphasize that these conclusions are drawn from preliminary evidence of the benefits of KnowYourBeat. Thus, it can be noted that the validity of the conclusions is related to the replication of the study in other contexts to ensure the feasibility of the approach. However, it was possible to identify situations in which similar results could be obtained. 7 CONCLUSION Software ecosystem is a new way of creating software, born from the need of responsiveness over the emergent requirements soft- ware products face everyday. Companies start to see that they no longer can develop entire software products, so they reach out of their walls for resources, like expertise and open source compo- nents. At the same time they offer stable market share and features where external developers may find a niche where to extend their competitive advantages. Brought into the light of recent SECO literature, we discussed it structure, such as governance strategies and niche players flavours, and its maturity as platform, keystone organization and infrastruc- ture provider. Regarding interoperability aspect, we analysed the Spotify ecosys- tem under the aegis of state-of-the-art literature, to understand which solutions they are proposing for the known challenges this kind of structure brings. Furthermore, the KnowYourBeat devel- opment process and constraints gave us a internal view of the ecosystem whereas the faced challenges contributed to this work. This research allows us to suggest evolutions that may extend its robustness and health. In short, this paper brings a software ecosystem assessment, towards organizational, social and technical aspects, focusing on interoperability. Through the knowledge acquired on this work, es- pecially on KnowYourBeat development, we present improvement suggestions for the ecosystem, regarding interoperability aspect As future works, there are some aspects that can be deepened. For instance, we pretend to analyze other aspects as reputation, fidelty and niche creation regards Spotify ecosystem. Also we aim for collect empirical data to draw conclusions about this SECO’s ma- turity and robustness. We seek to expand KnowYourBeat to explore further the ecosystem infraestructure, beyond the interoperability aspect. ACKNOWLEDGMENTS We would like to thank CNPq, CAPES, FAPEMIG, Jonathan Wils- son21, creator and maintainer of Spotify Web API SDK for PHP, for his unconscious help to this work. We also thank the reviewers for their valuable contributions. 21https://github.com/jwilsson REFERENCES [1] V. R Basili and D. M. Weiss. 1984. A Methodology for Collecting Valid Software Engineering Data. IEEE Trans. Softw. Eng SE-10, no. 6 (1984), 728–738. [2] Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. 2013. Recommender systems survey. Knowledge-based systems 46 (2013), 109– 132. [3] Jan Bosch. 2010. Architecture challenges for software ecosystems. In Proceedings of the Fourth European Conference on Software Architecture: Companion Volume. ACM, 93–95. [4] Jan Bosch. 2017. Speed, Data, and Ecosystems: Excelling in a Software-Driven World. CRC Press. [5] Henrik Bærbak Christensen, Klaus Marius Hansen, Morten Kyng, and Konstanti- nos Manikas. 2014. Analysis and design of software ecosystem architectures– Towards the 4S telemedicine ecosystem. Information and Software Technology 56, 11 (2014), 1476–1492.', 'nos Manikas. 2014. Analysis and design of software ecosystem architectures– Towards the 4S telemedicine ecosystem. Information and Software Technology 56, 11 (2014), 1476–1492. [6] Rodrigo Pereira dos Santos. 2016. Managing and monitoring software ecosystem to support demand and solution analysis. Ph.D. Dissertation. Universidade Federal do Rio de Janeiro. [7] Abhijit Dubey and Dilip Wagle. 2007. Delivering software as a service. The McKinsey Quarterly 6, 2007 (2007), 2007. [8] Roy T Fielding and Richard N Taylor. 2002. Principled design of the modern Web architecture. ACM Transactions on Internet Technology (TOIT) 2, 2 (2002), 115–150. [9] Martin Fowler. 2010. Richardson Maturity Model: steps toward the glory of REST. Online at http://martinfowler.com/articles/richardsonMaturityModel. html (2010). [10] Google. 2017. Material design - Introduction. https://material.io/guidelines/introduction-goals. Accessed 2017-05-24. [11] Dick Hardt. 2012. The OAuth 2.0 authorization framework. (2012). [12] Slinger Jansen and Michael A Cusumano. 2013. Defining software ecosystems: a survey of software platforms and business network governance. Software ecosystems: analyzing and managing business networks in the software industry 13 (2013). [13] Slinger Jansen, Anthony Finkelstein, and Sjaak Brinkkemper. 2009. A sense of community: A research agenda for software ecosystems. InSoftware Engineering- Companion Volume, 2009. ICSE-Companion 2009. 31st International Conference on. IEEE, 187–190. [14] Konstantinos Manikas. 2016. Revisiting software ecosystems research: A longi- tudinal literature study. Journal of Systems and Software 117 (2016), 84–103. [15] Konstantinos Manikas, Krysztof Wnuk, and Arisa Shollo. 2015. Defining decision making strategies in software ecosystem governance. Department of Computer Science, University of Copenhagen (2015). [16] Mic. 2016. How Does Spotify Make Money? Here’s the Business Model Behind the Streaming Service. https://mic.com/articles/137400/how-does-spotify-make- money-here-s-the-business-model-behind-the-streaming-service. Accessed 2017-05-24. [17] Leonard Richardson. 2008. Justice will take us millions of intricate moves. In International Software Development Conference (QCon). [18] Spotify. 2017. New Endpoint: Recently Played Tracks. https://developer.spotify.com/news-stories/2017/03/01/new-endpoint-recently- played-tracks/. Accessed 2017-06-02. [19] Spotify. 2017. Spotify Artists Blog. https://artists.spotify.com/blog. Accessed 2017-05-24. [20] Spotify. 2017. Spotify Artists FAQ. https://artists.spotify.com/faq/. Accessed 2017-05-24. [21] Spotify. 2017. Spotify Artists Guide. https://artists.spotify.com/guide. Accessed 2017-06-02. [22] Spotify. 2017. Spotify Developers Showcase. https://developer.spotify.com/showcase/. Accessed 2017-06-02. [23] Spotify. 2017. Understanding the Spotify Web API. https://labs.spotify.com/2015/03/09/understanding-spotify-web-api/. Ac- cessed 2017-06-02. [24] The Telegraph. 2015. Apple Music vs Spotify: How do the two streaming services compare? http://www.telegraph.co.uk/technology/2016/03/17/apple-music-vs- spotify-how-do-the-two-streaming-services-compare/. Accessed 2017-05-01. [25] Unicornomy. 2016. Spotify Business Model and How does Spotify Make Money. https://unicornomy.com/spotify-business-model-how-does-spotify- make-money/. Accessed 2017-05-24. [26] Ivo van den Berk, Slinger Jansen, and Lútzen Luinenburg. 2010. Software ecosys- tems: a software ecosystem strategy assessment model. In Proceedings of the Fourth European Conference on Software Architecture: Companion Volume. ACM, 127–134. [27] Peter Weill and Jeanne Ross. 2005. A matrixed approach to designing IT gover- nance. MIT Sloan Management Review 46, 2 (2005), 26. [28] Music Business Worldwide. 2016. Spotify is converting more people into paying subscribers than ever before. https://www.musicbusinessworldwide.com/spotify- is-converting-more-people-into-paying-customers-than-ever-before/. Accessed 2017-05-01.']","['SPOTIFY CHARACTERIZATION AS A SOFTWARE ECOSYSTEM This briefin reports scieitfc evideice oi Spotify beiin a softare ecosfyste,\ue2a1 takiin  ii  coisideratoi  state-oi-the-art defiitois aid coicepts\ue2a1 FINDINGS • Our analysis is based on recent  researches on softare ecosystems, to  defne traits te needed to see  in  Spotify in order to evfaluate it as a  ecosystem\ue2a1 • We fnd that Spotify is a monarchic,  privfately otned and privfately  coordinated plataiform • In our analysis, Spotify stll f ures as an  ad hoc ecosystem en a ement. It  already ta es responsibilites as a  platform and ecosystem  eystone by  ifacin  the challen es discussed abovfe. •  Presented aspects shot that there are  more than monetary contributons, but  it stll lac s ecosystem  ovfernance  tactcs to increase fdelity and nourish  lon  term relatonships. • To beter understand the interactons  and relatonships betteen niche players and the platform, KnotYourBeat tas  devfeloped. It helps user to understand  his musical taste and fnd son s he may  li e. • Throu h KnotYourBeat Devfelopment  te fnd that it is possible to exiplore a  niche enterin  on Spotifyys ecosystem. • We fnd throu h a case study that our  applicaton havfe a vfalue propositon  that tas reached by exitendin  Spotify  natvfe recommendaton system. • We drat su  estons about hot can  Spotify increase its ecosystem maturity,  throu h our fnds on see in  to exiplore  a niche on the ecosystem. • We su  est that the additon oif a  exitension mar et (li e Goo le Play or  Apple Store) could increase the  ecosystemys atractvfeness. • We su  est that levfera in  the API  maturity by implementn  the HATEOAS  standard could nourish hi her  levfels oif interoperability  betteen the ecosystemys  components. • We also su  est that Spotify  could levfera e its vfalue  propositon and its mar et share  and presence by creatn  a  technical standard ifor APIs on  audio streamin  industry,  allotin  applicatons to share  data and ifeatures amon  other  bi  players such as Tidal and  Apple Music Who is this briefin  or? Softare en ineerin  practtoners tho  tant evfaluate Softare Ecosystems and  understand hot  they diferent traits are  related to each other in the real torld. Where the finiins come  rom? All fndin s oif this briefn  tere exitracted  ifrom the  literature and direct  observfatons conducted by Vinícuis  Schetno, Re ina Bra a, José Maria Davfid  and Marco Araújo.  What is iiclunen ii this briefin? Our main fndin s and condensed  iniformaton about the ori inal tor  What is iot iiclunen ii this briefin? Methods, case study To access other evineice briefins oi  software einiieeriin: http://wwwllialu clbr/~ccbsoft2017/pr onramacao-sbcars/ For annitoial ii ormatoi about  NeNC: http://wwwluj lbr/ieic/ Schetno, Vinicius J., et al. ""SiSpotify characteritaton as a softare ecosystem.""Si Proceedin s oif the 11th Bratilian Symposium on Softare Comomponents, Architectures,  and Reuse. AComM, 2017.']","**Title:** Understanding Spotify as a Software Ecosystem

**Introduction:**
This Evidence Briefing aims to summarize the findings from a study that characterizes Spotify as a software ecosystem. The research explores the technical, organizational, and social aspects of Spotify, highlighting its evolution from a traditional product model to a dynamic software ecosystem that engages external developers and users. 

**Main Findings:**
1. **Software Ecosystem Definition:** Spotify is characterized as a software ecosystem (SECO) where interactions between various actors—such as users, developers, and content providers—are driven by value creation. These interactions are essential for the ecosystem's sustainability and growth.

2. **Organizational Structure:** Spotify operates as a monarchy within its ecosystem, where major decisions are centralized. This structure can streamline decision-making but may also limit diverse input from stakeholders, potentially impacting the ecosystem's health.

3. **Hybrid Value Creation Model:** Spotify employs a hybrid business model that combines proprietary and open-source contributions. While the core platform is proprietary, it allows for third-party applications that can either be proprietary or open-source, thus broadening its ecosystem.

4. **Interoperability Challenges:** The study identifies interoperability as a critical factor for ecosystem success. Spotify's APIs, while functional, lack certain standards (like HATEOAS) that could enhance discoverability and interaction among components, thereby improving user experience.

5. **KnowYourBeat Application:** The development of the KnowYourBeat application demonstrates how third-party tools can extend Spotify's native recommendation capabilities. By allowing users to input preferences based on recent listening history, the application enhances personalized recommendations.

6. **Monetization Strategies:** Spotify generates revenue through premium subscriptions and advertisements. The balance between free and premium users is crucial, as each group contributes differently to the ecosystem's financial viability.

7. **Recommendations for Improvement:** The study suggests establishing a more robust extension market to facilitate developer engagement and enhance the visibility of successful applications. Additionally, implementing HATEOAS standards could improve API usability and integration.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, product managers, and organizational leaders interested in understanding software ecosystems, particularly in the context of music streaming services like Spotify.

**Where the findings come from?**
The findings presented in this briefing are derived from a comprehensive analysis of Spotify's ecosystem, including a case study on the KnowYourBeat application, which was developed to evaluate the ecosystem's capabilities.

**What is included in this briefing?**
This briefing includes an overview of Spotify's ecosystem characteristics, organizational structure, interoperability challenges, and recommendations for enhancing its ecosystem.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research:**
Contact Vinicius J. Schettino at vinicius.schettino@ice.ufjf.br

**ORIGINAL RESEARCH REFERENCE:**
Schettino, V. J., Braga, R., David, J. M. N., & Araújo, M. A. P. (2017). Spotify Characterization as a Software Ecosystem. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017. DOI: 10.1145/3132498.3133836"
"['Retrospective for the Last 10 years of Teaching So/f_tware Engineering in UFC’s Computer Department Rossana M. de Castro Andrade∗ Universidade Federal do Cear´a Fortaleza, Cear´a, Brasil rossana@ufc.br Ismayle de Sousa Santos Universidade Federal do Cear´a Fortaleza, Cear´a, Brasil ismaylesantos@great.ufc.br Italo Linhares de Ara´ujo† Universidade Federal do Cear´a Fortaleza, Cear´a, Brasil italoaraujo@great.ufc.br Bruno Sab´oia Arag˜ao‡ Universidade Federal do Cear´a Fortaleza, Cear´a, Brasil bruno@great.ufc.br Fernanda Siewerdt§ Universidade Federal do Cear´a Fortaleza, Cear´a, Brasil fernandasiewerdt@great.ufc.br ABSTRACT So/f_tware Engineering (SE) is a discipline of Computer Science dedi- cated to teaching topics related to so/f_tware development. It involves a wide variety of topics, so the teaching of SE is a challenge, espe- cially to make the discipline a/t_tractive to the students. /T_herefore, in the last 10 years, the process of teaching SE has been applied and improved continually in the computer science course of the Computer Science Department of the Federal University of Cear´a. /T_his paper presents a current version of this process, as well as the results of questionnaires answered by students and monitors. /T_he activities related to the discipline of SE are also presented, in- cluding dynamics and the practical use of the theory through the development of so/f_tware applications by the students. CCS CONCEPTS •Social and professional topics → So/f_tware engineering edu- cation; KEYWORDS So/f_tware Engineering, Teaching-Learning Process, So/f_tware Engi- neering Teaching-Learning Experience ACM Reference format: Rossana M. de Castro Andrade, Ismayle de Sousa Santos, Italo Linhares de Ara´ujo, Bruno Sab´oia Arag˜ao, and Fernanda Siewerdt. 2017. Retrospective for the Last 10 years of Teaching So/f_tware Engineering in UFC’s Computer Department . In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. DOI: 10.1145/3131151.3131179 ∗Bolsista do CNPq de Produtividade em Desenvolvimento Tecnol ´ogico e Extens ˜ao Inovadora (DT) 2 † Bolsista de Doutorado da CAPES ‡ Bolsista da Fundac ¸˜ao Cearense de Pesquisa e Cultura §Bolsista do Programa de Educac ¸˜ao `a Docˆencia da UFC Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131179 1 INTRODUC ¸ ˜AO A disciplina de Engenharia de So/f_tware (ES) tem destaque no curso de Computac ¸˜ao por estar diretamente relacionada ao desenvolvi- mento de so/f_tware [14]. No entanto, ´e necess ´ario esforc ¸o para manter a aten c ¸˜ao dos alunos, especialmente por essa disciplina apresentar muito conte´udo em que algumas vezes ´e ministrado em apenas um semestre. O desa/f_io´e envolver o aluno de forma a impulsionar sua pro- dutividade durante as atividades da disciplina. Assim, diferentes trabalhos tem apresentado solu c ¸˜oes para apoiar o ensino da ES nos cursos de graduac ¸˜ao [4, 5, 10, 13]. Boa parte desses trabalhos tˆem destacado o uso de atividades pr´aticas com o desenvolvimento de so/f_tware no intuito de apoiar o ensino de ES. Essa abordagem pr´atica permite que os alunos /f_ixem melhor os conceitos a partir da sua aplicac ¸˜ao [9], al´em de motivar o aprendizado [15]. Em um survey recente, trabalho realizado por Portela et al. [9]1, revelou-se que 89% dos professores e 89% dos alunos consideram a realizac ¸˜ao', 'survey recente, trabalho realizado por Portela et al. [9]1, revelou-se que 89% dos professores e 89% dos alunos consideram a realizac ¸˜ao de projetos de so/f_tware efetiva para o aprendizado. Em um trabalho anterior [2] foi proposta uma metodologia de ensino te´orico-pr´atica adotada na disciplina de ES do Departamento de Computac ¸˜ao (DC) da Universidade Federal do Cear´a (UFC). O principal diferencial dessa metodologia foi alinhar as aulas te´oricas com o projeto de desenvolvimento de so/f_tware, de forma a estimular o aluno a executar o que foi aprendido durante as aulas. Em outro artigo dos autores [3] foram apresentadas as melhorias incorporadas nessa metodologia de ensino, juntamente com os resultados de question ´arios aplicados para coletar a opini ˜ao de uma turma de alunos quanto `a metodologia. Ambos os trabalhos, no entanto, focavam em atividades espec´ı/f_icas da disciplina, n˜ao apresentando uma vis˜ao geral de todas as atividades executadas. Para tratar desse aspecto, este artigo descreve o processo de ensino-aprendizagem que foi aplicado ao longo dos ´ultimos 10 anos na disciplina de ES no DC/UFC. Dessa forma, este artigo estende os trabalhos anteriores [ 2, 3] detalhando as atividades, pap ´eis e artefatos relacionados. Adicionalmente, para analisar o processo foram realizadas trˆes avaliac ¸˜oes: (i) an´alise do desempenho dos alunos durante a disci- plina; (ii) comparac ¸˜ao do feedback da turma de 2017 com o da turma de 2015 quanto `a disciplina, apresentado no artigo de Andrade et al. 1Realizado com 47 alunos e 23 professores 20 instituic ¸˜oes de ensino p´ublicas e privadas 358', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil R. M. C. Andrade et al. [3]; e (iii) an´alise da percepc ¸˜ao dos monitores quanto `as atividades executadas por eles durante a disciplina. Espera-se que este processo possa ajudar outros professores de ES, pois, de maneira geral, os resultados obtidos foram positivos tanto para os alunos quanto para os monitores e o professor. O restante deste artigo est´a organizado da seguinte forma: na Sec ¸˜ao 2 s ˜ao discutidos os trabalhos relacionados `a metodologias, processos e pr ´aticas de ensino de ES; na Se c ¸˜ao 3 ´e apresentado o processo de ensino de ES de/f_inido com base na experiˆencia no ensino dessa disciplina no Departamento de Computac ¸˜ao da UFC; na Sec ¸˜ao 4 ´e feita uma an ´alise dos resultados obtidos com esse processo; na Sec ¸˜ao 5 s˜ao sumarizadas as principais lic ¸˜oes aprendidas durante esses anos de ensino de ES; e, /f_inalmente, a Sec ¸˜ao 6 conclui o artigo e apresenta perspectivas de trabalhos futuros. 2 TRABALHOS RELACIONADOS Na literatura existem diversos trabalhos para apoiar o ensino de ES, que v˜ao desde m´etodos de ensino a propostas de jogos educativos. Nesta sec ¸˜ao ´e feita uma discuss˜ao dos trabalhos relacionados mais recentes, comparando-os com as contribuic ¸˜oes deste artigo. Vale ressaltar, no entanto, que n˜ao foram encontrados trabalhos relaci- onados que apresentassem um processo detalhando as atividades te´oricas e pr´aticas envolvidas no ensino de ES, objetivo deste artigo. Barbosa e Nelson [ 4] discutem sobre os desa/f_ios da execuc ¸˜ao de atividades pr´aticas de ES em disciplinas de cursos a dist ˆancia. Esses autores tamb´em executaram question´arios com os alunos a /f_im de avaliar, dentre outras quest˜oes, a contribuic ¸˜ao do trabalho pr´atico para o aprendizado. Tavares et al. [15] apresentam um es- tudo emp´ırico para veri/f_icar a efetividade das atividades pr´aticas no melhoria do desempenho do estudante quanto ao aprendizado do m´etodo Scrum[12]. Os autores tamb´em aplicaram um question´ario para coletar a opini˜ao dos estudantes. Rocha et al. [11] discutem so- bre o projeto desenvolvimento de so/f_tware com o Scrum para apoiar o aprendizado de alunos da disciplina de ES de um curso t´ecnico em inform´atica. O presente artigo, por outro lado, prop˜oe um processo que envolve todas as atividades da disciplina de ES. Dessa forma, este artigo n˜ao se limita a discutir as atividades pr´aticas como nos trabalhos de Barbosa e Nelson [ 4], Tavares et al. [15] e Rocha et al. [11]. Outro diferencial, ´e que a avaliac ¸˜ao descrita neste artigo envolve tamb´em uma an´alise das notas dos alunos e uma coleta da perspectiva dos monitores quanto `a disciplina. Portela et al. [10] apresenta o FRAMES, que ´e um framework de apoio ao ensino dos t´opicos de ES. Esse framework ´e composto por quatro m´odulos: Plano de Ensino, Modelo Pedag ´ogico, Projeto Pr´atico, Avaliac ¸˜ao de Aprendizagem. Esses m´odulos, por sua vez, s˜ao compostos por itens de ensino que podem ser instanciados para o ensino das unidades de conhecimento (e.g., Engenharia de Requi- sitos, Processos de So/f_tware). A principal diferenc ¸a em relac ¸˜ao `a este trabalho ´e a de/f_inic ¸˜ao de um processo de/f_inindo pap´eis, res- ponsabilidades e artefatos relacionados as diferentes atividades executadas ao decorrer da disciplina de ES. Assim, esse processo n˜ao ´e direcionado `as unidades de conhecimento, mas `as atividades relacionadas com o ensino de ES. Billa e Cera [5] prop˜oem o ensino da ES por meio de uma abor- dagem de ensino-aprendizagem baseada em problemas. Nessa abor- dagem, equipes de alunos prop ˜oem um sistema computacional que solucione um problema real, praticando assim os conceitos de ES e o trabalho em equipe. No processo descrito no presente artigo, existe um subprocesso dedicado a descrever atividades e artefatos envolvidos no desenvolvimento de uma projeto de so/f_tware com foco no ensino de ES. Al ´em disso, s ˜ao discutidas outras pr ´aticas,', 'envolvidos no desenvolvimento de uma projeto de so/f_tware com foco no ensino de ES. Al ´em disso, s ˜ao discutidas outras pr ´aticas, como dinˆamicas, e tamb´em atividades de monitoramento que v˜ao al´em das atividades do projeto de so/f_tware. Silva e Vasconcelos [13] prop˜oem a utilizac ¸˜ao de um ambiente integrado desenvolvido para automatizar os processos de ES, como um apoio ao ensino-aprendizagem da ´area. A ˆenfase do artigo est´a no uso de duas ferramentas do ambiente relacionadas aos processos de Gerˆencia de Projetos e de Ger ˆencia de Requisitos. O presente artigo, por outro lado, n˜ao foca em uma tecnologia de suporte, mas sim em apresentar o processo aplicado na UFC, que descreve as atividades que se relacionam com o ensino-aprendizagem de ES. Medeiros et al. [6] apresentam um jogo web multiplayer chamado GameES para o ensino da Engenharia de So/f_tware. Esse jogo ´e baseado em perguntas e respostas em um ambiente virtual e pode ser utilizado para apoiar o ensino de todos os t´opicos da Engenharia de So/f_tware. Outro jogo para apoiar o ensino de ES´e o SimulES [8], que simula um jogo de cartas cujo objetivo ´e produzir um so/f_tware espec´ı/f_ico. Existem tamb´em jogos para auxiliar no ensino de t´opicos espec´ı/f_icos da engenharia de so/f_tware, como o iTestLearning [7] para o ensino de teste de so/f_tware e o SCRUMIA [16], que apoia o ensino dos conceitos do Scrum. Este artigo, por sua vez, n ˜ao apresenta um jogo de apoio ao ensino de ES, mas sim um processo descrevendo as atividades de suporte ao ensino de uma disciplina de ES. Deste modo, os jogos supracitados podem ser utilizados nas atividades pr´aticas previstas no processo. 3 PROCESSO Nesta sec ¸˜ao ´e apresentado o processo de ensino de ES com base na experiˆencia adquirida na disciplina de ES no curso de Ciˆencia da Computac ¸˜ao na UFC nos ´ultimos 10 anos. Na grade curricular, h´a apenas uma disciplina obrigat´oria de 60 horas e que ´e ofertada no 5º semestre. Nela, s˜ao abordados temas como Processos e Modelos de Processo de So/f_tware, M´etodos ´Ageis, Engenharia de Requisitos, Modelagem de sistemas, Projeto de arquitetura, Implementa c ¸˜ao, Testes, Evoluc ¸˜ao e Gerˆencia de Projetos. O processo tem como objetivo identi/f_icar pap´eis, artefatos e atividades na disciplina de Engenharia de So/f_tware para combinar atividades te´oricas e pr´aticas, com vistas a um maior aprendizado pelos alunos. A Figura 1 apresenta tal processo, e, em seguida, s˜ao detalhados os pap´eis e as atividades executadas em cada etapa. Ele envolve trˆes pap´eis, cada um com responsabilidades diferen- tes. O primeiro ´e o papel do Professor, que tem como principais atribuic ¸˜oes planejar a disciplina, ministrar aulas e avaliar o apren- dizado. O segundo ´e o do Monitor, que procura auxiliar o professor no planejamento e execuc ¸˜ao das atividades da disciplina. O terceiro papel ´e o do Aluno, que participa das aulas pr ´aticas e te ´oricas e executa atividades relacionadas aos trabalhos pr´aticos. 3.1 Planejar Disciplina • Descric ¸˜ao: De/f_inir oPlano da Disciplina, o qual especi/f_ica os marcos da disciplina, as atividades te ´oricas e pr´aticas, os assuntos a serem ministrados por aula e os meios de 359', 'Retrospective for the Last 10 years of Teaching SE in UFC/DC SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figura 1: Processo de Ensino-Aprendizagem para a disciplina de Engenharia de So/f_tware comunicac ¸˜ao a serem utilizados. Al ´em disso, nessa ativi- dade tamb´em ´e de/f_inida aLista de Monitores e, com base no conhecimento t´ecnico (e.g., conhecimento em Android) dos monitores envolvidos, s˜ao marcadas no Plano da Disciplina as aulas extras que ser ˜ao ofertadas. Geralmente, como a equipe de monitores envolve mais de trˆes pessoas, tamb´em ´e de/f_inido o l´ıder dos monitores, de forma a facilitar a comunicac ¸˜ao Professor-monitores. • Respons´avel: Professor • Participantes: - • Entrada da atividade: Ementa, Lista de Alunos (ou pre- vis˜ao da quantidade de alunos), Lista de Candidatos a Mo- nitoria, /Q_uestion´ario para o Per/f_il dos Monitores • Sa´ıda da atividade: Plano da Disciplina, Lista de Monitores, Per/f_il dos Monitores 3.2 Preparar Ambiente de Comunicac ¸˜ao • Descric ¸˜ao: Coletar e-mail dos alunos e n´umero de telefone do Professor ou dos Monitores para con/f_igurar as ferra- mentas de comunicac ¸˜ao a serem utilizadas na disciplina. • Respons´avel: Monitor • Participantes: - • Entrada da atividade: Plano da Disciplina, Lista de Alunos • Sa´ıda da atividade: Ambiente de comunica c ¸˜ao con/f_igu- rado composto por: (i) uma lista de e-mail para comunicac ¸˜ao entre Professor e Monitor; (ii) uma lista de e-mail para comunicac ¸˜ao entre Professor, Monitor e Aluno; (iii) um grupo no WhatsApp2 para comunicac ¸˜ao entre Professor e Monitor; e (iv) uma P´agina no Facebook3 para comunicac ¸˜ao entre Monitor e Aluno. 3.3 Identi/f_icar Per/f_il dos Alunos • Descric ¸˜ao: Identi/f_icar per/f_il dos alunos para checar a ne- cessidade de mudanc ¸as no plano da disciplina (e.g., mais aulas extras sobre um determinado t´opico). Dessa forma, 2h/t_tps://www.whatsapp.com/ 3h/t_tps://pt-br.facebook.com/ com base nos resultados do question´ario aplicado pelo Mo- nitor, o Professor pode atualizar o Plano de Disciplina para re/f_letir`as necessidades dos discentes. • Respons´avel: Professor • Participantes: Monitor, Aluno • Entrada da atividade: /Q_uestion´ario para o Per/f_il dos Alu- nos, Plano de Disciplina • Sa´ıda da atividade: Per/f_il dos Alunos, Plano de Disciplina atualizado 3.4 Monitorar e Controlar Atividades da Disciplina • Descric ¸˜ao: Durante toda a disciplina, o Professor, com o aux´ılio do Monitor, deve avaliar o aprendizado do aluno por meio de Provas e Listas de Exerc´ıcios. Al´em disso, ques- tion´arios devem ser aplicados para capturar a opini˜ao dos alunos quanto `as aulas pr´aticas ministradas por monitores, atividades pr´aticas, dinˆamicas, dentre outros. Os resultados desses question´arios s˜ao importantes para, por exemplo, identi/f_icar ac ¸˜oes necess´arias para motivar os alunos. • Respons´avel: Professor • Participantes: Monitor, Aluno • Entrada da atividade: Provas, Listas de Exerc´ıcios, /Q_ues- tion´arios para coletar a percepc ¸˜ao dos alunos • Sa´ıda da atividade: Avaliac ¸˜ao dos alunos com relac ¸˜ao as provas e exerc´ıcios, ac ¸˜oes a serem executadas para garantir a motivac ¸˜ao dos alunos e o ensino-aprendizagem de ES. 3.5 Ministrar Aulas Te ´oricas e Pr´aticas • Descric ¸˜ao: Ministrar as aulas de car ´acter te ´orico e/ou pr´atico para auxiliar o Aluno no processo de aprendiza- gem dos t´opicos de ES. Essas aulas s˜ao acompanhadas por pequenas atividades, exerc´ıcios que podem ser feitos du- rante a aula ou extra-classe. Adicionalmente, dinˆamicas4 tamb´em podem ser utilizadas para manter o interesse do aluno na disciplina. 4Nos ´ultimos anos, a aplicac ¸˜ao da dinˆamica “O cliente mandou” tem exibido resultado positivos. 360', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil R. M. C. Andrade et al. • Respons´avel: Professor • Participantes: Monitor, Aluno • Entrada da atividade: Plano da Disciplina, Listas de Exerc´ıcios • Sa´ıda da atividade: Listas de Exerc´ıciosrespondidas em classe ou extra-classe 3.6 Ministrar Aulas Extras • Descric ¸˜ao: Ministrar aulas extras de forma a complemen- tar a grade da disciplina de ES. Geralmente essas aulas s˜ao relacionadas ao uso de ferramentas espec ´ı/f_icas e outros t´opicos que auxiliam os alunos na conduc ¸˜ao da atividade de Projeto de Desenvolvimento de So/f_tware. Por serem aulas extras aos assuntos cobrados durante a disciplina, n˜ao existem artefatos de sa´ıda para esta atividade. • Respons´avel: Monitor • Participantes: Aluno • Entrada da atividade: Plano da Disciplina • Sa´ıda da atividade: - 3.7 Apresentar Pap ´eis e Pro/f_issionais • Descric ¸˜ao: Proporcionar o contato dos discentes com pro- /f_issionais da ind´ustria por meio de relatos destes quanto `as atividades pr´aticas executadas durante o desenvolvimento de so/f_tware. Geralmente, s˜ao convidados sete pro/f_issionais com os seguintes pap´eis: analista de sistemas, analista de requisitos, arquiteto de so/f_tware, analista de testes, gerente de desenvolvimento de so/f_tware, gerente de TI,designer e desenvolvedor. • Respons´avel: Professor • Participantes: Aluno • Entrada da atividade: Plano da Disciplina • Sa´ıda da atividade: - 3.8 De/f_inir Equipes • Descric ¸˜ao: De/f_inir as equipes dos alunos para a execuc ¸˜ao do trabalho /f_inal da disciplina: projeto de um sistema de so/f_tware. • Respons´avel: Professor, Aluno • Participantes: - • Entrada da atividade: Plano da Disciplina • Sa´ıda da atividade: Lista das Equipes 3.9 Planejar e Executar Projeto de Desenvolvimento de So/f_tware O subprocesso “Planejar e Executar Projeto de Desenvolvimento de So/f_tware” trata das atividades relacionadas a implementac ¸˜ao de um so/f_tware pelos alunos no decorrer da disciplina. Nesse subprocesso, cada grupo de alunos representa uma empresa de so/f_tware, e os alu- nos assumem pap´eis, tais como: desenvolvedor, testador e gerente. A experiˆencia mostrou que essa de/f_inic ¸˜ao de pap´eis ´e interessante por permitir que os alunos experimentem na pr´atica as atribuic ¸˜oes de/f_inidas para cada papel no desenvolvimento de so/f_tware. Dessa forma, esse projeto ´e importante, pois permite aos alunos exercitarem os conhecimentos adquiridos durante as aulas te´oricas. Al´em disso, essa atividade proporciona outros aprendizados, como: trabalho em equipe, proatividade e lideranc ¸a. A Figura 2 apresenta o processo de desenvolvimento de so/f_tware da disciplina de ES. A maioria das atividades s˜ao subprocessos que geram artefatos do desenvolvimento de so/f_tware. A Tabela 1 apre- senta detalhes sobre esses artefatos, enquanto a Figura 3 descreve o processo “Desenvolver e Validar Artefato”, que´e usado em todos os subprocessos de “Planejar e Executar Projeto de Desenvolvimento de So/f_tware”. A primeira atividade do processo “Desenvolver e Validar Ar- tefato”, diz respeito `a disponibilizac ¸˜ao de templates pelo Profes- sor/Monitor. Esses templates s ˜ao importantes para dar um dire- cionamento para o aluno do que se espera do artefato e tamb ´em facilita a avaliac ¸˜ao do aprendizado. Ao todo, foram desenvolvidos oito templates para a disciplina de ES da UFC, um para cada artefato descrito na Tabela 1. Na segunda atividade, as equipes desenvol- vem os artefatos. Por exemplo, na atividade Detalhar Requisitos do Projeto (ver Figura 2), o Aluno cria o documento de especi/f_icac ¸˜ao de requisitos que descreve o que deve ser implementado durante o projeto da disciplina. Um vez que o artefato´e criado e enviado para o Professor/Monitor, ele pode avaliar o artefato (i.e., atribuir uma nota) e sugerir me- lhorias, que s ˜ao ent ˜ao repassadas ao aluno, para a corre c ¸˜ao de acordo com as sugest˜oes indicadas. Finalmente, ap´os as correc ¸˜oes,', 'lhorias, que s ˜ao ent ˜ao repassadas ao aluno, para a corre c ¸˜ao de acordo com as sugest˜oes indicadas. Finalmente, ap´os as correc ¸˜oes, as equipes dos alunos enviam os artefatos corrigidos para o Profes- sor/Monitor, que por sua vez, pode incrementar a nota do artefato de/f_inida anteriormente com base nas correc ¸˜oes feitas. 3.10 Avaliar para Evoluir o Processo • Descric ¸˜ao: Avaliar o desempenho dos alunos durante a disciplina para veri/f_icar a adequac ¸˜ao do processo quanto ao ensino-aprendizagem de ES aos alunos. Essa atividade envolve coletar a opini ˜ao dos monitores para identi/f_icar poss´ıveis ac ¸˜oes de melhoria no processo. • Respons´avel: Professor • Participantes: Monitor • Entrada da atividade: /Q_uestion´arios para coletar a percepc ¸˜ao dos monitores, Manual do Monitor • Sa´ıda da atividade: Ac ¸˜oes para a pr ´oxima turma de ES, Mudanc ¸as no processo da disciplina, Manual do Monitor atualizado 4 AVALIAC ¸˜AO Para avaliar o processo de ensino-aprendizagem de ES foram exe- cutadas as seguintes avalia c ¸˜oes: (i) An ´alise do desempenho dos alunos, por meio das notas na disciplina; (ii) Survey [17] por meio de um question´ario online para identi/f_icar a percepc ¸˜ao dos alunos quanto a disciplina de ES; (iii) Survey [17] por meio de um ques- tion´ario online aplicado aos monitores para coletar feedback deles com relac ¸˜ao a disciplina. Nas pr´oximas subsec ¸˜oes s˜ao apresentados e discutidos os resul- tados dessas avaliac ¸˜oes. 4.1 An ´alise do desempenho dos alunos Para avaliar o desempenho dos alunos durante a disciplina de ES, foram analisadas as notas dos discentes nos ´ultimos sete anos, entre 2010 e 2016. A Figura 4 apresenta a distribuic ¸˜ao das notas desses alunos pelos instrumentos de avalia c ¸˜ao. No caso das provas, o 361', 'Retrospective for the Last 10 years of Teaching SE in UFC/DC SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figura 2: Processo Planejar e Executar Projeto de Desenvolvimento de So/f_tware Figura 3: Processo Desenvolver e Validar de Artefato professor avaliava o conhecimento dos alunos e nos casos dos projetos, os monitores analisavam o hist´orico de revis˜oes a /f_im de identi/f_icar o papel de cada um na construc ¸˜ao do artefato. Nos trˆes primeiros anos (2010, 2011 e 2012), foram utilizadas trˆes notas (projeto de desenvolvimento de so/f_tware, primeira e segunda provas) para de/f_inir a m´edia /f_inal dos alunos. A partir de 2013, as listas de exerc´ıcios foram adicionadas como mais uma avaliac ¸˜ao para estimular os alunos a estudarem para /f_ixar os conhecimentos. Observando as notas dos alunos ´e poss´ıvel notar um crescimento das mesmas ao longo dos anos, exceto no ano de 2014, onde houve uma queda das m´edias das notas das avaliac ¸˜oes. A m´edia da nota dos projetos de desenvolvimento de so/f_tware no ano de 2010 foi 6,65, enquanto no ano de 2016 foi de quase 8,0. Representando assim um crescimento de 20.3%. Acredita-se que o impacto desse cres- cimento seja oriundo das melhorias feitas no processo de ensino- aprendizagem ao longo dos anos. Um exemplo do que pode ter levado `a esse aumento ´e que a partir de 2015 as equipes de alu- nos passaram a ser tratadas como “empresas desenvolvedoras de so/f_tware”, e dessa forma, os alunos foram estimulados a concluir com mais qualidade o projeto de desenvolvimento de so/f_tware da disciplina. Al´em disso, em 2015, a formac ¸˜ao das empresas, simulando um ambiente real, passou a considerar a possibilidade de estimular habilidades como trabalho em equipe, organiza c ¸˜ao do trabalho, proatividade, disciplina, e comunica c ¸˜ao. Dessa forma, os alunos continuaram se organizando em equipes com trˆes membros (como nos anos anteriores a 2015), por ´em o professor e os monitores faziam um sorteio para formar empresas com seis integrantes. Ao trazer para a disciplina um ambiente mais pr ´oximo do real, com v´arias pessoas envolvidas e diferentes pap´eis e responsabilidades, os alunos passaram a ver o projeto de desenvolvimento de so/f_tware de uma forma mais pro/f_issional. Essa mudanc ¸a ocorreu de tal forma que percebeu-se um aumento na complexidade e relev ˆancia dos projetos. Isso tamb´em impactou na conclus˜ao dos mesmos, pois at´e 2014 muitas equipes n˜ao conseguiam concluir o so/f_tware. Por exemplo, em 2014, existiam aplicac ¸˜oes com uma complexi- dade menor, como um aplicativo para pesquisa de opini˜ao, enquanto em 2015 foi desenvolvido um projeto que veri/f_icava quais ingredi- entes vocˆe tinha em casa e sugeria uma receita. J´a no caso do ano de 2016, foi desenvolvida uma aplicac ¸˜ao, chamada DoeSempre5, com o intuito de orientar os usu´arios sobre doac ¸˜oes de sangue e plaquetas na cidade de Fortaleza. Essa aplica c ¸˜ao foi al´em da disciplina e se tornou um produto dispon´ıvel na loja de aplicativos do Google6. As notas das provas, 1 ª Avaliac ¸˜ao Parcial (AP) e 2 ª AP, nos gr´a/f_icos da Figura 4 oscilam em torno de 6,30 na 1ª AP e 5,87 na 2ª AP. Apesar das avaliac ¸˜oes te´oricas se apresentarem como notas 5h/t_tps://www.doesempre.com/ 6h/t_tp://play.google.com/ 362', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil R. M. C. Andrade et al. Tabela 1: Lista de artefatos produzidos durante o projeto de so/f_tware Atividade/SubProcesso Artefato Descric ¸˜ao do artefato De/f_inir Escopo Descric ¸˜ao Informal Descric ¸˜ao geral da aplicac ¸˜ao a ser desenvolvida, contendo t´ıtulo, sigla, atribuic ¸˜oes dos pap´eis dos membros da equipe e principais funcionalidades Realizar Estudo de Viabili- dade Relat´orio de Estudo de Viabilidade Estudo sobre viabilidade /f_inanceira, t´ecnica, organizacional e de cronograma, considerando necessidades reais e analisando principais riscos Elaborar Termo de Abertura Termo de Abertura Documento de aprova c ¸˜ao do projeto pelos clientes e pela equipe, contendo prop´osito e escopo do projeto e do produto, premissas e restri c ¸˜oes, marcos e recursos humanos envolvidos De/f_inir Plano de Projeto Plano de Projeto Planejamento do projeto, contendo modelo de processo utilizado, gerenciamento de riscos, planejamento de aquisic ¸˜oes, planejamento de recursos humanos, estru- tura anal´ıtica do projeto (EAP) e cronograma Detalhar Requisitos do Pro- jeto Relat´orio de Especi/f_icac ¸˜ao de Requisitos Especi/f_icac ¸˜ao dos requisitos do usu´ario, requisitos do sistema (funcionais e n˜ao funcionais), diagramas e descric ¸˜oes dos casos de uso ou est´orias de usu´ario, mo- delos do sistema (representados atrav´es de diagramas de atividades), arquitetura do sistema e evoluc ¸˜ao do sistema De/f_inir Projeto da Aplicac ¸˜ao Projeto da Aplicac ¸˜ao Documento que contem diagramas de classe, sequˆencia e estados da aplicac ¸˜ao Implementar Aplicac ¸˜ao C´odigo C´odigo desenvolvido pela equipe na plataforma Android contendo execut´avel Executar Testes / Elaborar Plano de Testes Plano de Testes Documento descrevendo os requisitos da aplicac ¸˜ao a serem testados, estrat´egias de teste, ambientes de teste, recursos humanos, riscos e contingˆencias e especi/f_icac ¸˜ao dos testes detalhando procedimentos e casos de teste Gerar Relat´orio de Testes Relat´orio de Execu c ¸˜ao dos Testes Relat´orio dos testes realizados na aplicac ¸˜ao, tanto no dispositivo m´ovel quanto no emulador. Os testes devem respeitar o plano de testes e considerar caracter´ısticas da aplicac ¸˜ao e os resultados em ambientes e dispositivos diferentes Apresentar Projeto Apresentac ¸˜ao em Slides Apresentac ¸˜ao do projeto da aplicac ¸˜ao a ser realizada pelos membros da equipe. Deve-se considerar p ´ublico alvo, funcionalidades principais, vantagens sobre outras aplicac ¸˜oes existentes no mercado e resultados dos testes abaixo da m´edia 7,0, geralmente, os alunos tinham notas melhores nos exerc´ıcios e no projeto de desenvolvimento de so/f_tware. Uma poss´ıvel explicac ¸˜ao para isso ´e que os alunos se dedicavam mais `as atividades pr´aticas. Figura 4: M ´edia das notas dos alunos por avaliac ¸ ˜ao nos ´ultimos 7 anos A Figura 5 mostra o gr ´a/f_ico debox plot com as medianas das notas /f_inais dos alunos nos´ultimos 7 anos. O ano de 2014 cont´em a mediana mais baixa, com o valor de 5,9. Enquanto isso, o ano que apresentou a maior mediana foi 2013 com o valor de 7,1. Apesar dessa diferenc ¸a, os demais anos tiveram medianas com valores pr´oximos. Tamb´em deve ser ressaltado que nos dois ´ultimos anos, 2015 e 2016, os alunos tiveram notas mais lineares em comparac ¸˜ao aos demais anos. Em 2016, tamb´em podem ser observados v´arios outliers, mas n ˜ao foram feitos estudos para identi/f_icar o porquˆe dessa quantidade. Ainda ´e poss´ıvel associar as notas dos projetos com os modelos de processo escolhidos, como ilustrado na Figura 6. No ano de 2010, as equipes escolheram quatro diferentes modelos de processo: incre- mental, espiral, prototipac ¸˜ao e cascata. O modelo que apresentou um melhor resultado foi o espiral com a m´edia de 8,2. Em 2011, os modelos de processo escolhidos foram os mesmos do ano anterior. Por outro lado, o que obteve melhor resultado foi o', 'um melhor resultado foi o espiral com a m´edia de 8,2. Em 2011, os modelos de processo escolhidos foram os mesmos do ano anterior. Por outro lado, o que obteve melhor resultado foi o incremental, com m´edia 6,8. Acredita-se que isso tenha acontecido porque, utilizando-se do modelo incremental, em cada entrega da disciplina, uma nova funcionalidade ´e apresentada. Em 2013, nenhuma equipe optou pelo modelo espiral, por´em es- colheram os outros trˆes modelos usados anteriormente. A prototipac ¸˜ao foi o que obteve a melhor m´edia: 7,7. Uma das equipes que optou pelo incremental utilizou o Scrum para gerenciamento do projeto e obteve a nota 6,3. Em 2014, os alunos optaram pelos mesmos modelos de processo, e dessa vez os projetos que seguiram a prototipac ¸˜ao tiveram a m´edia de 9,1. O Scrum foi usado mais uma vez para gerenciar o projeto de uma equipe do modelo incremental, e essa equipe de alunos obteve a nota 6,3 para o projeto de so/f_tware da disciplina. 363', 'Retrospective for the Last 10 years of Teaching SE in UFC/DC SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figura 5: Mediana das notas /f_inais dos alunos de ES nos ´ultimos 7 anos Figura 6: M´edia das notas por processos de desenvolvimento utilizados nos projetos /f_inais Nos dois ´ultimos anos, 2015 e 2016, os alunos optaram por utilizar o incremental ou a prototipa c ¸˜ao. Nesse caso, as m ´edias foram similares, com a diferenc ¸a de apenas 0,5 e 0,7 pontos entre os dois modelos de processo. Sendo que em 2015 o modelo incremental teve a maior m ´edia, enquanto que em 2016 a maior m ´edia /f_icou com os projetos que usaram prototipac ¸˜ao. Finalmente, com base no gr´a/f_ico da Figura 6,´e poss´ıvel perceber que a prototipac ¸˜ao e o incremental foram os modelos de processo mais escolhidos, estando presentes em todos os anos. Os dois mo- delos tiveram esses resultados por requererem entregas parciais validadas com os monitores antes do produto /f_inal. Contudo, as equipes que escolheram o modelo incremental tiveram as melhores m´edias na maioria dos anos investigados. 4.2 Survey com os alunos 4.2.1 Avalia c ¸˜ao da aula sobre carreiras em ES. Uma das melho- rias feitas no processo em 2015 [3] foi a inclus˜ao de apresentac ¸˜oes feitas por pro/f_issionais graduados que trabalhavam em projetos de desenvolvimento, pesquisa e extens˜ao do laborat´orio GREat7, execu- tados em parcerias com empresas privadas [1]. Essas apresentac ¸˜oes evidenciavam os diferentes pap´eis e responsabilidades dentro de um ambiente real de trabalho, despertando a curiosidade dos alunos quanto a carreira de ES. Em 2015 e 2017 foram levados para a disciplina pro/f_issionais que desempenhavam os seguintes pap ´eis: i) gerente de projetos; ii) analista de sistemas; iii) analista de requisitos; iv) analista de testes; e v) arquiteto de so/f_tware. Ap´os a aula com as apresentac ¸˜oes desses pro/f_issionais, os alunos responderam um question´ario com perguntas relacionadas a vis˜ao geral dos cargos e o quanto essas apresentac ¸˜oes eram motivadoras para eles. A Figura 7 apresenta os resultados destes question ´arios. Em 2015, dos 37 alunos que responderam o formul´ario, 89.1% concor- daram ou concordaram fortemente que /f_icaram motivados com as apresentac ¸˜oes dos pro/f_issionais. J´a em 2017, das 27 avaliac ¸˜oes 89% concordaram ou concordaram fortemente que /f_icaram motivados com essas apresentac ¸˜oes. Apenas um aluno respondeu que n ˜ao /f_icou motivado com essa aula em 2015, o que representa 2,7%. E, apenas trˆes alunos se mostraram indiferentes com a apresentac ¸˜ao nos dois anos. Figura 7: Feedback dos alunos em relac ¸˜ao `a motivac ¸˜ao ocasi- onada pela aula de carreiras em ES Esses dados sugerem que o contato com os pro/f_issionais contri- bui para que os alunos permanec ¸am motivados com a disciplina. Uma poss´ıvel explicac ¸˜ao para essa motiva c ¸˜ao ´e o conhecimento da carreira com pro/f_issionais que a exercem no dia-a-dia. Alguns dos alunos, passam inclusive a se identi/f_icar com algum dos pap´eis, como desenvolvedor ou analista de teste. 4.2.2 Avalia c ¸˜ao da disciplina. Tamb´em foram aplicados ques- tion´arios a /f_im de obter ofeedback dos alunos visando a melhoria das aulas e do processo de aprendizagem. Baseado nisso, os alunos responderam quest˜oes acerca das aulas e das dinˆamicas apresenta- das. A Figura 8 apresenta o resultado da opini˜ao dos alunos sobre a disciplina de ES do DC/UFC. Em 2015, ao serem questionados se estavam gostando da disciplina, dos 24 alunos que preencheram, 75% responderam positivamente, sendo 20,8% como concordo forte- mente e 54,2% responderam que concordam. /Q_uatro alunos /f_icaram 7h/t_tp://www.great.ufc.br 364', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil R. M. C. Andrade et al. neutros quanto a essa quest ˜ao. Por /f_im, 8,4% dos respondentes a/f_irmaram que n˜ao estavam gostando das aulas, sendo a metade como discordo e a outra metade como discordo fortemente. Figura 8: Feedback dos alunos sobre as aulas No ano de 2017, os resultados foram parecidos, apresentando uma ligeira variac ¸˜ao nas respostas positivas. Ao todo, trinta pes- soas preencheram o question ´ario. Desse total, 21 pessoas, o que corresponde a 70%, disseram estar gostando da forma que as aulas estavam sendo conduzidas. Desse total, 20% concordando forte- mente e 50% apenas concordando. Apenas 3,3% a/f_irmaram que discordam fortemente quanto a estar gostando das aulas da disci- plina de ES. Apesar de n˜ao ter sido realizado um estudo para identi/f_icar as cau- sas que levaram a um aumento das respostas negativas, acredita-se que o momento da execuc ¸˜ao do question´ario possa ter in/f_luenciado. Em 2015 os question´arios foram aplicados ap´os algumas dinˆamicas e em 2017 foi antes da realizac ¸˜ao de algumas dinˆamicas previstas para a disciplina. Al´em disso, ressaltamos que a maioria dos alunos da turma continuou aprovando a disciplina. Durante as aulas, foram tamb ´em executadas din ˆamicas como a dinˆamica “O Cliente mandou”[3]. Nessa dinˆamica, um monitor desempenha o papel de cliente passando aos analistas de requisi- tos de cada empresa as suas necessidades e expectativas para que cada uma fac ¸a estimativas considerando desde tempo e custos ne- cess´arios para o desenvolvimento da aplicac ¸˜ao, at´e os tipos de testes que podem ser realizados. Ao /f_inal dessa dinˆamica, todas as estima- tivas levantadas pelas equipes de alunos s˜ao colocadas no quadro para uma discuss˜ao com a turma de como existem discrep ˆancias nas estimativas e como a disciplina de ES ´e importante. Tanto em 2015 quanto em 2017 essas e outras dinˆamicas foram aplicadas, sendo a citada anteriormente a ´unica avaliada. A Figura 9 cont´em os resultados obtidos com a an´alise dos resultados desses for- mul´arios com relac ¸˜ao a a/f_irmac ¸˜ao “Eu estou gostando das dinˆamicas e atividades intra-classe da disciplina”, a qual os alunos poderiam concordar fortemente, concordar, se manter neutro, discordar ou discordar fortemente. Em 2015, 87,5% das 24 respostas concordaram que estavam gostando das din ˆamicas, sendo 33,3% concordando fortemente. Apenas tr ˆes discentes responderam que estavam se sentindo neutros em relac ¸˜ao a execuc ¸˜ao dessas dinˆamicas em sala de aula. Figura 9: Feedback dos alunos sobre as dinˆamicas aplicadas Em 2017, por sua vez, dos 30 alunos que responderam, 63,4% (19 alunos) a/f_irmaram estar gostando das dinˆamicas, enquanto oito alu- nos se colocaram como neutros. Nesse ano, dois alunos discordaram e 1 discordou totalmente da a/f_irmac ¸˜ao colocada no question´ario, in- dicando que eles n˜ao estavam satisfeitos com as dinˆamicas aplicadas. Cabe destacar que o per/f_il dos pr´oprios monitores que executaram essa atividade, que nesse ano desempenhavam pela primeira vez essa func ¸˜ao, pode ter impactado nos resultados. Embora tenha aumentado a quantidade de alunos insatisfeitos com as dinˆamicas em 2017, acredita-se que assim como os projetos de desenvolvimento de so/f_tware, elas tem aumentado o interesse dos discentes na disciplina de ES. 4.3 Survey com os monitores Para coletar o ponto de vista dos monitores, um question´ario foi ela- borado e enviado a quinze monitores que participaram dos ´ultimos dez anos da disciplina. Ao todo, foram recebidas 12 respostas, apre- sentadas na Figura 10. Figura 10: Feedback dos monitores quanto a disciplina de ES No formul´ario tinham quest˜oes sobre a forma de comunicac ¸˜ao dos monitores com os alunos, as atividades que os monitores mais se sentiram atra´ıdos a desempenhar e quais foram as mais dif´ıceis. Em relac ¸˜ao `a comunicac ¸˜ao, em 2007, ela ocorria apenas atrav´es de', 'se sentiram atra´ıdos a desempenhar e quais foram as mais dif´ıceis. Em relac ¸˜ao `a comunicac ¸˜ao, em 2007, ela ocorria apenas atrav´es de listas de e-mails e presencialmente, enquanto em 2017, ela passou 365', 'Retrospective for the Last 10 years of Teaching SE in UFC/DC SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil a ser feita por meio de redes sociais (Facebook e Whatsapp), lista de e-mails e o pr ´oprio sistema da universidade, o SIGAA8. Essas ferramentas foram escolhidas a /f_im de minimizar a probabilidade de falhas na comunicac ¸˜ao entre professor, monitor e aluno. Sobre as atividades, a que mais atraiu os monitores foi ministrar aulas, com 6 votos. Tal´ındice de respostas ocorreu devido`a possibi- lidade de aprimorar a did´atica do monitor. Al´em disso, ela recebeu um voto de atividade mais dif´ıcil, que segundo o respondente foi devido `as atribuic ¸˜oes associadas, como preparar aula com materiais e exerc´ıcios. Tamb´em pode ser citada como uma das atividades que mais atraem os monitores, a rela c ¸˜ao com os alunos na execu c ¸˜ao dos projetos, onde os monitores interagiram diretamente com as equipes desempenhando um papel de stakeholder. Em tal func ¸˜ao, os monitores davam direcionamentos que auxiliavam os discentes no desenvolvimento do projeto de so/f_tware. A ´ultima opc ¸˜ao votada dentre as atividades mais atraentes, e que recebeu apenas um voto, foi a de gerenciar os demais monitores. Em- bora tenha sido considerada atrativa, o monitor que escolheu essa atividade tamb´em a colocou como sendo dif´ıcil. Tal classi/f_icac ¸˜ao ocorre devido `as quest ˜oes inerentes `a gerˆencia de pessoal, como alocac ¸˜ao de atividades e gerˆencia de comunicac ¸˜ao. Os monitores tamb´em identi/f_icaram as atividades que acharam mais dif´ıceis durante a monitoria da disciplina de ES. A mais vo- tada foi a correc ¸˜ao dos artefatos desenvolvidos pelos alunos. Essa atividade recebeu seis votos e foi assim classi/f_icada pelo esforc ¸o em- pregado na correc ¸˜ao das vers˜oes dos artefatos. Outro fator que foi considerado como o mais dif´ıcil para os monitores foi a comunicac ¸˜ao com os alunos. Embora os meios de comunicac ¸˜ao fossem evoluindo ao longo dos anos, alguns monitores tiveram di/f_iculdades para en- trar em contato com os membros das equipes, pois muitos acabavam n˜ao interagindo nos meios de comunicac ¸˜ao utilizados. 5 LIC ¸ ˜OES APRENDIDAS Durante os ´ultimos 10 anos de ensino de ES, v ´arias lic ¸˜oes foram aprendidas. Elas foram identi/f_icadas durante a execuc ¸˜ao das ati- vidades, atrav´es da observac ¸˜ao dos resultados obtidos durante a disciplina e por meio das discuss˜oes geradas na reuni˜ao de retros- pectiva da disciplina, realizada ao /f_im de cada semestre entre o Professor e os Monitores. A seguir, s˜ao descritas as lic ¸˜oes aprendidas (LA) de acordo com a seguinte estrutura: (i) Atividade do processo: momento da execuc ¸˜ao do processo onde foi identi/f_icada a oportunidade de melhoria; (ii) Experiˆencia: fato observado durante a disciplina; (iii) Problema: quest˜oes identi/f_icadas durante ou ap´os a execuc ¸˜ao de uma atividade; e (iv) Lic ¸˜ao aprendida: a c ¸˜oes sugeridas com base nos problemas observados a /f_im de obter melhoria cont´ınua. LA-1: De/f_inir as equipes com a participac ¸˜ao dos alunos • Atividade do Processo: De/f_inir Equipes; • Experiˆencia: Ao deixar os pr ´oprios alunos de/f_inirem as equipes, eles se agrupavam com base na a/f_inidade cons- tru´ıda no dia-a-dia; • Problema: Os alunos n˜ao adquiriam experiˆencia em traba- lhar com pessoas com as quais n˜ao tinham contato/a/f_inidade; • Lic ¸˜ao Aprendida: De/f_inir as equipes em dois momentos. Primeiro, os alunos se re´unem em grupos de trˆes e depois 8h/t_tps://si3.ufc.br/sigaa/verTelaLogin.do o Professor sorteia esses grupos para formar equipes de seis alunos. Assim, cada equipe continha integrantes que poderiam n˜ao ter a/f_inidade entre si. LA-2: Tratar equipes como “Empresa de so/f_tware” ao inv´es de “equipe de alunos” • Atividade do Processo: De/f_inir Equipes; • Experiˆencia: Ao formar as equipes, os alunos organiza- vam as atividades sem atribui c ¸˜oes bem de/f_inidas. Geral-', '“equipe de alunos” • Atividade do Processo: De/f_inir Equipes; • Experiˆencia: Ao formar as equipes, os alunos organiza- vam as atividades sem atribui c ¸˜oes bem de/f_inidas. Geral- mente n˜ao havia boa divis˜ao das responsabilidades; • Problema: Di/f_iculdades no entendimento das atividades e responsabilidades dos pap´eis envolvidos no desenvolvi- mento de so/f_tware; • Lic ¸˜ao Aprendida: Tratar as equipes como empresas de de- senvolvimento, onde os alunos devem assumir um ou mais papel(´eis) relacionados ao desenvolvimento de so/f_tware. LA-3: Utilizar diferentes meios de comunicac ¸˜ao • Atividade do Processo: Preparar ambiente de comunicac ¸˜ao; • Experiˆencia: Pouca participac ¸˜ao dos alunos por meio das listas de e-mail e falta de acompanhamento do Plano da Disciplina disponibilizado em uma p´agina web; • Problema: Os alunos /f_icavam com impedimentos que re- sultavam em trabalhos mal /f_inalizados e havia muitos atra- sos nas entregas dos artefatos; • Lic ¸˜ao Aprendida: Utilizar, al´em das listas de e-mail, ou- tros meios de comunicac ¸˜ao. Um exemplo ´e o Facebook, que, na turma de ES da UFC, ´e usado para alertar os alunos sobre prazos e tamb´em como ponte-direta entre o Aluno e o Monitor. LA-4: Utilizar ferramentas de gerenciamento de atividades • Atividade do Processo: Monitorar e Controlar Ativida- des da Disciplina; • Experiˆencia: Grande n ´umeros de artefatos a corrigir de- vido ao fato de as turmas de ES geralmente possu´ırem mais de 40 alunos; • Problema: Atividades atrasadas gerando impacto noPlano da Disciplina; • Lic ¸˜ao Aprendida: Utilizar alguma ferramenta de gerenci- amento de atividades dos monitores. Atualmente, usa-se o Trello9. LA-5: Elaborar um Manual do Monitor • Atividade do Processo: Monitorar e Controlar Ativida- des da Disciplina; • Experiˆencia: Falta de envolvimento dos monitores em todas as atividades da disciplina; • Problema: Equipe de monitores sem experi ˆencia pr´evia pelo fato de ser composta somente por novos integrantes. Isso resultava em problemas de compreens ˜ao acerca das atividades a serem realizadas; • Lic ¸˜ao Aprendida: De/f_inir e atualizar anualmente o Ma- nual do Monitor: documento que descreve as atribuic ¸˜oes do monitor na disciplina e determina as formas de comunicac ¸˜ao, bem como guia correc ¸˜oes de artefatos e dinˆamicas em sala. LA-6: Boni/f_icar participac ¸˜ao em aulas extras 9h/t_tp://trello.com.br 366', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil R. M. C. Andrade et al. • Atividade do Processo: Ministrar Aulas Extras; • Experiˆencia: Pouca participac ¸˜ao dos alunos nas aulas ex- tras que complementavam o aprendizado durante a disci- plina; • Problema: Falta de interesse por parte dos alunos em participar das aulas extras devido ao fato dos hor ´arios dessas n˜ao se adequarem `a maioria dos alunos; • Lic ¸˜ao Aprendida: Usar a lista de presenc ¸a como boni/f_icac ¸˜ao na nota do aluno. Al ´em disso, abordar, nas aulas extras, temas relevantes para o desenvolvimento do projeto /f_inal. LA-7: Prover incentivos al´em do contexto da disciplina • Atividade do Processo: Planejar e Executar Projeto de Desenvolvimento de So/f_tware; • Experiˆencia: Mesmo com o acompanhamento pr ´oximo das equipes pelos monitores, tirando d´uvidas e indicando melhores pr´aticas, observou-se falta de est´ımulo dos alunos no desenvolvimento dos projetos; • Problema: Os alunos continuavam desestimulados pelas di/f_iculdades relacionadas a programac ¸˜ao para no /f_inal da disciplina o produto ser descontinuado; • Lic ¸˜ao Aprendida: Incentivos como o lanc ¸amento do resul- tado da disciplina na loja de aplicativos ou premiac ¸˜ao com livros impulsionam a turma a se motivar a aprender uma nova tecnologia e entregar um trabalho de maior qualidade. LA-8: Incentivar alunos em todo o desenvolvimento do so/f_tware • Atividade do Processo: Planejar e Executar Projeto de Desenvolvimento de So/f_tware; • Experiˆencia: Apoio dos monitores oferecido apenas para a parte de documentac ¸˜ao do projeto /f_inal; • Problema: Ajuda insu/f_iciente, uma vez que os alunos apresentavam di/f_iculdade ao desenvolver para dispositivos m´oveis e n˜ao se esforc ¸avam para gerar o produto completo; • Lic ¸˜ao Aprendida: Incentivo aos alunos em todo o desen- volvimento do so/f_tware.´E importante que a tecnologia utilizada seja de conhecimento do monitor e preferencial- mente do aluno tamb´em. Isso reduz os problemas com o aprendizado. 6 CONCLUS ˜AO No presente artigo foi apresentado um processo de ensino da Enge- nharia de So/f_tware, resultado da melhoria continua da metodologia de ensino empregada nos ´ultimos 10 anos no Departamento de Computac ¸˜ao da Universidade Federal do Cear´a. Para avaliar o processo, foi conduzida uma an ´alise do desem- penho dos alunos, que indicou melhora das notas dos alunos no decorrer dos anos. Al´em disso, foi coletada a percepc ¸˜ao dos alunos e dos monitores quanto `as atividades da disciplina. A maioria dos alunos concordou que a aula apresentando os pro/f_issionais de en- genharia de so/f_tware motivava o estudo de ES. A/f_irmaram tamb´em estar satisfeitos com as aulas e dinˆamicas aplicadas na disciplina. O question´ario com os monitores, por sua vez, foi interessante por revelar as atividades mais atraentes e mais dif´ıceis do processo do ponto de vista do monitor. Dessa forma, o processo descrito neste artigo pode ser aplicado para melhorar o ensino-aprendizado da ES. Tamb´em cabe ressaltar que o processo pode ser adaptado para utilizac ¸˜ao em disciplinas de ES em outras instituic ¸˜oes. Como trabalhos futuros, pretende-se aplicar jogos educacionais, bem como coletar dados de mais turmas de alunos. Al´em disso, as lic ¸˜oes aprendidas devem continuar sendo aplicadas e novas lic ¸˜oes extra´ıdas do processo de forma a tornar o ensino mais atrativo para os alunos e tamb´em buscando facilitar o aprendizado. REFERˆENCIAS [1] Rossana M. C. Andrade, Val´eria Lelli, Rute Castro, and Ismayle S. Santos. 2017. Fi/f_teen Years of Industry and Academia Partnership: Lessons Learned from a Brazilian Research Group. In4TH International Workshop on So/f_tware Engineering Research and Industrial Practice. [2] Rossana M. C. Andrade, Fabiana G. Marinho, Val´eria L. Leit˜ao, and Lincoln S. Rocha. 2008. Uma Proposta de Metodologia para o Ensino de Engenharia de So/f_tware. InF´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES).', 'Rocha. 2008. Uma Proposta de Metodologia para o Ensino de Engenharia de So/f_tware. InF´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). [3] Rossana M. C. Andrade, Ismayle S. Santos, Italo L. Ara´ujo, and Rainara M. Car- valho. 2015. Uma Metodologia para o Ensino Te´orico e Pr´atico da Engenharia de So/f_tware. InF´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). In VI Congresso Brasileiro de So/f_tware: Teoria e Pr´atica (CBSo/f_t). [4] Marcelo W. Barbosa and Maria A. V. Nelson. 2015. Desa/f_ios do desenvolvimento de atividades pr´aticas de Engenharia de So/f_tware em grupo em cursos a distˆancia. In F´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). In VI Congresso Brasileiro de So/f_tware: Teoria e Pr´atica (CBSo/f_t). 11–22. [5] M´arcia Cera, Mateus D. Forno, and Vanessa G. Vieira. 2012. A Proposal to Teach So/f_tware Engineering based on Problem Solving.Brazilian Journal of Computers in Education 20, 03 (2012). [6] Rodrigo A. de Medeiros, Irlan A. T. Moreira, Natan M. Barros, Cicilia R. M. Leite, Rommel W. de Lima, and Lizianne P. S. Marques. 2013. GameES: Um Jogo para a Aprendizagem de Engenharia de So/f_tware. InXXIV Simp´osio Brasileiro de Inform´atica na Educac ¸˜ao (SBIE). In II Congresso Brasileiro de Inform ´atica na Educac ¸˜ao (CBIE 2013). [7] Virg´ınia Farias, Carla Moreira, Emanuel Coutinho, and Ismayle S. Santos. 2012. itest learning: Um jogo para o ensino do planejamento de testes de so/f_tware. In F´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). [8] Elizabeth S. Monsalve, Julio C. S. P. Leite, and Vera M. B. Werneck. 2015. Transpa- rently Teaching in the Context of Game-based Learning: the Case of SimulES-W. In 2015 IEEE/ACM 37th IEEE International Conference on So/f_tware Engineering, Vol. 2. 343–352. [9] Carlos S. Portela, Alexandre M. L. Vasconcelos, and Sandro R. B. Oliveira. 2015. An´alise da Relevˆancia dos T´opicos e da Efetividade das Abordagens para o Ensino de Engenharia de So/f_tware. InF´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). In VI Congresso Brasileiro de So/f_tware: Teoria e Pr´atica (CBSo/f_t). 34–45. [10] Carlos S. Portela, Alexandre M. L. Vasconcelos, and Sandro R. B. Oliveira. 2016. FRAMES: Um Framework para o Ensino-Aprendizagem dos T´opicos de Enge- nharia de So/f_tware dos Curr´ıculos de Referˆencia da ACM/IEEE e SBC. In F´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). In VII Congresso Brasileiro de So/f_tware: Teoria e Pr´atica (CBSo/f_t). 41–52. [11] Fabio G. Rocha, Rosimeri F. Sabino, and Ronald H. L. Acipreste. 2015. A Me- todologia Scrum como mobilizadora da pr´atica pedag´ogica: Um olhar sobre a Engenharia de So/f_tware. InF´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). In VI Congresso Brasileiro de So/f_tware: Teoria e Pr´atica (CBSo/f_t). 23–33. [12] Ken Schwaber and Mike Beedle. 2001. Agile So/f_tware Development with Scrum (1st ed.). Prentice Hall PTR, Upper Saddle River, NJ, USA. [13] Simone V. Silva and Aline P. V. de Vasconcelos. 2014. Ambiente Integrado como Apoio ao Ensino da Engenharia de So/f_tware. InF´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). [14] Ian Sommerville. 2010.So/f_tware Engineering(9th ed.). Addison-Wesley Publishing Company, USA. [15] Cleiton Tavares, Fischer Ferreira, Eduardo Fernandes, and Johnatan Oliveira. 2016. Scrum in Practice: Evaluating an Activity to Support Agile So/f_tware Development Learning. In F´orum de Educac ¸˜ao em Engenharia de So/f_tware (FEES). In VII Congresso Brasileiro de So/f_tware: Teoria e Pr´atica (CBSo/f_t). 89–100. [16] Christiane G. v. Wangenheim, Rafael Savi, and Adriano F. Borga/t_to. 2013. SCRU- MIA—An educational game for teaching SCRUM in computing courses. Journal of Systems and So/f_tware86, 10 (2013), 2675 – 2687. DOI:h/t_tp://dx.doi.org/h/t_tps: //doi.org/10.1016/j.jss.2013.05.030 [17] Claes Wohlin, Per Runeson, Martin H ¨ost, Magnus C. Ohlsson, Bj ¨orn Rgnell, and Anders Wessl´en. 2012. Experimentation in So/f_tware Engineering. Springer', '//doi.org/10.1016/j.jss.2013.05.030 [17] Claes Wohlin, Per Runeson, Martin H ¨ost, Magnus C. Ohlsson, Bj ¨orn Rgnell, and Anders Wessl´en. 2012. Experimentation in So/f_tware Engineering. Springer Publishing Company, Incorporated. 367']","['TEACHING SOFTWARE ENGINEERING IN DC/UFC This briefin reports scieitfc eeiieice oi  the experieice of teachiin softare  einiieeriin iuriin the last iecaie ii the  Feieral Uiieersity of Ceará. FINDINGS The main fnning is the Softaae Engineeaing (SE) teaching  paocess, as tell as the aesults obtainen tith its use nuaing the last ten yeaas.   The  paocess  has  thaee aoles: paofessoa, tho ne - fnes the scope ann the aciviies peafoamen nuaing the couase; monitoa, tho is an assistant of the pao- fessoa in the execuion of the aciviies; ann stu - nent, tho takes paat in paacical ann theoay classes ann executes the aciviies aelaten to the paacical toak. To sum up, this paocess involves aciviies aelaten to:  \uf02d The couase planning, such as “Paepaaing the communicaion enviaonment” ann “Inenifying the Stunents’ Paofle”; \uf02d The execuion of paacical ann theoay classes accoaning  to  the  couase  planning.  Foa  in - stance, foa the paacical classes, classaoom ny - namics  aae  usen  to  impaove  the  stunents leaaning. These  nynamics  aae  aelaten  to  the concepts annaessen in the couase. The objec - ive is to use the softaae engineeaing in paac - ice. \uf02d The nevelopment of softaae by the stunents. They  shouln  nefne  the  scope  ann  aequiae- ments of the applicaion.  \uf02d The nevelopment of the assets (e. g., Paoject Chaatea,  Scope,  Paoject  Management  Plan, Test  Plan)  aelaten  to  the  softaae  nevelop- ment paoject. Using an inteaacive cycle, stu - nents nevelop the assets, theaeas the moni - toas ann the paofessoa valinate them ann senn impaovement ips to the stunents, tho shouln coaaect the assets accoaning to the aeviet.  In geneaal, tith the use of the paocess, te obseave that the stunents’ gaanes have incaeasen ovea the yeaas. The  paocess  monels  most  usen  by  the  stunents nuaing the softaae nevelopment teae the Paoto - typing ann the Iteaaive ann Incaemental Paocess. They also paesenten the best aesults  in the fnal evaluaion. Basen on a suavey tith the monitoas, tto aciviies teae consineaen the most challenging ones: teach- ing classes ann checking the assets. In the fast one, the monitoas shouln suppoat the paofessoa nefn - ing ann paepaaing the classes. In the othea one, the monitoas shouln evaluate the assets caeaten by the stunents. We inenifen eight lessons leaanen.  \uf02d To  use  the  paocess,  the  stunents  shouln  be sepaaaten into teams tith at least six mem - beas. Also, this gaoup shouln be taeaten as a softaae nevelopment company. In this case, each membea assumes a aole, as managea, ae - quiaements  analyst  oa  test  analyst,  among otheas. \uf02d The fast lesson conceans to the nefniion of the stunent gaoups. When the stunents oaga - nizen  the  gaoups  themselves,  they  usually foamen gaoups by peasonal afnity. Then, to simulate a aeal scenaaio of the softaae nevel - opment  innustay,  the  gaoups  teae  nefnen aannomly. \uf02d Iniially, te usen only e-mail in oua communi - caion.  Hotevea,  tith  the  evoluion  of  the communicaion menia, te also use the system of the univeasity, e-mail, Whatsapp, ann Face - book to a betea inteaacion tith the stunents. \uf02d The use of a tool to manage ann contaol the aciviies of the couase is caiical to the success of the stunent’s paoject. We suggest the use of Taello, an online Kanban boaan.  \uf02d To  aegistea  the  aciviies  aelaten  to  the  SE couase, te obseaven the neen foa caeaing a manual tith the aesponsibiliies ann the acivi- ies to be peafoamen by monitoas.  \uf02d Foa the contents applien nuaing the caeaion of  the  assets,  but  not  exploaen  nuaing  the couase, the monitoas paepaaen ann taught ex - taa classes. Foa example, the UML niagaams oa Annaoin technologies to help the stunents ne - veloping the fnal applicaion. Also, to simu - late the stunents to be paesent in the classes, te gave them a bonus on theia gaanes. \uf02d Also in oanea to simulate the stunents, te ne - fnen  a  neliveay  of  the  applicaion  in  app stoaes, like Google Play.        Keywords:', 'te gave them a bonus on theia gaanes. \uf02d Also in oanea to simulate the stunents, te ne - fnen  a  neliveay  of  the  applicaion  in  app stoaes, like Google Play.        Keywords: Teaching-leaaning paocess Softaae Engineeaing       Who is this briefin for? Paofessoas ann any softaae  engineeaing paaciioneas tho tant to  teach Softaae Engineeaing. Where the fidiins come from? All fnnings of this baiefng teae  extaacten faom the analysis of the last  ten yeaas of the teaching of Softaae  Engineeaing in DC/UFC connucten by  Annaane et al.   What is iicluded ii this briefin? The paocess nefnen ann the aciviies  ann aoles involven in the teaching of  Softaae Engineeaing, as tell as the  lessons leaanen nuaing its use. To access other evideice briefins  oi sofware einiieeriin: htp://ttt.lia.ufc.ba/ccbsof2017/ For additoial iiformatoi about  Group of Computer Networks,  Sofware Einiieeriin aid Systems  (GREat): htp://ttt.gaeat.ufc.ba']","**Title:** Enhancing Software Engineering Education: Insights from a Decade of Experience

**Introduction:**  
This briefing summarizes key findings from a retrospective study on the teaching of Software Engineering (SE) at the Federal University of Ceará (UFC) over the past ten years. The goal is to share effective practices and lessons learned that can inform educators and practitioners involved in Software Engineering education.

**Main Findings:**  
1. **Continuous Improvement in Teaching Practices:** The teaching approach for SE has evolved significantly over the last decade, with a focus on integrating theoretical knowledge with practical application. This dual approach has enhanced student engagement and learning outcomes.

2. **Student Feedback and Performance:** Regular feedback from students and teaching assistants has been instrumental in refining the course structure. Surveys indicated that 89% of students found project-based learning effective for understanding SE concepts, leading to an increase in average student grades from 6.65 in 2010 to nearly 8.0 in 2016.

3. **Role of Practical Projects:** The curriculum emphasizes the development of software projects, allowing students to apply theoretical concepts in real-world scenarios. This hands-on experience fosters skills such as teamwork, problem-solving, and project management. Projects have included applications that are now available on platforms like Google Play.

4. **Dynamic Learning Environment:** The incorporation of interactive dynamics, such as ""The Client Asked,"" has proven effective in maintaining student interest and promoting collaboration among peers. These activities simulate real-world scenarios, enhancing the learning experience.

5. **Challenges and Adaptations:** Despite positive outcomes, challenges such as varying levels of student participation and communication issues were identified. The study suggests utilizing diverse communication tools (e.g., social media) to enhance engagement and streamline information flow among students and instructors.

6. **Lessons Learned:** Several key lessons emerged from the experience, including the importance of defining roles within project teams, treating student groups as software development companies, and continually assessing and adapting teaching methods based on feedback and performance data.

**Who is this briefing for?**  
This briefing is intended for educators, curriculum developers, and software engineering practitioners interested in improving the teaching and learning processes in Software Engineering.

**Where the findings come from?**  
The findings are derived from a comprehensive analysis of the teaching practices in the Software Engineering course at UFC, supported by surveys and performance evaluations conducted over the last decade.

**What is included in this briefing?**  
This briefing includes insights into the teaching-learning process, activities involved in the course, and the lessons learned from the implementation of various teaching strategies.

To access other evidence briefings on software engineering:  
[http://www.lia.ufc.br/~cbsoft2017/](http://www.lia.ufc.br/~cbsoft2017/)

For additional information about the Group of Computer Networks, Software Engineering, and Systems (GREat):  
[http://www.great.ufc.br](http://www.great.ufc.br)

**Original Research Reference:**  
Andrade, R. M. de C., Santos, I. de S., Araújo, I. L. de, Aragão, B. S., & Siewerdt, F. (2017). Retrospective for the Last 10 years of Teaching Software Engineering in UFC’s Computer Department. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017. DOI: [10.1145/3131151.3131179](https://doi.org/10.1145/3131151.3131179)"
"['Test case prioritization: a systematic review and mapping of the literature Heleno de S. Campos Junior Federal University of Juiz de Fora Juiz de Fora, MG, Brazil helenocampos@ice.u/f_jf.br Marco Antˆonio P. Ara´ujo Federal University of Juiz de Fora / Federal Institute of Southeast MG Juiz de Fora, MG, Brazil marco.araujo@ice.u/f_jf.br Jos´e Maria N. David Federal University of Juiz de Fora Juiz de Fora, MG, Brazil jose.david@u/f_jf.edu.br Regina Braga Federal University of Juiz de Fora Juiz de Fora, MG, Brazil regina.braga@u/f_jf.edu.br Fernanda Campos Federal University of Juiz de Fora Juiz de Fora, MG, Brazil fernanda.campos@u/f_jf.edu.br Victor Str¨oele Federal University of Juiz de Fora Juiz de Fora, MG, Brazil victor.stroele@ice.u/f_jf.br ABSTRACT Test case prioritization (TCP) techniques aim to reorder test cases execution according to a goal. One common goal is fault detec- tion, in which test cases that have a higher chance of detecting a fault are executed /f_irst than the remaining test cases. /T_he goal of this study is to investigate TCP empirical studies in order to synthesize reported eﬀectiveness results and provide a basis for future research. We conducted a systematic literature mapping to characterize TCP empirical studies and a systematic literature review to analyze reported TCP techniques eﬀectiveness results. Among selected studies from 1999 to 2016, we found that there is a high amount of empirical studies evaluating many TCP techniques. However, when we applied our quality assessment criteria, most of them were discarded, indicating that they might have methodologi- cal problems. Analyzed studies reported results of coverage-based TCP techniques. Furthermore, we found that some context factors regarding faults, test cases of the application being tested and cov- erage granularity considered by TCP techniques may signi/f_icantly aﬀect the eﬀectiveness of their execution. /T_hese results suggest that more rigorous empirical methodology is needed when evaluating TCP techniques and also, authors need to compare eﬀectiveness results of their proposed TCP techniques with well established techniques to generate more evidences. Furthermore, our analysis of signi/f_icant factors on TCP techniques eﬀectiveness can guide researchers when planning empirical evaluations and help them choosing features to compose new techniques. CCS CONCEPTS •So/f_tware and its engineering→ So/f_tware veri/f_ication and validation; Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5326-7/17/09. . .$15.00 DOI: 10.1145/3131151.3131170 KEYWORDS Regression testing, so/f_tware testing, systematic literature review, systematic literature mapping ACM Reference format: Heleno de S. Campos Junior, Marco Antˆonio P. Ara´ujo, Jos´e Maria N. David, Regina Braga, Fernanda Campos, and Victor Str¨oele. 2017. Test case priori- tization: a systematic review and mapping of the literature. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017,10 pages. DOI: 10.1145/3131151.3131170 1 INTRODUCTION Regression tests are executed a/f_ter so/f_tware modi/f_ication in order to ensure that previously developed so/f_tware parts are working and that newly developed source code behaves like it is supposed to. Sometimes regression tests take a long time to execute. /T_here', 'to ensure that previously developed so/f_tware parts are working and that newly developed source code behaves like it is supposed to. Sometimes regression tests take a long time to execute. /T_here are examples in the literature that report regression suites that take around 1000 hours to /f_inish execution [3]. For this reason, some form of optimization is needed to ensure that the develop- ment work/f_low is not compromised. Regression test optimization techniques are divided into three types [ 21], minimization or re- duction, selection and prioritization. Minimization techniques are concerned with removing redundant test cases from execution. Se- lection techniques choose a speci/f_ic set of test cases according to some criteria to be executed. Prioritization techniques reorder exe- cution of the test cases according to some criteria. While selection and minimization techniques exclude test cases for a regression test execution, prioritization techniques only reorder them, being sometimes more reliable and cost-eﬀective [3]. One common criterion for test case prioritization (TCP) is fault detection. In this case, test cases are ordered in a manner that those with higher probability of detecting faults are executed /f_irst. Diﬀerent approaches are used by TCP techniques to de/f_ine this probability, some examples are based on historical failure data, test coverage, requirements and system models [21]. One way to investigate these techniques is by performing a secondary study. Secondary studies are used to aggregate and synthesize studies related to some subject available in the literature. More speci/f_ically, systematic literature reviews or mappings are secondary studies that use a systematic procedure to allow their reproducibility. 34', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Campos Junior et al. Existing test case prioritization secondary studies include a sys- tematic literature review [ 17], in which they analyze TCP tech- niques published in studies between 1997 and 2011 in order to investigate research gaps regarding tools, metrics and artifacts used by authors in the domain. /T_hey also investigate if available TCP techniques are language independent. Catal and Mishra [1] perform a systematic literature mapping aiming to identify which TCP techniques exist in the literature. /T_hey also evaluate trends in the area, providing a basis for improve- ment of future research. /T_hey concluded that it is necessary more secondary studies in the domain. /T_hey also suggest that new studies need to compare their proposed techniques with other techniques to provide a basis for evidence aggregation and comparison. Fur- thermore, another study [10] was published in 2013 that describes diﬀerent TCP techniques. In this paper, we aim to investigate TCP empirical studies to syn- thesize reported TCP techniques eﬀectiveness results and provide a basis for future research through a systematic literature review and mapping. Our focus diﬀer from existing secondary studies in the sense that they analyze studies up to 2011, while we analyze also more contemporary ones. Furthermore, we investigate diﬀerent re- search questions, like which TCP techniques achieve best reported eﬀectiveness results and context factors that aﬀect their results. /T_he remaining of this paper is organized as follows. Section 2 describes the planning and conduction phases of our systematic literature mapping and review. Section 3 and 4 describe the results from our systematic mapping and review, respectively. In Section 5 results are discussed and conclusions are presented in Section 6. 2 SYSTEMATIC MAPPING AND REVIEW Secondary studies usually are used to aggregate primary studies and evidences from the literature in order to answer research ques- tions [19]. Systematic Literature Mappings (SLM) are secondary studies which aim at giving an overview about the state of the prac- tice or research about some speci/f_ic subject. Systematic Literature Reviews (SLR) are used to deeply investigate some topic, typically aggregating evidences collected by other studies in order to answer more elaborate research questions [19]. In this sense, we perform a SLM and SLR on test case prioriti- zation studies to give an overview about the topic and investigate results obtained by individual primary studies on using test case prioritization techniques. /T_his mapping and review is based on the guidelines provided by Kitchenham and Charters [11]. In the next sections we describe the planning, conducting and reporting of the results. 2.1 Mapping and review planning During the planning phase of systematic mapping or review, general goals and the protocol are outlined. A protocol is responsible for al- lowing the reproduction of the review in the future and also to guide the researcher during the conduction phase. Our designed protocol was validated by 4 diﬀerent so/f_tware engineering researchers with more than 10 years of experience. 2.1.1 Mapping (MQ) and review (RQ) questions. • MQ1: Which are the most active researchers on TCP tech- niques? • MQ2: Which publication venues have more primary stud- ies on TCP? • MQ3: Which are the most investigated TCP approaches (we consider TCP approach as the category of the technique e.g. coverage-based or history-based)? • MQ4: What kind of evaluation metrics are most frequently used in primary TCP studies? • MQ5: What is the existing infrastructure to support the diﬀerent activities for TCP evaluations? • RQ1: What are the existing empirical evidences for TCP techniques? – RQ1.1: Which techniques achieve the best results in terms of eﬀectiveness for common applications (i.e. applications that appear in more than one diﬀerent study)?', 'techniques? – RQ1.1: Which techniques achieve the best results in terms of eﬀectiveness for common applications (i.e. applications that appear in more than one diﬀerent study)? – RQ1.2: Which study context factors can aﬀect eﬀec- tiveness results obtained by TCP techniques? – RQ1.3: How TCP eﬀectiveness results vary according to the used granularity (i.e. the scale used for a TCP technique input data)? 2.1.2 Inclusion and exclusion criteria. Inclusion (IC) and exclu- sion criteria (EC) are used to select relevant papers to be considered in the review and mapping. /T_hus, the process to check if a paper is pertinent or not, includes the following criteria: • IC1: primary study presenting TCP techniques • IC2: primary study comparing TCP techniques • EC1: study does not quantitatively evaluate the eﬀective- ness of any TCP technique • EC2: study proposes a TCP technique that is designed to a speci/f_ic type of so/f_tware. e.g. service-oriented, embedded, product lines • EC3: study is not wri/t_ten in English • EC4: proceedings call or short paper. /T_he main reason why only primary studies are considered is due to the fact that our research questions aim at comparing TCP eﬀectiveness results, which normally are reported on these type of studies. Studies that only present a TCP technique but do not evaluate its eﬀectiveness are discarded, since they do not add any information to answer our systematic review questions. Further- more, techniques that are designed to be used on speci/f_ic type of so/f_tware such as service-oriented, embedded systems, so/f_tware product lines, safety-critical systems are also excluded, since their usage is very speci/f_ic. 2.1.3 /Q_uery string.A/f_ter objectives, research questions and se- lection criteria are outlined, sources of papers and a search string are de/f_ined. /T_his study considers electronic databases as source of papers, which are selected based on the following requirements: • Able to process a query string with boolean expressions • provide papers on computer science domain. • available to be accessed from the authors’ institution. Given these requirements, /f_ive diﬀerent electronic databases are selected: ACM Digital Library, Science@Direct, IEEE Xplore, Sco- pus and Springer. /T_he boolean search string is de/f_ined as:regression AND (testing OR test) AND (prioritisation OR prioritization). It was set to be applied to title, abstract and keywords of the papers in the 35', 'Test case prioritization: a systematic review and mapping of the literature SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil electronic databases, except for ACM and Springer, which did not oﬀer this option. No time period was speci/f_ied to be applied. 2.1.4 /Q_uality assessment.Inclusion and exclusion criteria alone do not ensure that selected papers are reliable enough to be ana- lyzed. For this reason, Kitchenham and Charters [11] suggest the use of a quality assessment for selected papers. /T_his methodology includes the de/f_inition of questions about selected studies, which are weighted according to each answer. In this sense, we de/f_ine /f_ive questions regarding the methodology used in the primary studies. • Authors explicitly de/f_ined the used dataset • authors explicitly de/f_ined the used measures • statistical signi/f_icance tests were used on the results in order to draw conclusions – authors de/f_ined which statistical tests were used • authors described procedures used for evaluation of TCP techniques. Each aﬃrmative answer for the questions is given a value of 1 when fully agreed with its statement. If the agreement is not total, a value of 0.5 is set. Otherwise, the value is 0. /T_hus, the maximum score is 5. We consider a threshold score of 2.5 in order to include a paper to answer the systematic mapping questions and a score of 4 to include the paper in the systematic review analysis. A higher score for the systematic review analysis is used due to the fact that evidence aggregation is performed and including a paper that does not meet a minimum standard in its evaluation can bias the obtained results. To de/f_ine the threshold score for the systematic review, we consider that the only criteria that a study can possibly receive a 0 value is the fourth one. /T_hat is, if a study received a 0 in the fourth criteria, but even so, achieved a total score of 4, it is included in our analysis. 2.1.5 Data extraction. Extracted data from each paper includes title, authors, publication year, publication venue and study con- clusions. For each technique that each paper propose, we collect technique name and description, language type, input method, approach, granularity of the approach and type of the technique (static or dynamic). Finally, for each empirical study reported in each paper, we collect the evaluation type, artifacts used, compared techniques, metrics, supporting tools and when applicable, reported eﬀectiveness results. 2.2 Review and mapping conduction Conduction of the review and mapping started with the application of the search string on selected electronic databases, according to the protocol. 1563 papers were retrieved in this step. /T_heir title were read in order to discard oﬀ topic papers. Since the search string used was generalist, aiming to retrieve as much studies as possible, 993 oﬀ topic papers were discarded, representing a reduction of 63.5%. 277 papers were also discarded in this step since they were iden- ti/f_ied as duplicated. /T_he remaining 293 papers had their abstract, introduction and conclusion read. 91 were excluded according to the exclusion criteria and 1 for being identi/f_ied as duplicated. /T_he reduction in this step was 31%. 136 papers advanced to the next step, which includes the full reading of their text. From this step, 25 papers were excluded according to the exclusion criteria and 3 were identi/f_ied as duplicated. /T_he reduction in this step was 18.4%. Figure 1: Review conduction process. Table 1: Rank of authors per number of publications. Author Papers Gregg Rothermel 17 Hyunsook Do 8 Dan Hao 7 Lu Zhang 7 Lingming Zhang 6 Ladan Tahvildari 5 Mark Harman 5 Bogdan Korel 5 Zhenyu Chen 4 Author Papers Hong Mei 4 Sebastian Elbaum 4 Siavash Mirabab 4 Zheng Li 4 Bo Jiang 4 W. K. Chan 4 Luay Tahat 4 George Koutsogian- nakis 4 108 papers were accepted according to the inclusion and exclusion criteria. /T_he next step was applying the quality assessment criteria', 'W. K. Chan 4 Luay Tahat 4 George Koutsogian- nakis 4 108 papers were accepted according to the inclusion and exclusion criteria. /T_he next step was applying the quality assessment criteria to evaluate their methodology, according to our protocol. A/f_ter applying this /f_ilter, 90 papers from 1999 to 2016 remained. /T_hose papers had their data extracted to answer our systematic mapping questions. Furthermore, these 90 accepted papers went through another /f_ilter to answer our systematic review questions. In total, 13 studies were considered for the systematic review. An overview of the conduction process is depicted in Figure 1. 3 SYSTEMATIC MAPPING REPORT As described in Section 2.1.5, diﬀerent data were collected from accepted studies. Our mapping questions were mostly motivated by already existing systematic mappings and reviews. Altogether 90 studies were considered in this systematic litera- ture mapping. /T_hese studies were wri/t_ten by 175 diﬀerent authors. Due to space limitations, we list those that participated in at least four papers in Table 1, answering MQ1. A complete listing is avail- able in our external appendix1. MQ2 is concerned with venues in which most TCP techniques primary studies were published, according to our criteria. To answer this, the number of papers per venue is in Table 2. A total of 64 diﬀerent TCP techniques are proposed in the se- lected studies. /T_hese techniques can be divided into categories according to diﬀerent features [1, 17]. If we consider, for example, programming languages on which they can be applied, we have language independent, object oriented, binary code and those that did not state this information. Considering their input method, we have source code, binary form of the source code, call graphs, requirements/speci/f_ications, the whole system being tested, fault matrix, time budget, test suite, coverage information, source code change information, test input 1h/t_tps://helenocampos.github.io/tcpreview mapping/ 36', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Campos Junior et al. Table 2: Papers number per venue. Publication venue Papers IEEE Transactions on So/f_tware Engineering 9 Lecture Notes in Computer Science 6 International Conference on So/f_tware Maintenance 5 International Computer So/f_tware and Applications Conference 4 International Conference on So/f_tware Engineering 4 International Symposium on So/f_tware Testing and Analysis 4 So/f_tware Testing, Veri/f_ication and Reliability 3 International Symposium on So/f_tware Reliability Engineering 3 International Conference on So/f_tware /Q_uality, Reliability and Security 3 International Conference on So/f_tware Testing, Veri/f_ication and Vali- dation 3 International Symposium on Foundations of So/f_tware Engineering 3 International Journal of So/f_tware Engineering and Knowledge Engi- neering 3 Journal of Systems and So/f_tware 3 So/f_tware /Q_uality Journal 3 Others 34 Table 3: Most investigated approaches. Approach Amount of techniques Coverage based 31 History based 12 Modi/f_ication based 8 Similarity based 8 Genetic based 7 Model based 5 Requirements based 4 Search based 4 Fault based 2 Program structure based 2 Oracle based 1 Fault diagnosis based 1 data, test execution history, execution trace and diﬀerent so/f_tware quality metrics. Considering their type, we have static, dynamic and hybrid tech- niques. Considering their granularity, techniques that use method, statement, function, class, test case, branch, block and block of binary form information. Considering their base approach, a previous secondary study [17] identi/f_ied 6 diﬀerent types of approaches. We identi/f_ied 6 more. /T_hey are genetic based, modi/f_ication based, coverage based, history based, fault based, similarity based, requirements based, model based, oracle based, fault diagnosis based, search based and program structure based techniques. As MQ3 is concerned with the most frequently used approaches, their usage is in Table 3. Note that some techniques can be considered in more than one category approach. Example of this is the Bayesian Network technique[14], which uses information about source code change and coverage, thus, can be considered as coverage based and modi/f_ication based. A total of 24 diﬀerent TCP eﬀectiveness metrics are used by selected studies. However, many of them are only used in one or two studies. A rank of most used metrics is shown in Table 4, answering MQ4. /T_he most commonly used TCP eﬀectiveness metric is the Average Percentage of Faults Detected, proposed by Rothermel et al. [16]. Its values range from 0 to 100 percent, where higher values mean faster fault detection rates. Authors use diﬀerent tools to support their activities during TCP techniques execution and evaluation. We collected data about used tools for each activity in selected studies, which are depicted in Table 5, answering MQ5. A large amount of tools is used for source Table 4: Most used TCP eﬀectiveness metrics. Metric Amount of studies APFD (Average Percentage of Faults Detected) 63 APFDc (Cost-cognizant Average Percentage of Faults Detected) 8 f-measure 6 Relative Position 5 APSC (Average Percentage of Statement Coverage) 4 code coverage information gathering and source code mutation. /T_his can possibly be explained by the vast majority of TCP tech- niques that are coverage based, as shown in Table 3. Furthermore, source code mutation tools are o/f_ten used by authors to seed arti/f_i- cial faults into a so/f_tware so that they can measure the eﬀectiveness of TCP techniques. It is also notable a huge amount of studies that use the So/f_tware Infrastructure Repository2 (SIR) to /f_ind artifacts to be used in their evaluation. In fact, it is the only cited repository that we found among the analyzed studies. /T_his section outlined selected studies for our systematic literature mapping and answered our /f_ive mapping questions. Due to space', 'that we found among the analyzed studies. /T_his section outlined selected studies for our systematic literature mapping and answered our /f_ive mapping questions. Due to space limitations these information can be further explored in our external appendix, through a database that we constructed with the collected data. In the next section, we present a deeper analysis of the data through a systematic literature review. 4 SYSTEMATIC REVIEW REPORT Our systematic literature review questions are mainly concerned with empirical results obtained by using TCP techniques reported by selected studies. Moreover, we are also interested on investigating relationships among factors that aﬀect the eﬀectiveness of these techniques. In order to answer RQ1, we selected papers that were not dis- carded in our quality assessment. Furthermore, we analyzed results reported using APFD metric, since it is the most used eﬀective- ness metric among the studies. We only extracted APFD data for the most compared techniques. /T_he main reason is that if we use TCP techniques that have few available APFD results, we would not have a reliable base to compare them. /T_hus, 13 studies were considered in the analysis for the Systematic Review. Table 6 displays APFD results collected from studies that evaluate TCP techniques on common applications. We consider common application those that are used in more than one diﬀerent study. It is worth noting that this table is limited to mean results reported by selected studies. In order to answer RQ1.1, we highlighted the highest APFD achieved for each application (row). Diﬀerent factors can aﬀect these results and one should not exclusively use this table to infer the best TCP technique. Instead, we provide diﬀerent quantitative analysis over data collected during this review, aiming to identify relevant factors that can aﬀect the results obtained by using TCP techniques. Possible APFD factors candidates considered in this review are motivated by Do and Rothermel [ 4] study. /T_hey make a qualita- tive analysis of results obtained by the use of TCP techniques on /f_ive diﬀerent studies. /T_hey consider as possible factors: program size, indicated by Lines of Code (LOC) metric; test case source, i.e. 2h/t_tp://sir.unl.edu/ 37', 'Test case prioritization: a systematic review and mapping of the literature SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Table 5: Tools used for each diﬀerent activity in selected TCP primary studies. Activity Tools amount Tools name Code coverage information 12 CodeCover, gcov, Emma, Cobertura, JaCoCo, Cantata++, mma, Sofya, xSuds, LDRA Testbed, ATAC, Fault Tracer Source code mutation 11 PIT, MuJava, Proteum, MutGen, Javalanche, Jumble, Major Mutation Framework, Jester, Sofya, Zoltar Prioritization tool/framework 4 xSuds, Apros, MOTCP+, MOTCP Source code metrics information 4 SLOCCount, ckjm, CLOC, SWT-Metrics Execution trace information 4 valgrind, daikon, AspectJ, gcov Bytecode manipulation/analysis 4 FaultTracer tool, ASM, Galilleo, Sofya Change analysis 3 sandmark, diﬀ, Celadon Linear integer programming solver: GUROBI optimization, ILOG CPLEX; test scripts generation/creation: TSL, AutoBlackTest; test execution information: time, ant; so/f_tware repository: SIR; artifacts traceability: Traceclipse; multi-objective optimization framework: Jmetal; analysis of dynamic binary code: Vulcan; Bayesian Network framework: Smile Library; fault localization: Zoltar; test analysis: xSuds; fault history: LDRA Testbed;clustering: Matlab; information retrieval framework: Indri toolkit; control /f_low graph information: Aristotle; refactoring diﬀ: ref-/f_inder. Table 6: APFD results reported by selected studies for common applications. App T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14 T15 T16 T17 A1 0.179[9] 0.763[13] 0.97[5] 0.761[13] 0.896[6] 0.87[5] 0.947[13] 0.916[6] 0.894[20] 0.71[5] 0.723[13] 0.5[12] 0.187[9] 0.953[13] 0.86[8] 0.844[8] 0.96[5] 0.896[6] 0.96[5] 0.927[6] 0.897[20] 0.822[8] 0.813[8] 0.517[6] 0.57[6] 0.504[6] 0.565[6] A2 0.866[20] 0.776[8] 0.704[8] 0.902[20] 0.788[8] 0.789[8] A3 0.644[20] 0.665[8] 0.684[8] 0.647[20] 0.556[8] 0.57[8] A4 0.789[15] 0.182[9] 0.501[15] 0.685[15] 0.174[9] 0.785[15] 0.733[15] 0.84[7] A5 0.675[15] 0.118[9] 0.513[15] 0.637[15] 0.141[9] 0.706[15] 0.702[15] 0.72[7] A6 0.598[15] 0.195[9] 0.514[15] 0.504[15] 0.178[9] 0.596[15] 0.606[15] 0.72[7] A7 0.739[15] 0.5[9] 0.6[15] 0.714[15] 0.523[9] 0.748[15] 0.758[15] 0.749[7] A8 0.764[15] 0.179[9] 0.572[15] 0.748[15] 0.184[9] 0.768[15] 0.808[15] 0.771[7] A9 0.758[15] 0.318[9] 0.559[15] 0.755[15] 0.328[9] 0.748[15] 0.761[15] 0.752[7] A10 0.771[15] 0.106[9] 0.54[15] 0.96[22] 0.966[22] 0.623[15] 0.102[9] 0.768[15] 0.665[15] 0.808[7] A11 0.923[16] 0.364[9] 0.834[16] 0.993[22] 0.997[22] 0.918[16] 0.371[9] 0.922[16] 0.92[16] 0.931[7] A12 0.061[9] 0.791[13] 0.587[13] 0.51[5] 0.786[13] 0.579[13] 0.792[6] 0.84[5] 0.921[13] 0.815[13] 0.842[6] 0.64[5] 0.761[13] 0.708[13] 0.066[9] 0.954[13] 0.845[13] 0.4484[8] 0.416[8] 0.52[5] 0.792[6] 0.87[5] 0.853[6] 0.52[8] 0.504[8] 0.818[6] 0.833[6] 0.819[6] 0.834[6] A13 0.076[9] 0.617[13] 0.588[13] 0.34[5] 0.608[13] 0.573[13] 0.601[6] 0.77[5] 0.883[13] 0.872[13] 0.687[6] 0.6[5] 0.605[13] 0.696[13] 0.095[9] 0.869[13] 0.843[13] 0.34[5] 0.605[6] 0.74[5] 0.688[6] 0.633[6] 0.68[6] 0.635[6] 0.685[6] A14 0.101[9] 0.719[13] 0.483[13] 0.68[5] 0.73[13] 0.48[13] 0.729[6] 0.97[5] 0.845[13] 0.83[13] 0.914[6] 0.61[5] 0.784[13] 0.664[13] 0.245[9] 0.885[13] 0.861[13] 0.407[8] 0.699[8] 0.68[5] 0.724[6] 0.97[5] 0.912[6] 0.599[8] 0.591[8] 0.613[6] 0.636[6] 0.619[6] 0.639[6] A15 0.766[18] 0.744[18] A16 0.821[18] 0.821[18] A17 0.769[18] 0.801[18] A18 0.708[2] 0.597[2] 0.742[2] A19 0.6[6] 0.624[6] 0.465[6] 0.755[6] 0.763[6] 0.868[6] 0.768[6] 0.94[6] A20 0.8[6] 0.806[6] 0.802[6] 0.945[6] 0.711[6] 0.845[6] 0.705[6] 0.84[6] Techniques summary: T1: random order, T2: statement ART, T3: additional branch coverage, T4: total branch coverage, T5: total statement coverage, T6: additional statement coverage, T7: total function coverage, T8: total block coverage, T9: additional block coverage, T10: total method coverage, T11: additional method coverage, T12: additional function coverage, T13: additional coverage (generic), T14:', 'branch ART, T15: Bayesian Network (method coverage), T16: Bayesian Network with feedback (method coverage), T17: Bayesian Network (block coverage). Applications summary: A1: XML-Security, A2: Jdepend, A3: Checkstyle, A4: tcas, A5: schedule2, A6: schedule, A7: tot info, A8: print tokens, A9: print tokens2, A10: replace, A11: space, A12: ant, A13: jmeter, A14: jtopas, A15: Altitude Switch (ASW), A16: Wheel Brake System (WBS), A17: Flight Guidance System (FGS), A18: NoiseGen, A19: Galileo, A20: NanoXML. who developed the test cases; test case type, number of faults in each version of the tested program and the type of these faults. In addition to the factors analyzed by them, we also considered type of seeded faults, TCP techniques and test case granularity. Each factor and its values for selected studies are described in Table 7. 4.1 APFD factors analysis For all analysis, the dependent variable is the APFD result obtained from selected studies. Usually the process followed by studies to obtain APFD values involves executing regression test suites from Table 7: Possible APFD factors. Factor Values (treatments) Faults type Seeded / real Average number of faults per version Continuous values Test case source Provided / generated Test case type JUnit / TSL Program size (LOC) Continuous values Faults seeding type Manual / mutation TCP technique coverage granularity Branch / statement / block / method and function Test case granularity Method level / class level 38', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Campos Junior et al. diﬀerent applications, using diﬀerent TCP techniques. For this reason, usually, each study reports a number of diﬀerent APFD values. /T_he signi/f_icance level for our analysis was established as 0.05 and the statistical tool used is Minitab 17 3. /T_he following subsections report analysis for each considered factor. 4.1.1 Faults type. /T_his analysis is performed considering 1 fac- tor (Fault type) and 2 treatments (Seeded and Real) design. /T_he hypothesis are: • H0: /T_he means of TCP techniques execution results ob- tained using seeded and real faults are equal. • H1: /T_he means of TCP techniques execution results ob- tained using seeded and real faults are signi/f_icantly diﬀer- ent. For this analysis, 274 diﬀerent APFD values are used. Among these values, 260 are results obtained when faults were seeded into the application being tested and 14 when real faults were used. As we have 274 diﬀerent values, Kolmogorov-Smirnov is used to test normality of data. With a p-value < 0.010, which is less than the established level of signi/f_icance 0.05, the sample has a non normal distribution. As the data distribution is not normal, a non-parametric hypothesis test is used. In this case, we employ Kruskal-Wallis test. /T_he result output is displayed in Listing 1. Listing 1: Kruskal-Wallis test for fault type factor. Kruskal −W a l l i s T e s t : APFD v e r s u s FAULT TYPE FAULT TYPE N Median Ave Rank Z r e a l 14 0 , 9 1 9 0 2 1 4 , 3 3 , 7 2 s e e d e d 260 0 , 7 0 0 5 1 3 3 , 4 −3 ,72 O v e r a l l 274 1 3 7 , 5 H = 1 3 , 8 7 DF = 1 P = 0 , 0 0 0 H = 1 3 , 8 7 DF = 1 P = 0 , 0 0 0 ( a d j u s t e d f o r t i e s ) With a p-value of 0.00, which is less than the established sig- ni/f_icance level of 0.05, the null hypothesis can be rejected. /T_hese results show evidence towards the alternative hypothesis that the means of TCP techniques execution results obtained using seeded and real faults are signi/f_icantly diﬀerent. Since the means are dif- ferent, one can compare the mean results obtained using real and seeded faults. In this sense, this result indicates that when the faults present in the so/f_tware being tested in an empirical evaluation are real, the APFD result tends to achieve higher values. /T_his result can be biased since the amount of data points for real and seeded values are very diﬀerent, but can be considered as an initial evidence that this factor is relevant. 4.1.2 Average amount of faults per version./T_his analysis is per- formed considering the factor average amount of faults per version on APFD results. As the variable ”amount of faults” is continuous, i.e. has numerical values instead of categorical, a regression analy- sis is performed in order to check if a relationship between amount of faults and APFD exists for the collected data. /T_he hypothesis are: • H0: /T_here is no signi/f_icant relationship between amount of faults and APFD. • H1: /T_here is a signi/f_icant relationship between amount of faults and APFD. 3www.minitab.com Figure 2: Plot for linear model of amount of faults vs. APFD. A linear model was constructed by the Minitab tool, which re- ported a p-value of 0.009. As it is less than the established sig- ni/f_icance level of 0.05, this indicates that we can reject the null hypothesis. /T_hus, evidence points towards the alternative hypothe- sis that there is a signi/f_icant relationship between the amount of faults and APFD. /T_he generated model is depicted in Figure 2. 4.1.3 Test case source. /T_his analysis is performed considering 1 factor (test case source) and 2 treatments (provided and generated) design. Provided test cases mean that the original developers of the application developed the test cases. On the other hand, generated means that researchers developed the test cases in order to evaluate something on the application. In this sense, the hypothesis for this analysis are:', 'means that researchers developed the test cases in order to evaluate something on the application. In this sense, the hypothesis for this analysis are: • H0: /T_he means of TCP techniques execution results ob- tained using provided and generated test cases are equal. • H1: /T_he means of TCP techniques execution results ob- tained using provided and generated test cases are signi/f_i- cantly diﬀerent. For this analysis, 274 diﬀerent APFD values are used. Among these values, 238 are results obtained when test cases are generated by researchers and 36 when they are provided with the application being tested and were created by real developers. As we have 274 diﬀerent values, Kolmogorov-Smirnov is used to test normality of data. With a p-value < 0.010, which is less than the established level of signi/f_icance 0.05, the sample has a non normal distribution. As the data distribution is not normal, a non-parametric hypoth- esis test is used. In this case, we employ Kruskal-Wallis test. /T_he result output is displayed in Listing 2. Listing 2: Kruskal-Wallis test for test case source factor. Kruskal −W a l l i s T e s t : APFD v e r s u s TEST CASE SOURCE TEST CASE SOURCE N Median Ave Rank Z g e n e r a t e d 238 0 , 7 1 9 5 1 4 1 , 5 2 , 1 4 p r o v i d e d 36 0 , 5 8 0 0 1 1 1 , 1 −2 ,14 O v e r a l l 274 1 3 7 , 5 H = 4 , 5 9 DF = 1 P = 0 , 0 3 2 H = 4 , 5 9 DF = 1 P = 0 , 0 3 2 ( a d j u s t e d f o r t i e s ) With a p-value of 0.032, which is less than the established sig- ni/f_icance level of 0.05, the null hypothesis can be rejected. /T_his shows evidence towards the alternative hypothesis that the means 39', 'Test case prioritization: a systematic review and mapping of the literature SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil of TCP techniques execution results obtained using provided and generated test cases are signi/f_icantly diﬀerent. Since the means are diﬀerent, one can compare the mean results obtained using pro- vided and generated test cases. In this sense, this result indicates that when the test cases are generated by researchers, the results obtained by the use of TCP techniques are be/t_ter than when using provided test cases created by the application developers. /T_his also indicates that the factor test case source is relevant to the APFD result. 4.1.4 Test case type. /T_his analysis is performed considering 1 factor (test case type) and 2 treatments (JUnit and TSL) design. Among the test suites used from selected studies, we identi/f_ied two diﬀerent types. /T_hose that use the JUnit framework and those that use test suites constructed using a Test Speci/f_ication Language (TSL). Do and Rothermel [4] call them traditional test suites. Our hypothesis for this analysis are: • H0: /T_he means of TCP techniques execution results ob- tained using JUnit and TSL test suites are equal. • H1: /T_he means of TCP techniques execution results ob- tained using JUnit and TSL test suites are signi/f_icantly diﬀerent. For this analysis, 267 diﬀerent APFD values are used. Among these values, 142 are results obtained when test cases type is JU- nit and 125 when they are TSL. As we have 267 diﬀerent values, Kolmogorov-Smirnov is used to test normality of data. With a p- value < 0.010, which is less than the established level of signi/f_icance 0.05, the sample has a non normal distribution. As the data distribution is not normal, a non-parametric hypoth- esis test is used. In this case, we employ Kruskal-Wallis test. /T_he result output is displayed in Listing 3. Listing 3: Kruskal-Wallis test for test case type factor. Kruskal −W a l l i s T e s t : APFD v e r s u s TEST SUITE TYPE TEST SUITE TYPE N Median Ave Rank Z J U n i t 142 0 , 6 8 6 0 1 3 5 , 8 0 , 4 0 TSL 125 0 , 7 1 4 0 1 3 2 , 0 −0 ,40 O v e r a l l 267 1 3 4 , 0 H = 0 , 1 6 DF = 1 P = 0 , 6 8 9 H = 0 , 1 6 DF = 1 P = 0 , 6 8 9 ( a d j u s t e d f o r t i e s ) With a p-value of 0.689, which is bigger than the established signi/f_icance level of 0.05, the null hypothesis can not be rejected. /T_his suggests that there is no diﬀerence between results obtained using JUnit and TSL test suites. /T_hus, we can not say, for the data that we used, that test case type is a signi/f_icant factor on APFD results. 4.1.5 Program size. Do and Rothermel [4], consider the metric Lines of Code (LOC) to represent programs size. We also do so, as every selected study reported this metric. /T_he variable then is continuous and we should use a regression analysis to verify if a relationship between program size and APFD result exists. Our hypothesis are: • H0: /T_here is no signi/f_icant relationship between program size measured by LOC metric and APFD. • H1: /T_here is a signi/f_icant relationship between program size measured by LOC metric and APFD. Figure 3: Plot for linear model of LOC vs. APFD. A linear model was constructed using the Minitab tool, which reported a p-value of 0.708. As it is bigger than the established signi/f_icance level of 0.05, this indicates that we can not reject the null hypothesis, suggesting that there is no signi/f_icant relationship between program size measured by LOC and APFD. /T_he generated model is depicted in Figure 3. Do and Rothermel [4] use a classi/f_ication for program size ac- cording to LOC value. /T_hey divide programs in small, medium and large. /T_hey consider small programs those with LOC smaller than 10k, medium as bigger than 10k and smaller than 100k and large as larger than 100k. Considering those categories, we can perform a hypothesis test like the previous ones. /T_hus, our hypothesis are:', '10k, medium as bigger than 10k and smaller than 100k and large as larger than 100k. Considering those categories, we can perform a hypothesis test like the previous ones. /T_hus, our hypothesis are: • H0: /T_he means of TCP techniques execution results ob- tained using small, medium and large programs are equal. • H1: /T_he means of TCP techniques execution results ob- tained using small, medium and large programs are signi/f_i- cantly diﬀerent. For this analysis, 274 diﬀerent APFD values are used. Among these values, 2 are results obtained when the program used is large, 114 when they are medium sized and 158 are small. As we have 274 diﬀerent values, Kolmogorov-Smirnov is used to test normality of data. With a p-value < 0.010, which is less than the established level of signi/f_icance 0.05, the sample has a non normal distribution. As the data distribution is not normal, a non-parametric hypoth- esis test is used. In this case, we employ Kruskal-Wallis test. /T_he result output is displayed in Listing 4. Listing 4: Kruskal-Wallis test for program size factor. Kruskal −W a l l i s T e s t : APFD v e r s u s LOC CATEGORY LOC CATEGORY N Median Ave Rank Z l a r g e 2 0 , 6 6 0 5 1 2 6 , 3 −0 ,20 medium 114 0 , 6 7 2 5 1 3 6 , 4 −0 ,20 s m a l l 158 0 , 7 2 0 0 1 3 8 , 5 0 , 2 3 O v e r a l l 274 1 3 7 , 5 H = 0 , 0 9 DF = 2 P = 0 , 9 5 8 H = 0 , 0 9 DF = 2 P = 0 , 9 5 8 ( a d j u s t e d f o r t i e s ) With a p-value of 0.958, which is bigger than the established signi/f_icance level of 0.05, the null hypothesis can not be rejected. /T_his suggests that there is no diﬀerence between results obtained using small, medium and large programs. /T_hus, we can not say, for 40', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Campos Junior et al. the used data, that program size is a signi/f_icant factor on APFD results. /T_his result is compatible with our regression analysis. 4.1.6 Seeded fault type. /T_his analysis is performed considering 1 factor (Seeded fault type) and 2 treatments (mutation or manual) design. A study [4] investigate if mutation faults are representative of real faults in real systems. /T_he use of representative faults is important to allow generalization of results obtained on empirical studies when evaluating TCP techniques. Some authors [5, 9, 12] prefer to manually seed faults into the so/f_tware under test to repre- sent real faults, usually following a systematic approach to avoid bias. /T_hus, our hypothesis for this analysis are: • H0: /T_he means of TCP techniques execution results ob- tained using mutated and manually seeded faults are equal. • H1: /T_he means of TCP techniques execution results ob- tained using mutated and manually seeded faults are sig- ni/f_icantly diﬀerent. For this analysis, 260 diﬀerent APFD values are used. Among these values, 160 are results obtained when faults were manu- ally seeded into the application being tested and 100 when mu- tation was used to seed the faults. As we have 260 diﬀerent values, Kolmogorov-Smirnov is used to test normality of data. With a p- value < 0.010, which is less than the established level of signi/f_icance 0.05, the sample has a non normal distribution. As the data distribution is not normal, a non-parametric hypoth- esis test is used. In this case, we employ Kruskal-Wallis test. /T_he result output is displayed in Listing 5. Listing 5: Kruskal-Wallis test for seeded fault type factor. Kruskal −W a l l i s T e s t : APFD v e r s u s SEEDED FAULT TYPE SEEDED FAULT TYPE N Median Ave Rank Z manual 160 0 , 6 3 3 5 1 0 9 , 9 −5 ,59 m u t a t i o n 100 0 , 7 6 4 5 1 6 3 , 5 5 , 5 9 O v e r a l l 260 1 3 0 , 5 H = 3 1 , 2 9 DF = 1 P = 0 , 0 0 0 H = 3 1 , 3 0 DF = 1 P = 0 , 0 0 0 ( a d j u s t e d f o r t i e s ) With a p-value of 0.00, which is less than the established signi/f_i- cance level of 0.05, the null hypothesis can be rejected. /T_his shows evidence towards the alternative hypothesis that the means of TCP techniques execution results obtained using mutated and manually seeded faults are signi/f_icantly diﬀerent. Since the means are dif- ferent, one can compare the mean results obtained. In this sense, this result indicates that when the faults present in the so/f_tware being tested in an empirical evaluation were seeded using mutation, the APFD result tends to achieve higher values. /T_his could be due to the fact that when mutation is used, a higher number of faults are generated and as our analysis of average amount of faults per version indicated, there is a relationship between the amount of faults and APFD results. 4.1.7 TCP technique granularity. /T_his analysis considers the factor granularity of the TCP technique. /T_he possible values for the data we have are: branch, statement, block and method/function. In this case, the granularity of TCP technique is concerned with the coverage of source code elements, since all the data we obtained are results from applying coverage-based TCP techniques. Our general hypothesis for this analysis are: Table 8: Comparison of TCP techniques granularity. Comparison Normal Test P-value Signif. Statement vs. branch x Kruskal-Wallis 0.012 ✓ Statement vs. block x Kruskal-Wallis 0.001 ✓ Statement vs. method/function x Kruskal-Wallis 0.000 ✓ Block vs. method/- function ✓ One-way ANOVA 0.834 x Branch vs. block ✓ One-way ANOVA 0.333 x Branch vs. method/- function x Kruskal-Wallis 0.149 x • H0: /T_he means of TCP techniques execution results ob- tained using branch, statement, block and method/function granularity are equal. • H1: /T_he means of TCP techniques execution results ob- tained using branch, statement, block and method/function', 'tained using branch, statement, block and method/function granularity are equal. • H1: /T_he means of TCP techniques execution results ob- tained using branch, statement, block and method/function granularity are signi/f_icantly diﬀerent. For this analysis, 216 diﬀerent APFD values are used. Among these values, 40 use techniques that consider branch granularity, 81 statement, 37 block and 58 method/function. As we have 216 diﬀerent values, Kolmogorov-Smirnov is used to test normality of data. With a p-value < 0.010, which is less than the established level of signi/f_icance 0.05, the sample has a non normal distribution. As the data distribution is not normal, a non-parametric hypoth- esis test is used. In this case, we employ Kruskal-Wallis test. /T_he result output is displayed in Listing 6. Listing 6: Kruskal-Wallis for TCP techniques granularity. Kruskal −W a l l i s T e s t : APFD v e r s u s GRANULARITY GRANULARITY N Median Ave Rank Z b l o c k 37 0 , 7 5 5 0 1 2 8 , 1 2 , 1 0 branch 40 0 , 7 4 8 0 1 1 1 , 9 0 , 3 8 method / f u n c t i o n 58 0 , 7 6 6 5 1 2 6 , 6 2 , 5 8 s t a t e m e n t 81 0 , 6 3 7 0 8 4 , 9 −4 ,30 O v e r a l l 216 1 0 8 , 5 H = 2 0 , 1 9 DF = 3 P = 0 , 0 0 0 H = 2 0 , 1 9 DF = 3 P = 0 , 0 0 0 ( a d j u s t e d f o r t i e s ) With a p-value of 0.00, which is less than the established signi/f_i- cance level of 0.05, the null hypothesis can be rejected. /T_his shows evidence towards the alternative hypothesis that the means of TCP techniques execution results obtained using branch, statement, block and method/function granularity are signi/f_icantly diﬀerent. /T_his suggests that the granularity used by TCP techniques is also a signi/f_icant factor to APFD results. But with this analysis, we can not say which one achieves be/t_ter results than the other. For this reason, we perform a paired comparison between each of them. A summary of the paired analysis is described in Table 8. More details about this analysis can be found on our external appendix. Given the results obtained by the paired comparison analysis, we can group the mean results for the diﬀerent granularities according to Table 9. Results that are in the same grouping do not have a signi/f_icant statistical diﬀerence. /T_his suggests that we can not say that results obtained by TCP techniques that use block, branch and method/function granularity diﬀer between them, but they all achieve signi/f_icantly be/t_ter results than those techniques that use statement granularity. 41', 'Test case prioritization: a systematic review and mapping of the literature SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Table 9: Mean results comparison for each granularity. Granularity APFD mean Grouping Block 0.755 A Branch 0.748 A Method/function 0.766 A Statement 0.637 B 4.1.8 Test granularity. /T_he last possible factor we consider to impact APFD is test granularity. /T_his factor is important when JUnit tests are being used, because diﬀerent granularity levels can be applied. A developer can choose to have each test class as a single test case or group diﬀerent test cases in methods that are within a class. A problem however arises with the use of method level granularity, since the JUnit framework, up to the version 4.9, do not allow that test cases that are in diﬀerent classes be executed arbitrarily. To overcome this problem, researchers need to extend the framework and implement the feature themselves [4]. For this analysis we have two diﬀerent values for the factor test granularity, class level and method level. Our hypothesis are: • H0: /T_he means of TCP techniques execution results ob- tained using class level and method level test granularity are equal. • H1: /T_he means of TCP techniques execution results ob- tained using class level and method level test granularity are signi/f_icantly diﬀerent. We consider only studies that explicitly stated the test granular- ity. /T_hus, we have 110 diﬀerent APFD values, where 55 use class level and 55 use method level tests. As we have 110 diﬀerent val- ues, Kolmogorov-Smirnov is used to test normality of data. With a p-value > 0.150, which is bigger than the established level of signi/f_icance 0.05, the sample has a normal distribution and thus we use a parametric hypothesis test. In this case, we use One-way ANOVA test. /T_he result output is displayed in Listing 7. Listing 7: One-way ANOVA test for test granularity factor. One−way ANOVA: APFD v e r s u s TEST GRANULARITY A n a l y s i s o f V a r i a n c e S o u r c e DF Adj SS Adj MS F −Value P −Value TEST GRANULARITY 1 0 , 1 4 0 5 0 , 1 4 0 4 8 6 , 2 0 0 , 0 1 4 E r r o r 108 2 , 4 4 7 2 0 , 0 2 2 6 6 T o t a l 109 2 , 5 8 7 7 Means TEST GRANULARITY N Mean StDev 95% CI c l a s s l e v e l 55 0 , 6 7 1 8 0 , 1 4 0 5 ( 0 , 6 3 1 5 ; 0 , 7 1 2 0 ) method l e v e l 55 0 , 7 4 3 3 0 , 1 5 9 9 ( 0 , 7 0 3 0 ; 0 , 7 8 3 5 ) P o o l e d StDev = 0 , 1 5 0 5 3 0 With a p-value of 0.014, which is less than the established signif- icance level of 0.05, the null hypothesis can be rejected. /T_his shows evidence towards the alternative hypothesis that the means of TCP techniques execution results obtained using class level and method level test granularity are signi/f_icantly diﬀerent. /T_his suggests that the test granularity used by TCP techniques is also a signi/f_icant factor to APFD results. Since the means are diﬀerent, one can an- alyze their diﬀerence and note that method level achieves be/t_ter results than class level. Table 10: Summary of best APFD achieved on each applica- tion used by selected papers on their empirical evaluations. Application Technique APFD XML-Security ART-st 0.970 Jdepend add-cov 0.902 Checkstyle t-block-cov 0.684 tcas a-block-cov 0.840 schedule2 a-block-cov 0.720 schedule a-block-cov 0.720 tot info t-block-cov 0.758 print tokens t-block-cov 0.808 print tokens2 t-block-cov 0.761 replace t-stmt-cov 0.966 space t-stmt-cov 0.997 ant a-stmt-cov 0.954 jmeter a-branch-cov 0.883 jtopas a-branch-cov 0.970 Altitude Switch (ASW) t-branch-cov 0.766 Wheel Brake System (WBS) t-branch-cov and a-fn-cov 0.821 Flight Guidance System (FGS) a-fn-cov 0.801 NoiseGen a-fn-cov 0.742 Galileo BNA-block 0.940 NanoXML a-fn-cov 0.945 Techniques summary: ART-st: statement ART; add-cov: additional coverage; t-block-cov: total block coverage; a-block-cov: additional block coverage; t- stmt-cov: total statement coverage; a-stmt-cov: additional statement coverage;', 't-block-cov: total block coverage; a-block-cov: additional block coverage; t- stmt-cov: total statement coverage; a-stmt-cov: additional statement coverage; a-branch-cov: additional branch coverage; t-branch-cov: total branch cover- age; BNA-block: additional bayesian network with block coverage; a-fn-cov: additional function coverage Table 11: Summary of our relevant factors /f_indings. Factor P-value Eﬀect on APFD Fault type 0.000 Signi/f_icant Average amount of faults per version 0.009 Signi/f_icant Test case source 0.032 Signi/f_icant Test case type 0.689 Not signi/f_icant LOC 0.958, 0.708 Not signi/f_icant Seeded faults type 0.000 Signi/f_icant TCP technique granularity 0.000 Signi/f_icant Test granularity 0.014 Signi/f_icant /T_his result may occur due to the fact that, as explained before, when JUnit is running a class containing tests, it must execute all of its test methods before moving to the next class. /T_his can aﬀect the APFD results, since a fault may be detected by a test case that is in the next class to be executed. If a method level granularity is used, the JUnit framework would execute a method from a class and would move directly to the other class the method, reducing the amount of executed tests without detecting a fault. 4.2 Results summary To answer RQ1.1, we highlighted in bold font the best result for each application (row) in Table 6. A summary of these best results is shown in Table 10. RQ2.2 is answered through the analysis presented in the previous section, which is summarized in Table 11. According to collected data, the factors fault type, average amount of faults per version, test case source, seeded faults type, TCP technique granularity and test granularity have a signi/f_icant impact on APFD results obtained by selected studies in this review. Section 4.1.7 answers RQ1.3, showing that there is no signi/f_i- cant diﬀerence between results obtained by TCP techniques using branch, block and method/function granularity, but they all achieve 42', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Campos Junior et al. be/t_ter APFD than those techniques that use statement granularity. /T_he mean results for each granularity can be observed in Table 9. 5 DISCUSSION AND THREATS TO VALIDITY All techniques that were considered in our review analysis are coverage-based. /T_his indicates the necessity for more quality em- pirical studies regarding diﬀerent TCP techniques. /T_here are plenty of empirical studies, as demonstrated by the number of accepted studies in the mapping (90), but when our quality assessment was applied, only thirteen studies remained. Furthermore, the amount of rejected papers in this mapping and review suggests that authors are proposing many TCP techniques but they are not empirically evaluating them. /T_his fact can be a problem. Since there is a low amount of empirical studies, practitioners and researchers do not have a reliable amount of empirical evidence to choose a suitable TCP technique for their needs. Considering the possible factors that we could test their signif- icance on APFD results for the data that we collected, we found that 6 out of 8 candidate factors achieved a signi/f_icant result. When compared to Do and Rothermel [4] qualitative analysis, we could con/f_irm all 3 factors that they considered relevant to the APFD result. /T_hey are fault type, average amount of faults per version and test case source. Regarding the threats to validity of our review, we tried to per- form a search as broad as possible. However, studies were discarded from analysis based on our quality assessment protocol, which is composed of requirements that we believe that make an empirical study result reliable to be aggregated. For this reason, our results may not re/f_lect the reality and empirical studies must be carried to con/f_irm our /f_indings. Moreover, analyzing the TCP techniques that were compared, one can note that all of them are coverage-based. Other important threats are the interaction of diﬀerent factors that may aﬀect the APFD results of the analyzed studies and the diﬀerent context and designs of the experiments to evaluate the eﬀectiveness of TCP techniques, which may aﬀect their outcome. 6 CONCLUSIONS In this paper we described a systematic literature mapping and review about the test case prioritization /f_ield. We investigated most used approaches, metrics and tools for diﬀerent activities. Moreover, we investigated reported TCP techniques eﬀectiveness results in order to identify relationships with some evaluation context factors and provide a basis for future research. Our results suggest that more rigorous empirical methodology is needed in future studies, since poor methodology hinder the aggregation of evidences. In our analysis, for example, even though there are studies that report eﬀectiveness results of techniques that are not coverage based, they were discarded from the process, since they do not meet our quality assessment criteria. Furthermore, it is also necessary that authors compare their proposed techniques with well established ones in literature, since these comparisons can help researchers and practitioners to choose a technique that is suitable for their need. Six evaluation context factors were identi/f_ied as signi/f_icant on TCP techniques eﬀectiveness by our analysis. /T_hey are related to faults and test cases of the applications being tested and also to the coverage granularity that a TCP technique uses when executed. /T_his result however is limited to our speci/f_ic context of coverage- based TCP techniques and must be further evaluated with more data in the future. /T_hese factors can help researchers build new techniques and also be/t_ter plan their evaluations. As future work, we plan to further evaluate context factors that aﬀect TCP techniques eﬀectiveness in order to help practitioners choosing suitable TCP techniques. REFERENCES', 'As future work, we plan to further evaluate context factors that aﬀect TCP techniques eﬀectiveness in order to help practitioners choosing suitable TCP techniques. REFERENCES [1] C. Catal and D. Mishra. Test case prioritization: a systematic mapping study. So/f_tware /Q_uality Journal, 21(3):445–478, 2013. [2] D. Di Nardo, N. Alshahwan, L. Briand, and Y. Labiche. Coverage-based test case prioritisation: An industrial case study. In So/f_tware Testing, Veri/f_ication and Validation (ICST), 2013 IEEE Sixth International Conference on, pages 302–311. IEEE, 2013. [3] H. Do, S. Mirarab, L. Tahvildari, and G. Rothermel. /T_he eﬀects of time constraints on test case prioritization: A series of controlled experiments. IEEE Transactions on So/f_tware Engineering, 36(5):593–617, 2010. [4] H. Do and G. Rothermel. On the use of mutation faults in empirical assessments of test case prioritization techniques. IEEE Transactions on So/f_tware Engineering, 32(9):733–752, 2006. [5] H. Do, G. Rothermel, and A. Kinneer. Prioritizing junit test cases: An empirical assessment and cost-bene/f_its analysis.Empirical So/f_tware Engineering, 11(1):33– 70, 2006. [6] S. Eghbali and L. Tahvildari. Test case prioritization using lexicographical order- ing. IEEE Transactions on So/f_tware Engineering, 42(12):1178–1195, 2016. [7] S. Elbaum, A. G. Malishevsky, and G. Rothermel. Test case prioritization: A family of empirical studies. IEEE transactions on so/f_tware engineering, 28(2):159–182, 2002. [8] C. Fang, Z. Chen, K. Wu, and Z. Zhao. Similarity-based test case prioritization using ordered sequences of program entities.So/f_tware /Q_uality Journal, 22(2):335– 361, 2014. [9] D. Hao, X. Zhao, and L. Zhang. Adaptive test-case prioritization guided by output inspection. In Computer So/f_tware and Applications Conference (COMPSAC), 2013 IEEE 37th Annual, pages 169–179. IEEE, 2013. [10] A. Jatain and G. Sharma. A systematic review of techniques for test case prioriti- zation. International Journal of Computer Applications, 68(2), 2013. [11] B. Kitchenham and S. Charters. Guidelines for performing systematic literature reviews in so/f_tware engineering, 2007. [12] A. Marche/t_to, M. M. Islam, W. Asghar, A. Susi, and G. Scanniello. A multi- objective technique to prioritize test cases. IEEE Transactions on So/f_tware Engi- neering, 42(10):918–940, 2016. [13] H. Mei, D. Hao, L. Zhang, L. Zhang, J. Zhou, and G. Rothermel. A static ap- proach to prioritizing junit test cases. IEEE Transactions on So/f_tware Engineering, 38(6):1258–1275, 2012. [14] S. Mirarab and L. Tahvildari. A prioritization approach for so/f_tware test cases based on bayesian networks. In International Conference on Fundamental Ap- proaches to So/f_tware Engineering, pages 276–290. Springer, 2007. [15] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold. Test case prioritization: An empirical study. In So/f_tware Maintenance, 1999.(ICSM’99) Proceedings. IEEE International Conference on, pages 179–188. IEEE, 1999. [16] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold. Prioritizing test cases for regression testing. IEEE Transactions on so/f_tware engineering, 27(10):929–948, 2001. [17] Y. Singh, A. Kaur, B. Suri, and S. Singhal. Systematic literature review on regression test prioritization techniques. Informatica (Slovenia), 36(4):379–408, 2012. [18] M. Staats, P. Loyola, and G. Rothermel. Oracle-centric test case prioritization. In So/f_tware Reliability Engineering (ISSRE), 2012 IEEE 23rd International Symposium on, pages 311–320. IEEE, 2012. [19] C. Wohlin, P. Runeson, M. H ¨ost, M. C. Ohlsson, B. Regnell, and A. Wessl ´en. Experimentation in so/f_tware engineering. Springer Science & Business Media, 2012. [20] K. Wu, C. Fang, Z. Chen, and Z. Zhao. Test case prioritization incorporating ordered sequence of program elements. In Proceedings of the 7th International Workshop on Automation of So/f_tware Test, pages 124–130. IEEE Press, 2012.', 'ordered sequence of program elements. In Proceedings of the 7th International Workshop on Automation of So/f_tware Test, pages 124–130. IEEE Press, 2012. [21] S. Yoo and M. Harman. Regression testing minimization, selection and prior- itization: a survey. So/f_tware Testing, Veri/f_ication and Reliability, 22(2):67–120, 2012. [22] Z. Q. Zhou, A. Sinaga, and W. Susilo. On the fault-detection capabilities of adaptive random test case prioritization: Case studies with large test suites. In System Science (HICSS), 2012 45th Hawaii International Conference on, pages 5584–5593. IEEE, 2012. 43']","['TEST CASE PRIORITIZATION: RELEVANT FACTORS This  briefin  reports  scieitfc  evideice oi  coverane-based  test  case prioritzatoi techiiquzes efectveiess. FINDINGS •  Approximately  36%  of  test  case prioritiatoon  (TCP)  techoniqutes available ion primary sttdies from literattre  are  coverage-based. Followed by history-based (approx. 14%)  aond  modifcatoon-based (approx. 9%). • Fiondiongs  reported  ion  this  briefong coonsider oonly coverage-based test case prioritiatoon techoniqutes. • Overall,  the  TCP  techoniqutes  that achieved the higher efectveoness restlts  ion  most  projects  ion coontrolled  experimeonts  were ""additoonal ftonctoon coverage"", ion 5 difereont projects, followed by ""total  block  coverage""  ion  4 difereont projects. • Most compared test case prioritiatoon techoniqutes ion difereont projects are ""total  braonch  coverage""  (18 projects)  aond  ""additoonal statemeont coverage"" (14 projects). • Test  case  prioritiatoon  techoniqutes efectveoness  restlts  may  vary accordiong  to  difereont  coontext factors regardiong the project beiong tested. • Fatlts type does impact oon test case prioritiatoon  techoniqutes efectveoness. Techoniqutes tsed oon projects with real fatlts teond to achieve higher efectveoness thaon those with seeded fatlts. • Amotont of fatlts ion the project beiong tested  does  impact  oon  TCP techoniqutes  efectveoness.  Its efectveoness grows as amotont of fatlts also grows. • Test case sotrce doest impact oon TCP techoniqutes  efectveoness. Techoniqutes tsed oon projects with test  cases  geonerated  by researchers teond to achieve higher efectveoness thaon those with test cases  provided  by  origional developers. • Test case type doeson\'t seem to impact oon TCP techoniqutes efectveoness. JUonit aond TSL test stites seems to achieve similar efectveoness wheon prioritied. • Program siie (meastred by LOC) also doeson\'t seem to impact oon TCP techoniqutes efectveoness. • Type of seeded fatlts does impact oon TCP  techoniqutes  efectveoness. Fatlts seeded ionto the sofware throtgh mttatoon seem to achieve higher  efectveoness  wheon prioritied  thaon  those  seeded maontally. • TCP techoniqute graontlarity does impact oon its efectveoness. • Block,  braonch  aond  method/ftonctoon graontlarity  seem  to  perform equtally well oon TCP techoniqutes. Oon the  other  haond,  statemeont graontlarity may be onot worth to tse, sionce TCP techoniqutes tsiong this  graontlarity  achieve  worse efectveoness,  despite  requtiriong more  processiong  power  to calctlate. • Test graontlarity does impact oon TCP techoniqute  efectveoness. Techoniqutes that tse method level seem  to  achieve  higher efectveoness thaon those with class level. • Coonsideriong the most tp-to-date JUonit versioon (5.0.1), there is ono optoon to exectte tonit tests at method level. That is, arbitrary execttoon of  test  cases  (methods)  from difereont classes. • A persoon ionterested oon execttong tonit tests at the method level oneed to rely oon third-party libraries. Who is this briefin  or? Sofware eongioneeriong practtooners who waont to make decisioons abott test case  prioritiatoon techoniqutes based oon  scieontfc evideonce. Also, researchers  who waont to desigon onew test case  prioritiatoon techoniqutes. Where the finiins come  rom? All fondiongs of this briefong were  extracted from the systematc  literattre review aond mappiong  coondtcted by Campos Jtonior et al.   What is a systematc reeiee? cion.tfpe.br/eseg/systematc-reviews What is iiclunen ii this briefin? The maion fondiongs of the origional  systematc review aond mappiong. What is iot iiclunen ii this briefin? Details abott the review process aond  ioncltded sttdies. To access other eeineice briefins  oi sofeare einiieeriine http://cbsof.org/cbsof2012/ For annitoial ii ormatoi about  NEiCe http://www.tjf.br/oneonc/ ORIGINAL RESEARCH REFERENCE', 'ioncltded sttdies. To access other eeineice briefins  oi sofeare einiieeriine http://cbsof.org/cbsof2012/ For annitoial ii ormatoi about  NEiCe http://www.tjf.br/oneonc/ ORIGINAL RESEARCH REFERENCE Campos Jtonior, H. S., Araújo, M. A. P., David, J. M. N., Braga, R., Campos, F., & Ströele, V. (2012, September). Test case prioritiatoon: a systematc review aond mappiong of the literattre. Ion Proceediongs of the 31st Braiiliaon Sympositm oon Sofware  ongioneeriong (pp. 34-43). ACM. https://doi.org/10.1145/3131151.3131120']","**Title: Enhancing Test Case Prioritization for Improved Fault Detection**

**Introduction:**
This evidence briefing summarizes findings from a systematic review and mapping of the literature on test case prioritization (TCP) techniques. The goal is to synthesize reported effectiveness results and identify significant factors that influence the success of these techniques in detecting faults during software testing. This information is crucial for software engineers and researchers aiming to optimize regression testing practices.

**Main Findings:**
1. **Prevalence of Empirical Studies**: The review identified a substantial number of empirical studies (90) evaluating various TCP techniques from 1999 to 2016. However, many studies were discarded due to methodological issues, highlighting a need for improved empirical rigor in this field.

2. **Coverage-Based Techniques Dominance**: Most effective TCP techniques reported in the literature are coverage-based. These techniques prioritize test cases based on their ability to cover different parts of the code, thus increasing the likelihood of fault detection.

3. **Significant Factors Influencing Effectiveness**: The analysis revealed six key context factors that significantly affect TCP effectiveness:
   - **Fault Type**: Results indicate that using seeded faults generally yields higher APFD (Average Percentage of Faults Detected) values compared to real faults.
   - **Average Number of Faults**: A positive correlation exists between the number of faults per version and APFD results; more faults lead to better detection rates.
   - **Test Case Source**: Test cases generated by researchers tend to result in higher APFD values compared to those provided by original developers.
   - **TCP Technique Granularity**: The granularity of coverage (e.g., branch, statement, block, method) significantly impacts effectiveness, with block and method/function granularities generally performing better than statement granularity.
   - **Test Granularity**: Method-level tests outperform class-level tests in terms of APFD results, suggesting that finer granularity can enhance detection capabilities.

4. **Need for Comparative Studies**: The review emphasizes the importance of comparing new TCP techniques against established ones to build a stronger evidence base for their effectiveness.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, researchers, and educators involved in software testing and quality assurance. It provides insights into effective TCP practices and informs decisions related to test case prioritization strategies.

**Where the findings come from?**
All findings are based on a systematic literature review and mapping conducted by Heleno de S. Campos Junior et al. (2017), which analyzed empirical studies on TCP techniques published from 1999 to 2016.

**What is included in this briefing?**
This briefing includes a summary of the key findings regarding TCP effectiveness, significant influencing factors, and recommendations for future research directions. It also highlights the need for improved empirical methodologies in evaluating TCP techniques.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research group:**
[https://helenocampos.github.io/tcpreview](https://helenocampos.github.io/tcpreview)

**Original Research Reference:**
Heleno de S. Campos Junior, Marco Antonio P. Araújo, José Maria N. David, Regina Braga, Fernanda Campos, and Victor Ströele. 2017. Test case prioritization: a systematic review and mapping of the literature. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, DOI: [10.1145/3131151.3131170](https://doi.org/10.1145/3131151.3131170)"
"[""Testing context-aware software systems: Unchain the context, set it free! Santiago Matalonga  Facultad de Ingeniería  Universidad ORT Uruguay  Montevideo, Uruguay  smatalonga@uni.ort.edu.uy  Guilherme H. Travassos  Universidade Federal do Rio de Janeiro  PESC/COPPE  Rio de Janeiro, Brazil  ght@cos.ufrj.br   1  Abstract— Background: In the era of digitalization , context  awareness has become more important to allow software  systems adaptation. Therefore, the quality assurance of such  systems must consider the variation of context. However,  there is a  lack of software quality  technologies doing so,  which can increase the risk of failure of these systems.  Objective/Aim: To evidence recent advances regarding the  testing of context -aware software systems  (CASS), focusing  on the abstraction that context should freely vary during test  execution as it does in production environments.  Method: Based on knowledge acquired with quasi- Systematic Literature Reviews, we evaluate current testing  approaches and discuss benefits and l imitations of applying  the observation above about CASS.  Results: It was not possible to observe any software testing  technology supporting the unconstrained variation of  context during testing a CASS . The practitioners, meanwhile,  can use three main evidence-based strategies to test CASS: to  assure functional correctness before turning to testing   context-aware requirements; to design test cases to target  context variables and to take advantage of automatic testing  tools as much as possible.  Conclusions: To allow context to vary during testing freely ,  seems plausible . It provides a new frame of thought to  enable the design of novel technologies to improve our  capacity of testing CASS.  CCS Concepts  Software and its engineering  \uf0e0 Software creation and  management \uf0e0 Software verification and validation  Keywords. Software Testing , Context-aware,  Context-aware software testing.                                                                           Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components of  this work owned by others than the author(s) must be honored. Abstracting  with credit is permitted. To copy otherwise, or republish, to post on servers or  to redistribute to l ists, requires prior specific permission and/or a fee.  Request permissions from Permissions@acm.org.    SBES'17, September 20–22, 2017, Fortaleza, CE, Brazil© 2017   Copyright is held by the owner/author(s). Publicati on rights licensed to ACM.  ACM ISBN 978-1-4503-5326-7/17/09…$15.00  https://doi.org/10.1145/3131151.3131190  ACM Reference format  S. Matalonga, G. H. Travassos. 2017. Testing context -aware  software systems: Unchain the context, set it free! In  Proceedings of 31st Brazilian Symposium on Software  Engineering, Fortaleza, Ceará, Brazil, September 2017 (XXXI  SBES), 5 pages.  https://doi.org/10.1145/3131151.3131190  1. Introduction  Context-aware software systems  (CASS) are a subtype of   ubiquitous systems. They use context information to aid  users in achieving their tasks. This capacity of sensing the  application context has found useful in several application  domains. Platform providers like Google are taking advantage  of the myriad of sensors available in smartphones to provide  contextual APIs to softwa re developers  (https://developers.google.com/awareness/). In turn,  application providers are using these devices in domains  such as smart traffic routing like Waze, health care, athletes  training   and performance enhancement, among others. The IoT and  manufacturing industry are also adopting context -aware  software systems to optimize production and reduce costs  [1]."", 'training   and performance enhancement, among others. The IoT and  manufacturing industry are also adopting context -aware  software systems to optimize production and reduce costs  [1].  As context -aware applications become pervasive and  more ambitious, so does the cost of their failures. Imagine the  remote-monitoring health care system described in [2],  dispatching an ambulance with a doctor for kilometers across  India for mistakenly interpreting a push -up for fainting.  Likewise, imagine a CASS designed for optimizing a  horizontally-integrated supply chain as the one envisioned in  [3]. In this system, the factory capacity and inventory are  managed by software, signaling work orders downstream,  misjudging any variable can result in costly inefficiencies  either for excessive inventory or additional waiting times.  These examples show the importance of assuring the  quality of CASS. In particular, in this work, we are interested  in the current practices and technologies available for testing  CASS, which is particularly challenging. Traditionally,  designing a suitable set of test cases to verify a software  system requires that the test designer makes tradeoff  decisions between the test suite comprehensiveness and its  execution effort. In CASS, context becomes a factor and the  test designer  capacity in planning  a possible test suit  worsens.  The definition of context is elusive [4][5]. To the best of  our knowledge, the most used context definition comes fr om  Dey and Abowd [6]: “ Context is a ny information that can be  used to characterize the situation of entities that are  considered relevant to the interaction between user and  250', 'application.” At design time, context can be abstracted  to  context variables and inputs from sensors will be regarded  the representation of such variables. At run-time, context can  take many forms, and even be represented  by values not  foreseen for the application [7], [8].   Current testing approaches for CASS primarily rely on  two strategies. Either they exploit extensive computation  resources to simulate possible contextual input, or they  artificially constrain the context values to make testing  decisions manageable . However, both strategies impose  artificial limitations and threat the effectiveness of testing,  since these conditions are not present in the intended  production environment of context-aware software systems.  A recent proposal to guide the design and execution of  CASS test cases observed that context should freely vary  during test execution as it does in production environments [7].  Therefore, during the design and execution of test cases, the  test designer  should not treat context variables as regular  test input ones. First, because test input variables are  deterministic, that is, the test designer has control over the  test input variables and can define their values at design time.  Context variables, by th eir nature, are not deterministic .  Dourish [5] makes a case for  a phenomenological view of  context for context-aware software. The author argues about  a dual nature of context. In this view, an application can be  designed to be aware to a context variable (and to the  changes in its values)  – deterministic. The same software  system can occasionally react  to a context variable –  phenomenological. We argue that this phenomenological  view of context represents an adequate framework when  dealing with testing CASS. For instance, a self-driving car can  be designed to turn left upon detection of a road cross  (deterministic). However, designing a test case for this  scenario, cannot account for all possible hurdles and  obstacles that can appear at any crossroad –  phenomenological. Notice that this advocacy for non - determinism is not in contrast to traditional testing  guidelines about non -determinism (see  https://martinfowler.com/articles/nonDeterminism.html).  We see it as a necessary extension to accommodate for the  nature of CASS. In our envisioned CASS test cases, the test  oracle must give a clear expectation of the outcome of the  test, albeit with the necessary flexibility to accommodate to  variations in context. B oth, test input and system under test  must be within the control of the tester, yet context variables  should not be constrained to deterministic values (M2).  To set fixed testing values for context variables during  testing, leads to artificial constraints, limiting testing samples  and trials regarding the CASS under test. These artificial  constraints are enacted by setting context variables to  predefined values (for instance location [9]) or simulating  those values through a software infrastructure [10].  Secondly, while test input variables impact only on the  system under test, context variables have an impact both on  the system under test and on the test oracle.   The CASS under test can adapt its behavior after sensing a  variation in the context. Therefore, for testing purposes, the  test oracle must also be aware of the variations of context to  provide an appropriate expected result regarding the test  case. Having performed two quasi-systematic literature  reviews on different aspects of testing [7],[8], we claim that  no available technology can support the variation of context  during the testing of CASS.  Therefore, this paper discusses a new perspective for  dealing wit h the testing of CASS and present current  recommendations for those practitioners looking to test this  sort of software system by comparing and contrasting  current approaches against our proposed abstraction (i.e. the', 'recommendations for those practitioners looking to test this  sort of software system by comparing and contrasting  current approaches against our proposed abstraction (i.e. the  free variation of context during the t esting). Furthermore,  practical and methodological limitations of the approach are  presented, aiming at to motivate researchers to work with  these software technological challenges.  This paper is organized  as follows . First, we discuss  practical and method ological motivations for our novel  approach. In section –“A new abstraction for testing context- aware software systems” - we describe the elements of the  new approach and how it addresses the methodological  motivations. We then go on to discuss envisioned attributes  and limitations of the approach. Finally, we close this paper  with some conclusions and reflections.  2. Testing Context-Aware Software Systems  without imposing artificial context constraints  The research results discussed here comes from the CNPq  CAcTUS project. The goal of this project is to understand how  software engineers are currently assuring the quality of CASS.  The CAcTUS project follows an evidence-based approach to  software research. The results of two quasi-systematic  literature reviews (qSLR) [7], [8]  support our propositions.   This section will first describe the implications for the  industry and then discuss the methodological motivations  arising from the acquired evidence.  2.1. Practical contributions  Through our observations of current approaches used for  testing CASS, we abstract the following practical  contributions that the industry can currently apply when  testing such type of software system[12]:   P1.  Assure functional correctness before turning to  testing context aware requirements.   When the contextual input is considered  for designing  test cases, the test input space grows. Therefore, before  tackling the problem of verifying how the software system  behaves under a changing context, check the system for i ts  functionality in constrained contextual environments.   P2.  Design test cases to target context variables.   Context variables are the designed parts of the software  system where context is sensed. Therefore, the test designer  should take care of such variables, as explicitly recommended  in [13].   P3. Take advantage of automatic testing tools as much as  possible.   The most popular approach  to deal with the unforeseen  variation of context is to use platforms that can simulate  pseudo-random input for context variables (for instance [10],  [14], [15]).   Practitioners can use these previous recommendations to  test CASS because they do not require any special  infrastructure (see in section “Practical limitations of the  proposed approach ,” PL3). The current software testing  technologies can enact them.  Nevertheless, we argue that they do not capture the  phenomenological nature of context. Therefore, these  approaches increase the risk of not achieving appropriate  coverage of the system under test.  2.2 Methodological motivations  In t his section , we  argue for the development of a  technology to enable the variation of context during testing.   .     251', 'Figure 1. Abstraction of the elements of a test case allowing the variation of context    M1. Context influences the test cases [8]  This influence is explicit in two test case elements: the  test item and the test oracle. Since the Test Item is context - aware, when executing the test case, the Test Item can sense  the context and react by adapting its behavior. T he Test  Oracle must match the Test Item behavior change to provide  a suitable expectation of results to evaluate the Test Case.  M2. The application context, the test input, and the test  environment are different entities and should also be  distinguished during testing [7].  The ISO/IEC/IEEE 29119 Standard for Software and  Systems testing [16] defines Test Input as: “ the input to  which the Test Item will be stimulated. ” Moreover, the same  standard defines the Test Environment as “ the facilities,  hardware, software, firmware, procedures, and  documentation intended for or used to perform the software  testing.” Therefore, for a Test Case, both Test Inputs and Test  Environment should be completely deterministic. The test  designer should be able to define the test inputs (variable  types and values), and the test environment (hardware,  operating systems, database, and others). In  contrast, for  context, only the variable type can be determined during the  test planning. Pre -setting context variable values leads to  fixating context, which leads to observing the test item in an  artificial environment. Therefore, limiting the possibili ty of  identifying context-related faults (which results in M3).  M3. Context must be left free to change during the test  case execution [7].  In a production environment, a CASS is not constrained   by the values that it can accept from its context sensors. For  the execution of a Test Case, the CASS should not be imposed  by artificial constraints that could limit its  interaction with  the context.   Back to the self-driving car example of the introduction ,  this car can be programmed to accept the full range of values  included in the sensors specification. However, during  testing, it is not feasible to design scenarios f or all possible  values. Therefore, higher abstraction oracles must be set.  M4. Fuzzy Test Oracles.   As a corollary of the previous two results, we argue that  Fuzzy (non-deterministic) Test Oracles are needed  to  evaluate the test cases results. Since contex t has influence in  the Test Case, then the Test Oracle cannot be deterministic.  Therefore a behavior expectation should be sufficient to pass  judgment on the suitability of the Test Item behavior  modification given the current context and Test Inputs.  3. A new abstraction for testing context-aware  software systems  Figure 1 depicts the relationships between the  methodological motivations and the elements of a test case.  In this figure, Context – represented by a clou d – is not  completely within the scope of the Test Case. It relates how  the test case cannot control all possible contextual inputs  (M3).   The arrows for the context towards the system under test  and the test oracle, convey the influence that the context has  on both of these items of a test case ( M1). As per this  influence, the test oracle must adapt to the contextual  changes to give appropriate and relevant expectations to  evaluate the test output (M4).   Finally, the context, the test input, and the test  environment are different entities that must be differentiated  (M2). Test input and context are different in nature. The test  input is deterministic. Moreover, though some inputs can be  generated outside of the scope of the test case, the tester  knows these  (hence, we represent it as an arrow not as a  cloud). In addition , the context and test environment since  the latter one should be defined  in the system under test  252', 'requirements. Moreover, though some environments are not  feasible to reproduce during test  time, this brings on a  different problem than that of the variation of context.  4. Limitations of the proposed approach  This section presents some limitations that must be  considered when generalizing the arguments from the  previous section. This section is also divided  into the  discussion of the current practical applications and  methodological limitations.  5. Practical limitations of the proposed approach  PL1. The proposed abstraction is in its early stages, and it  is not completely clear how it can contribute to   improving the quality of CASS.   The proposed abstraction was introduced in [7]. In [17],  the abstraction supported the proposal of a test suit design  process, which has only been evaluated in vitro with students.  There is no discussion about how this differentiation applies  to the unit testing/code level.   PL2. There are  no available testing strategies to support  practitioners on designing test cases for CASS, which  consider the behavior changes regarding the Test  Items and Test Oracles.  In [18], the authors compare which test types (as defined  by ISO/IEC 25010) are being used to design test cases for  CASS. Their results show that the current solution is to adapt  traditional test design  strategies to overcome the context  variation.   PL3. There is currently no computerized infrastructure to  support the proposed approach.   To the best of our knowledge, no technical infrastructure  enables the practical implementation of this abstraction yet .  Even whether practitioners design test cases using the  proposed abstraction, no tools nor approaches allow them to  plan or execute CASS testing considering the variation of  context during testing execution.  •\uf020 Methodological limitations of the proposed approach  ML1. Reviewed literature is relatively narrow in  comparison to the available technical literature.   Contributions presented in this paper all come from two  independently conducted secondary studies. Both qSLRs  share the same Population (“Sensibility to context,” the same  keywords in the search engines). Thus, we could crudely  estimate the amount of missed primary sources. Table 1  presents the overlaps between both qSLRs. Using the Lincoln- Petersen estimator for Mark and Recapture method [19], we  can estimate that either research did not consider five  potential primary sources.    Reference In [8] In [7]  Alsos and Dahl [20]  Yes  Amalfitano, D., et al. [14] Yes Yes  Canfora, G. et al. [21]  Yes  Chan W.K., Chen, T., and Lu, H. [22] Yes   Chan W.K. et al. [23] Yes   Frederiks, E.M., DeVries, B., and Cheng, B.  [24]  Yes   Griebe and Gruhn [25] Yes   Jang, B., Long, X. and Gao, X. [26] Yes Yes  Merdes, M., et al. [27] Yes Yes  Micskei, Z. et al. [28] Yes   Propp, S., Buchholz, G., and Forbrig, P.[29] Yes   Ryan and Gonsalves [30]  Yes  Satoh, I [9]  Yes  Tse, H. et al. [31] Yes Yes  Wang, H. and Chan, W.K. [32] Yes Yes  Wang, H., Chan, W.K, and Tse, H [33] Yes   Wang H., Zhai, K. and Tse, H [34] Yes   Wang, Z. Elbaum, S., and Rosenblum, D.S.  [13]  Yes Yes  Ye, C. et al. [35] Yes   Table 1 Evidence-based technologies regarding CASS  testing  ML2. Industrial experts have not evaluated the proposed  abstraction:   The only evaluation of the test cases design process  observing context variation was an in-vitro observational  study. Without proper industrial validation, it is not possible  to warrant that this abstraction is anything else than a  theoretical construct with a limited perspective of industrial  use. However, the initial results did not suggest it.  6. On the issue of Quality assurance of context- aware software system, where do we stand?  There is a need for developing and deploying high quality  CASS. Four out of the 12 worst “software glitches” in 2015  were caused by CASS  (see  http://www.computerworlduk.com/galleries/infrastructure', 'CASS. Four out of the 12 worst “software glitches” in 2015  were caused by CASS  (see  http://www.computerworlduk.com/galleries/infrastructure /top-10-software-failures-of-2014-3599618/). T he industry  has very few alternatives for assuring the quality of such  systems. Available software testing technologies rely heavily  on simulation or extensive computation. We believe that t his  solution is not feasible for all parties involved with the  construction of CASS.   New testing technologies dealing with the issues brought  by the variation of context and its influence on systems’  behaviors are needed. Such technologies need to be  experimentally validated to provide feasible solutions for the  industry. Since the demand for CASS is continuously growing  while such technologies are being developed and validated,  the current and viable alternatives for the industry are P1,  P2, and P3, as di scussed in Section II – practical  contributions.    7. Summarizing remarks  This paper describes the implications of a n ew  abstraction for testing CASS, where test input and context  variables are differentiated. This proposal aims at dealing  with the issue t hat context variation worsens the test  designers capacity to make trade-off decisions between test  coverage and test effort. The authors argue that software  testing technologies must evolve to address the context - awareness characteristic of software systems. Currently, the  testing approaches  deal with the variation of context by  either fixating context values or simulating them based on  computerized infrastructure. In either case, the test item is  not receiving contextual information from the context, but  through artificial methods. Thereby, limiting the test case  validity. The authors advocate the idea that context  should  vary freely during test execution as it does in the production  environments.  The proposed abstraction can guide the test designer into  recognizing the different nature of both types of variables  into the test case: deterministic test inputs under the test  designer control and context variables, which should capture  253', 'the possible variations of context. So far, the proposed  abstraction has been enacted  in a test suit design process,  which has been subject to limited in-vitro evaluations.  Further research is needed to validate both the  abstraction and the new software testing technologies that  must be derived  from it. We are currently working wi th  industrial and academic partners to design an appropriate  environment to support its experimental evaluation on  testing CASS.  Acknowledgement  Authors thank the CAcTUS team members for the  thoughtful discussion of the ideas in this paper. The “CAcTUS  - Context-Awareness Testing for Ubiquitous Systems”  project partially financed by CNPq – Universal 14/2013  Number 484380/2013 -3. Prof. Travassos is a CNPq  Researcher.  References  [1] M. Weinberger, D. Bilgeri, and E. Fleisch, “IoT business models in an  industrial context,” - Autom., vol. 64, no. 9, Jan. 2016.  [2] S. Sridevi, B. Sayantani, K. P. Amutha, C. M. Mohan, and R. Pitchiah,  “Context Aware Health Monitoring System,” in Medical Biometrics.  Second Internationa l Conference, (ICMB) , hong kong, China, 2010,  pp. 249–257.  [3] S. Wang, J. Wan, D. Li, and C. Zhang, “Implementing Smart Factory of  Industrie 4.0: An Outlook,” Int. J. Distrib. Sens. Networks , vol. 2016,  2016.  [4] C. Perera, A. Zaslavsky, P. Christen, and D. Georgakopoulos,  “Context aware computing for the internet of things: A survey,”  IEEE Commun. Surv. Tutorials, vol. 16, no. 1, pp. 414–454, 2014.  [5] P. Dourish, “What we talk about when we talk about context,” Pers.  Ubiquitous Comput., vol. 8, no. 1, pp. 19–30, 2004.  [6] G. D. Abowd, A. K. Dey, P. J. Brown, N. Davies, M. Smith, and P.  Steggles, “Towards a Better Understanding of Context and Context - Awareness,” in Computing Systems, vol. 40, 1999, pp. 304–307.  [7] S. Matalonga, F. Rodrigues, and G. H. Tra vassos, “Characterizing  testing methods for context -aware software systems: Results from  a quasi-systematic literature review,”  J. Softw. Syst.Vol 131. pp 1 -21- Aug., 2017.  [8] I. de S. Santos, R. M. de C. Andrade, L. S. Rocha, S. Matalonga, K. M.  de Olive ira, and G. H. Travassos, “Test case design for context - aware applications: Are we there yet?,” Inf. Softw. Technol. , vol. 88,  pp. 1–16, Aug. 2017.  [9] I. Satoh, “Software testing for mobile and ubiquitous computing,” in  The Sixth International Symposium o n Autonomous Decentralized  Systems, 2003. ISADS 2003., 2003, no. Section 2.  [10] L. Capra, W. Emmerich, and C. Mascolo, “CARISMA: Context -Aware  Reflective middleware System for Mobile Applications,” IEEE Trans.  Softw. Eng., vol. 29, pp. 929–945, 2003.  [11] F. Rodrigues, S. Matalonga, and G. H. Travassos, “CACTUS Technical  report. Systematic literature review protocol: Investigating context  aware software testing strategies,” 2014. [Online]. Available:  www.cos.ufrj.br/~ght/cactus_pr012014.pdf. [Accessed: 11 -Mar- 2016].  [12] S. Matalonga, F. Rodrigues, and G. H. Travassos, “Challenges in  Testing Context Aware Software Systems,” in Systematic and  Automated Software Testing, 2015.  [13] Z. W. Z. Wang, S. Elbaum, and D. S. Rosenblum, “Automated  Generation of Contex t-Aware Tests,” in 29th International  Conference on Software Engineering (ICSE’07) , 2007.  [14] D. Amalfitano, A. R. Fasolino, P. Tramontana, and N. Amatucci,  “Considering Context Events in Event -Based Testing of Mobile  Applications,” in 2013 IEEE Sixth Int ernational Conference on  Software Testing, Verification and Validation Workshops , 2013, pp.  126–133.  [15] A. Bertolino, G. De Angelis, F. Lonetti, and A. Sabetta, “Let the  puppets move! Automated testbed generation for service -oriented  mobile applications,” in Proc. of the 34th EUROMICRO Conference on  Software Engineering and Advanced Applications , 2008, pp. 321 – 328.  [16] ISO/IEC/IEEE 29119 -1:2013, “Software and systems engineering  Software testing Part 1:Concepts and definitions,” ISO/IEC/IEEE  29119-1:2013, pp. 1–64, Sep. 2013.', '328.  [16] ISO/IEC/IEEE 29119 -1:2013, “Software and systems engineering  Software testing Part 1:Concepts and definitions,” ISO/IEC/IEEE  29119-1:2013, pp. 1–64, Sep. 2013.  [17] F. Rodrigues, S. Matalonga, and G. H. Travassos, “CATS Design: A  Context-Aware Test Suite Design Process,” in Proceedings of the 1st  Brazilian Symposium on Systematic and Automated Software Testing  - SAST, 2016, pp. 1–10.  [18] S. Matalonga, F. Rodrigues, and G. H. Travassos, “Matching Context  Aware Software Testing Design Techniques to ISO/IEC/IEEE  29119,” in 2015 Spice Conference, 2015, pp. 33–44.  [19] A. Grimm, B. Gruber, and K. Henle, “Reliability of Different Mark - Recapture M ethods for Population Size Estimation Tested against  Reference Population Sizes Constructed from Field Data,” PLoS One,  vol. 9, no. 6, p. e98840, 2014.  [20] O. A. Alsos and Y. Dahl, “Toward a best practice for laboratory - based usability evaluations of mobi le ICT for hospitals,” Proc. 5th  Nord. Conf. Human -computer Interact. Build. Bridg. - Nord. ’08, p. 3,  2008.  [21] G. Canfora, F. Mercaldo, C. A. Visaggio, M. D’Angelo, A. Furno, and C.  Manganelli, “A case study of automating user experience -oriented  performance testing on smartphones,” in Proceedings - IEEE 6th  International Conference on Software Testing, Verification and  Validation, ICST 2013, 2013, pp. 66–69.  [22] W. K. Chan, T. Y. Chen, H. Lu, T. H. Tse, and S. S. Yau, “A  metamorphic approach to integra tion testing of context -sensitive  middleware-based applications,” in Proceedings - International  Conference on Quality Software, 2005, vol. 2005, pp. 241–249.  [23] W. K. Chan, T. Y. Chen, H. Lu, T. H. Tse, and S. S. Yau, “Integration  testing of context -sensitive middelware -based applications: a  metamorphic approach,” Int. J. Softw. Eng. Knowl. Eng., vol. 16, no. 5,  pp. 677–703, Oct. 2006.  [24] E. M. Fredericks, B. DeVries, and B. H. C. Cheng, “Towards run -time  adaptation of test cases for self -adaptive syst ems in the face of  uncertainty,” Proc. 9th Int. Symp. Softw. Eng. Adapt. Self -Managing  Syst. - SEAMS 2014, pp. 17–26, 2014.  [25] T. Griebe and V. Gruhn, “A model -based approach to test  automation for context -aware mobile applications,” in Proceedings  of the 29th Annual ACM Symposium on Applied Computing - SAC ’14,  2014, pp. 420–427.  [26] B. Jiang, X. Long, and X. Gao, “MobileTest: A tool supporting  automatic black box test for software on smart mobile devices,” in  Proceedings - International Conference on S oftware Engineering ,  2007.  [27] M. Merdes, R. Malaka, D. Suliman, B. Paech, D. Brenner, and C.  Atkinson, “Ubiquitous RATs: How Resource -Aware Run-Time Tests  Can Improve Ubiquitous Software System,” in 6th International  Workshop on Software Engineering and Middleware, SEM 2006 ,  2006, pp. 55–62.  [28] Z. Micskei, Z. Szatmári, J. Oláh, and I. Majzik, “A Concept for Testing  Robustness and Safety of the Context -Aware Behaviour of  Autonomous Systems,” in Lecture Notes in Computer Science , 2012,  pp. 504–513.  [29] S. Propp, G. Buchholz, and P. Forbrig, “Task model -based usability  evaluation for smart environments,” in Lecture Notes in Computer  Science (including subseries Lecture Notes in Artificial Intelligence  and Lecture Notes in Bioinformatics) , 2008, vol. 5247 L NCS, pp. 29– 40.  [30] C. Ryan and A. Gonsalves, “The effect of context and application  type on mobile usability: An empirical study,” in Conferences in  Research and Practice in Information Technology Series , 2005, vol.  38, pp. 115–124.  [31] T. H. Tse, S. S.  Yau, W. K. K. Chan, H. Lu, and T. Y. Y. Chen, “Testing  context-sensitive middleware -based software applications,” in  Proceedings of the 28th Annual International Computer Software  and Applications Conference (COMPSAC ). , 2004, pp. 458–466.  [32] H. Wang an d W. K. Chan, “Weaving Context Sensitivity into Test  Suite Construction,” in 2009 IEEE/ACM International Conference on', 'and Applications Conference (COMPSAC ). , 2004, pp. 458–466.  [32] H. Wang an d W. K. Chan, “Weaving Context Sensitivity into Test  Suite Construction,” in 2009 IEEE/ACM International Conference on  Automated Software Engineering, 2009, pp. 610–614.  [33] H. Wang, W. K. Chan, and T. H. Tse, “Improving the Effectiveness of  Testing Perva sive Software via Context Diversity,” ACM Trans.  Auton. Adapt. Syst., vol. 9, no. 2, p. 9:1--9:28, 2014.  [34] H. Wang, K. Zhai, and T. H. Tse, “Correlating context -awareness and  mutation analysis for pervasive computing systems,” in Proceedings  - International Conference on Quality Software, 2010, pp. 151–160.  [35] C. Ye, S. C. Cheung, J. Wei, H. Zhong, and T. Huang, “A study on the  replaceability of context -aware middleware,” in Proceedings of the  First Asia -Pacific Symposium on Internetware - Internetware ’09,  2009, pp. 1–10.    254']","['TESTING CONTEXT-AWARE SOFTWARE SYSTEMS:  UNCHAIN THE CONTEXT, SET IT FREE! This  briefin  reports  scieitfc evideice oi Testin Coitext-Aware Software Sysstemssa Backnrouid \uf0b7 Context-awareness  sofware systems (CASS)  are pervasive;  \uf0b7 There is a lack of sofware quality technologies  considering  the variation of context in CASS \uf0b7 Context-variation  is  an  intrinsic property of a CASS Method \uf0b7 By  refecting  on  current approaches  identiied  by  two quasi-systematic literature reviews (see  sidebar).  To  discuss  current approaches  and  limitation  for testing CASS. Fiidiins Restrictois  of  curreit  techiolonies  for testin CASS! \uf0b7 Imspose  artfcial  coistraiis  to coitexta \uf0b7 Coitext  variatoi  is  ai  iitriisic pheiomseioi of CASSa Therefore, \uf0b7 Curreit approaches caiiot  possiblys accouit for all coitext  variables valuesa  \uf0b7 Theys limsit test space coverane of a test suite for CASSa Proposed solutoi To test CASS without imsposiin  artfcial coitext coistraiis! Practcal coitributois P1. Assure functional correctness before  turning to testing context aware  requirements. P2. Design test cases to target context  variables.  P3. Take advantage of automatic testing  tools as much as possible. Methodolonical msotvatois M1. Context infuences the test case M2. The application context, the test  input, and the test environment are  different entities and should also be  distinguished during testing. M3. Context must be lef free to change  during the test case execution M4. Fuzzy test oracles must evolve to  account for M1, M2, and M3. Who is this briefin for? Sofware engineering practitioners who  need to test context-aware sofware  systems. Where the fidiins come from? All indings of this brieing are evidence  based and have been extracted from two  quasi-systematic literature reviews: 1. Matalonga, S., Rodrigues, F., Travassos, G.H.: Characterizing testing methods  for context-aware sofware systems:  Results from a quasi-systematic  literature review. J. Syst. Sofw. 131, 1– 21 (2017). 2. Santos, I. de S., Andrade, R.M. de C.,  Rocha, L.S., Matalonga, S., de Oliveira,  K.M., Travassos, G.H.: Test case design  for context-aware applications: Are we  there yet? Inf. Sofw. Technol. 88, 1–16 (2017). What is iicluded ii this briefin? Abstractions and discussions as to current  alternatives for testing CASS. Discussion of limitations of current  approaches. What is iot iicluded ii this  briefin? Details of the quasi-systematic literature  reviews. For additoial iiformatoi about  Testin CASSSSg CAcTUS Project (CNPq 484380/2013-3) http://lens-ese.cos.ufrj.br/ese/? portolio=projeto-cactus']","**Title: Enhancing Testing Practices for Context-Aware Software Systems**

**Introduction:**
This briefing aims to summarize recent advancements in the testing of context-aware software systems (CASS), emphasizing the importance of allowing context to vary freely during testing, mirroring real-world conditions. The insights presented are based on research conducted by Matalonga and Travassos, which highlights the limitations of current testing practices and proposes strategies for improvement.

**Core Findings:**
1. **Current Limitations in Testing CASS:** Traditional testing approaches for CASS often impose artificial constraints on context variables, leading to ineffective testing scenarios. Many existing technologies do not support the unconstrained variation of context during testing, which is crucial for accurately assessing software performance in real-world conditions.

2. **Proposed Strategies for Improvement:** The authors recommend three evidence-based strategies for practitioners:
   - **Ensure Functional Correctness First:** Before testing context-aware requirements, verify that the system functions correctly under constrained contextual conditions.
   - **Design Context-Targeted Test Cases:** Test cases should specifically target context variables to assess how the system reacts to different contextual inputs.
   - **Utilize Automated Testing Tools:** Employ available automated testing tools to simulate various context scenarios, even though these may not fully capture the phenomenological nature of context.

3. **The Need for a New Testing Paradigm:** The research advocates for a shift in testing philosophy, suggesting that context should be allowed to change freely during testing execution. This approach recognizes the non-deterministic nature of context and emphasizes the need for fuzzy test oracles that can adapt to variations in context.

4. **Implications for Future Research and Practice:** The study calls for the development of new testing technologies that can accommodate the unique challenges posed by context-aware systems. It encourages further research to validate the proposed abstraction and to create methodologies that improve the quality assurance processes for CASS.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, quality assurance professionals, and researchers interested in improving testing methodologies for context-aware software systems.

**Where do the findings come from?**
The findings are derived from quasi-systematic literature reviews conducted by Matalonga and Travassos, focusing on the testing practices for context-aware software systems and their implications for quality assurance.

**What is included in this briefing?**
The briefing includes a summary of the core findings regarding current limitations in testing CASS, proposed strategies for improvement, and the necessity for a new testing paradigm that accommodates the variability of context.

To access other evidence briefings on software engineering, visit: https://dl.acm.org/citation.cfm?doid=3131151.3131190

**Original Research Reference:**
Matalonga, S., & Travassos, G. H. (2017). Testing context-aware software systems: Unchain the context, set it free! In Proceedings of the 31st Brazilian Symposium on Software Engineering (SBES 2017), Fortaleza, CE, Brazil, 5 pages. https://doi.org/10.1145/3131151.3131190"
"['Testing Game: An Educational Game to Support So/f_tware Testing Education Pedro Henrique Dias Valle Instituto de Ciências Matemáticas e de Computação (ICMC/USP) São Carlos, São Paulo 13560-970 pedrohenriquevalle@usp.br Rafaela Vilela Rocha Instituto de Ciências Matemáticas e de Computação (ICMC/USP) São Carlos, São Paulo 13560-970 rafaela.vilela@gmail.com José Carlos Maldonado Instituto de Ciências Matemáticas e de Computação (ICMC/USP) São Carlos, São Paulo jcmaldon@icmc.usp.br ABSTRACT Software testing is an essential activity for software product quality assurance. For historical reasons, there is lack of quali/f_ied profes- sionals in this area as well as of studentsmotivation in learning software testing related contents. To mitigate these problems, some approaches have been proposed in educational games perspective. Among them, there is the Testing Game, which is an educational game to support software testing education. The objective of this paper is to describe and analyze the development of the Testing Game. The method AIMED which is an agile method to support the development of open educational resources is used. This method supports the identi/f_ication of aspects essential related to the quality of educational resources. Evidence are provided that the Testing Game considered positive aspects in the development of the game, such as: de/f_inition of project scope, licensing, availability of source code in repositories, among others. On the other hand, we identi/f_ied some limitations in the development of the game, such as: prioritiza- tion and revision of artifacts, user tutorials, database, among others. It is important to emphasize that this description contributed to highlight the phases and functionalities of the game, as well as to perform an evaluation to verify if the Testing Game considered the necessary features for the development of educational resources, identifying limitations in the game, giving directions for future work in the evolution of the game and of similar initiatives. CCS CONCEPTS •Software and its engineering → Software veri/f_ication and validation; Software testing and debugging; KEYWORDS Software Testing Education, Educational Resources, Games ACM Reference format: Pedro Henrique Dias Valle, Rafaela Vilela Rocha, and José Carlos Maldonado. 2017. Testing Game: An Educational Game to Support Software Testing Education. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. DOI: 10.1145/3131151.3131182 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131182 1 INTRODUÇÃO Nos últimos anos, a indústria de desenvolvimento de software tem se deparado com a exigência de produtos de software de alta qua- lidade, e consequentemente as atividades de VV&T - Veri/f_icação, Validação e Teste têm sido aprimoradas, em especial as atividades de testes, com a utilização de técnicas, critérios e ferramentas para a execução de teste [10]. O teste de software tem por objetivo execu- tar programas ou modelos com entradas especí/f_icas, analisando se eles se comportam conforme o esperado, realizando uma análise de- talhada com o intuito de levá-los a falhar e, posteriormente eliminar os defeitos que originaram as falhas [10]. O teste envolve quatro fases: planejamento de teste, projeto de caso de teste, execução de', 'os defeitos que originaram as falhas [10]. O teste envolve quatro fases: planejamento de teste, projeto de caso de teste, execução de testes e avaliação dos resultados dos testes [10]. Apesar do teste de ser reconhecido como uma atividade essencial no desenvolvimento de produtos de software com alta qualidade [10, 24], há uma carência de pro/f_issionais quali/f_icados nessa área [9], uma vez que esses pro/f_issionais possuem di/f_iculdades em aplicar técnicas, critérios e ferramentas de teste. Isso pode estar relacionado com a de/f_iciência na formação dos pro/f_issionais de teste ou por desmotivação no ambiente de trabalho e também pelas estratégias de alocação e responsabilização desses pro/f_issionais nas equipes de desenvolvimento e teste [28]. Para veri/f_icar como os pro/f_issionais de teste são capacitados Valle, Barbosa e Maldonado (2015) [28] analisaram os currículos de refe- rência propostos pela Sociedade Brasileira de Computação (SBC) e pela Association for Computing Machinery (ACM) para cursos de graduação da área de Computação. Esses currículos fornecem apoio e diretrizes para a de/f_inição, implantação e avaliação de cursos. Em geral, os currículos de referências em computação indicam que os conteúdos de teste devem ser abordados na disciplina de Engenha- ria de Software [ 1, 15, 17, 22, 27]. Além disso, foram realizadas pesquisas nos currículos das melhores universidades do Brasil e ex- terior. Em geral, observou-se que os cursos de graduação da área de computação não proporcionam aos estudantes uma visão integrada dos conteúdos de teste de software com outras disciplinas [28]. Outro fator que contribui para a de/f_iciência na formação de pro/f_is- sionais de teste é a carência de ambientes que motivem os estudantes a realizarem atividades relacionadas com o teste de software, pois há uma di/f_iculdade em ensinar esse conteúdo por meio de abordagens tradicionais que utilizem apenas aulas teóricas [8, 25]. Para mitigar esses problemas, algumas iniciativas têm sido propostas para moti- var e capacitar os pro/f_issionais de teste de software como: módulos educacionais, ensino de teste com programação, jogos educacionais, entre outras [29]. Dentre essas iniciativas, encontram-se os jogos educacionais que são considerados importantes ferramentas que facilitam a aprendizagem de conceitos e ideias sobre os conteúdos 289', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Pedro Henrique Dias Valle, Rafaela Vilela Rocha, and José Carlos Maldonado educacionais [6]. Eles motivam os jogadores enquanto eles jogam, permitindo adquirir conhecimentos educacionais combinados com a diversão [19]. No domínio de teste de software foram identi/f_icados 7 diferen- tes jogos, sendo eles: iTest Learning [3], Jogo das 7 Falhas [ 11], TestEG [7], JETS [5], U-TEST [26], iLearnTest [20] e JoVeTest [2]. No entanto, esses jogos apresentam algumas limitações como a ausência de conteúdos sobre o teste estrutural e mutação. Desta forma, desenvolveu-se um jogo educacional, denominado Testing Game [30], para auxiliar o ensino de teste de software em conjunto com programação. Esse jogo está disponível por meio da plataforma Web e aborda conteúdos de teste relacionados com as técnicas de teste funcional, estrutural e baseado em defeitos. Para avaliar o Testing Game, utilizou-se uma metodologia ex- perimental de/f_inida por Shull, Carver e Travassos (2011) [23] que é composta por quatro etapas: i) estudo de viabilidade; ii) estudo de observação; iii) estudo de caso (ciclo de vida); e iv) estudo de caso (indústria). Até o momento, o Testing Game foi avaliado utili- zando apenas a primeira etapa que é o estudo de viabilidade. Essa avaliação pode ser consultada em [30, 31]. É importante ressaltar que é necessário veri/f_icar se os jogos abordam grande parte dos aspectos essenciais a qualidade de re- cursos educacionais antes de serem inseridos em ambientes educa- cionais [32]. Desta forma, o objetivo desse trabalho foi utilizar o método AIMED [21](em inglês, Agile, Integrative and open Method for open Educational resources Development ) para auxiliar a descri- ção do Testing Game. Essa descrição contribuiu para a realização de uma segunda avaliação do jogo, em que foram analisadas as particularidades necessárias para o desenvolvimento de recursos educacionais, identi/f_icando os aspectos que não foram considera- dos no desenvolvimento do jogo e que poderão ser utilizados como trabalhos futuros para sua evolução e iniciativas similares. Este trabalho está organizado da seguinte forma: Na Seção 2 são apresentados os principais conceitos sobre teste de software. Além disso, é apresentada uma descrição do método AIMED que foi utilizado para auxiliar a descrição do Testing Game. Na Seção 3 é apresentada a descrição do jogo utilizando os processos do método AIMED. Na Seção 4 é discutido os aspectos que foram identi/f_icados por meio da descrição e que podem ser utilizados na evolução do jogo. Por /f_im, na Seção 5 são apresentadas as conclusões /f_inais e possíveis trabalhos futuros. 2 REFERÊNCIAL TEÓRICO Nesta seção são apresentados os principais conceitos sobre teste de software e as principais técnicas de teste de software, as quais foram consideradas no Testing Game. Além disso, é apresentada uma descrição do método AIMED utilizado para auxiliar o desenvol- vimento de recursos educacionais. Neste artigo, o método AIMED foi utilizado para auxiliar a descrição do Testing Game. 2.1 Teste de Software Nos últimos anos, a indústria de software tem aprimorado o desen- volvimento de produtos de software de alta qualidade. Para atender a essa exigência as atividades de VV& T - Veri/f_icação, Validação e Teste têm sido aprimoradas, em especial as atividades de teste de software, com a utilização de técnicas, critérios e ferramentas para auxiliar a execução de testes [10, 18]. O teste de software tem por objetivo executar programas ou modelos com entradas especí/f_icas, veri/f_icando se eles comportam-se de acordo com o esperado, com o intuito de levá-los a falhar e, posteriormente, eliminar os defeitos que originaram as falhas [10]. O teste de software é amplamente reconhecido como uma atividade essencial em qualquer processo de desenvolvimento de software bem sucedido [12]. A atividade de teste de software possui diferentes critérios que', 'reconhecido como uma atividade essencial em qualquer processo de desenvolvimento de software bem sucedido [12]. A atividade de teste de software possui diferentes critérios que de/f_inem quais são partes constituintes do produto que precisam ser testadas para eventualmente revelar a presença de defeitos no software. Cada critério divide o domínio de entrada em diferentes subdomínios e, consequentemente, diferentes, em geral in/f_initos, conjuntos de casos de teste podem ser derivados para satisfazer um determinado critério. Em geral, é impossível garantir a inexistência de defeitos no software, então o teste é utilizado na prática para dar uma segurança da qualidade do produto desenvolvido [10]. Os critérios de teste de software, em nível de programa, podem ser classi/f_icados em três diferentes técnicas:Funcional, Estrutural e Baseada em Defeitos [10]. O Teste Funcional, ou teste de caixa-preta, é baseado apenas na especi/f_icação do produto em teste. Os critérios desta técnica podem ser aplicados em qualquer fase do teste de software (unidade, integração, sistema e aceitação), independentemente do paradigma utilizado, pois não se analisam detalhes de implementação [4, 10, 14]. Nessa técnica, os detalhes de implementação não são considerados e consequentemente não se conhece a estrutura interna do programa em teste. Essa técnica identi/f_ica apenas erros relacionados com o mal funcionamento de software e, em geral, não garantem que as partes críticas ou essenciais do produto em teste sejam exercita- das [10]. O Teste Estrutural, ou teste de caixa branca, é baseado na im- plementação do software a ser testado, isso requer a execução de partes ou componentes do software. Os critérios pertencentes a esta técnica são classi/f_icados em três categorias que são:critérios baseados em complexidade , critérios baseados em /f_luxo de controle e critérios baseados em /f_luxos de dados. Geralmente, no Teste Estrutural é utilizado uma representação do programa a ser testado, denominado “Grafo de Fluxo de Controle” (GFC) ou “Gráfo de Programa” [18]. Cada nó do GFC representa uma execu- ção indivisível do código que termina em uma instrução simples ou condicional [10]. O Teste de Mutação ou Análise de Mutantes é o critério de teste mais conhecido da técnica de Teste Baseado em Defeitos. Este critério adiciona possíveis defeitos ao programa uniformemente. Para isso, utilizam-se os defeitos que são frequentemente cometidos pelos desenvolvedores. No teste de mutação, o programa testado é alterado várias vezes, criando um conjunto de programas alter- nativos (mutantes) [10, 16]. O principal objetivo deste critério é mostrar que o programa em teste não possui determinados tipos de defeitos [13]. A partir das três técnicas de teste de software apresentadas, é possível obter de forma sistemática um conjunto de casos de teste e/f_icaz com relação à atividade de teste de software, e possivelmente revelar falhas nos programas testados. É importante ressaltar que essas técnicas não devem ser utilizadas separadamente, já que as mesmas são complementares na execução da atividade de teste. 290', 'Testing Game: An Educational Game to Support So/f_tware Testing Education SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Sendo assim, essas técnicas devem ser utilizadas em uma estratégia incremental de teste [10]. 2.2 Método AIMED O AIMED (Agile, Integrative and open Method for open Educational resources Development ) é um método ágil que integra práticas de design pedagógico, design de jogo, modelagem de simulação, en- genharia de software e gerenciamento de projetos; para auxiliar o desenvolvimento de recursos educacionais abertos e/f_icientes e e/f_icazes. Dentre os recursos educacionais considerados estão: jogos educacionais, jogos sérios, simulações interativas e artefatos gami/f_i- cados [21]. Uma visão geral do método AIMED pode ser observada na Figura 1. Figura 1: Visão Geral do Método AIMED O método AIMED contém cinco macroprocessos com catorze processos [21], sendo eles: (1) Processos Organizacionais: (1) Gerência – Executado ao longo da pré-produção e produção; (2) Licenciamento e (3) Publicação – Executados ao /f_inal da produção do recurso educacional; (2) Processo de Pré-produção: (4) Planejamento Inicial – Exe- cutado logo após a concepção e aprovação do projeto (pri- meira atividade do processo de gerência); (3) Processos de Produção: (5) Análise e Planejamento da ite- ração, (6) Projeto Iterativo, (7) Implementação Incremental, (8) Integração, teste e revisão da iteração. Estes processos são iterativos. (4) Processos de Pós-produção: (9) Ambiente e Manutenção, (10) Execução e (11) Avaliação da Aprendizagem. Estes processos são realizados após a entrega do recurso educa- cional. (5) Processos de Apoio: (12) Veri/f_icação dos Artefatos, (13) Validação, (14) Projeto Experimental. Estes processos são realizados ao longo de todo o ciclo de vida do recurso educacional. O método AIMED é fundamentado em métodos ágeis quanto à produção rápida e contínua de artefatos [ 21]. Então, todos os artefatos que são entregues devem ser planejados, produzidos e avaliados em cada iteração. Os processos e atividades do método AIMED podem ser visualizados na Figura 2. Figura 2: Visão Geral das Atividades do Método AIMED Conforme mencionado na Seção 1, o método AIMED foi utilizado para auxiliar a descrição do Testing Game. Essa descrição pode ser observada na próxima seção. 3 DESCRIÇÃO DO TESTING GAME Nesta seção é apresentada a descrição do Testing Game por meio do método AIMED. A descrição do jogo foi dividida em 3 iterações que correspondem as três técnicas de teste consideradas no jogo, sendo elas: teste funcional, teste estrutural e teste baseado em defeitos. 3.1 Processos Organizacionais Os processos organizacionais estão relacionados com a gestão de projetos, licenciamento e publicação, os quais estão detalhados a seguir: 3.1.1 Gerência. O processo de Gerência contém as atividades e tarefas de planejamento e execução do projeto, controle de seus riscos e monitoramento de seu progresso. Essas atividades são executadas ao longo do desenvolvimento do recurso educacional. Esse processo é transversal aos processos de Pré-Produção e de Produção do método AIMED. Nesta etapa foram de/f_inidos os requi- sitos do jogo, porém eles não foram descritos formalmente. Além disso, identi/f_icou-se a visão do produto que está relacionada com as interfaces, módulos e tipos de licenças utilizadas. 3.1.2 Licenciamento. O processo de Licenciamento contém ati- vidades e tarefas para de/f_inir políticas de licenciamento. Essas atividades promovem a garantia dos direitos de propriedade intelec- tual e o compartilhamento do recurso desenvolvido. Para auxiliar o desenvolvimento do jogo foi utilizado o motor de jogos Construct 2 com a licença Personal que é limitada quanto ao uso comercial. No entanto, é importante ressaltar que todos os direitos dos produtos desenvolvidos são dos desenvolvedores. Desta forma, o Testing Game foi disponibilizado por meio de uma licença open source . 3.1.3 Publicação. O processo de Publicação contém atividades', 'desenvolvidos são dos desenvolvedores. Desta forma, o Testing Game foi disponibilizado por meio de uma licença open source . 3.1.3 Publicação. O processo de Publicação contém atividades e tarefas para criar os tutoriais e disponibilizar o recurso educa- cional desenvolvido em repositórios. Até o momento não foram 291', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Pedro Henrique Dias Valle, Rafaela Vilela Rocha, and José Carlos Maldonado desenvolvidos tutoriais para auxiliar a utilização do jogo. É impor- tante ressaltar que para utilizar o jogo não é necessário tutoriais de instalação, pois o jogo está disponível na plataforma Web, então não é necessário a instalação de bibliotecas e software. O Testing Game contém algumas dicas e enunciados durante as fases para facilitar o entendimento dos estudantes. Quanto a disponibilização do recurso educacional, o jogo está disponível no Bitbucket1 que é um serviço de hospedagem de projetos semelhannte ao GitHub e GitLab. O código do Testing Game em HTML5 está disponível em: https://bitbucket.org/pedrohdvalle/testinggamehtml5. 3.2 Processo de Pré-produção: O processo de Planejamento Inicial contém atividades e tarefas de planejamento do recurso educacional que será desenvolvido, con- tendo as necessidades de se desenvolver esse recurso educacional, tanto pedagógicas quanto técnicas. 3.2.1 Definição das necessidades pedagógicas.Nesta etapa, deve- se analisar as necessidades pedagógicas (aprendizagem, treina- mento e avaliação) e fazer um planejamento inicial pedagógico. Para isso, foram de/f_inidos os seguintes itens: • Contexto e Problema: O teste de software é reconhecido como uma atividade importante na garantia de qualidade de produtos de software. No entanto, há uma carência de pro/f_issionais quali/f_icados nessa área e uma desmoti- vação por parte dos estudantes em aprender conteúdos relacionados com o teste de software. Para amenizar es- ses problemas, diferentes iniciativas têm sido propostas para auxiliar o ensino de teste de software, dentre elas, encontram-se os jogos educacionais. Desta forma, realizou- se o desenvolvimento de um jogo educacional denominado Testing Game. • Público-Alvo: O público-alvo do jogo são alunos de gra- duação que cursam disciplinas com conteúdos relacionados com o teste de software. O jogo pode ser utilizado como um material de apoio às aulas de teste de software, pois os estudantes podem treinar suas habilidades adquiridas em sala de aula. • Conteúdo: Os conteúdos considerados no jogo abordam as três principais técnicas de teste de software, a saber: teste funcional, teste estrutural e teste baseado em defeitos. O livro “Introdução ao Teste de Software” [ 10] foi utili- zado para a elaboração dos conteúdos abordados no jogo e também como literatura complementar. Na Tabela 1 são apresentados os conteúdos abordados no jogo para cada técnica de teste considerada. • Avaliação: Essa atividade é responsável por avaliar o de- sempenho dos estudantes com a utilização do recurso edu- cacional. Até o momento, não foi avaliado o desempenho dos estudantes com a utilização do Testing Game. No en- tanto, pretende-se realizar essa avaliação como trabalhos futuros. 1Disponível: https://bitbucket.org/ 3.2.2 Definição das necessidades técnicas. Nesta etapa, deve-se analisar as necessidades técnicas do jogo e realizar um planeja- mento inicial para o desenvolvimento. Para isso, foram de/f_inidas as seguintes atividades: • Objetivos de Design: O Testing Game é um jogo com di- mensão grá/f_ica 2D para a plataforma Web. Nesse jogo, o estudante é representado por um avatar que pode realizar diversas ações, tais como: andar, correr, pular e atirar. O jogo contém 3 níveis que correspondem às três técnicas de teste de software, e cada nível contém fases que cor- respondem aos critérios de teste relacionados às técnicas consideradas. Cada nível do jogo é representado por uma porta que pode ser aberta com chaves encontradas durante o jogo. Para auxiliar a aprendizagem dos estudantes, o jogo contém módulos teóricos para os conteúdos aborda- dos. Para ter acesso a esse conteúdo é necessário clicar no ícone representado por um livro no menu superior do lado direito no jogo. A pontuação do jogo é obtida a partir dos acertos dos estudantes.', 'dos. Para ter acesso a esse conteúdo é necessário clicar no ícone representado por um livro no menu superior do lado direito no jogo. A pontuação do jogo é obtida a partir dos acertos dos estudantes. • Objetivos de Arte: Para a de/f_inição doavatar, inimigos (representados por diferentes avatares em que os jogadores devem eliminar para continuar os desa/f_ios propostos no jogo), objetos e os cenários do jogo foram selecionadas imagens disponibilizadas na internet com licenças gratuitas. Essas imagens foram reutilizadas nas diferentes fases do jogo. • Objetivos Técnicos: O Testing Game considerou a dinâ- mica single player , ou seja, possibilita a participação de apenas um estudante no jogo. Para auxiliar na progra- mação do jogo, utilizou-se o motor de jogos Construct 2. Esse motor de jogos foi selecionado a partir de uma mapeamento sistemático realizado. Os sons utilizados fo- ram obtidos por meio de repositórios disponibilizados na internet com licenças gratuitas. 3.3 Processos de Produção O objetivo principal do processo de Produção é desenvolver o re- curso educacional, de forma iterativa e incremental. Para a descri- ção do Testing Game foram de/f_inidas 3 iterações, as quais corres- pondem aos três níveis do jogo. Essas iterações estão descritas a seguir: 3.3.1 Iteração 1 - Teste Funcional. A Iteração 1 representa o primeiro nível do Testing Game, no qual são abordados os conteúdos relacionados com a técnica de teste funcional. Nesse nível há 8 fases que correspondem aos critérios de teste funcional, tais como: particionamento em classes de equivalência e análise de valor limite. • Análise e Planejamento: Neste processo, analisou-se e priorizou os artefatos produzidos e reusados na fase sobre teste funcional. Algumas imagens foram reusadas de um repositório disponível na internet, tais como portas, que representam os níveis do jogo, ícone de conteúdo teórico, inimigos, tiros, personagem do jogo. Outras imagens foram criadas utilizando o software Microsoft PowerPoint 2013. Além disso, alguns artefatos foram criados a partir dos recursos disponibilizados pela Construct 2, tais como: casos 292', 'Testing Game: An Educational Game to Support So/f_tware Testing Education SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Tabela 1: Conteúdos abordados no Testing Game Técnicas de Teste de Software Conteúdo Teste Funcional Critérios de Teste de Software, Especi/f_icação do Programa BubbleSort, Critério de Particionamento em Classes de Equivalência, Tabela de Classe de Equivalência, Critério de Análise de Valor Limite e Conjunto de Caso de Teste Mínimo. Teste Estrutural Grafo de Fluxo de Controle, Critério Todos-Nós, Critério Todas-Arestas, Usos de Variável (de/f_inição, uso computacional, uso predicativo), Grafo Def-Uso, Critério Todas-De/f_inições, Critério Todos-Usos, Conjunto Mínimo de Casos de Teste e Caminho Não-Executável. Teste Baseado em Defeitos Operadores de Mutação em Java, Programas Mutantes, Mutantes Equi- valentes, Escore de Mutação e Conjunto de Caso de Teste Mínimo. de teste, enunciados, labels de tempo e pontuação, entre outros. • Projeto Iterativo: Neste processo, de/f_iniram-se as fases do jogo para o nível que corresponde ao teste funcional. Para avançar para a próxima fase, o estudante deve concluir a fase atual com sucesso. Neste nível foram de/f_inidas 8 fases que abordam os critérios de particionamento em classe de equivalência, análise de valor limite, conjunto de caso de teste mínimo, entre outros. A seguir, apresenta-se uma breve descrição das fases do teste funcional. – Fase 1: Nesta fase, o jogador deve arrastar os crité- rios para a técnica de teste que os correspondem. As técnicas consideradas estão representadas por dois retângulos. Quando o jogador arrastar um critério para a técnica correta, um sinal sonoro é realizado indicando que o jogador acertou, caso contrário, um sinal sonoro é emitido indicando que a ação realizada é incorreta, e o critério volta para seu local. Nesta fase está disponível um módulo de conteúdo teórico sobre o teste funcional. Na Figura 3, pode-se visualizar a interface da Fase 1. Figura 3: Primeira Fase do Teste Funcional – Fase 2: Nesta fase é apresentado ao jogador a especi- /f_icação do programa utilizado, a saber: Bubble Sort, para aplicar os conceitos de teste durante a execução do jogo. A especi/f_icação diz que o programa Bubble Sort só aceitará vetores com o tamanho igual a 4. Em seguida, é apresentado um enunciado para o jogador que diz que ele deve encontrar vetores com o tamanho igual a 4. Para isso, o jogador deve eliminar os inimi- gos e os vetores inválidos. Após o jogador completar o desa/f_io, é apresentada uma lista com os vetores váli- dos e inválidos da fase. Na Figura 4 é apresentada a interface da Fase 2. Figura 4: Segunda Fase do Teste Funcional – Fase 3: A terceira fase é semelhante a fase 2. A única diferença é que o jogador deve encontrar vetores com tamanho igual a 4 e que eles contenham apenas nú- meros inteiros conforme a especi/f_icação do programa Bubble Sort. – Fase 4: Nesta fase, o jogador deve preencher uma tabela de equivalência a partir da especi/f_icação do programa Bubble Sort. Nela, há um módulo com con- teúdo teórico sobre o critério “Particionamento em Classes de Equivalência”. – Fase 5: Nesta fase, o jogador deve encontrar casos de teste válidos para satisfazer o critério de “Particio- namento em Classes de Equivalência”. Para isso, ele deve eliminar os inimigos e os casos de teste que são inválidos. Na Figura 5 é apresentada a interface da Fase 5. – Fase 6: Nesta fase, o jogador deve selecionar os casos de teste disponíveis na tela para as classes de equiva- lência do programa Bubble Sort. – Fase 7: Nesta fase, o jogador deve analisar a classe de equivalência Tamanho do Vetorpara encontrar todos os casos de teste disponíveis na fase para satisfazer o critério “Analise de Valor Limite”. Para isso, o jogador 293', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Pedro Henrique Dias Valle, Rafaela Vilela Rocha, and José Carlos Maldonado Figura 5: Quinta Fase do Teste Funcional deve eliminar todos os inimigos e os casos de teste que são inválidos. – Fase 8: Por /f_im, nesta fase, o jogador deve apenas pegar a chave que abre a porta do próximo nível do jogo. O próximo nível corresponde aos conteúdos relacionados com o teste estrutural. • Implementação Incremental: O processo de Implemen- tação Incremental contém atividades e tarefas de criação, revisão, reúso e remixagem de artefatos. Esses artefatos incluem arte, áudio e vídeo. Os recursos de imagem e áudio foram obtidos por meio de websites sob licença gratuita. A programação das fases foi realizada com o auxílio da Construct 2. Os Recursos de vídeos e banco de dados não foram considerados na versão atual do jogo. • Integração, Testes e Revisão: Após a de/f_inição, criação e reutilização dos artefatos, realizou-se a integração dos re- cursos de arte, multimídia e conteúdo. Em seguida, houve a integração das fases para que juntas formassem o primeiro nível do jogo. Para veri/f_icar a e/f_iciência das fases desen- volvidas foram testadas suas principais funcionalidades. Após os testes realizados, a Iteração 1 foi aprovada como um recurso educacional pronto. 3.3.2 Iteração 2 - Teste Estrutural. A Iteração 2 representa o segundo nível do Testing Game com 10 fases. Nesse nível são abordados os conteúdos relacionados com o teste estrutural. • Análise e Planejamento: Neste processo foram identi/f_i- cados e de/f_inidos os artefatos utilizados para o desenvolvi- mento da Iteração 2. Alguns artefatos foram reutilizados da Iteração 1, tais como: portas, ícone de conteúdo teórico, moedas, personagens, inimigos, entre outros. No entanto, alguns artefatos foram criados exclusivamente pra o desen- volvimento da Iteração 2, sendo eles: lupa para representar código fonte, grafos de /f_luxo de controle, imagens deback- ground, bandeiras, entre outros. É importante ressaltar que todos esses artefatos foram obtidos por meio de licenças gratuitas. • Projeto Iterativo: Neste processo foram de/f_inidas as fa- ses da Iteração 2. Assim como na Iteração 1, os estudantes só têm acesso a próxima fase se concluirem a fase atual com sucesso. Neste nível contêm 10 fases com conteúdos relacionados com os critérios Todos-Nós e Todos-Usos, Uso de Variável (de/f_inição, uso computacional e uso predica- tivo), Grafo de Fluxo de Controle, Grafo Def-Uso, conjunto mínimo de teste, caminho não-executável e outros. – Fase 1: Na primeira fase, é apresentado ao jogador o código do programa Bubble Sort. Em seguida, o jogador deve encontrar o GFC que corresponde ao código do programa. Para isso, ele deve eliminar os GFCs que não correspondem ao programa e eliminar os inimigos durante os desa/f_ios que são propostos. Nesta fase está disponível um módulo de conteúdo teórico sobre o teste estrutural. Na Figura 6, pode-se visualizar a interface da fase 1. Figura 6: Primeira fase do Teste Estrutural – Fase 2: Na segunda fase, o jogador deve encontrar casos de teste para satisfazer o critério “Todos-Nós” para o programa Bubble Sort. Quando o jogador en- contra um caso de teste válido, o GFC é atualizado na tela para demonstrar quais os nós foram cobertos. É importante ressaltar que o jogador deve eliminar os casos de teste inválidos e os inimigos. – Fase 3: A terceira fase é semelhante a fase 2 desta Iteração. No entanto, ao invés de encontrar casos de teste para satisfazer o critério “Todos-Nós”, o jogador deve encontrar casos de teste para satisfazer o critério “Todas-Arestas”. – Fase 4: Na quarta fase, o jogador deve encontrar o conjunto mínimo de casos de teste para satisfazer os critérios “Todos-Nós” e “Todas-Arestas”. Para isso, o jogador deve eliminar os casos de teste que são inváli- dos ou que fornecem a mesma cobertura dos casos de teste que foram selecionados. – Fase 5: Na quinta fase, o jogador deve apenas ob-', 'jogador deve eliminar os casos de teste que são inváli- dos ou que fornecem a mesma cobertura dos casos de teste que foram selecionados. – Fase 5: Na quinta fase, o jogador deve apenas ob- servar os usos das variáveis do programa Max. Esse programa foi utilizado para que os jogadores pudes- sem entender quais são os tipos de usos de variáveis para que nas próximas fases eles utilizassem esses conceitos no programa Bubble Sort. Na Figura 7 é ilustrada a interface da fase 5. – Fase 6: Na sexta fase, o jogador deve encontrar todas as variáveis do programa Bubble Sort que tiveram uso predicativo. Para isso, os jogadores devem eliminar os inimigos e as variáveis que não tiveram uso predi- cativo. No /f_inal dessa fase é apresentado ao jogador 294', 'Testing Game: An Educational Game to Support So/f_tware Testing Education SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Figura 7: Quinta fase do Teste Estrutural o GFC com as variáveis que tiveram uso predicativo. Nesta fase está disponível um módulo de conteúdo teórico sobre os critérios da família de /f_luxo de dados. – Fase 7: A sétima fase é semelhante a fase 6. No en- tanto, ao invés de encontrar as variáveis que tiveram uso predicativo, o jogador deve encontrar todas as variáveis do programa Bubble Sort que tiveram uso computacional. – Fase 8: Na oitava fase, o jogador deve observar o grafo Def-Uso do programa Bubble Sort e preencher uma tabela que demostra em quais nós do grafo as variáveis foram de/f_inidas. Na Figura 8, pode-se visualizar a interface da fase 8. Figura 8: Oitava fase do Teste Estrutural – Fase 9: Na nona fase, o jogador deve encontrar as as- sociações requeridas para satisfazer o critério “Todos- Usos” para o programa Bubble Sort. Para isso, o joga- dor deve eliminar as associações incorretas que forem encontradas durante a fase. – Fase 10: Por /f_im, na décima fase, o jogador deve en- contrar o caminho do GFC do programa Bubble Sort que é um caminho não-executável. Para isso, o jogador deve eliminar os caminhos executáveis do GFC. Nesta subfase está disponível um módulo de conteúdo teó- rico que explica o que é um caminho não-executável. • Implementação Incremental: O processo de Implemen- tação Incremental é responsável pela criação, revisão, reúso e remixagem de artefatos. Os recursos de imagem e áudio utilizados neste nível foram obtidos sob licença gratuita. Para auxiliar a programação das fases foi utilizado a Cons- truct 2. É importante ressaltar que os recursos de vídeos e banco de dados não foram considerados na versão atual do jogo. • Integração, Testes e Revisão: Após a de/f_inição, criação e reutilização dos artefatos, realizou-se a integração dos re- cursos de arte, multimídia e conteúdo. Em seguida, houve a integração das fases para formarem o segundo nível do jogo. É importante ressaltar que foram testadas as princi- pais funcionalidades desse nível. Para isso, foram geradas combinações entre as funcionalidades e uma sequência de passos que representam a execução do jogo. Em seguida, a Iteração 2 foi aprovada como um recurso educacional pronto. 3.3.3 Iteração 3 - Teste de Mutação.A Iteração 3 representa o terceiro e último nível do Testing Game. Neste nível são abordados conteúdos relacionados com o teste baseado em defeitos, especi/f_i- camente o critério de teste de mutação. • Análise e Planejamento: Nesse processo foram iden- ti/f_icados e criados artefatos que foram utilizados para o desenvolvimento da Iteração 3. Para esta Iteração foram criados poucos artefatos, pois grande parte dos artefatos utilizados foram reusados das Iterações 1 e 2. • Projeto Iterativo: Neste processo foi de/f_inido o programa educacional das fases da Iteração 3. Neste nível contém 5 fases com conteúdos sobre operadores de mutação em Java, programas mutantes, mutantes equivalentes, escore de mutação e conjunto de caso de teste mínimo. Assim como nas Iterações 1 e 2, os estudantes só têm acesso a próxima fase se conseguirem concluir a fase atual com sucesso. – Fase 1: Na primeira fase, o jogador deve encontrar o programa mutante gerado a partir dos operadores de mutação: arimético, atribuição, relacional e lógico. Para isso, o jogador deve eliminar todos os programas mutantes que não foram gerados por essas classes de operadores de mutação. Na Figura 9 é apresentada a interface da fase 1 do teste de mutação. Figura 9: Primeira fase do Teste de Mutação – Fase 2: Na segunda fase, o jogador deve encontrar casos de teste que eliminem os programas mutantes do 295', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Pedro Henrique Dias Valle, Rafaela Vilela Rocha, and José Carlos Maldonado programa denominado Op. Para isso, o jogador deve eliminar os inimigos e os casos de teste inválidos. – Fase 3: A fase 3 é semelhante a fase 2. No entanto, o jogador deve encontrar casos de teste para elimi- nar os mutantes do programa Bubble Sort. Ele pode visualizar no código quais as mutações foram elimi- nadas com a seleção do caso de teste. Além disso, o jogador pode visualizar seu score de mutação que está disponível no topo da tela no lado direito de forma /f_ixa. – Fase 4: Nessa fase, o jogador deve eliminar os progra- mas mutantes que são considerados mutantes equiva- lentes. Para isso, o jogador deve eliminar os inimigos, barreiras e mutantes que não são equivalentes. O lo- cal que houve mutação é marcado com cor vermelha no código do programa Bubble Sort. Na Figura 10 é apresentada a interface da fase 4 do teste de mutação. Figura 10: Quarta fase do Teste de Mutação – Fase 5: Por /f_im, na quinta e última fase, o jogador deve selecionar casos de teste para eliminar os programas mutantes. Nessa fase são apresentados aos jogadores 3 códigos de programas diferentes, o primeiro contém uma estrutura apenas sequencial, o segundo contém uma estrutura condicional e o último uma estrutura de repetição. Desta forma, os jogadores podem observar que é possível aplicar o teste de mutação em qualquer estrutura de programa. • Implementação Incremental: O processo de Implemen- tação Incremental considera a criação, revisão, reúso e remixagem de artefatos. Esses artefatos incluem arte, áu- dio e vídeo, os quais foram obtidos por meio de licença gratuita. É importante ressaltar que até o momento não foram considerados os recursos de vídeos e banco de dados. Porém, pretende-se eliminar essas limitações na próxima versão do jogo. • Integração, Testes e Revisão: Após a de/f_inição de cada conteúdo abordado na Iteração 3, considerou-se os recur- sos de arte e áudio assim como nas Iterações 1 e 2. Após a integração desses recursos, realizou-se os testes com as principais funcionalidades do Testing Game. Para isso, criou-se um conjunto de passos que representavam a exe- cução do jogo. Por /f_im, a Iteração 3 foi aprovada como recurso educacional pronto para auxiliar o ensino de teste de software. 3.4 Processos de Pós-produção Neste processo, o recurso educacional já foi desenvolvido e pode ser utilizado como instrumento de apoio à aprendizagem de con- teúdos educacionais. Para isso, é necessário considerar a execução, ambiente e avaliação da aprendizagem conforme apresentado a seguir: • Ambiente: O Testing Game está disponível por meio pla- taforma Web no servidor do Scirra Arcade2. É importante ressaltar que para ter acesso ao jogo não é necessário insta- lar recursos, tais como: banco de dados, aplicativos,plugins, entre outros. • Execução: Nesta etapa, os estudantes já podem utilizar o Testing Game como um recurso educacional de apoio ao en- sino de teste de software. Até o momento, o Testing Game foi utilizado apenas no estudo de viabilidade realizado para avaliar a qualidade e usabilidade do jogo [30]. • Avaliação da Aprendizagem: Na versão atual do jogo, não foram realizados experimentos com estudantes para veri/f_icar a aprendizagem adquirida com a utilização do Testing Game. Porém, pretende-se realizar uma avaliação como trabalhos futuros para avaliar a aprendizagem dos estudantes. No entanto, foi realizado um estudo de via- bilidade [23] para avaliar a usabilidade e a qualidade do jogo sob o ponto de vista dos estudantes com relação à motivação, experiência do usuário e aprendizagem. 3.5 Processos de Apoio Esses processos estão relacionados com a veri/f_icação e validação dos artefatos. Além disso, são considerados estudos experimentais. É importante ressaltar que estes processos são realizados ao longo de todo o ciclo de vida do recurso educacional. A seguir, apresenta-se a descrição desses processos:', 'importante ressaltar que estes processos são realizados ao longo de todo o ciclo de vida do recurso educacional. A seguir, apresenta-se a descrição desses processos: • Veri/f_icação e Validação:Os processos veri/f_icação e vali- dação são processos transversais na metodologia de desen- volvimento de recursos educacionais, eles avaliam as saídas dos processos (Pré-Produção, Produção e Pós-Produção). Desta forma, os artefatos gerados foram avaliados para serem reusados em outras iterações, tanto em relação ao desenvolvimento correto entre os processo (veri/f_icação) quanto se o jogo está sendo implementado corretamente e completamente, conforme necessidades/requisitos dos usuários (validação). • Estudos Experimentais: Para avaliar a usabilidade e qua- lidade do Testing Game com relação à motivação, experi- ência do usuário e aprendizagem, realizou-se um estudo de viabilidade de acordo com uma metodologia experimental [23]. Nesse estudo, aproximadamente 85,64% dos estudan- tes avaliaram a qualidade do jogo de forma positiva sob o ponto de vista dos estudantes. Quanto à usabilidade, foram identi/f_icados poucos problemas [30, 31]. 2Disponível em: https://goo.gl/50pmWP 296', 'Testing Game: An Educational Game to Support So/f_tware Testing Education SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil 4 DISCUSSÃO O Testing Game foi desenvolvido utilizando um processo ad-hoc. No entanto, é importante ressaltar que os jogos educacionais devem ser avaliados antes de serem utilizados como recursos educacio- nais para veri/f_icar se eles possuem boa qualidade [32]. Para isso, utilizou-se o método AIMED para auxiliar a descrição e avaliação do desenvolvimento do jogo, veri/f_icando se o Testing Game contém os principais aspectos essenciais à qualidade de recursos educacionais. Por meio dessa descrição, identi/f_icaram-se aspectos positivos no desenvolvimento do jogo, tais como: i) de/f_inição do escopo, pois foram de/f_inidas as técnicas e critérios de teste abordadas no jogo; ii) visão de produto (licença, módulos, interfaces) bem de/f_inida; iii) disponibilização do código fonte em repositórios; iv) disponibi- lização de módulos extras com conteúdos teóricos abordados no jogo; v) estabelecimento de licença software livre; vi) de/f_inição de conteúdos abordados em cada nível; e vii) acesso ao jogo por meio de um link, dispensando a utilização de tutoriais de acesso. No entanto, alguns aspectos do jogo podem ser revisitados na evolução do jogo. Dentre eles, destacam-se: i) priorização e revisão de artefatos; ii) ausência de tutoriais para auxiliar os estudantes na utilização do jogo; iii) ausência do estabelecimento de uma lista de requisitos; iv) ausência de tutorial para os estudantes como um guia de uso (telas/menus, funcionalidade/fase, comandos); v) falta de divisão de papéis no desenvolvimento do jogo; vi) ausência de um planejamento inicial com os principais riscos; vi) ausência de uma avaliação para veri/f_icar a aprendizagem dos estudantes na utilização do jogo; e vii) ausência de banco de dados para armazenar os dados dos estudantes. Com relação ao Testing Game, pretende-se desenvolver uma nova versão para atender as limitações identi/f_icadas por meio da descrição do jogo e implementar a característica demultiplayer, pos- sibilitando que os estudantes troquem experiências entre si. Além disso, pretende-se integrar ferramentas de teste para auxiliar os estudantes na execução de testes. Com relação ao método AIMED, seria interessante considerar a evolução do método para auxiliar o desenvolvimento de recursos educacionais com equipes de desen- volvimento geogra/f_icamente distribuídas. Na atividade responsável pelo licenciamento do recurso educacional seria interessante uma lista com as principais licenças para os recursos educacionais. Além disso, na atividade responsável pela programação poderia haver uma lista com sugestões de ferramentas de apoio ao desenvolvi- mento dos recursos educacionais. 5 CONCLUSÃO E TRABALHOS FUTUROS O teste de software é reconhecido como uma atividade importante para o desenvolvimento de produtos de software de boa qualidade. No entanto, há uma carência de pro/f_issionais quali/f_icados nessa área e uma di/f_iculdade em ensinar teste de software por meio de aulas teóricas tradicionais. Para amenizar esses problemas, desenvolveu- se um jogo denominado Testing Game [30]. Porém, é necessário realizar uma avaliação dos jogos antes de serem inseridos em ambi- entes educacionais para veri/f_icar se eles possuem boa qualidade. Para isso, realizou-se uma descrição e avaliação do desenvolvi- mento do Testing Game por meio do método AIMED. Essa descrição auxiliou a identi/f_icação de aspectos positivos do jogo e aspectos que não foram considerados no jogo, tais como: tutoriais de uso, lista de requisitos, planejamento de riscos, entre outros. Esses aspectos serão considerados na evolução do jogo para que o Testing Game possa contemplar grande parte dos itens essenciais do método AI- MED. É importante ressaltar que o Testing Game aborda conteúdos relacionados com os testes estrutural e baseado em defeitos, os quais não foram contemplados nos jogos de teste que foram identi-', 'MED. É importante ressaltar que o Testing Game aborda conteúdos relacionados com os testes estrutural e baseado em defeitos, os quais não foram contemplados nos jogos de teste que foram identi- /f_icados. Além disso, o jogo foi avaliado de forma positiva quanto à sua qualidade e usabilidade, e consequentemente pode ser utili- zado como instrumento motivador da aprendizagem de conteúdos relacionados com o teste. 6 AGRADECIMENTOS Os autores agradecem à Coordenação de Aperfeiçoamento de Pes- soal de Nível Superior (CAPES) e o Conselho Nacional de Desen- volvimento Cientí/f_ico e Tecnológico (CNPq) pelo apoio concedido a este trabalho. REFERÊNCIAS [1] ACM and IEEE. 2013. Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science. (2013). [2] Ana. Karoline. T. Barbosa, Larissa L. E. Neves, and Arilo C. Dias Neto. 2016. JoVeTest - Jogo da Velha para Auxiliar no Ensino e Estudo de Teste de Software. In IX Fórum de Educação em Engenharia de Software . SBC, Maringá, Brasil, 65–76. [3] Carla IM Bezerra, Emanuel F Coutinho, Ismayle S Santos, José Maria Monteiro, and Rossana MC Andrade. 2014. Evolução do Jogo Itest Learning para o Ensino de Testes de Software: Do Planejamento ao Projeto. InXIX Conferência Internacional sobre Informática na Educação (TISE) . Nuevas Ideas En Informática Educativa, Fortaleza, Brasil. [4] E M Bizerra Junior, D Silva Silveira, M Lencastre Pinheiro Menezes Cruz, and FJ Araujo Wanderley. 2012. A Method for Generation of Tests Instances of Models from Business Rules Expressed in OCL. Latin America Transactions 10, 5 (2012), 2105–2111. [5] Tarcila Gesteira da Silva and Felipe Martins Muller. 2012. Jogos Sérios em Mun- dos Virtuais: uma abordagem para o ensino-aprendizagem de teste de Software . Master’s thesis. Universidade Federal de Santa Maria. [6] Sara De Freitas and Paul Maharg. 2011. Digital games and learning . Bloomsbury Publishing, New York. [7] Bruno CÉSAR de Oliveira. 2013. TestEG - UM SOFTWARE EDUCACIONAL PARA O ENSINO DE TESTE DE SOFTWARE. In Monogra/f_ia de graduação apresentada ao Departamento de Ciência da Computação da Universidade Federal de Lavras . UFLA, Lavras, Brasil, 1–92. [8] Draylson Micael de Souza, So/f_ia Larissa da Costa, Nemesio Freitas Duarte Filho, and Ellen Francine Barbosa. 2013. Um Estudo Experimental do Ambiente Prog- Test no Ensino de Programação. In X Workshop Latinoamericano Ingeniería de Software Experimental (ESELAW) . CIbSE. [9] J. C.; de Souza, D. M.; Maldonado and E. F. Barbosa. 2012. Aspectos de Desen- volvimento e Evolução de um Ambiente de Apoio ao Ensino de Programação e Teste de Software. In XXIII Simpósio Brasileiro de Informática na Educação . SBC, Rio de Janeiro, Brasil, 1–10. [10] M.E. Delamaro, J.C. Maldonado, and M. Jino. 2016. Introdução ao teste de software (2 ed.). Elsevier, Rio de Janeiro. [11] Lucio Lopes Diniz and Rudimar Luis Scaranto Dazzi. 2011. JOGO DAS SETE FA- LHAS: UM JOGO EDUCACIONAL PARA APOIO AO ENSINO DO TESTE CAIXA PRETA. In Computer on the Beach . Universidade do Vale do Itajaí, Florianópolis, 1–10. [12] Gordon Fraser and Andrea Arcuri. 2013. Whole test suite generation. IEEE Transactions on Software Engineering 39, 2 (2013), 276–291. [13] Yue Jia and M. Harman. 2011. An Analysis and Survey of the Development of Mutation Testing. IEEE Transactions on Software Engineering 37, 5 (Sept 2011), 649–678. [14] M.A. Khan and M. Sadiq. 2011. Analysis of black box software testing techniques: A case study. In International Conference and Workshop on Current Trends in Information Technology (CTIT) . IEEE, Dubai, United Arab Emirates, 1–5. [15] Richard J LeBlanc, Ann Sobel, Jorge L Diaz-Herrera, Thomas B Hilburn, and others. 2006. Software Engineering 2004: Curriculum Guidelines for Undergra- duate Degree Programs in Software Engineering. (2006). [16] P. Lochab, A. Singhal, and A. Bansal. 2014. Generation of mutation operators for', 'duate Degree Programs in Software Engineering. (2006). [16] P. Lochab, A. Singhal, and A. Bansal. 2014. Generation of mutation operators for aspect-oriented software systems. In V International Conference Con/f_luence The Next Generation Information Technology Summit . IEEE, Noida, India, 748–752. [17] Chengying Mao. 2008. Towards a question-driven teaching method for software testing course. In International Conference on Computer Science and Software 297', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Pedro Henrique Dias Valle, Rafaela Vilela Rocha, and José Carlos Maldonado Engineering. IEEE, Wuhan, China. [18] Glenford J Myers, Corey Sandler, and Tom Badgett. 2011. The art of software testing. John Wiley & Sons, New Jersey, USA. [19] José Francisco Barbosa Neto and Fernando de Souza da Fonseca. 2013. Jogos edu- cativos em dispositivos móveis como auxílio ao ensino da matemática. RENOTE 11, 1 (2013). [20] T. P. B. Ribeiro and A. C. R. Paiva. 2015. iLearnTest: Educational game for learning software testing. In X Iberian Conference on Information Systems and Technologies (CISTI). IEEE, Aveiro, Portugal, 1–6. [21] R.V. Rocha, P.H.D. Valle, J.C. Maldonado, I.I. Bittencourt, and S. Isotani. 2017. AIMED: Agile, Integrative and open Method for open Educational resources Development. In XVII IEEE International Conference on Advanced Learning Tech- nologies. IEEE, Timisoara, Romania, 1–6. [22] SBC. 2005. Currículo de Referência para Cursos de Graduação em Bacharelado em Ciência da Computação e Engenharia de Computação. (2005). [23] Forrest Shull, Jeﬀrey Carver, and Guilherme H. Travassos. 2001. An Empirical Methodology for Introducing Software Processes. In VIII European Software En- gineering Conference Held Jointly and IX International Symposium on Foundations of Software Engineering (ACM SIGSOFT) . ACM, New York, USA, 288–296. [24] Rodolfo Adamshuk Silva, Evandro Westphalen Carlos Gomes, and Simone Nas- ser Matos. 2012. Plano de Teste para Validação do Subframework de Análise Semântica de Fórmulas. In IX International Conference on Information Systems and Technology Management (CONTECSI) . São Paulo, Brasil, USP, 4182–4208. [25] Joanna Smith, Joe Tessler, Elliot Kramer, and Calvin Lin. 2012. Using peer review to teach software testing. In IX annual international conference on International computing education research . ACM, Melbourne, Australia, 93–98. [26] Marcello Thiry, Alessandra Zoucas, and Antônio C da Silva. 2011. Empirical study upon software testing learning with support from educational game.. In XXIII International Conference on Software Engineering and Knowledge Engineering (SEKE). IEEE, Miami Beach, USA. [27] Heikki Topi, Joseph S Valacich, Ryan T Wright, Kate Kaiser, Jay F Nunamaker Jr, Janice C Sipior, and Gert-Jan de Vreede. 2010. IS 2010: Curriculum guidelines for undergraduate degree programs in information systems. Communications of the Association for Information Systems (2010). [28] P. H. D. Valle, E. F. Barbosa, and J. C. Maldonado. 2015. CS curricula of the most relevant universities in Brazil and abroad: Perspective of software testing education. In XVII International Symposium on Computers in Education (SIIE) . IEEE, Setúbal, Portugal, 62–68. [29] Pedro Henrique Dias Valle, Ellen Francine Barbosa, and José Carlos Maldonado. 2015. Um Mapeamento Sistemático sobre Ensino de Teste de Software. In XXVI Simpósio Brasileiro de Informática na Educação . SBC, Maceió, Brasil, 71 – 80. [30] Pedro Henrique Dias Valle and José Carlos Maldonado. 2016. Jogos educacionais: uma contribuição para o ensino de teste de software . Master’s thesis. Universidade de São Paulo. [31] Pedro Henrique Dias Valle, Armando Maciel Toda, Ellen Francine Barbosa, and José Carlos Maldonado. 2017. Educational Games: A Contribution to Software Testing Education. In XLVII Annual Frontiers in Education (FIE) . IEEE. [32] Pedro Henrique Dias Valle, Ricardo Ferreira Vilela, Paulo Afonso Parreira Júnior, and Ana Carolina Gondim Inocêncio. 2013. HEDEG-Heurísticas para Avaliação de Jogos Educacionais Digitais. In XVIII Conferência Internacional sobre Informática na Educação (TISE) . Nuevas Ideas En Informática Educativa, Porto Alegre, Brasil. 298']","[""Who is this briefin  or? Software engineering pratiioners   who want to make detis ions  about the us e of the Tes ing Game to  s upport s oftware tes ing edutaion bas ed on s tieniit evidente. Where the fidiins come  rom? All indings  of this  brieing were  extratted from the us e of the  AIMED method to des tribe and  analyze the development of the  Testing Game tondutted by Valle  et al.   What is iicluded ii this briefin? The main indings  on the  development of the Testing Game. Evidente tharatteris its  through a  des tripion of the game,  idenifying pos iive as petts  and  limitaions  in the game. What is iot iicluded ii this  briefin? Informaion about the phas es  of  the game, the tontents  addres s ed  and res ults  of feas ibility s tudy.  To access other evideice  briefins oi softare  einiieeriin: http:////www.lia.uft.br//ctbs oft2017 For additoial ii ormatoi  about  http:////www.labes .itmt.us p.br//s ite A GAME TO SUPPORT SOFTWARE TESTING EDUCATION This  briefng  reports  scientifc  evidence on  the  use  of  the  AIMED  method  to describe and analyze the development of an educational game to support softare testing education. FINDINGS The aim of this  paper was  to des tribe and analyze the development of the  Testing Game  us ing the AIMED method. \uf0b7 Organizational Processes  In this  protes s ,  we ideniied the view of the produtt  regarding to litens es , modules , interfates . Bes ides  that, we deined the litens e of  the game and the repos itory to s tore the  s ourte tode of the game. \uf0b7 Pre-Production Process  In this  protes s ,  we deined the iniial planning whith  tontains  two ativiies : deiniion of  pedagogital and tethnital requirements .  In addiion, we es tablis hed the target  audiente, whith is  undergraduate  s tudents , s oftware tes ing tontents  and  the as s es s ment performed to evaluate  the quality and us ability of the Testing  Game. \uf0b7 Production Processes  In this  protes s , we  organized the des tripion of the game  into three interations  whith torres pond  to tes t tethniques , s uth as  Funtional  Tes ing, Struttural Tes ing and Defetts g Bas ed Tes ing.   For eath interation, we applied the  Analys is  and Planning, Iteraive Projett,  Intremental Implementaion, Integraion,  Tes ing and Review of arifatts . We planned the phas es  of the game and  s eletted the arifatts  of the repos itories   available in the internet to us e in the  development of the game. Finally, we  implemented and integrated the s eletted  arifatts  in eath interation. \uf0b7 Post-Production Processes  In this   protes s , we deined the environment in  whith the game is  hos ted, named as   Stirra Artade s erver. It is  important to  highlight that the Testing Game is  ready  to be us ed to s upport s oftware tes ing  edutaion. However, it s hould be  highlighted that it has  been us ed s o far  only for the game evaluaion. \uf0b7 Support Processes  In this  protes s , we  evaluated the generated arifatts  to be  us ed in other iteraions . We als o  performed a feas ibility s tudy to evaluate  the quality and us ability of the Testing  Game. Through the des tripion of the game, we idenify  s ome pos iive as petts , s uth as : \uf0b7 The deiniion of the game's  s tope s inte  the tethniques  and tes t triteria that  s hould be addres s ed in iniial planning.  \uf0b7 View of produtt (litens e, modules ,  interfates ) wellgdeined. \uf0b7 The s ourte tode available on repos itories . \uf0b7 Addiional modules  available with the  theoreital tontents  addres s ed in the  game. \uf0b7 Es tablis hment of open s ourte litens e for  the game. \uf0b7 Deiniion of s oftware tes ing tontents   addres s ed in eath level. \uf0b7 Eas y attes s  to the game by means  of a  link. Des pite the pos iive as petts  rais ed, we ideniied  s ome points  that tan be revis ited in the next  vers ion of the game, s uth as : \uf0b7 Prioriizaion and revis ion of arifatts . \uf0b7 Latk of tutorials  to s upport s tudents  in  the us e of the Testing Game."", 'vers ion of the game, s uth as : \uf0b7 Prioriizaion and revis ion of arifatts . \uf0b7 Latk of tutorials  to s upport s tudents  in  the us e of the Testing Game. \uf0b7 Latk of an es tablis hed requirements  lis t. \uf0b7 Latk of game tutorial for s tudents  to us e  of the game (i.e., s treens , menus ,  funtionality, phas e, tommands ), \uf0b7 Latk of divis ion of roles  in the game  development. \uf0b7 Latk of an iniial planning with the main  ris ks . \uf0b7 Latk of an experiment to as s es s  the  s tudents  learning after they play the  Testing Game. \uf0b7 Do not s ave s tudents  informaion in a  databas e. Regarding we have us ed the AIMED method to  des tribe the game s ome improvements  need to be tons idered for the method upgrade: \uf0b7 Provide s upport the development of  edutaional res ourtes  with geographitally dis tributed development teams .  \uf0b7 Es tablis h during the ativity for litens ing  the edutaional res ourte a lis t of the main litens es  for edutaional res ourtes .  \uf0b7 Deine a lis t of s ugges ions  of tools  to  s upport the development of edutaional  res ourtes  during the programming  ativiies . Through the res ults , we intend to s olve the  ideniied problems  and tondutt a tas e s tudy,  applying this  game in a s oftware tes ing tours e, to  evaluate s tudents ’ performante in this  tontext. We als o intend to tons ider the implementaion of  new features  to integrate s oftware tes ing tools   into the Testing Game, allowing  s tudents  tes ing  their programs  ins ide the game environment.    ORIGINAL RESEARCH REFERENCE Valle, P. H. D., Rotha, R. V., & Maldonado, J. C.; Tes ing Game: An Edutaional Game to Support Software Tes ing Edutaion. In Proteedings  of the 31s t Brazilian Sympos ium on  Software Engineering (SBES) @ VIII Brazilian Conferente on Software: Theory and Pratite (CBSoft 2017), Fortaleza, 2017.']","**Title: Enhancing Software Testing Education Through Gamification**

**Introduction:**
This Evidence Briefing explores the development and evaluation of the Testing Game, an educational tool designed to improve software testing education. The briefing aims to summarize the key findings from the research conducted by Valle, Rocha, and Maldonado, highlighting the game's development process, its educational value, and areas for improvement.

**Core Findings:**
1. **Educational Value**: The Testing Game addresses the lack of qualified professionals in software testing by providing an engaging platform for students to learn essential testing techniques, including functional, structural, and mutation testing. It promotes active learning through gameplay, which can enhance student motivation and understanding of complex concepts.

2. **Development Process**: The game was developed using the AIMED method, which emphasizes agile practices and the integration of educational design principles. This approach facilitated the identification of key aspects necessary for high-quality educational resources, such as project scope, licensing, and source code availability.

3. **Positive Aspects**: The development of the Testing Game included several strengths:
   - Clear definition of project scope and educational objectives.
   - Availability of the source code in open repositories, promoting transparency and collaboration.
   - Inclusion of theoretical modules that support gameplay, enhancing the educational experience.

4. **Identified Limitations**: Despite its strengths, the game also has limitations that could be addressed in future iterations:
   - Lack of user tutorials and guidance, which may hinder new users from fully engaging with the game.
   - Insufficient prioritization and revision of development artifacts, which could improve overall quality.
   - Absence of a structured evaluation mechanism to assess student learning outcomes from using the game.

5. **Future Directions**: The authors suggest several improvements for future versions of the Testing Game, including the addition of multiplayer features to foster collaboration among students and the integration of testing tools to support practical learning experiences.

**Who is this briefing for?**
This briefing is intended for educators, curriculum developers, and software engineering practitioners interested in innovative approaches to teaching software testing. It highlights the potential of gamification in educational settings and provides insights into the development and evaluation of educational games.

**Where the findings come from?**
The findings are derived from the development and evaluation of the Testing Game, as detailed in the research conducted by Valle, Rocha, and Maldonado, presented at the SBES 2017 conference.

**What is included in this briefing?**
This briefing includes an overview of the Testing Game's development process, its educational implications, strengths, limitations, and suggestions for future enhancements.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the authors and their work:**
Pedro Henrique Dias Valle: [pedrohenriquevalle@usp.br](mailto:pedrohenriquevalle@usp.br)  
Rafaela Vilela Rocha: [rafaela.vilela@gmail.com](mailto:rafaela.vilela@gmail.com)  
José Carlos Maldonado: [jcmaldon@icmc.usp.br](mailto:jcmaldon@icmc.usp.br)

**Original Research Reference:**
Valle, P. H. D., Rocha, R. V., & Maldonado, J. C. (2017). Testing Game: An Educational Game to Support Software Testing Education. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. DOI: [10.1145/3131151.3131182](https://doi.org/10.1145/3131151.3131182)"
"['Evaluating an Automatic Text-based Test Case Selection using a Non-Instrumented Code Coverage Analysis Claudio Magalhães ∗ Centro de Informática Recife, Brazil cjasm@cin.ufpe.br João Andrade Centro de Informática Recife, Brazil jlan@cin.ufpe.br Lucas Perrusi Centro de Informática Recife, Brazil lbp@cin.ufpe.br Alexandre Mota Centro de Informática Recife, Brazil acm@cin.ufpe.br ABSTRACT During development, systems may be tested several times. In general, a system evolves from change requests, aiming at improving its behavior in terms of new features as well as ﬁxing failures. Thus, selecting the best test plan in terms of the closeness between test cases and the changed code and its dependencies is pursued by industry and academia. In this paper we measure the coverage achieved by an auto- matic test case selection based on information retrieval that relates change requests and test cases. But instead of using oﬀ-the-shelf coverage tools, like JaCoCo, we propose a way of obtaining code coverage of Android apk’s without instru- mentation. This was a basic requirement of our industrial partner. We performed some experiments on this industrial partner and promising results were obtained. Keywords Information Retrieval; Test case selection and prioritization; Code coverage 1. INTRODUCTION Nowadays it is usual that software evolves iteratively and incrementally. From iteration to iteration, some changes oc- cur to ﬁx failures or to improve functionality. Such changes are documented through Change Requests (or simply CRs). As changes can potentially introduce faults, it is good prac- tice to test the system periodically by using several diﬀerent approaches [1]. As the system size grows, the amount of tests to check its behavior grows as well. Change requests based testing present an interesting opportunity to avoid performing a huge amount of tests because it can focus on the changes performed during a certain period of time. That is, given a speciﬁc development period of time, one can get the corre- sponding CRs and select the most appropriate test cases to ∗This is our contact author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full cita- tion on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. SAST, September 18–19, 2017, Fortaleza, Brazil c⃝2017 ACM. ISBN 978-1-4503-5302-1/17/09. . . $15.00 DOI: https://doi.org/10.1145/3128473.3128478 be re-executed as well as the design and execution of new test cases. When source code and automated test cases are available, this selection can be performed safely and precisely [15]. Unfortunately in some contexts, where artifacts are textual documents (change requests and tests) as well as tests are executed manually1, test selection can be imprecise. And the amount of selected test cases with respect to change re- quests can be unfeasible to be executed. In such a situation, one has to consider some sort of selection criteria based on information retrieval [16] that best ﬁt this scenario. In this paper we use a selection process based on infor- mation retrieval that uses textual similarity and frequency as a means to select and prioritize test cases, proposed in previous work [10]. This is implemented in a tool named AutoTestPlan (or just ATP); it is already in use by our in- dustrial partner. AutoTestPlan is intended to replace the test architect activities completely. To accomplish this, we need to attest that its selection is as good as that of human beings. Currently, the test architects still validate our list', 'test architect activities completely. To accomplish this, we need to attest that its selection is as good as that of human beings. Currently, the test architects still validate our list of selected test cases as a temporary requirement. To have a clearer idea about the closeness achieved by AutoTestPlan with respect to the modiﬁed code regions, we performed monitored test sessions where we measured code coverage. We used the Android Device Monitor tool 2 pro- vided by Google. We created a tool named AutoTestCover- age (or just ATC) with that purpose. The main contributions of this paper are: •Code coverage of Android apks without instrumenta- tion (AutoTestCoverage, or just ATC); •Some experiments demonstrating the advantages of the proposed selection process attested by code cov- erage analysis focused on the modiﬁed regions. This paper is organized as follows. In Section 2 we in- troduce the main concepts used in this work, namely in- formation retrieval, test prioritization, and code coverage. Section 3 presents our proposed solution to the problem of test case selection related to change requests. In Section 4 we introduce how we identify which part of the source code is associated to the change requests and how we monitor its execution to calculate code coverage. We perform some experiments to analyse the advantages of our proposed pro- cess and these are reported in Section 5. Our conclusions, related and future work are discussed in Section 6. 1This is the case of our industrial partner: Motorola Mobil- ity. 2https://developer.android.com/studio/proﬁle/monitor.html', '2. BACKGROUND This section brieﬂy presents basic concepts related to in- formation retrieval, test case prioritization and code cov- erage analysis, focusing their applications in software engi- neering. 2.1 Regression Test Selection Eﬃcient regression testing is important, even crucial, for organizations with a large share of their cost in software de- velopment. This includes, among other tasks, identifying which test cases need to be re-executed (that is, regression test selection) to check whether the behavior of modiﬁed software was preserved. Regression test selection involves a trade-oﬀ between the cost for re-executing test cases, and the risk for missing faults introduced through side-eﬀects of changes to the software. Iterative development strategies and reuse are common means of saving time and eﬀort for the development. However they both require frequent retest- ing of previously tested functions due to changes in related code. The need for eﬃcient regression testing strategies is thus becoming more and more important. A test selection technique, in the context of regression testing, tries to ﬁnd a subset of test cases that satisﬁes cer- tain goals, relevant to a change in the system under test. The nature of such goals can vary considerably. Follow- ing the work reported in [14], the goals that can be com- pared and evaluated are inclusiveness, precision, eﬃciency, and generality. Inclusiveness measures the extent to which a technique chooses tests that will cause the modiﬁed program to produce diﬀerent output than the original program, and thereby expose faults caused by modiﬁcations. Precision measures the ability of a technique to avoid choosing tests that will not cause the modiﬁed program to produce diﬀerent output than the original program. Eﬃciency measures the computational cost, and thus, practicality, of a technique. Generality measures the ability of a technique to handle re- alistic and diverse language constructs, arbitrarily complex code modiﬁcations, and realistic testing applications. It is worth noting that the previous categories proposed by [14] are related to source code. But in our context we do not have access to source code. We can only access two kinds of artifacts: change requests and text based test cases. Therefore our goals are somewhat diﬀerent. We are inter- ested in measuring our eﬀectiveness in terms of keywords presence and frequency in such documents [12]. This is fur- ther detailed in the next section. 2.2 Information Retrieval and Impact Analy- sis Information retrieval (IR) applications are being increas- ingly used in software engineering problems [12]. IR focuses on search: given a massive collection of documents, IR at- tempts to ﬁnd the most relevant documents based on a user- query. Generally an IR employs three main tasks: text pre- processing, indexing, and retrieval. Preprocessing concerns text normalization, stop-word removal, and stemming. A text normalizer removes punctuation, performs case-folding, tokenizes terms, etc. In the stop-word removal phase, an IR application discards the frequently used terms such as prepositions, articles, and so on, to improve eﬃciency and reduce spurious matches. Finally, stemming combines vari- ants of the same term (for instance, see, seeing, saw) to improve term matching between query and document. Af- terwards, documents are indexed for fast retrieval. Once indexed, queries are submitted to the search engine, which returns a ranked-list of documents in response. Finally, the search engine is evaluated by measuring the quality of its output ranked-list relative to each user’s input query. In this paper we use the text processing framework Lucene [13]. Textual similarity is a critical part of our approach. The Information Retrieval community dealt with text similarity for a long time. Given a set of text documents and a user information needs represented as a set of words, or more', 'Information Retrieval community dealt with text similarity for a long time. Given a set of text documents and a user information needs represented as a set of words, or more generally as a free text, the information retrieval problem is to retrieve all documents relevant to the user. Beyond information retrieval, our work also uses concepts from impact analysis. Impact analysis is the identiﬁcation of the work products aﬀected by a proposed change request, either a correction of ﬂaws or a new feature request [2]. Our approach to impact analysis is based on the hypothesis that the set of revision comments of a ﬁle and the set of CRs that previously impacted it are a good descriptor of the ﬁle to support impact analysis of new CRs. We use textual similarity to retrieve past CRs similar to a new CR and to compute the impacted ﬁles. 2.3 Test case prioritization Test case prioritization intends to order test cases for re- gression testing in such a manner that test cases with higher priority executes earlier than those with lower priority, ac- cording to some performance criteria. This works as follows. Assume that T is a test suite, PT is the set of permutations of T and f is a function from PT to real numbers. The problem is to ﬁnd T′which belongs to PT , such that, for every T′′, T′′ belongs to PT and (T′′̸= T′) [f(T′) ≥f(T”)]. In the above deﬁnition, PT denotes the set of all possible prioritization or order ofT, f is the function which is applied to any such order, and returns an award value for it. In this work we use the material in the previous section to create a prioritized list of test cases, where f is a similarity function based on keywords frequency found in test cases and CRs. Thus we deviate slightly from the literature def- inition of test case prioritization because we do not intend to get an ordered list of test cases where higher priority executes earlier than those with lower priority, but instead that the lower priority can be possibly discarded whether the frequency f is too low. 2.4 Mobile Testing The term mobile testing refers to diﬀerent types of test- ing: native mobile app testing, mobile device testing, and mobile Web app testing [7]. We use mobile app testing to refer to “testing activities for native and Web applications on mobile devices using well-deﬁned software test methods and tools to ensure quality in functions, behaviors, perfor- mance, and quality of service, as well as features, such as mobility, usability, interoperationability, connectivity, secu- rity, and privacy” [7]. In our scenario, tests tend to focus on functionality and behavior. This kind of tests validates service functions, mo- bile Web APIs, external system behaviors, system-based in- telligence, and user interfaces (UIs) [7] 2.5 Code Coverage Analysis Code coverage is a kind of metric used to characterize', 'the degree to which some region of the source code of an application is executed after a test campaign [8]. It means a percentage, calculated by the division Codeexec Coderegion , exhibiting the amount of the source code that was executed (Codeexec) with respect to the total amount of source code that may be exercised (Coderegion). The higher the code coverage the bigger is the amount of source code executed during a testing execution. This suggests a lower chance of escaping undetected bugs com- pared to an application with lower code coverage. Several diﬀerent metrics can be used to calculate code coverage. For object-oriented applications, one can measure classes, meth- ods, statements, conditionals, etc., exercised. This list is in the ﬁned-grained direction. That is, class coverage is less accurate than conditionals coverage. In this paper we focus on methods coverage which is enough as a measure to address the closeness (the degree to which) that our proposed selection process can create test plans that cover modiﬁed regions based on change requests. The higher this percentage the better is our proposed selection process. In general, code coverage is performed by instrumenting the source code of the application and then getting coverage data from the instrumentation part. There are several tools that provides such an information. For instance, JaCoCo 3. However, sometimes we are not allowed to perform instru- mentation because testing is not made at development level. In this case we have to performing monitoring without mod- ifying already compiled applications. For Android, we use ddmlib tracers to listen methods calls while performing a test case execution. 3. PROPOSAL In this section we present our proposal for automatic TC selection and prioritization (More detailed in [10]). 3.1 Selection The proposed solution for the automatic TC selection and prioritization is to base this process on the CRs which are being resolved in current test campaign. The aim is to au- tomate the manual process, which is very costly and not always eﬀective. The automated process receives as input a set of CRs manually selected by the test team, and re- turns as output an ordered list of TCs which will be used to perform the tests (a test plan). The TCs are obtained from a general test repository main- tained by the company. As the general repository is very large (and frequently updated), all testing campaigns start by selecting from this repository a (possibly high) number of TCs related to the test goals (named as Master Plan—MP). This selection is traditionally executed manually, based on the test goals description and on the architect’s experience. As our aim is to automate the whole process of test plan creation, we must also address the selection and creation of this initial TC subset (from the Master Plan). This sub pro- cess is part of the Phase 1 of the general automation process, which is fully described below. Finally, note that although the Master Plan is already a subset of the general repository, it is usually still large, to preserve coverage of the code to be tested. Thus, the Master Plan is later reduced by the test architect to manually create 3http://www.eclemma.org /jacoco more objective/focused test plans (which is the main aim of our automation process). 3.1.1 Proposed Selection Process We start by receiving a Master Plan (MP) as input and a set of open Change Requests (CRs). We create a test plan per CR. This process counts on four main phases, de- tailed below (Figure 3.1.1 depicts the general architecture proposed for the general process). Phase 1 (MP Index File creation)- To ease the Test Plans creation, we automatically index the input MP to re- trieve TCs using keyword queries related to each open CR. Indexing is performed with Lucene Apache 4. This part is one module of the implemented system (Section 3.2). The indexing phase will be executed only once per product, since the MP does not change.', 'Indexing is performed with Lucene Apache 4. This part is one module of the implemented system (Section 3.2). The indexing phase will be executed only once per product, since the MP does not change. Phase 2 (Creation of CRs keyword based represen- tation) - this phase aims to create a keyword based repre- sentation of each input CR, counting on three steps: •Step 1 (Extraction of keywords from the input CRs): The keywords are extracted from previously deﬁned ﬁelds of the CR template - the ones with mean- ingful information for the task (e.g., title, product com- ponent, problem description). These ﬁelds were chosen with the help of a test architect; •Step 2 (Stopwords elimination): Each CR rep- resentation is then ﬁltered through the elimination of stopwords. These are words which are too frequent in the TC repository (and thus will not help to iden- tify a particular test), or which are not relevant in the current application. The list of stopwords is usu- ally built during the initial indexing process described above (Phase 1), since it may change according to the current MP vocabulary and words frequency. •Step 3 (Stemming): The list of keywords may also undergo a stemming process, to reduce inﬂected or de- rived words to their word stem/base form (for instance, messages ≈message). This process favors the retrieval of a higher quantity of TCs, so it should be optional and only used when necessary. Phase 3 (MP Index File consultation)- in this phase, the CRs keyword representations are used to search the MP index ﬁle to create the initial TC lists. This phase will be executed once for each input CR. •Step 1 (Queries creation): This step receives each CR representation as input and delivers one query per CR to be submitted to the search engine. Duplica- tions are eliminated. But duplicated words are placed at the beginning of the query, so they will have more importance during the search process. •Step 2 (Queries processing): each query created in Step 1 above is individually submitted to the search engine, retrieving a list of TCs related to the corre- sponding CR. These lists of TCs are automatically or- dered by the search engine according to the relevance 4https://lucene.apache.org/', 'Figure 1: Proposed process (General architecture) of each TC to the current query.5. Such individual or- dered lists are represented as the left-hand side (CR1, CR2, and CR3) of Figure 2. The obtained lists of TCs are given as input to Phase 4. Phase 4 (Test Plan Creation)- in this phase, the ob- tained lists of TCs are merged, originating the Test Plan to be executed. Note that diﬀerent queries (representing dif- ferent CRs) may retrieve the same TC from the index base. During the merging step, the exiting duplications are elim- inated. However, we understand that if a TC was retrieved by more than one query, it may be more relevant for the testing campaign as a whole. So, the duplicated TCs are prioritized in the ﬁnal ordered Test Plan. The right-hand side of Figure 2 illustrates the ﬁnal merged list. The above process will be repeated every time new build is tested and presents defects. However, the process starts from Phase 2. As said, Phase 1 will run only once per new product. The ﬁnal merged list will constitute the Test Plan to be executed in the test plan. This strategy was validated via an experiment presented in the following section. 3.2 The Test Selection Tool Prototype The automated process described above was implemented in a tool prototype named as AutoTestPlan(see Figure 3). It was implemented using Django Bootstrap 6, a high-level Python Web framework, bearing an MVC (Model-View- Controller) architecture. 4. CODE COVERAGE Our industrial partner does not allow code instrumenta- tion because where our job is being done is in the latest testing phase. At that point, code instrumentation is sup- posed to introduce bug detection false alarms. So it was necessary to use an alternative way to get code coverage: dynamically monitoring code execution. In this case, our solution is not allowed on time-sensitive tests because the application has some time performance degradation. But this was not a problem for the experiments presented in this paper (Basically, functional tests). Our proposed code cov- erage analysis is divided into four main steps explained in what follows. From the previous section, saw that can identify from which to which versions of an application a release notes is 5Relevance measures will be mentioned in Section 5. 6http://www.djangoproject.com/ related to. That is, all modiﬁcations documented in the re- lease notes (documented in change requests—CRs) in a cer- tain period must be also found in the corresponding source code. Although release notes do not indicate the speciﬁc source code, we can identify the respective source code by the time period documented in the release notes. Thus, we get two dates TOld and TNew . By approval from our industrial partner, we can access the application source code as well as the repository that contains all the most recent versions of that application cor- responding to at least the period from TOld to TNew . From this, we calculate the code diﬀerence between two versions of the application, one corresponding to TOld and the other to TNew . This is indeed the hardest part of our analysis be- cause this is not just getting ﬁle diﬀerences as provided by Git diﬀs. We use static analysis to detect new and modiﬁed methods, without worrying about source code line locations, spacing, etc. This allows to get all modiﬁed code in the right time period (from TOld to TNew ). We keep these methods in a set named ToBeCovered 4.1 Monitoring test executions We monitor the execution of the test plan, created by AutoTestPlan, to measure its eﬃcacy in terms of coverage of the modiﬁed regions associated to the release notes. We created a tool called AutoTestCoverage (or simply ATC) to perform this monitoring. Its goal is to know which methods were called during testing. First of all, to track all methods which were called during a test execution, we use an Android Library called “ddmlib”7 that provides Android background informations about runtime execution. This', 'methods which were called during a test execution, we use an Android Library called “ddmlib”7 that provides Android background informations about runtime execution. This process counts on four main phases detailed in Figure 4. Then the device is connected to a computer and we start the Android Debug Bridge (ADB). At this point, we indi- cate, as a parameter, which package of the application ADB has to monitor. This part concerning the package to mon- itor is very important. One has to indicate all companion packages related to the application and those packages must be visible to the ADB so that the right trace to those pack- ages can be tracked. To start logging trace ﬁles, the method startMethodTracer is called. This method called enables the proﬁling on the speciﬁc packages. The ATC tool saves log ﬁles (sequences of method calls— Full seq—involving methods in or outside the modiﬁed re- gion. That is, methods lying in the set ToBeCovered or 7https://android.googlesource.com/platform/tools/base/+/ tools r22/ddmlib/src/main/java/com/android/ddmlib', 'Figure 2: TC duplication elimination and reordering Figure 3: The AutoTestPlan prototype not) that we use in our third step. There is one log ﬁle for each test case or exploratory testing session. This is be- cause we need to know in which areas of the application a test exercised that application. This will help us in the fu- ture to choose test cases more precisely by integrating this information into the AutoTestPlan tool. Thus we have a mapping (from test case to method names) such as this one. TCk ↦→ m1(); m2(); ... ; mT () 4.2 Generating the Code Coverage Report We ﬁlter8 the log ﬁles provided by the AutoTestCoverage 8Assume that Seq ↓ Set yields a new sequence resulting from the sequence Seq by preserving only the elements in tool using the modiﬁed regions determined in Section 4.1. That is, we compute the new set Modifseq = Full seq ↓ToBeCovered We calculate the code coverage by simply performing the division between the amount of elements in sequences of Modifseq and the set ToBeCovered . Or: Coverage = #Modifseq #ToBeCovered (1) This, obviously, for each test case. The whole coverage has just to take care of simultaneous method names in diﬀerent test sequences. 5. EXPERIMENTS AND THREATS TO V A- LIDITY the set Set.', 'Figure 4: The AutoTestCoverage Prototype In this section we explain the experiments performed, and the threats to validity. 5.1 Performed Experiments Our proposed test selection prototype was used in four real experiments9 from our industrial partner, which we de- scribe in what follows. For each experiment we provide code coverage data to show the closeness between the test cases selected from our tool and the architects and the change requests that should be tested. The experiments were created with the help of test archi- tects from our industrial partner. We have to observe the following setup information. •CRs selection: the test architects selected two re- lease notes (one for the ﬁrst experiment and a second one for the other two experiments) to execute the two testing campaigns. These release notes have 9 CRs in the ﬁrst release notes and 224 CRs in the second release notes; •Corpus selection: the test architects manually se- lected 120 TCs from the general TC repository to de- ﬁne the Master Plan in the ﬁrst experiment and 116 TCs to deﬁne the second Master Plan. As we will see in what follows, this master plan of 116 TCs was also used in our third experiment for obtaining a corre- sponding comparison with a modiﬁed version of ATP (named here as ATP∗); In Tables 1, 2, 3, and 4 we have data collected from ex- periments 1, 2, 3 and 4, respectively. Each of these tables has 4 columns, explained as follows: •Selector - represents the agent that made the test case selection. In these tables we can ﬁnd: – Architect: human being that performs a manual selection; – ATP: tool that performs an automatic selection; – ATP∗: tool that performs an automatic selection without using a master plan. •TCs - represents the number of test cases selected; 9To get experimental data from our industrial partner with- out interfering with its usual schedule, we could only have 4 experiments. Selector TCs Coverage Failure Architect (62)120 51.74% 0 ATP (40)62 51.74% 0 Table 1: 1st experiment •Coverage - represents the percentage of coverage; •Failure - represents the number of failures found in the test campaign. In the ﬁrst experiment, the architect selected 120 test cases, deﬁning these as the master plan. AutoTestPlan (ATP) selected 62 test cases from this master plan. But due to hardware characteristics (unsupported features), from the 120 list only 62 could be executed and 40 from the 62 se- lected test cases of ATP. The unsupported test cases were deleted. Both executions (architects and ATP) had the same code coverage of 51.74%, but the ATP’s test plan is approx- imated 50% smaller than architects. This is outstanding because it cuts costs in practice while preserves the same coverage. Although we have achieved the same code cov- erage with less test cases, such a code coverage is too low and worrying. We already pointed this out to our industrial partner to update its test database more frequently as an attempt to improve such a code coverage. Selector TCs Coverage Failure Architect 175 4% 9 ATP∗ 166 4% 13 Table 2: 2th experiment In the second experiment (Table 2), the architect used all the 175 test cases of the master plan. ATP selected 166 test cases from these 175 test cases because in some circunstances we have to use the master plan created by specialised em- ployees. Interestingly, ATP has shrinked the 175 test cases into 166 test cases, while preserving its code coverage of 4%. Surprisingly, the ATP selection has produced more bugs than the architect selection. The main reasons associated to this are: (i) the test cases are somewhat vague, leaving the tester free to choose some variations in the smartphone in- teraction; (ii) the architect’s selection was executed in India and ATP’s selection was executed in Brazil. And just to', 'Selector TCs Coverage Failure Architect 116 53.45% 0 ATP 108 52.07% 0 Table 3: 3nd experiment make this scenario even clearer, the 9 bugs found using ar- chitect’s selection are contained in the 13 bugs found using ATP’s selection. In the third experiment, the architect selected 116 test cases for the master plan. ATP selected 108 test cases from them. Once again the coverage was too close: 53.45% from the architects and 52.07% from ATP. As already highlighted in the previous experiment, such a code coverage is too low. Selector TCs Coverage Failure Architect 116 53.45% 0 ATP∗ 211 58.57% 7 Table 4: 4rd experiment As ATP was being used—by a requirement from our in- dustrial partner—pruned by the master plan selected by test architects, we decided to perform a third experiment (only related to the 3 rd experiment). For this new experiment, we did not use the master plan selected from test architects. We used ATP directly in the whole test database of our in- dustrial partner, related to this product. ATP selected 211 test cases from more than 5500 test cases in about 4 min- utes. With 211 selected test cases, ATP got a code coverage of 58.57%. This 5.12% (58.57%-53.45%) code coverage gain was not so impressive, but it was enough to ATP’s test plan reveals 7 new failures not found in the company’s failures (change requests) database. This was really fantastic! 5.1.1 General discussion In the ﬁrst three experiments we noted that we could never have a plan with greater coverage than the master plan be- cause the master plan is always fully executed by our indus- trial partner. There is no further selection from test archi- tects. Even though, ATP has provided a close code coverage and with fewer test cases. This is an important advantage of our proposed test selection tool because these tests are ex- ecuted manually in general and this represents costs saving for our industrial partner. The second experiment was a very particular situation that currently should not occur in practice: A test plan ex- hibiting a very low code coverage related to the modiﬁed regions with a high number of failures found. This was ex- ceptionally caused by us. We tried to observe what happens if we use a shorter (compared to current practice) time pe- riod to perform a regression. As the master plan is some- what the same for a longer time period, we would like to observe how close it could be related to the most recent re- lease notes. As the experiment shows, the master plan is deﬁnitively not good enough, yielding an unacceptable code coverage. Besides that, and surprisingly, we also noted a considerable amount of failures found. And all failures en- countered using the architect’s plan were also found using the ATP plan. By deciding to avoid having the master plan as a prelim- inary ﬁlter in our fourth experiment, our attempt was two fold: (i) to increase code coverage beyond the master plan; (ii) eventually ﬁnding bugs. Both attempts were accom- plished but (i) was not so impressive because we had only a 5.12% gain in code coverage. But (ii) was very interest- ing because we can ﬁnd 7 new unreported failures with the small improvement from (i). This means that if the com- pany attempts to have more change requests related test cases, such a code coverage can increase and more bugs can be found. Besides that, ATP can become the de-facto stan- dard to ﬁnd the best master plans more dynamically. This is indeed already recognized by some collaborators of our industrial partner. From this 5.12% we have two points to further investi- gate: (a) the company’s test database has not enough test cases to achieve a higher code coverage and this implies that it has to be updated more directed and frequently, and (b) our selection algorithm can suﬀer from a huge amount of in- formation when discarding the master plan as a preliminary ﬁlter. We leave both as future work. 5.2 Threats to validity A ﬁrst threat we identiﬁed is related to the MP’s size. If', 'formation when discarding the master plan as a preliminary ﬁlter. We leave both as future work. 5.2 Threats to validity A ﬁrst threat we identiﬁed is related to the MP’s size. If a MP has less than 50 TCs, for example, the similarity algo- rithm cannot work with eﬃcacy and the automatic selection can result in the same 50 TCs. But in practice this is not a problem because in general MP’s size is at least of 100 TCs. We have another concern that is related to the availability of just one test team to work with. We are already collab- orating with other teams to perform similar experiments. This is mainly associated to the quality of the textual parts in the test cases as well as CRs. As pointed out in the previous threat, the text provided in CRs is totally informal and does not follow guidelines. Fortunately in test cases, this is not a concern because they are written carefully and reviewed to guarantee that they are clear and well deﬁned. Even though, we can try to ﬁnd some relationship between code changes and textual test cases if the code conforms to some convenient standard that can allow us to create natural text automatically. We intend to investigate this in the future as well. 6. CONCLUSION In this work we present real experiments performed on our industrial partner to quantify the quality of test cases selection in terms of code coverage. These test cases may be closely related to the modiﬁed code as result of change requests. We use AutoTestPlan to create test plans based on information retrieval. These experiments indicate that it is feasible to use AutoTestPlan to create such desired test plans. The coverage results were very close, even when we increased the number of test cases in the indexed base (dis- carding the master plan as a ﬁlter). The low code coverage in all experiments suggests that the company’s test database is not frequently updated to overcome the frequent modiﬁ- cations in code. Today test cases are created on demand by test architects. The goal of the work reported in [18] is completely aligned to ours. In particular, its industrial setting is very similar to ours and its use of natural language processing. However, there it deals with source code to better ﬁnd the related test cases (based on features) whereas we do not have ac- cess to source code. In our case, components are our main features, restricting considerably the amount of test cases', 'to be executed. The reﬁnement to components come from the keywords appearing simultaneously in the texts of the change requests and test cases. Another diﬀerence is that in [18], the authors deal with software product lines (SPL) and we do not address this problem speciﬁcally, although our context is based on SPL as well. The work reported in [9] inherits a similar strategy as ours in the sense of using information retrieval and similarity analysis. However, that work uses a speciﬁc ranking function with particular weights (in the direction of a previous work of ours [11]), etc., whereas we use the built-in functionality provided by Lucene [13]. Concerning test selection in particular, an interesting re- lated work is reported in [17]. In that work, test selection is indeed dealt with as test suite reduction where several criteria are used to reduce a test suite while retain as most as possible of its original properties in terms of source code and requirements coverage, etc. Its approach is more gen- eral than ours where we are more closely related to what is named requirements coverage in [17]. That is, we try to cre- ate test plans that covers similar requirements (keywords) in change requests as well as in test cases procedures. Several works use mathematical models, indeed transition systems, to select test cases such as [4, 6]. The work reported in [4] reduces a test plan by checking dependency in EFSM structures whereas [6] reduces by applying a similarity algo- rithm on transition systems. Although both use some kind of similarity algorithm like ourselves, they use some formal notation. The main diﬀerence to our work is exactly that we do not use any mathematical model except the similarity algorithm for natural language provided by the Lucene tool. We think our similarity criteria is more convenient because it is guided by change request whereas in the work reported in [6] it is related to the mathematical model itself. Thus this reduction simply discards similar test cases. In our case we discard those test cases not related to the current change requests. The work [3] presents an interesting approach, although restricted to model-based solutions like other works. It pro- vides a similarity strategy based on the number of identical transitions in test cases as graphs. Our paper however uses textual similarity, obtainig promising results from a code coverage perspective. The closest work to ours is the one reported in [5]. Like ours, the work [5] performs a similarity check based on test cases and change descriptions (or requests). But diﬀerently to ours, source code is used as well. In our case only infor- mal documents are used which complicates considerably the similarity algorithm. We intend to consider source code in the future but this is not done currently. As future work we intend to perform further experiments, focusing other features of the Motorola smartphones. We need to ﬁne-tune our selection algorithm when considering selection without a master plan as preliminary ﬁlter. We also need to try other variants of the merge strategy to see whether we can obtain better match results. And use code coverage data to shrink the ATP test plans even further. We also intend to make a continuous analysis of the results obtained with our tool against the architect’s selections to test a statistical hypothesis to completely replace the manual selection with the automatic one, improving the daily test process of Motorola Mobility. Another future work concerns investigating the quality of the tests available in the company’s test database as well as how to improve than, if necessary. A ﬁnal future work can be to try to ﬁnd some relation- ship between code changes and textual test cases if the code conforms to some convenient standard that can allow us to create natural text automatically. Acknowledgments. We would like to thank Alice Arashiro, Viviana Toledo, Eliot Maia, and Lucas Heredia from Mo-', 'create natural text automatically. Acknowledgments. We would like to thank Alice Arashiro, Viviana Toledo, Eliot Maia, and Lucas Heredia from Mo- torola Mobility, and Virginia Viana, Victor Bichler from the collaboration between CIn and Motorola Mobility. This re- search is supported by Motorola Mobility. 7. REFERENCES [1] Jørn Ola Birkeland. From a Timebox Tangle to a More Flexible Flow, pages 325–334. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. [2] G. Canfora and L. Cerulo. Impact Analysis by Mining Software and Change Request Repositories. In 11th IEEE International Software Metrics Symposium (METRICS’05), page 29. IEEE, 2005. [3] Emanuela G. Cartaxo, Patr´ ıcia D. L. Machado, and Francisco G. Oliveira Neto. On the use of a similarity function for test case selection in the context of model-based testing. Software Testing, Veriﬁcation and Reliability, 21(2):75–100, 2011. [4] Paolo Tonella Cu D. Nguyen, Alessandro Marchetto. Model based regression test reduction using dependence analysis. In In Proceedings of the International IEEE Conference on Software Maintenance, pages 214–223. IEEE, 2002. [5] Paolo Tonella Cu D. Nguyen, Alessandro Marchetto. Test case prioritization for audit testing of evolving web services using information retrieval techniques. In Web Services (ICWS), 2011 IEEE International Conference on, pages 636–643. IEEE, 2011. [6] Patr´ ıcia Duarte de Lima Machado Francisco Gomes de Oliveira Neto. Sele¸ c˜ ao autom´ atica de casos de teste de regress˜ ao baseada em similaridade e valores. In Revista de Inform´ atica Te´ orica e Aplicada:RITA, v20(2), pages 139–154, 2013. [7] Jerry Gao, Xiaoying Bai, Wei-Tek Tsai, and Tadahiro Uehara. Mobile application testing: A tutorial. Computer, 47(2):46–55, 2014. [8] Ferenc Horv´ ath, B´ ela Vancsics, L´ aszl´ o Vid´ acs,´Arp´ ad Besz´ edes, D´ avid Tengeri, Tam´ as Gergely, and Tibor Gyim´ othy. Test suite evaluation using code coverage based metrics. In 14th Symposium on Programming Languages and Software Tools, SPLST 2015 . CEUR-WS, 2015. [9] Manisha Khattar, Yash Lamba, and Ashish Sureka. Sarathi: Characterization study on regression bugs and identiﬁcation of regression bug inducing changes: A case-study on google chromium project. In Proceedings of the 8th India Software Engineering Conference, pages 50–59. ACM, 2015. [10] Cl´ audio Magalh˜aes, Alexandre Mota, Fl´ avia Barros, and Eliot Maia. Automatic selection of test cases for regression testing. In Brazilian Symposium on Systematic and Automated Software Testing,SAST, Maring´ a, Brasil, pages 1–8, 2016.', '[11] Cl´ audio Magalh˜aes, Alexandre Mota, and Eliot Maia. Automatically ﬁnding hidden industrial criteria used in test selection. In 28th International Conference on Software Engineering and Knowledge Engineering, SEKE’16, San Francisco, USA, pages 1–4, 2016. [12] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008. [13] Michael McCandless, Erik Hatcher, and Otis Gospodnetic. Lucene in Action: Covers Apache Lucene 3.0. Manning Publications Co., 2010. [14] Gregg Rothermel and Mary Jean Harrold. Analyzing regression test selection techniques. IEEE Trans. Softw. Eng., 22(8):529–551, August 1996. [15] Gregg Rothermel and Mary Jean Harrold. A safe, eﬃcient regression test selection technique. ACM Trans. Softw. Eng. Methodol., 6(2):173–210, April 1997. [16] Ripon K. Saha, Lingming Zhang, Sarfraz Khurshid, and Dewayne E. Perry. An information retrieval approach for regression test prioritization based on program changes. In Proceedings of the 37th International Conference on Software Engineering - Volume 1, ICSE ’15, pages 268–279, Piscataway, NJ, USA, 2015. IEEE Press. [17] August Shi, Alex Gyori, Milos Gligoric, Andrey Zaytsev, and Darko Marinov. Balancing trade-oﬀs in test-suite reduction. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2014, pages 246–256. ACM, 2014. [18] Michael Unterkalmsteiner, Tony Gorschek, Robert Feldt, and Niklas Lavesson. Large-scale information retrieval in software engineering - an experience report from industrial application. Empirical Software Engineering, pages 1–42, 2015.']","[""TEXT-BASED TEST CASE SELECTION EVALUATED BY CODE COVERAGE This  briefin  reports  scieitfc  evideice oi evaluatin  a tool for renressioi test case selectoi usiin a ioi-iistrumeeited code  coverane  aialysis  based  oi emepirical research experimeeits. FINDINGS \uf0b7 The fndings presented in this briefng show  the  evaluaton  of  the AutoTestPlan  tool  compared  with  a test architect using code coverage as a metric Selector TCs Coverag e Failure Architec t 120 51.74% 0 ATP 62 51.74% 0 Table 1: 1st experimeeit \uf0b7 Both  executons  (architects  and  ATP) had  the  same  code  coverage  of 51.74%, but the ATP's test plan is about 50%  smaller  than  that  proposed  by architects Selector TCs Coverag e Failure Architec t 175 4% 9 ATP 166 4% 13 Table 2: 2id experimeeit \uf0b7 ATP has shrunk the 175 test cases into 166 test cases while preserving its code coverage  of  4%  and  increasing  the number  of  failures  found.  The  main reasons associated to this are: (i) the test cases are somewhat vague, leaving the  tester  free  to  choose  some variatons  in  the  smartphone interacton; (ii) the architect's selecton was  executed  in  India,  and  ATP's selecton was executed in Braziil. Selector TCs Coverag e Failure Architec t 116 53.45% 0 ATP 108 52.07% 0 Table 3: 3rd experimeeit \uf0b7 Once again the coverage was too close: 53.45% from the architects and 52.07% from ATP.  Selector TCs Coverag e Failure Architec 116 53.45% 0 t ATP 211 58.57% 7 Table 4: 4th experimeeit \uf0b7 We used ATP directly in the whole test database  of  our  industrial  partner, related to this product. In this fourth experiment,  we  did  not  use  a preselected  test  cases  subset  (called master plans) as previous experiments \uf0b7 With 211 selected test cases, ATP got a code  coverage  of  58.57%.  This  was 5.12% superior to architect’s selecton, and it was  enough to ATP's  test plan reveal 7 new failures \uf0b7 We noted that we could never have a plan  with  greater  coverage  than  the master plan because the master plan is always fully executed by our industrial partner. \uf0b7 There is no further selecton from test architects.  Even  though,  ATP  has provided  a  close  code  coverage  and with fewer test cases \uf0b7 The  second  experiment  was  a  very partcular  situaton  that  currently should not occur in practce. We tried to observe what happens if we use a shorter (compared to current practce) tme period to perform a regression \uf0b7 By deciding to avoid having the master plan as a preliminary flter in our fourth experiment: o Our atempt to increase code coverage  beyond  the  master plan  was  not  so  impressive because we had only a 5.12% gain in code coverage. o Our  atempt  to  eventually fnding  bugs  was  very interestng because we found 7  new  unreported  failures with this small improvement      Keywords: Informaton Retrieval; Test Case selecton and  prioritziaton; Code coverage; Who is this briefin  or? Software engineering  practtoners who want to make  decisions about regression test  case selecton based on scientfc evidence. Where the fidiins come  rom? All fndings of this briefng were  extracted from the empirical  research conducted by Claudio  Magalhães et al.   What is iicluded ii this briefin? Briefng about evidence and the  main fndings of the study  conducted. What is iot iicluded ii this briefin? Detailed descriptons of what  was done and how the studies  were done.""]","**Title:** Enhancing Test Case Selection for Software Changes Using Text-Based Analysis

**Introduction:**  
This briefing presents findings from a study that evaluated an automatic test case selection process based on change requests and non-instrumented code coverage analysis. The aim is to provide insights into how this approach can improve testing efficiency and effectiveness, particularly in environments where manual testing is prevalent.

**Core Findings:**  
The research focused on the development of a tool named AutoTestPlan (ATP) that automates the selection and prioritization of test cases based on change requests (CRs). Key findings include:

1. **Non-Instrumented Code Coverage:** The study introduced a novel method to measure code coverage of Android applications without requiring code instrumentation. This was achieved using a tool called AutoTestCoverage (ATC), which dynamically monitors code execution during testing.

2. **Effective Test Case Selection:** ATP demonstrated the ability to select a smaller set of test cases while maintaining comparable code coverage to that achieved through manual selection by test architects. In one experiment, ATP selected 62 test cases, achieving 51.74% code coverage, which was the same as the architect's selection but with 50% fewer test cases.

3. **Varying Results Across Experiments:** While ATP produced similar coverage percentages to manual selections, the results varied across experiments. In some cases, ATP found additional bugs not identified by the manual process, suggesting that its automated selection process can uncover issues that human selection may overlook.

4. **Need for Updated Test Cases:** The overall code coverage achieved in the experiments was relatively low, indicating that the company’s test database requires more frequent updates to ensure comprehensive testing of modified code.

5. **Trade-offs in Test Selection:** The study highlighted a trade-off between the number of test cases and the effectiveness of coverage. While ATP can reduce the number of tests needed, ensuring that the selected tests are relevant to recent changes remains critical for effective testing.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, test managers, and quality assurance professionals involved in software testing and test case management, particularly in agile and iterative development environments.

**Where the findings come from:**  
The findings are based on experiments conducted in collaboration with Motorola Mobility, involving real-world testing scenarios and a comparative analysis of test case selection strategies.

**What is included in this briefing?**  
This briefing includes an overview of the automatic test case selection process, the impact of using non-instrumented code coverage analysis, and the implications for improving testing efficiency and effectiveness.

**What is NOT included in this briefing?**  
This briefing does not include detailed statistical metrics or exhaustive discussions of the experimental setup and results. It focuses on summarizing key insights and practical implications for practitioners.

To access other evidence briefings on software engineering:  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

For additional information about the research team and their work:  
[Centro de Informática, Recife, Brazil](http://www.cin.ufpe.br)

**Original Research Reference:**  
Claudio Magalhães, João Andrade, Lucas Perrusi, Alexandre Mota. ""Evaluating an Automatic Text-based Test Case Selection using a Non-Instrumented Code Coverage Analysis."" In Proceedings of the SAST, September 2017, Fortaleza, Brazil. DOI: [https://doi.org/10.1145/3128473.3128478](https://doi.org/10.1145/3128473.3128478)"
"['Aspects Influencing Feature-Oriented Software Comprehension Observations from a Focus Group Alcemir Rodrigues Santos∗ Universidade Federal da Bahia Salvador, Bahia alcemirsantos@dcc.ufba.br Ivan do Carmo Machado Universidade Federal da Bahia Salvador, Bahia ivanmachado@dcc.ufba.br Eduardo Santana de Almeida Universidade Federal da Bahia Salvador, Bahia esa@dcc.ufba.br ABSTRACT Feature-oriented software development has been considered as a reasonable way to address the ever increasing need of handling variability in software systems. However, we still lack to understand the influence of the use of different ways to implement variability on program comprehension and consequently on the effort they demand from developers, so they could successfully accomplish the assigned maintenance tasks. This paper addresses it presenting a qualitative study carried out as an focus group aimed at identify fac- tors influencing program comprehension. The findings of our study were grouped into four groups, including the followed strategies to understand feature-oriented software, the factors hindering and easing such understanding, as well as general observations on the effort demanded to maintain software either using Conditional Compilation or FeatureHouse. CCS CONCEPTS • Software and its engineering → Software product lines ; Maintaining software ; KEYWORDS Variability Representations, Program Comprehension, Conditional Compilation, FeatureHouse ACM Reference format: Alcemir Rodrigues Santos, Ivan do Carmo Machado, and Eduardo Santana de Almeida. 2017. Aspects Influencing Feature-Oriented Software Compre- hension. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017, 10 pages. https://doi.org/10.1145/3132498.3133838 1 INTRODUCTION Several options are available to use in feature-oriented software development (e.g., annotations, aspects, deltas, refinements, colors) [1]. Some of them are popular in industry, other are not. Regardless of their popularity, researchers have addressed the existing draw- backs to improve adoption and ease their use. However, despite ∗Corresponding author. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5325-0/17/09. . . $15.00 https://doi.org/10.1145/3132498.3133838 of the importance given to program comprehension in the litera- ture, little has been done concerning the influence of variability representations on feature-oriented software comprehension. Although some work have been done in particular mechanisms [5, 13], to the best of our knowledge, only two works [2, 12] had studied such topic since the first pilot [15]. However, Siegmund et al. addressed whether colors would provide better support to imple- ment variability instead of #ifdef annotations, where they found that colors could yield better results under some reservations. In turn, our previous work [12] addressed the implications on concept location tasks. Thus, there is a lack of studies in such a topic. This paper reports the fourth experimental study of a family [11] on feature-oriented software comprehension. The first study of the family addressed the concept location problem considering JavaScript systems [12]. Such study found no statistical difference in terms of response time and correctness of the answers regard- less of the used variability representation. The other two studies were replications from the Siegmund et al. [15] pilot that compared the effort of feature-oriented software comprehension while devel- opers addressed code written in Java using either Conditional Compilation or FeatureHouse. Again, no statistical difference', 'the effort of feature-oriented software comprehension while devel- opers addressed code written in Java using either Conditional Compilation or FeatureHouse. Again, no statistical difference was found regardless of the used variability representation. The present study gathered evidence from the point of view of a qualita- tive study (focus group [14]) on the aspects driving the developers in the process of understanding variable code implemented us- ing Conditional Compilationand FeatureHouse, instead of a quantitative study (controlled experiments). Such difference in the studies nature allows the researcher to make more flexible obser- vations without compromise rigor of the research being done. In addition to that, given the lack of studies in the topic, we expect our observations serve the purpose of pointing out aspects worth further investigations. The remainder of this paper is organized as follows. Section 2 presents related work. Section 3 presents the planning and the execution of the study. Section 4 presents the data collection pro- cedures. Section 5 presents and discusses the results of the study regarding the individual feedback collected from the participants. Section 6 discusses the participants’ answers to the focus group questions. Section 7 discusses the research questions in the light of the gathered data. Section 8 presents the threats to validity iden- tified during the evaluation. Finally, Section 9 draws concluding remarks. 2 RELATED WORK This section presents work we deemed as related to ours [12, 15]. Both work compared comprehension aspects between different variability representations. Siegmund et al. [15] work was the first', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil A. R. Santos et al. to investigate the influence of Conditional Compilationand FeatureHouse on program comprehension. Although preliminary, such a pilot study serves as the basis for further empirical work on the topic. They called Santos et al. [12] analyzed the influence of RiPLE-HC (a feature-oriented approach) in concept location tasks in JavaScript systems. The yielded results showed no additional effort when variability was introduced in the software development. None of them addressed the topic in a qualitative approach. Another set of related studies has focused on program com- prehension from the perspective of a given particular variability implementation mechanism. [ 2, 5, 7, 8, 13]. Siegmund et al. [ 2] carried out an empirical evaluation to analyze the effectiveness trade-off of using colors (instead of using preprocessing annota- tions). The study showed the use of colors could be lead to better results, in terms of program comprehension. However, there are some important drawbacks, such as the caution regarding color blindness. Schulze et al. [13] and Malaquias et al. [5] addressed the importance of the discipline of annotations reaching contrasting results with the most recent one claiming the undisciplined annota- tions should not be neglected. Melo et al. [7] showed the speed of bug finding decreases linearly with the degree of variability, while effectiveness of finding bugs is relatively independent of the degree of variability. Moreover, Melo et al. [8] showed that the presence of variability correlates with an increase in the number of gaze transitions between definitions and usages for fields and methods. Our work has a different purpose from the above work that provide researchers and practitioners, at least at some extent, with a definitive the answer to the addressed research question. Given the lack of research on the topic, we expect the answers to our research questions provide the research community with a considerable amount of aspects influencing feature-oriented comprehension that are worth further investigation. 3 STUDY SETTINGS This study was carried out as a focus group [ 14] after the partic- ipants perform tasks involving data-flow and feature precedence comprehension. This section presents the defined research ques- tions, the planning, and the execution of this experimental study. 3.1 Planning The planning of this study concerns its experimental design, the selection of participants, the performed tasks, and the supporting material available to the participants during the experiment session. Next, we detail each of them. 3.1.1 Design. The design of this study was a mix of controlled experiment with a focus group [14]. More specifically, the empirical study was performed in three phases: Phase 1 where the participants performed three technical tasks using Conditional Compilationand FeatureHouse (the tasks are described later). This phase serves the pur- pose of familiarizing the participants with the novelty of the FeatureHouse and its capabilities. Phase 2 where they fulfilled a feedback form. This phase serves the purpose of getting an individual perception of the participants. We thought this might be needed in case some of them stay speechless during the follow-up phase, what actually happened. It also worth notice the size of individual focus groups can vary from 3 to 12 participants [14]. Phase 3 was the actual execution of the focus group. The idea behind the focus group is to identify aspects impact- ing the comprehension of both variability representations (Conditional Compilationand FeatureHouse) through the coding of the dialogs’ transcriptions. In total, 10 graduate students took part in our experimental ses- sion. All of them were recruited from a Software Reuse course, which is regular course in the Federal University of Bahia’s Com- puter Science Graduate program. Each participant worked in an', 'sion. All of them were recruited from a Software Reuse course, which is regular course in the Federal University of Bahia’s Com- puter Science Graduate program. Each participant worked in an individual workstation until they accomplishedPhase 2. Afterwards, we brought them together to carry out Phase 3. 3.1.2 Target System. MobileMedia [3] was used for the techni- cal tasks of Phase 1. It is a small system written in less than 4 KLOC of Java code. More specifically, we used two versions available in the literature, one implemented using Conditional Compila- tion [3] and refactored version to FeatureHouse [15]. Although the variability points are the same in both versions (i.e., they are equivalent), their feature model are different. 3.1.3 Tasks. Table 1 describes each of the tasks performed by the participants in Phase 1. Table 1: Tasks defined for MobileMedia in the focus group session. Tasks Description Task 1 Find the exact place – Class, Line of Code, (and Feature folder for the FeatureHouse tasks) – where the video controller is being initialized in both implementations (Conditional Compilationand FeatureHouse); Task 2 Analyze the source-code in order to identify which features must be selected in order to make this part of the code (video controller initialization) available in a final product; Task 3 Is there any precedence between/among the features? Please, justify. Each participant performed each task twice, both in the Condi- tional Compilationand the FeatureHouse version of the target system. They took around 50 minutes to finish the tasks. The tasks were designed to force the participants to understand the source- code as a whole, including aspects of data-flow, which was needed to perform the Tasks 2 and 3. Task 1 is the easiest one and it was used to allow the participants to familiarize with the system. 3.2 Research Questions In the following, we present the research questions guiding our experimental study. We do not state any hypotheses, since our goal was to gather evidence from a qualitative standpoint. In this sense, no metric was previously defined, as the data shall be gathered from the coding of the textual transcripts from the focus group interviews and the applied feedback questionnaire. Next, we describe each research question.', 'Aspects Influencing Feature-Oriented Software Comprehension SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil RQ: What aspects do impact the developers comprehension of variability implementation in the maintenance of feature- oriented software? This research question aims at covering all aspects to be con- sidered while analyzing the comprehension bottlenecks involved in the maintenance process. In order to go deeper in the analysis of such aspects, we split this general question into three research questions. RQa : How do developers approach variability implemen- tation comprehension? Rationale: since the code is rearranged from Conditional Com- pilation to FeatureHouse and there is a different set of tools available for each of them, we believe developers might have to change their strategy to address unfamiliar code in the different variability representations. RQb : Which aspects hinder the comprehension of vari- ability implementation? Rationale: given the different nature of how developers program either in Conditional Compilationor FeatureHouse, in this question, we investigate specific factors from either of them hin- dering comprehension. RQc : Which aspects ease the comprehension of variabil- ity implementation? Rationale: conversely, the differences in the programming models may incur in different positive factors, each. In this question, we look for those aspects playing in favor of the variability representa- tion. 3.3 Execution In the experimental session, we took around 35 minutes to intro- duce the study and lay the foundations of the study to participants, including study goals and procedures. Next, in the Phase 1, we de- scribed the tasks they had to perform and defined about 40 minutes as the desired finishing time. The participants inspected both ver- sions of the systems then provided their answers in a form. In fact, we did not use this as a hard deadline and some participants took a couple of more minutes to finish and proceed with Phase 2. In the Phase 3, we met all participants in a room for about 40 minutes. Each participant was free to talk or not without limit to each speech and were allowed to left the activities room without finishing it or say nothing during the focus group session at anytime. 3.3.1 Subjects Characterization. We followed the questionnaire proposed by Siegmund et al. [16] to characterize the subjects pro- gramming experience, including programming languages and large software projects experience. The Software Reuse course all par- ticipants were enrolled includes the implementation of software product lines in the syllabus, therefore, we did not include questions regarding this topic. We asked the participants to compare themselves against their group-mates and professional developers with 20 years of experi- ence. This comparison shows approximately one third the partic- ipants see themselves as less or more experienced as their mates and 40% as equally experienced. In addition, only 10% self-assessed as more experienced than professional developers and 20% as much experienced as them. 3.3.2 Support Material. We decided to provide the complete FeatureIDE (v3.1.1 ) [4] and Eclipse (v4.5.2) infrastructure to the participants during the experiment session. This fact contributes to the observation of the actual impact of available tools in the participants comprehension tasks. The participants also had the feature model of both versions (Conditional Compilationand FeatureHouse) in hands while they analyzed the target system during the session. 3.3.3 Pilot. The original idea was to carry out a (quasi-) exper- iment [17] addressing data-flow tasks. In this sense, in order to balance the study in a way we could extract the best from it, we performed a pilot with three Master students from theTechnical School of Würzburg-Schweinfurt (FHWS) located in Germany, while they were working at UFBA in a research collaboration project', 'performed a pilot with three Master students from theTechnical School of Würzburg-Schweinfurt (FHWS) located in Germany, while they were working at UFBA in a research collaboration project between both institutions. The german students had only superfi- cial knowledge of SPL engineering, conditional compilation, and FeatureHouse in opposition to the background of those recruited from the Software Reuse course. From our previous experience with program comprehension experimentation, we found the tasks could be too hard even for the actual participants, what could lead the study to a failure. Therefore, in the actual session, we decided to use the tasks only as a familiarization with the MobileMedia code and to add a new phase, the focus group. In addition, they contributed with a list of factors with influence in their activities during the assigned tasks, which we considered while refining the study and helped in the construction of the individual feedback form with some items. 4 DATA COLLECTION This section presents how we collected the data to proceed with the analysis. As we anticipated in the design session, we collected data by three different ways: (i) the answers form in the Phase 1; (ii) the feedback form in the Phase 2; and (iii) the transcription of the focus group available in Portuguese language in the project’s website1. However, as we used Phase 1 as familiarization, we focus on the data collected in the last two phases. 4.1 Individual Feedback Collection This section describes how the feedback was individually collected from the participants after the tasks. One of the primary reasons we considered to construct such a questionnaire is that we anticipated some of the participants would have little contribution during the focus group session. In this sense, we designed one question –What was the influence of the following factors on the program comprehen- sion? – with a set of pre-defined impact factors concerning each of the paradigms addressed in this task (Conditional Compilation and FeatureHouse). Additionally, we asked two questions to an overall evaluation of how hard it was for them to perform the tasks. Table 2 shows the questions regarding the pre-defined factors concerning theConditional Compilationand theFeatureHouse paradigms, respectively. All the questions were rated in a five-point Likert-scale [10] with values ranging from “too easy/no impact” to “too hard/high impact”. We summarized these factors based on two main sources of evidence. The first, the experience acquired by 1http://rise.com.br/riselabs/vicc', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil A. R. Santos et al. the first author of this paper while refactoring the Conditional Compilation version of RiSEEvent [9] to FeatureHouse. The second, the evidence raised in the previous experimental studies [11]. Table 2: Predefined impact aspects used for the participants individual feedback. Conditional Compilation (CC-Q1) Amount of variation points; (CC-Q2) Scattered varia- tion points; (CC-Q3) Logical expressions in the variation points; (CC-Q4) Long fragments of variable code; (CC-Q5) Data-flow; (CC-Q6) Tool support. FeatureHouse (FH-Q1) Class refinements; (FH-Q2) Source-code organiza- tion; (FH-Q3) Features precedence; (FH-Q4) Data-flow; (FH-Q5) Method overloading; (FH-Q6) Method refinements; (FH-Q7) Tool support. 4.2 Focus Group Data Collection This section describes the questions used to collect data during the focus group session. Table 3 shows the questions. They are both closed- and open-ended questions. Since closed-ended questions lack of stimulus for the participant to give a long and elaborated answer, we did not include them among the questions while planning the questionnaire. However, they did happen in the focus group session to complement the open-ended questions during the discussion. We transcribed the audio recording of the focus group session to resort on the coding technique. More specifically, we used two cod- ing levels [14]: (i) open coding – where the main rule is to segment the transcription text into similar groupings in order to identify pre- liminary categories of information about the phenomenon under analysis; (ii) axial coding – the axial coding allows the researcher to concatenate similar ideas concerning specific aspects under a more general statement or concept/themes. 5 INDIVIDUAL FEEDBACK RESULTS This section presents the raw data and discuss the individual feed- back of the participants collected with the feedback form in the experimental study. 5.1 Influence Drivers Figure 1 summarizes the subjects rating of the impact the predefined factors detailed previously (Table 2). We can identify the rating for each factor in the figure by the identifiers from these tables (ID). The bars are ordered in decreasing order of impact, i.e., the predefined factors with highest impact comes first. If we look to each paradigm individually, we can see the “Long fragments of variable code” (CC-Q4) was rated as the most im- pactful factor among those from the Conditional Compilation paradigm, whereas the “quantity of class refinements” (FH-Q1) is by far the most impactful among those from the FeatureHouse paradigm. On the other hand, the “Tool support” for Conditional (a) CC 0% 10% 20% 30% 40% 50% 60% CC-Q1 CC-Q2 CC-Q3 CC-Q4 CC-Q5 CC-Q6 Amount\tof\tParticipants No\timpact Few\timpact Indiferent Cosiderable\tImpact High\tImpact (b) FH 0% 20% 40% 60% 80% FH -Q1 FH -Q2 FH -Q3 FH -Q4 FH -Q5 FH -Q6 FH -Q7 Amount\tof\tParticipants No\timpact Few\timpact Indiferent Cosiderable\tImpact High\tImpact Figure 1: Subjects rating of the predefined aspects impact. Compilation showed up with the lowest ratings, whereas only 10% of the participants found the FeatureHouse “Code organization” a factor of severe impact. In fact, the participants rated the influ- ence of most factors listed in the FeatureHouse tasks mostly as indifferent, with the exception of “Code organization” (FH-Q2) and “Tools support"" (FH-Q7), which most of them rated with few to no impact in the comprehension. 5.2 Tasks’ Difficulty Perception We relied on two general questions to assess the overall difficulty of the executed tasks: (i) “Generally speaking, how difficult were the tasks?” and(ii) “Generally speaking, how difficult was to understand the system source-code?”. Figure 2 shows the answers regarding each variability representation (Conditional Compilationand FeatureHouse). Their answers showed an equivalence in terms of difficulty. In addition, half of the participants found the tasks to have', 'each variability representation (Conditional Compilationand FeatureHouse). Their answers showed an equivalence in terms of difficulty. In addition, half of the participants found the tasks to have usual difficulty both in a general way or in terms of comprehension effort. 6 FOCUS GROUP QUESTIONS ANSWERS In this section, we proceed with the analysis of the focus group session transcriptions. The focus group questions can be divided in four main group of questions: (i) the questions concerning the strategies adopted by the software engineers while addressing code usingConditional Compilationand FeatureHouse; (ii) the ques- tions concerning the factors hindering the best performance of the software engineers; (iii) the questions concerning factors easing the maintenance work with each variability representation; and(iv) the general observations questions. Tables 4 and 5 enumerate all positive and negative points identified from the focus group partici- pants’ speeches. The “Category” column in the tables categorizes', 'Aspects Influencing Feature-Oriented Software Comprehension SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Table 3: Focus group questions. Order Questions Group Question 1 What were your first impressions of each paradigm? General Observations Question 2 What could make you more effective in your tasks? Facilitators Factors Question 3 What were your feelings about each paradigm? Fatigue, tiredness, paradigm preference. Hindering Factors Question 4 What tool or strategy have you adopted during the tasks to comprehend unfamiliar code? Have you changed the strategy to address the FeatureHouse code? Maintenance Strategies Question 5 The worse thing about Conditional Compilationis. . . Hindering Factors Question 6 The worse thing about FeatureHouse is. . . Hindering Factors Question 7 The best thing about Conditional Compilationis. . . Facilitators Factors Question 8 The best thing about FeatureHouse is. . . Facilitators Factors Question 9 While using Conditional Compilation, the first action I took was. . . Maintenance Strategies Question 10 While using FeatureHouse, the first action I took was. . . Maintenance Strategies Question 11 Would you like to elaborate a bit more about the maintenance difficulty of each approach? General Observations Question 12 Have you noticed precedence among features, regarding the methods refinements? General Observations (a) Understanding 0% 10% 20% 30% 40% 50% 60% Too\t Easy Easy Usual\tDifficulty Hard Too\t Hard Amount\tof\tParticipants FH IFDEF (b) Difficulty 0% 10% 20% 30% 40% 50% 60% Too\t Easy Easy Usual\tDifficulty Hard Too\t Hard Amount\tof\tParticipants FH IFDEF Figure 2: Answers to the questions to assess the overall dif- ficulty of the tasks and their understanding of the source code. each point as aspects inherent to the variability representation (“Inherent”), related to tools available (“Tools”), or related to the participants’ background knowledge. We discuss the more impor- tant of them in the following while presenting the answers to the focus groups questions. 6.1 Group 1: Maintenance Strategies Three questions are in this group: questions 4, 9, and 10. Each of these questions concern the way developers handle the compre- hension of unfamiliar source code. Depending on the strategy they used to accomplish their task, different kind of tools would yield dif- ferent results in terms of effectiveness. Next, we discuss the finding of each of these questions. Question 4 explicitly asked the participants about the tools and strategies they adopted while addressing the comprehension tasks. In addition, we asked whether they had to change their usual strat- egy to address unfamiliar code, because of the implementations using Conditional Compilationand FeatureHouse. Questions 9 and 10 asked the participants what were their first attitude to comprehend the source code using eitherConditional Compi- lation or FeatureHouse, respectively. The idea was to identify whether the first action follows a different pattern depending on the variability representation. We identified mainly two groups of participants. Those who relied only on the search tools and those who resort to additional tools. In the first group, a participant stated to have addressed both versions code with the same strategy, as shown by the following quote: “ I ended up by using the same strategy I used in the other. (. . . ) To search and than identify the artifacts. I repeated for both. ”Other two participants of this first group made use of their pre- vious game development experiences, which can be seen in the following quote: “ I have already worked a bit with game development, so it was straightforward to me to search for an “init” method. In the way that, in the place I found a variable declaration with this method call, than it should be the one I was looking for. It means I did not look to feature or other stuff, just looked for the declaration. ”On the other hand, participants of the second group approached', 'the one I was looking for. It means I did not look to feature or other stuff, just looked for the declaration. ”On the other hand, participants of the second group approached the code using different strategies. While using Conditional Com- pilation, they relied on the search tools only, whereas they resorted', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil A. R. Santos et al. Table 4: Positive points identified in the speeches. Description Category Conditional Compilation Reading the code is enough to understand the variability. Inherent It is good in cases of several variation points in the same file. Inherent The search is enough to find the feature annotations. Inherent Background knowledge was useful to the working strat- egy. Background Modularization and architecture may alleviate the anno- tation scattering problem. Practices The programming model is simple. Inherent Good programming practices may also alleviate the an- notation scattering problem. Practices FeatureHouse Variability, constraints, and configurations visualization are straightforward. Tools The configuration management tool had some use to un- derstand features constraints and boosted effectiveness. Tools Background knowledge was useful to the working strat- egy. Background The configuration management helps in the troubleshoot- ing and debugging. Tools Collaboration diagram is faster than search for annota- tions. Tools Valid configurations visualization. Tools Code readability (i.e.clean code) Inherent It is better than Conditional Compilationin cases of scattered variability. Inherent The visual organization is interesting. Inherent Collaboration diagram provides a good overview about each feature. Tools Code obfuscation is not a problem. Inherent FeatureIDE views are useful to novice programmers. Tools It has good traceability. Inherent To locate the maintenance-target code for maintenance seems straightforward. Inherent The feature code traceability can produce gains in the development. Inherent The error propagation seems to be lower than Condi- tional Compilationdue the feature-based modulariza- tion. Inherent The collaboration diagram is more comfortable to work with than a traceability matrix. Tools of the feature model/configuration management and the collabo- ration diagram tools to solve the tasks with FeatureHouse. One possible reason is that some developer are used to explore their options while maintaining software, while others prefer the search. The quote below shows the strategy of one of the participants. Table 5: Negative points identified in the speeches. Description Category Conditional Compilation Code obfuscation is a huge problem (Excessive amount of annotation). Inherent Lack of visualization tools supporting annotations (e.g., col- ors, block folding) Tools Lack of traceability tools (justified with lack of training) Tools Maintenance of Conditional Compilationmight be bot- tleneck for both large and small systems. Inherent Annotations turns the maintenance harder. Inherent The use of feature tags in comments hinder comprehension (e.g., searching process) when the annotation mechanism is not a native construct, such as in Java. Inherent Object-orientation (OO) does not require a feature-oriented mindset, which turns up to be too flexible to implement variability and requiring additional effort from developers to handle such construct. Inherent To locate the maintenance-target code requires global search. Inherent Conditional Compilationis likely to propagate errors in the code. Inherent FeatureHouse It is a FH problem the amount of clicks to reach the source code files. Inherent Lack of traceability tools (justified with the lack of training). Tools Lack of visualization tools support. Tools Too much duplicated classes. Inherent It is hard to keep a big picture of the implementation (e.g., memory allocation handling, method overriding.) Inherent It is required to understand the composition problem in order to comprehend how the variability is being implemented. Inherent Feature precedence is harder to notice for novices. Inherent Big effort needed to manage the shared resources. Inherent “ As soon as I discovered the FeatureHouse had that “small tree” (a reference to the FeatureIDE configura-', 'Big effort needed to manage the shared resources. Inherent “ As soon as I discovered the FeatureHouse had that “small tree” (a reference to the FeatureIDE configura- tion management editor.), I preferred to create a config- uration, to unselect all features non-mandatory, than I verified, selected the features I did not want and saw that everything built automatically. This made me solve my problem infinitely faster. ”It is worth notice that it isFeatureIDE who provides developers with several tools to support development with FeatureHouse. Such tools include the feature model, the configuration manage- ment (the small tree mentioned by the participant), the collaboration diagram, and others.FeatureHouse is only the mechanism behind the features composition. This fact reinforces the importance of tools while working with this variability representation, which consider as a valid and important research direction. Regarding their first action, we did not identify any different behavior other than the use of the global search tool by the partici- pants addressing Conditional Compilationcode. On the other', 'Aspects Influencing Feature-Oriented Software Comprehension SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil hand, we identified that some participants prefer to use first the col- laboration diagram, others the feature model [1] – the de facto SPL modeling mechanism – while using FeatureHouse. None of the participants mentioned the change of strategies to address unfamil- iar code. In fact, those participants who used to rely on search tools stick to them, whereas those who always consider the additional tools available also retained their usual behavior strategies. 6.2 Group 2: Hindering Factors Three questions are in this group: questions 3, 5, and 6. These questions aim at the identification of aspects that might hinder the comprehension of the source code. It is important to distinguish aspects of each variability representation hindering comprehension, because it would be possible to mitigate the effect of such factors in the daily activity of developers. Question 3 asked the participants to elaborate on the different feelings (e.g., Fatigue, tiredness, variability representation prefer- ence) during the execution of the technical tasks using each of the variability representations. Question 5 and 6 asked the participants about the worse thing they found while working either withCondi- tional Compilationor FeatureHouse during the comprehension tasks, respectively. The following quote regarding the superiority ofFeatureHouse concerning to the maintainability of the addressed variability rep- resentations points to the overall sentiment of most of the partici- pants: “ I have the feeling that it is unanimous that Feature- House is better than Conditional Compilation. By thinking on the maintenance of that Conditional Compilation code, it would be a headache to me. ”Later, the same participant claimed the maintenance issue would appear regardless of the system’s size. Although none of the other participants raised objections, some raised arguments in favor of a balance between both Conditional Compilationand Feature- House highlighting the importance of the best use of each one, as the following quote shows: “ The Conditional Compilationcode is highly obfus- cated. However, if you use the search tool to look for annotations it makes your life easier. You go straight to the point. You click Find and all the annotations you expect to find will be in your hands. At the same time, when using FeatureHouse you can identify them in the (collaboration) diagram. You go faster to the right point. Therefore, I believe you need to know how to use the available resources. It is a balance. ”Regarding the hindering aspects themselves, the participants mostly mentioned factors from the Conditional Compilation representation. Besides the hindering aspects already known (Code obfuscation,Excessive amount of annotations ), we also identified the following factors. The use of feature tags in the comments: according to the participants, this practice can both cause misunderstandings and turn the search for annotations in time of maintenance harder. This is aJava specific issue due the non-native use of annotations through Antenna and similars. For instance, in C/C++, this issue may happen only in the case that comments contain feature TAGs. The lack of feature-orientation mindset: according to them, differently from the FeatureHouse, the OO paradigm does not force the software engineer to think in terms of features, which requires additional effort for them to handle such additional construct. Regarding the hindering aspect identified in the FeatureHouse variability representation, we extracted the following from partici- pants observations. There is too much duplicated classes: according to them, the amount of classes with the same name turns it harder to un- derstand and follow the code. For instance, it is hard to keep track memory allocation and deallocation –, which may not', 'amount of classes with the same name turns it harder to un- derstand and follow the code. For instance, it is hard to keep track memory allocation and deallocation –, which may not be a problem in Java, but certainly is in C/C++ – or method overriding. It is hard to keep a big picture: according to them, still the number of duplicated classes, in conjunction with the amount of clicks needed to get to the actual code in the features folder hinder comprehension and consequently the execution of maintenance tasks. 6.3 Group 3: Facilitators Factors Three questions are in this group: questions 2, 7, and 8. These questions aim at the identification of aspects that might ease the comprehension of the source code. It is important to enumerate the aspects of each variability representation hindering compre- hension, because it would be possible to use them as inspiration to improve the supporting tools of the daily work of developers. Question 2 asked the participants about what could make their work more effective while using eitherConditional Compilation or FeatureHouse. In addition, the questions 7 and 8 asked them about the best thing they found about working with Conditional Compilation and FeatureHouse, respectively. The participants pointed out the lack of tools supporting anno- tations as a set of effective-oriented enhancements that might help in the comprehension and consequently in the maintenance activi- ties. Among these tools, the participants mentioned visualization tools and also tools to help with the identification of annotations of interest. “ A software visualization tool for this (to be more effec- tive) would help. For both variability representations. ”Additionally, the available tools are dependent on the IDE and the participants who do not use it in daily work had difficulties to address their tasks. “ For me, to both variability representations, as I do not use Eclipse, I found difficulties to use the tools functionalities and to look for the files. ”', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil A. R. Santos et al. 6.4 Group 4: General Observations Three questions are in this group: questions 1, 11 and 12. Question 1 asked the participants about their first impressions of each variabil- ity representation, whereas the Question 11 asked the participants to elaborate a more about the difficulty to maintain the code written in each variability representation. Question 12 asked whether the participants noticed any kind of precedence among features, re- garding the methods refinements in the variability representations. Regarding their impressions, although the participants did not present any strong objections to the use of Conditional Com- pilation, it seems they would prefer to use the FeatureHouse to implement variability. They claimed the tool support available to FeatureHouse helped them to better approach the tasks. The following quotes show such a feeling: “ The variability representation withConditional Com- pilation is interesting when we have a concentration of variation points inside a single source code file. (. . . ) However, when the variability starts to be scattered, then the FeatureHouse seems to be interesting, be- cause we can easily see the project structure. The bad side is the many clicks we need to reach the source code files. ” “ There is also that tool, which shows the feature “Base” and where it was implemented. . . [Collaboration Dia- gram] Exactly. I think it helps to get an overview of how the feature is implemented. On the other hand, while using Conditional Compilationit just use the search to find the annotations, which is also in- teresting and you can also walk through the source code. ”Regarding maintainability (Question 11), the participants pointed out the benefits of the improved traceability of FeatureHouse in comparison with the Conditional Compilation. Besides, the fact that the development team may change constantly, according to the participants this quality of FeatureHouse would be benefi- cial in such situations, which demands maintenance knowledge management – usually accomplished with the traceability matrix. “ Traceability is the biggest need. Because something that worries me is how scattered is the source code, since the more scattered the code, the bigger the error proneness of the source code. [WithFeatureHouse,] I would go straight to that package and perform the needed changes. ”Moreover, none of the participants claimed to have perceived the existing precedence among features in FeatureHouse in binding time. In fact, one of them clearly stated he did not perceive, instead he pointed out to potential issues while programming with Fea- tureHouse. As in FeatureHouse several duplicated classes might exist throughout the entire project, the management of shared re- sources (e.g., memory) might be problematic, which we discussed next. 7 ANALYSIS OF FINDINGS In this section, we discuss our macro research question by answer- ing their sub-questions. This can help future research to pursue the improvement of processes and tool support for maintenance tasks of software in the presence of variability. To this end, we take into consideration the raw data presented in the previous sections and the possible implications of the identified influence factors on program comprehension to future research. RQa : How do developers approach variability implemen- tation comprehension? According to the presented answers for the questions of the Group 1, participants relied on the search tools regardless of the variability representation. More specifically, global search to find all the Conditional Compilationannotation tags and local search inside the files of the target feature folder. In addition, we can point out that they also made use of visualization tools available in the FeatureIDE to support FeatureHouse development, such as the “Collaboration Diagram”, the “Feature Model"", and the “Configura-', 'out that they also made use of visualization tools available in the FeatureIDE to support FeatureHouse development, such as the “Collaboration Diagram”, the “Feature Model"", and the “Configura- tion Management”. Moreover, the background knowledge of each participant has heavily influenced in the way they approached the tasks. While some participants with game development background just searched for similar keywords, the one with traceability matrix experience preferred to use the collaboration diagram. RQb : Which aspects hinder variability implementation com- prehension? According to the presented answers for the questions of the Group 2, the factors making the tasks accomplishment harder are different regarding the variability representations. InConditional Compilation, the scattering of feature annotations, the code ob- fuscation caused by the excessive amount of annotations, as well as the use of annotation tags inside the comments are the most mentioned factor hindering variability comprehension. Conversely, in the FeatureHouse, the number of occurrences of the dupli- cated classes and the amount of clicks needed to reach the source code files were the ones more emphasized as the factors hindering program comprehension in this variability representation. In this study, tasks demanded basically comprehension effort. The focus of the participants’ complaints regarding the bottlenecks of each variability representation were the lack of proper visual- ization tools and search. Thus, they suggested an improvement in the set of visualization and search tools available. They raised these enhancements as a mean to improve their effectiveness in the maintenance tasks. RQc : Which aspects ease variability implementation com- prehension? According to the presented answers for the questions of the Group 3, the factors making the tasks accomplishment easier are also different regarding the variability representations. In Condi- tional Compilation, participants were unanimous that the search tool is the first tool they have in mind to accomplish their tasks. They also found the programming model simpler than the Fea- tureHouse option, since it was enough to read the code to become aware of its purpose. On the other hand, in FeatureHouse, the code organization was pinpointed as an interesting factor of this variability representation. Additionally, participants mentioned the', 'Aspects Influencing Feature-Oriented Software Comprehension SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil collaboration diagram and the configuration management tools highly useful to accomplish the tasks. RQ: What aspects do impact the developers comprehen- sion of variability implementation in the maintenance of feature-oriented software? Each of the sub-questions have led us to identify some of the re- ferred aspects subject of investigation under this research question. As those aspects were already discussed previously, we now focus on those highlighted in the presented answers for the questions from the Group 4. This group concerned general observations re- garding the maintainability of the code using each of the variability representations under investigation in this paper. The available tools supporting the maintenance activities have high impact on the first impressions of the participants of our focus group. By comparing Tables 4 and 5, we can notice that traceability and visualization are pointed as both positive and negative points for FeatureHouse. Regarding traceability, participants mentioned the link between the feature model and feature folders as positive, whereas others did not recall from the training session prior to the tasks and pointed to the lack of tools supporting, for instance, class refinements (i.e., duplicated classes). In fact, both visualization and traceability are closely related in the FeatureIDE, as the refine- ments links can be seen in an “Outline” visualization, but none of the participants seemed to have used it. In addition to the search tools and the feature models available of both representations, there were a couple of tools available only for FeatureHouse (e.g., collaboration diagram). What might have favored this representation against theConditional Compilation regarding the traceability of artifacts. Such difference was not a misplanned action, but rather a reproduction of the situation they would be exposed in the real world. To the best of our knowledge, apart from the configuration management used in the Linux Ker- nel build system (KBuild), which has functionalities similar to the configuration management of the FeatureIDE, there is no equiva- lent tool to the collaboration diagram or the FeatureIDE Outline2 available to Conditional Compilation. All these observed points are aspects to further investigation to better understanding the influence of such factors on the program comprehension of software in the presence of variability. 8 THREATS TO VALIDITY In this section, we discuss potential threats to the validity of this em- pirical study. We believe that presenting such detailed information may contribute to further research and replications of this study [17], which may be built upon the results presented herein. Next, we detail the main threats according to external, internal, construct, and conclusion validity. External validity . The existence of confounding parameters (e.g. such as the lack of familiarity with the IDE) may threatens this validity. We mitigate then by performing familiarization exer- cises prior to starting of Phase 1. Moreover, as a qualitative study, the results cannot be generalized to all software engineers and practitioners. However, this is the fourth study of a family of com- plementary studies providing evidence from different sources [11], 2The FeatureIDE Outline is a tool that aggregates all the duplicated classes in only one tree view. which allow us to observe and make observations based on the observations made throughout the experimentation process. In- deed, this study provides insights for further investigations in other contexts by the research community. Internal validity. There are possible threats that may happen without the researcher’s knowledge affecting individuals from dif- ferent perspectives, such as (i) the maturation and effect, which we mitigated maturation disregarding their answers to the tasks', 'without the researcher’s knowledge affecting individuals from dif- ferent perspectives, such as (i) the maturation and effect, which we mitigated maturation disregarding their answers to the tasks and focusing on their feedback and free speeches data. As for the learning effect, in this study, we were not addressing the influence of each variability representation on comprehension but aspects in- fluencing the participants’ comprehension. In this sense, we believe the analysis of one system implemented with both mechanisms does not affects our discussion because we are not interested in the result of their actions, but in the working environment aspects influencing them; (ii) the amount of tasks and its instrumentation concerning the artifacts and forms used during the study session. We mitigated the effect of the number of tasks with the use of representative ones, such as concept location, data-flow compre- hension. Moreover, we counted on the support of more experienced researchers to review the study design; (iii) fatigue – question 3 of the focus group explicitly questions about this confounding param- eter and none of the answers mentioned it, therefore we believe in the minimization of this effect. Construct validity. It might have to do with the preoperational explanation of constructs, which means that the concepts are not well defined before translated to measures or treatments. We miti- gate this threat by using the same concepts throughout the whole family of studies and using only consolidated experimental designs already used in the literature. The inability of mapping the par- ticipants speeches to their profile prevented a deeper analysis of influencing factors, which we mitigated by the contrasting their individual feedback and speeches against the findings from our previous studies on the topic. The tool support available to each variability representation might also represent a validity threat. Although the FeatureIDE provides significant support for both Antenna and FeatureHouse, the provided support may not be equivalent. In addition, confounding constructs may affect the find- ings. For instance, the difficulty of each task might have affected the participants perception of the impact of the variability represen- tations on the tasks comprehension. We believe this threat could be mitigated since participants felt mostly the same level of difficulty. 9 CONCLUDING REMARKS This section summarizes the findings of this study. We used a qual- itative method called focus group to identify potential aspects to produce initial insights and guide future research on the feature- oriented software comprehension. More specifically, to compare and understand the differences and implications of the use of vari- ability representations, Conditional Compilationand Feature- House. We could group our main findings in four main categories regard- ing the aspects that might have influence on the comprehension and consequently the maintenance of feature-oriented software. These groups are enumerated in the following:', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil A. R. Santos et al. Approach Strategies: by understanding how the software en- gineers address the comprehension of unfamiliar code can produce insights on the construction of more effective meth- ods, processes, and tools to support maintenance of feature- oriented software. Among our findings in this category are the use of tools other than the search to address unfamil- iar code. We conjecture that visualization tools, such as the “Collaboration Diagram” and the “Configuration manage- ment” can contribute to the comprehension code using both Conditional Compilationand FeatureHouse. Hindering Factors: by understanding what makes the com- prehension tasks harder, we can build tools to facilitate such an important process in the software maintenance. Among our findings in this category are issues already well known by the practitioners using annotations [ 6], the excessive amount of annotations and the highly scattering of them. Regarding the compositional approach, we found the num- ber of duplicated classes, as well as the amount of clicks to reach the source code as bottlenecks of the existing code organization tools and are worth further investigation and improvements. Facilitators Factors: by understanding what makes the com- prehension tasks easier, we can concentrate the effort on research to make these factors of some use to enhance the already available tools to comprehend software. Among our findings in this category are the simple programming model of the Conditional Compilation and the good way of code organization of the FeatureHouse. This finding cor- roborates with those in the hindering factors category, since although the participants like the way the code is organized, they were uncomfortable with the amount of clicks to get to it. General Observations: by understanding the feelings of the software engineers regarding to the first contact with un- familiar variability representations, we can also look for improvements in such aspects causing negative and lack of motivated of software engineers facing the decision of whether use one or another option. Among our findings in this category is the difficulty to novice developers usingFea- tureHouse to perceive the importance of the precedence among the features in the binding time. This fact should be more explicitly addressed in the supporting tools. Addi- tionally, the participants pointed out the traceability is an important asset in the comprehension of such kind of code, which we agree and suggest also further investigation in the facet. We are aware this study only could not fulfill the gap on the in- fluence of the use of different variability representations on feature- oriented software comprehension. However, we believe the points enumerated earlier this section might point out the direction of the next steps of the research community. ACKNOWLEDGEMENTS This work is partially supported by INES, grant CNPq/465614/2014- 0, and FAPESB grants BOL0820/2014 and JCB0060/2016. REFERENCES [1] Sven Apel, Don Batory, Christian Kästner, and Gunter Saake. 2013. Feature- Oriented Software Product Lines: Concepts and Implementation. Springer-Verlag, Berlin, Germany. [2] Janet Feigenspan, Christian Kästner, Sven Apel, Jörg Liebig, Michael Schulze, Raimund Dachselt, Maria Papendieck, Thomas Leich, and Gunter Saake. 2013. Do background colors improve program comprehension in the #ifdef hell? Empirical Software Engineering 18, 4 (2013), 699–745. [3] Eduardo Figueiredo, Nelio Cacho, Claudio Sant’Anna, Mario Monteiro, Uira Kulesza, Alessandro Garcia, Sérgio Soares, Fabiano Ferrari, Safoora Khan, Fer- nando Castor Filho, and Francisco Dantas. 2008. Evolving Software Product Lines with Aspects: An Empirical Study on Design Stability. In Proceedings of the 30th International Conference on Software Engineering. ACM, New York, NY, USA, 261–270.', 'Lines with Aspects: An Empirical Study on Design Stability. In Proceedings of the 30th International Conference on Software Engineering. ACM, New York, NY, USA, 261–270. [4] Thomas Leich, Sven Apel, Laura Marnitz, and Gunter Saake. 2005. Tool support for feature-oriented software development: featureIDE: an Eclipse-based approach. In Proceedings of the 2005 OOPSLA workshop on Eclipse technology eXchange. ACM, 55–59. [5] Romero Malaquias, Márcio Ribeiro, Rodrigo Bonifácio, Eduardo Monteiro, Flávio Medeiros, Alessandro Garcia, and Rohit Gheyi. 2017. The Discipline of Preprocessor-Based Annotations - Does #ifdef TAG n’t #endif Matter. InPro- ceedings of the 25th International Conference on Program Comprehension (ICPC’17). 297–307. https://doi.org/10.1109/ICPC.2017.41 [6] Flávio Medeiros, Christian Kästner, Márcio Ribeiro, Sarah Nadi, and Rohit Gheyi. 2015. The Love/Hate Relationship with the C Preprocessor: An Interview Study. In Proceedings of the 29th European Conference on Object-Oriented Programming. Dagstuhl Publishing, Prague, Czech Republic, 495–518. [7] Jean Melo, Claus Brabrand, and Andrzej noneWasowski. 2016. How Does the Degree of Variability Affect Bug Finding?. In Proceedings of the 38th International Conference on Software Engineering. ACM, New York, NY, USA, 679–690. [8] Jean Melo, Fabricio Batista Narcizo, Dan Witzner Hansen, Claus Brabrand, and Andrzej Wasowski. 2017. Variability Through the Eyes of the Programmer. In Proceedings of the 25th International Conference on Program Comprehension (ICPC ’17). IEEE Press, Piscataway, NJ, USA, 34–44. https://doi.org/10.1109/ICPC.2017.34 [9] Paulo Anselmo Mota Silveira Neto, Taijara Loiola Santana, Eduardo Santana Almeida, and Yguaratã Cerqueira Cavalcanti. 2016. RiSE Events: A Testbed for Software Product Lines Experimentation. In Proceedings of the 1st International Workshop on Variability and Complexity in Software Design. ACM, New York, NY, USA, 12–13. [10] Likert Rensis. 1932. A technique for the measurement of attitudes. Archives of Psychology 22, 140 (1932), 5–55. [11] Alcemir Rodrigues Santos. 2017. Investigating Feature-Oriented Software Com- prehension. Ph.D. Dissertation. Universidade Federal da Bahia. to be published prior to the conference. [12] Alcemir Rodrigues Santos, Ivan Carmo Machado, and Eduardo Santana Almeida. 2016. RiPLE-HC: Javascript Systems Meets SPL Composition. In Proceedings of the 20th International Systems and Software Product Line Conference. ACM, New York, NY, USA, 154–163. [13] Sandro Schulze, Jörg Liebig, Janet Siegmund, and Sven Apel. 2013. Does the discipline of preprocessor annotations matter?: a controlled experiment. ACM SIGPLAN Notices 49, 3 (2013), 65–74. [14] Forrest Shull, Janice Singer, and Dag I.K. Sjøberg. 2007. Guide to Advanced Empirical Software Engineering. Springer-Verlag New York, Inc., Secaucus, NJ, USA. [15] Janet Siegmund, Christian Kästner, Jörg Liebig, and Sven Apel. 2012. Compar- ing Program Comprehension of Physically and Virtually Separated Concerns. InProceedings of the 4th International Workshop on Feature-Oriented Software Development. ACM, New York, NY, USA, 17–24. [16] Janet Siegmund, Christian Kästner, Jörg Liebig, Sven Apel, and Stefan Hanenberg. 2014. Measuring and modeling programming experience. Empirical Software Engineering 19, 5 (2014), 1299–1334. https://doi.org/10.1007/s10664-013-9286-4 [17] Claes Wohlin, Per Runeson, Martin Höst, Magnus C. Ohlsson, Björn Regnell, and Anders Wesslén. 2012. Experimentation in Software Engineering. Springer-Verlag, Berlin, Germany.']","['THE COMPREHENSION OF FEATURE-ORIENTED SOFTWARE  This  briefin  reports  evideice  oi  the aspects  iifleiciin  featlre-orieited software  compreheisioi  based  oi  the resllts of a focls nrolp sessioi. FINDINGS   We could group our main fndings in four categories  regarding  the  aspects  that might  have  infuence  on  the comprehension  and  consequently  the maintenance  of  feature-oriented software. These groups are enumerated in the following: Approach  Stratenies: By  understanding how the software engineers address the comprehension  of  unfamiliar  code  can produce  insights  on  the  constructon  of more  efectve  methodse  processese  and tools to support maintenance of feature- oriented software. \uf0b7 Among  our  fndings  in  this  category are  the  use  of  tools  other  than  the search  to  address  unfamiliar  code. We  conjecture  that  visualizaton toolse  such  as  the  “Collaboraton Diagram”  and  the  “Confguraton management”  can  contribute  to  the comprehension  code  using  both Conditional  Compilation  and FeatureHouse. Hiideriin Factors: By understanding what makes  the  comprehension  tasks  hardere we  can  build  tools  to  facilitate  such  an important  process  in  the  software maintenance. \uf0b7 Among  our  fndings  in  this  category are issues already well known by the practtoners  using  annotatonse  the excessive amount of annotatons and the highly scatering of them.  \uf0b7 Regarding  the  compositonal approache  we  found  the  number  of duplicated  classese  as  well  as  the amount of clicks to reach the source code  as  botlenecks  of  the  existng code organizaton tools and are worth further  investgaton  and improvements. Facilitators  Factors: by  understanding what  makes  the  comprehension  tasks easiere we can concentrate the e ort on research to make these factors of some use  to  enhance  the  already  available tools to comprehend software.  \uf0b7 Among  our  fndings  in this  category are the simple programming model of the Conditonal Compilaton and the good way of code organizaton of the FeatureHouse.  This  fnding corroborates  with  those  in  the hindering  factors  categorye  since although the partcipants like the way the  code  is  organizede  they  were uncomfortable  with  the  amount  of clicks to get to it. Geieral Observatois:  by understanding the  feelings  of  the  software  engineers regarding  to  the  frst  contact  with  un- familiar  variability  representatonse  we can also look for improvements in such aspects  causing  negatve  and  lack  of motvated  of  software  engineers  facing the  decision  of  whether  use  one  or another opton.  \uf0b7 Among our fndings in this category is the  difculty  to  novice  developers using  FeatureHouse to  perceive  the importance of the precedence among the features in the binding tme. This fact  should  be  more  explicitly addressed in the supportng tools.  \uf0b7 The  partcipants  pointed  out  the traceability  is  an  important  asset  in the  comprehension  of  such  kind  of codee  which  we  agree  and  suggest also further investgaton in the facet. These fndings point out research gaps on the  infuence  of  the  use  of  diferent variability  representatons  on  feature- oriented software comprehension.  Keywords: Feature-oriented Software Program Comprehension Focus group Who is this briefin  or? Software engineering researchers who  want directons to further  investgatons on the comprehension of feature-oriented software Where the finiins come  rom? All fndings of this briefng were  extracted from a focus group  conducted by Santos et al. What is iiclunen ii this briefin? The main fndings of the original study  report. Evidence summary through a brief  descripton of each group of aspects  identfed in the study. What is iot iiclunen ii this briefin? Additonal informaton not presented in the original study report. Detailed descriptons about the studies  analyzed in the original study report.', 'What is iot iiclunen ii this briefin? Additonal informaton not presented in the original study report. Detailed descriptons about the studies  analyzed in the original study report. To access other evineice briefins  oi softare einiieeriine htp://www.lia.ufc.br/ccbsoftt2017//en/ xi-sbcars/ For annitoial ii ormatoi about  RiSE Labse htp://www.rise.com.br ORIGINAL RESEARCH REFERENCE Alcemir Rodrigues Santos et al. Aspects Infuencing Feature-Oriented Software Comprehension: Observatons from a Focus Group. t2017/. 11th Brazilian Symposium of  Software Componentse Architecturee and Reuse. htp://dx.doi.org/10.1145/31131t2498.31131318318.']","**Title:** Understanding Factors Influencing Feature-Oriented Software Comprehension

**Introduction:**  
This evidence briefing summarizes key findings from a qualitative study that explored factors impacting the comprehension of feature-oriented software. Conducted through a focus group, the research aims to shed light on how different variability representations, specifically Conditional Compilation and FeatureHouse, affect developers' understanding and maintenance efforts.

**Main Findings:**  
1. **Approach Strategies:** Developers employed various strategies to comprehend feature-oriented software. While many relied on search tools, those familiar with visualization tools (like collaboration diagrams) found them beneficial in understanding the code structure. The choice of strategy was often influenced by prior experience, indicating a need for tailored training and tool support.

2. **Hindering Factors:** Key challenges identified include:
   - **Conditional Compilation:** Excessive annotations and code obfuscation hindered comprehension. The scattering of feature annotations made it difficult for developers to locate relevant information quickly.
   - **FeatureHouse:** Issues such as duplicated classes and the number of clicks required to navigate to source code files complicated understanding. Newer developers struggled with recognizing feature precedence during implementation.

3. **Facilitators of Comprehension:** Several factors eased the comprehension process:
   - **Conditional Compilation:** Developers appreciated the simplicity of the programming model and found that reading the code directly often sufficed for understanding.
   - **FeatureHouse:** Code organization and the availability of tools like the collaboration diagram enhanced comprehension. Participants noted that the traceability provided by FeatureHouse was a significant advantage over Conditional Compilation.

4. **General Observations:** Participants expressed a preference for FeatureHouse due to its tool support, which aided in navigating complex code. However, they also recognized that both approaches have their merits and suggested a balanced use depending on the context.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, educators, and researchers interested in feature-oriented software development and program comprehension.

**Where the findings come from?**  
The findings are derived from a focus group study involving ten graduate students from the Federal University of Bahia, who performed tasks using both Conditional Compilation and FeatureHouse representations.

**What is included in this briefing?**  
This briefing includes insights into the strategies developers use, the factors that hinder or facilitate comprehension, and general observations regarding the maintainability of feature-oriented software.

**What is NOT included in this briefing?**  
This briefing does not include detailed quantitative metrics or statistical analyses from the study.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about this research:**  
Contact Alcemir Rodrigues Santos at alcemirsantos@dcc.ufba.br.

**Original Research Reference:**  
Alcemir Rodrigues Santos, Ivan do Carmo Machado, and Eduardo Santana de Almeida. 2017. Aspects Influencing Feature-Oriented Software Comprehension. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017. https://doi.org/10.1145/3132498.3133838"
"['Investigating the Variability Impact on the Recovery of Software Product Line Architectures: An Exploratory Study Mateus Passos Soares Cardoso Federal University of Bahia Salvador, Brazil mateuspsc@dcc.ufba.br Crescencio Lima Federal University of Bahia Federal Institute of Bahia Salvador, Brazil crescencio@gmail.com Eduardo Santana de Almeida Federal University of Bahia Salvador, Brazil esa@dcc.ufba.br Ivan do Carmo Machado Federal University of Bahia Salvador, Brazil ivanmachado@dcc.ufba.br Christina von Flach G. Chavez Federal University of Bahia Salvador, Brazil flach@ufba.br ABSTRACT The Product Line Architecture (PLA) of a Software Product Line (SPL) is the core architecture that represents a high-level design for all the products of an SPL, including variation points and variants. If PLA documentation is missing, it can be recovered by reverse engineering the products. The recovered PLA is a relevant asset for developers and architects, that can be used to drive specific activities of SPL development and evolution, such as, understanding its structure and its variation points, and assessing reuse. This paper presents an exploratory study that investigated the effectiveness of recovered PLAs to address variability identification and support reuse assessment. We recovered the PLA of 15 open source SPL projects using the PLAR, a tool that supports PLA recovery and assessment based on information extracted from SPL products’ source code. For each project, reuse assessment was supported by existing reuse metrics. The yielded results revealed that the number of products used in PLA recovery affected the variability identification, and the number of optional features affected the components reuse rate. These findings suggest that a minimum set of representative products should be identified and selected for PLA recovery, and the component reuse rate is a candidate metric for SPL reuse assessment. CCS CONCEPTS • Software and its engineering → Software product lines ; Software architectures ; Empirical software validation ; KEYWORDS Software Product Lines; Product Line Architecture; Variability; Product Line Architecture Recovery ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5325-0/17/09. . . $15.00 https://doi.org/10.1145/3132498.3133835 ACM Reference format: Mateus Passos Soares Cardoso, Crescencio Lima, Eduardo Santana de Almeida, Ivan do Carmo Machado, and Christina von Flach G. Chavez. 2017. Inves- tigating the Variability Impact on the Recovery of Software Product Line Architectures: An Exploratory Study. In Proceedings of SBCARS 2017, Fort- aleza, CE, Brazil, September 18–19, 2017, 10 pages. https://doi.org/10.1145/3132498.3133835 1 INTRODUCTION Many companies – mainly in the automotive, aerospace, and elec- tronics industries – develop a portfolio of related software products, conceived to satisfy similar, but not identical, needs of their cus- tomers. A Software Product Line (SPL) is a set of software systems that share a common and variable set of features satisfying the specific needs of a particular market segment [18]. SPL engineering supports the development and management of a product portfolio, highlighting the commonalities and variabilities, promoting reuse, and fostering customization. The adoption of the SPL paradigm brings benefits, including improved product reliability, faster time to market, and reduced costs [1]. The development of an SPL involves the implementation of dif- ferent structures, processes, interfaces, and activities, therefore it is relevant for SPL engineers to pay sufficient attention to its archi-', 'ferent structures, processes, interfaces, and activities, therefore it is relevant for SPL engineers to pay sufficient attention to its archi- tecture [11]. The Product Line Architecture (PLA) can be defined as (i) the core architecture that represents a high-level design for all the products of an SPL, including variation points and variants documented in the variability model [18], or (ii) an architecture for a family of products that describes the mandatory, optional, and variable components1 in the SPL, and their interconnections [ 9]. The PLA is one of the most valuable SPL assets because it contains the core components of the SPL as well as the variable ones [5] in a structure that encompasses the behavior from which software products are developed [14]. Despite the benefits associated with SPL [1, 18], its development is considered expensive. For this reason, companies adopt SPL by developing a set of software products that share common charac- teristics, adding or removing functions from products [19]. With the growth of products portfolio, the management of variability 1In this paper we considered a PLA component as the concrete classes, abstract classes, and interfaces.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil M. P. S. Cardoso et al. and reuse becomes a complex task [ 20], especially if there is no architecture to support it. In this context, the PLA plays an important role to allow the SPL evolution and keep complexity under control. PLA assessment can be supported by metrics to be collected during the recovery process. For instance, reuse metrics such as those proposed by Zhang et al. [27] – Structure Similarity Coefficient (SSC), Structure Variability Coefficient (SVC) and Component Reuse Rate (CRR) [27] – and the metrics proposed by Oliveira-Junior et al. [17] – ClassOptional, ClassMandatory, and PLTotalVariability could be used to pro- vide insights about the way PLA components are reused within SPL products, that may be useful for SPL developers, during main- tenance tasks. For single systems, software architecture can be recovered and documented from source code or other available information [4, 7]. Software Architecture Recovery (SAR) approaches share the goal of documenting software architecture and provide solutions to prob- lems such as the absence of documented software architecture, and the need for detecting violations between conceptual and im- plemented architectures [7]. In the SPL domain the PLA provides information about the common and variable components, providing useful information for software reuse. Although the architecture description is part of the SPL adoption process, not all projects have a PLA documented. The PLA recovery can help developers with the SPL evolution and maintenance tasks. In previous work [16], we reported the results of a literature re- view undertaken to investigate research work that brings together the fields of software product lines and software architecture recov- ery. Several approaches to PLA recovery [ 12, 20] were identified, as well as research trends and gaps. We have found out that few SAR tools support variability identification, an essential feature for PLA recovery. Additional features such as support for PLA assessment, were not found either. Finally, these tools were mostly not available for use. These gaps motivated us to develop the PLAR Tool, a PLA recovery and assessment tool [3]. This paper presents the results of an exploratory study conducted to assess the PLA recovered from a set of open-source SPL projects. The PLAR tool supported PLA recovery and assessment. The main contributions of this paper are (i) the recovery of the PLA from 15 SPL projects and (ii) the assessment of those 15 PLAs through reuse metrics. In this paper, we discovered that is not necessary to use all products in order to recover the PLA, the recovery process can be improved by using only the most significant configurations. The PLA recovery process described in the paper was used on SPL projects from different domains and sizes. The remainder of the paper is organized as follows. The ex- ploratory study conducted to investigate the recovered PLA is pre- sented as study design (Section 2), study execution (Section 3), analysis (Section 4), and interpretation of results (Section 5). Sec- tion 6 discusses related work, and Section 7 presents concluding remarks and recommendations for future work. 2 STUDY DESIGN In this Section, we present research questions (RQ), hypotheses, metrics, and discuss the analysis procedure defined for our study. Table 1: GQM model for Goal 1 Goal Purpose Verify Issue if the number of products Object has an impact on the PLA variabil- ity identification precision Viewpoint from the SPL architect point of view Question RQ1 Does the number of SPL products used in PLA recovery impact the variability identification precision in the PLA? Metrics SSC, SVC, CRR, Optional Features , Number of Products, and ClassOptional We used the Goal/Question/Metric (GQM) approach [ 24] be- cause measurement is defined in a top-down fashion, from goals to metrics. This study encompasses two related facets: number of', 'ClassOptional We used the Goal/Question/Metric (GQM) approach [ 24] be- cause measurement is defined in a top-down fashion, from goals to metrics. This study encompasses two related facets: number of products impacting the precision to identify the variability in the PLA, and number of optional features impacting reuse rate. Each facet led to different research questions and hypotheses, as discussed next. 2.1 Research Questions We defined two GQM models in this study, each one addressing one facet. Based on the goals, we defined the research questions and related them to the set of metrics under evaluation [17, 27]. GQM Model 1: Table 1 describes the GQM model for the fol- lowing goal: “Verify if the number of products has an impact on the PLA variability identification precision”. Related to this goal, we defined the following research question: Does the number of SPL products used in PLA recovery impact the variability identi- fication precision in the PLA? The product generation tool provides the number of SPL products used in PLA recovery. The PLTotalVariability metric is used to estimate the total number of variable components expected in the PLA, and the SVC metric is used to calculate the overall variability of PLA components. Moreover, we verified the metrics values with different number of products in the comparison. GQM Model 2: Table 2 describes the GQM model for the fol- lowing goal: “Identify correlations between the number of optional features and the component reuse rate of the PLA”. Based on this goal we defined the following RQ: Is there any correlation between the number of optional features in the SPL and the component reuse rate of the recovered PLA? The product generation tool provides the number of optional features in the SPL. We used this information together with theSSC metric value to estimate the impact of optional features on the PLA component reuse rate. 2.2 Hypotheses In order to answer the research questions, we postulated the fol- lowing hypotheses: RQ.1 Hypotheses', 'Investigating the Variability Impact on the Recovery of SPL Architectures SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Table 2: GQM model for Goal 2 Goal Purpose Identify Issue if there is any correlation between Object the number of optional features and the component reuse rate of the PLA Viewpoint from the SPL architect point of view Question RQ2 Is there any correlation between the number of optional features in the SPL and the component reuse rate of the re- covered PLA? Metrics SSC, SVC, CRR, Optional Features H0a The number of SPL products analyzed does not influence the variability identification precision. H1a The variability identification precision is influenced by the number of SPL products analyzed. RQ.2 Hypotheses H0b There is no relation between the number of optional features in the SPL and the component reuse rate values in the recovered PLA. H1b There is a relation between the number of optional fea- tures in the SPL and the component reuse rate values in the recovered PLA. 2.3 Metrics Table 3 describes the metrics used in this study. The recovered PLA is evaluated based on the metrics analysis of its components with the SSC, SVC, and CRR metrics. The amount of common and variable components of the PLA is also measured and support the calculation of SSC, SVC and CRR. The PLA recovery activity collects the following data from the SPL project: number of SPL optional features, number of SPL varia- tion points, number of SPL products (M) number of common com- ponents (CC ), number of variable components ( CV ), number of common relations (RC ) and number of variable relations (RV ). Data to be collected by PLA assessment includes reuse measurement values for PLA components and relations. 2.4 Analysis Procedure The method to evaluate the variability identification precision of the recovered was based on correlation analysis. We analyzed met- rics concerning both the number of products and related with the variability identification (SVC, RSVC, PLTV, ClassOptional (CO), OptionalRelation (OR), and optional-Features). The analysis examines whether the number of products (M) influences the vari- ability identification rate of individual PLAs. To test our hypothesis related to RQ1, we analyzed the result from the correlation analysis. As statistical tests, we applied the Spearman rank correlation [6], which is a non-parametric test that is used to measure the degree of association between two variables. Our method for evaluating the effectiveness of the recovered PLA to support reusability evaluation was based on the analysis of CRR, which examines whether the number of optional features (OF) influences the component reuse rate of PLA. To test our hypothesis related to RQ2, we compared the CRR values from 15 SPL projects for testing the null hypothesis that the number of optional features has no influence on the CRR. As statistical tests, we applied the ANOVA [15] to identify if at least one SPL presented different CRR value, and the Tukey test [15] to perform a pairwise comparison between the values. 3 STUDY OPERATION 3.1 Preparation Fifteen open source SPL projects from different domains were selected for this exploratory study, based on the following cri- teria: lack of documented PLA and source code written in Java. Table 4 summarizes our sample and presents the number of fea- tures (mandatory and optional), classes and products of each SPL, and the tool used for product generation. 3.2 Execution Figure 1 shows the main activities, inputs, and outputs of the PLA recovery process executed for this study. The selected SPL projects were subject to product generation, information extraction with STAN4J, and PLA recovery with the PLAR Tool. Only valid product configurations were used. A product config- uration is valid if it obeys the SPL feature model dependencies [1]. For any SPL with a potential high number of products (e.g. Prop4J can have 5K products), we used the T-Wise method [10] (with t = 2)', 'For any SPL with a potential high number of products (e.g. Prop4J can have 5K products), we used the T-Wise method [10] (with t = 2) to generate only a subset of SPL products. T-Wise builds only the most significant products, based on the SPL feature model. Some SPL projects (DPL, VOD, Zip Me, and GOL) had existing generated products available, so that we could skip product generation. For each selected SPL project, we extracted a module dependency graph (MDG) based on the analysis of the products’ source code, with the support of Stan4J 2. The MDG represents the concrete classes, abstract classes, interfaces, and the relationship among them. This is done because there are different mechanisms to imple- ment the variability (we could enlist conditional compilation, inher- itance, parameterization, and overloading as the most widely used ones [2, 21]) and different composers (e.g. Featurehouse, AHEAD, CIDE, and so on [22]). By generating the products, it was possible to recover the PLA from different SPL projects (implemented using #IFDEF directives or FeatureIDE [22]) independently of the variability implementation mechanism. The set of extracted graphs served as input to PLA recovery, with the support of the PLAR Tool [3]. The PLAR tool analyzes the MDG files to identify the variability at the architectural level by comparing the components. The main output is the PLA, represented as Module View, Class Diagram, and Design Structure Matrix (DSM). The tool also provides a metrics report, that contains the calculation of the metrics presented on table 3, which is used to assess the PLA. 2http://stan4j.com/', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil M. P. S. Cardoso et al. Table 3: Metrics used to evaluate the PLA. Metric Description Formula Source SSC SSC calculates the overall similarity between PLA components. |CC | |CC |+|CV | [27] SVC SVC calculates the overall variability between PLA components. |CV | |CC |+|CV | [27] CRR Calculates the component reuse rate of each component of the PLA Í i Ex (Mi ) |M| ×100% [27] ClassOptional Calculates the number of classes implementing the optional features ÍCV [17] ClassMandatory Calculates the number of classes implementing the mandatory features ÍCC [17] PLTotalVariability Estimates the number of variable components found on PLA ÍRCV + ÍCV [17] Legend: CC - T otal number of Common Components;CV - Total number of Variable Components;M - Total number of SPL products;RCV - Total number of Variable Relations; Ex (Mi )- returns 1, if component i is present in the product architecture, 0 otherwise. Table 4: SPL Projects analyzed and Metrics collected for PLAs SPL #F #FM #OF #P #C Gen. SSC SVC RSSC RSVC CO OR CM MR PLTV DPL 5 3 2 12 4 NA 0.5 0.5 0.3 0.7 2 2 2 1 4 VOD 11 6 5 32 42 NA 0.8 0.2 0.7 0.3 10 23 32 55 33 Zip Me 7 2 5 32 31 NA 0.8 0.2 0.7 0.3 6 14 25 32 20 GOL 21 12 9 65 21 NA 0.6 0.4 0.7 0.3 8 11 13 24 19 GPL 38 18 20 155 15 CD 0.6 0.4 0.4 0.6 6 23 9 16 29 Prop4J 13 0 13 31 14 FH 0.1 0.9 0.0 1.0 13 50 1 0 63 BankAccount 6 0 6 24 2 FH 1.0 0.0 1.0 0.0 0 0 2 1 0 BankAccountv2 8 0 8 72 3 FH 0.7 0.3 0.5 0.5 1 1 2 1 2 DesktopSearcher 22 6 16 462 41 AH 0.3 0.7 0.1 0.9 30 134 11 14 164 Elevator 6 0 6 20 5 FH 1.0 0.0 1.0 0.0 0 0 11 29 0 E-mail 6 0 6 40 3 FH 1.0 0.0 1.0 0.0 0 0 3 4 0 ExamDB 3 0 3 8 4 FH 1.0 0.0 1.0 0.0 0 0 4 5 0 PayCard 3 0 3 6 7 FH 0.7 0.3 0.4 0.6 2 5 5 3 7 PokerSPL 11 2 9 28 8 FH 0.5 0.5 0.3 0.7 4 5 4 2 9 UnionFind 10 2 8 6 4 FH 1.0 0.0 1.0 0.0 0 0 4 4 0 Legend: [#F] Features [#FM] Mandatory Features [#OF] Optional Features [#P] Product [#C] Classes [Gen.] Product Generator [NA] Not Available [CD] CIDE [FH] FeatureHouse [AH] AHEAD [CO] ClassOptional [OR] OptionalRelation [CM] ClassMandatory [MR] MandatoryRelation [PLTV] PLTotalVariability Figure 1: The overall recovery process: Activities, inputs and outputs. 3.3 Data collection For each SPL project studied, the PLA was recovered and metrics were collected using the PLAR tool. The complete data set used in this exploratory study is available at the study website3. 3 https://sites.google.com/view/sbcars2017-mpassos/home', 'Investigating the Variability Impact on the Recovery of SPL Architectures SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil 4 DATA ANALYSIS This section presents the statistical analysis of the treatment vari- ables relating to the data items gathered in the study. First, we present some descriptive statistics for the dependent and indepen- dent variables; next, we present the analysis of each SPL data – because the SPL projects used different techniques to implement the variability. In order to evaluate the recovered PLA, we measured SSC, SVC, and CRR values. The SPL projects with a high SSC value and a low SVC value indicate that the PLA is mostly composed of common components. Conversely, projects with high SVC value and low SSC value indicate that the PLA is mostly composed of variable components. 4.1 Descriptive Statistics Table 4 presents the metric results of the recovered PLAs. The SSC metric is used to calculate the overall similarity of PLA components; the maximum value is 1. The SVC metric calculates the general variability of PLA components; the maximum value is 1. The RSSC and RSVC metrics are similar to SSC and SVC, respectively. However, they are used to measure the similarity and variability of relations among PLA components. The metrics ClassMandatory calculates the number of classes implementing the mandatory features and ClassOptional calcu- lates the number of classes implementing the optional features. Besides, the metrics OptionalRelation and MandatoryRelation use the same principle to calculate the number of mandatory and optional relations. It is possible to perform these calculations be- cause PLAR analyzes the classes and relations captured in the MDG files. PLTotalVariability is a metric that estimates the PLA vari- ability. Table 4 shows the ClassOptional and OptionalRelation metrics. The CRR metric is missing from the overview presented in Ta- ble 4. This metric provides a measure for each PLA component and relation, as it calculates the amount of products (ratio) that have a specific component or relation. Components with CRR of 100% indicates that the component is present in all the products. Values above 50% mean that the component was used at least in half of SPL products. The ClassVP and ComponentVariable metrics were not men- tioned in Table 4. The reason is that those metrics only indicate whether a specific PLA component is either variable or not. Such information could be visualized from the output files generated by PLAR. Figure 2 shows a boxplot with the distribution of the CRR values for each SPL. The values range from 0 to 100, in which lower values indicate the component is reused only in a small set of the products, and the higher values indicate the component is reused in a lot of products. Next, we detail the SPL projects that comprised variable elements, as observed in the recovered PLA. 4.2 Draw Product Line Results Draw Product Line (DPL) implements five features (three manda- tory and two optional) that allow the configuration of a small num- ber of products. The SSC value indicates variability in 50% of the PLA components, while the RSVC value indicates variability in 2/3 of the PLA relations. Table 5: CRR Measures for DPL components Component CRRpair CRR8 CRRall BasicRectangle 100.0 62.5 66.7 Canvas 100.0 100.0 100.0 Line 50.0 50.0 66.7 Main 100.0 100.0 100.0 Table 5 presents the CRR values for the PLA components. The CRRpair, CRR8 and CRRall columns present the CRR value for re- covery based on two products, eight products, and all SPL products configurations, respectively. The CRR measures for Canvas and Main (two classes implement- ing common features) are 100%. For BasicRectangle, the CRRpair measure is 100% and variability could not be identified. The CRR8 decreased 35.5% (reaching 62.5%) and CRRall increased 4.2% (reach- ing 66.7%). ForLine, theCRRpair measure is 50% and some variability could be identified. The CRR8 remains 50% and CRRall increased', 'decreased 35.5% (reaching 62.5%) and CRRall increased 4.2% (reach- ing 66.7%). ForLine, theCRRpair measure is 50% and some variability could be identified. The CRR8 remains 50% and CRRall increased 16.7% – reaching 66.7%. Further investigation on these results is needed to confirm whether changes performed on DPL components that presented CRR values above 50% could impact many DPL products [12]. 4.3 Video on Demand Results Video on Demand (VOD) implements eleven features (six manda- tory and five optional) that provide the creation of 32 products. The SSC value for VOD is 0.77, indicating that 32 products reused most of its PLA components. We found similar results for the PLA relations (see Table 4). There are 32 classes implementing the mandatory features and 10 classes implementing the optional features in the VOD SPL. The classes implementing the optional features had aCRR of 50%, that is, these components were used by half of the SPL products. However, some relations (VOD boxplot outliers in Figure 2) presented a CRR of 25% and 3.25% indicating that few products used them. For this reason, the refactoring of the components involved in this relation should be considered [ 27]. Due to space limitation, information about the PLA and the Table with the CRR values for the 42 classes of the VOD SPL project are only available at the study website. 4.4 Zip Me Results Zip Me has 32 generated products. The SSC measure was 0.8 indi- cating the components were similar in 80% of the products. The RSSC was 0.7, also a high value for reuse of relations (see Table 4). The PLA components presented high CRR values – 25 common and 6 variable components – above 50%. The PLA relations also presented high CRR values – 32 common and 14 optional relations. From the optional relations, 11 presented a CRR value above 50%; the other relations had a CRR of 25%. These components, which presented low CRR values, are the outliers Figure 2 shows.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil M. P. S. Cardoso et al. Figure 2: Boxplot of Component Reuse Rate per PLA 4.5 Game of Life Results Game of Life (GOL) has 65 generated products. The SSC value was 0.62, indicating a larger amount of common components rather than variable ones (see Table 4). Although the GOL results indicated low SSC values, some com- ponents were present in almost every GOL product (98% and 73%). We found similar results for CRR and PLA relations. Figure 3 shows the DSM for GOL PLA. Rows and columns head- ers of a DSM are named after PLA components. The darker items represent the commonalities of the PLA while the lighter items represent the variable components. The tool colors the dependency between two common components with darker color, and between a variable component and another component using lighter color. The GOL DSM shows that, unlike GPL and Prop4J, there is not a central node, i.e. a component that is related to almost every PLA component. The DSM also allows to visualize that PLA relations are scattered in the components. We also noticed that some PLA components did not have any relation; we believe this is due to fea- tures not implemented or discarded while their classes still remain in source code. Figure 3: Design Structure Matrix for GOL 4.6 Graph Product Line Results Graph Product Line (GPL) has 155 products. The SSC value was 0.6 indicating more common than variable components. However, the opposite happened with the relations – theSSC value was 0.42. This scenario indicated that most of the PLA relations were variable. According to Zhang et al. [27], this is a symptom of bad component reuse, suggesting a potential candidate for improvement. The CRR values were high. However, we identified that some com- ponents, such asCycleWorkSpaceand GlobalVarsWrapper, presented a low CRR value. Furthermore, the majority of relations presented low CRR values. The components Graph and Vertex have relation- ships with all the other components. It deserves further investiga- tion whether these two classes present the God class smell [8]. 4.7 Prop4J Results Prop4J has no mandatory features, implementing only optional features. In addition, for the absence of mandatory features, this SPL project allowed the generation of 5029 possible configuration of products based on optional features. For this reason, we instantiated 11 products using T-Wise method configuration [ 10]. After the PLA recovery based on a subset of 11 products, we identified only one common component (Node) and 13 variable components. All the recovered relations are variable. Figure 2 also reflected this information. For instance, Node is one outlier of the Prop4J boxplot. Figure 4: Design Structure Matrix for Prop4J', 'Investigating the Variability Impact on the Recovery of SPL Architectures SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Figure 4 shows the DSM for Prop4J PLA. TheNode component is common to the 11 products analyzed. However, the relations were variable confirming the CRR values. These results suggest that the PLA needs maintenance to improve the CRR values. In Prop4J DSM, we identified two central classes,Node and Prop4J test that relate to almost every class present in the PLA, which is an indicative of the God class smell [8] and require further investi- gation. 4.8 BankAccountV2 Results BankAccountV2, the second version of the BankAccount SPL, in- troduced new features to the SPL. The implementation of these new features required new classes and relations, which required the identification of variability in classes and relations that were absent from the previous version of this SPL. The SSC value was 0.7, meaning that most of components and relations are common to 70% of the products. 4.9 DesktopSearcher Results DesktopSearcher allowed the generation of 462 products. The values for SSC and RSSC were considered low [27]. The SSC value was 0.26 which means that almost all of its classes are variable among the products. The CRR presented the same behavior. In most cases, the values were lower than 0.5. Theses values indicated that the SPL products tend to have ex- clusive products that require specific features in only part of the products [1] which we believe to be the cause of the lowCRR values found in this SPL project. 4.10 PayCard Results PayCard is a small-sized SPL project, with respect to the amount of product configurations, and classes. The project presents a value of 0.71 for SSC, i.e., most of its classes are common to all products. However, the dependencies among classes presented a value of 0.37 for RSSC, which means that most of the relations are variable. 4.11 PokerSPL Results PokerSPL is another small-sized SPL project, regarding the number of classes and products. It presented a 0.5 SSC value meaning that half of its classes are common to all products. The CRR values could be higher since most of the variability in this SPL was found on the values assumed by some of its classes attributes. Accordingly, the CRR values for the SPL relations present similar results. A possible explanation is that most of the SPL variability is implemented in the values of class attributes. 5 DISCUSSION In this section, we interpret the results and discuss the findings by answering the research questions. 5.1 Answers to the Research Questions For the first research question, we verified ifthe number of products has an impact on the PLA variability identification precision. Figure 5 shows SSC (darker color) and SVC (lighter color) metrics of four SPL projects (DPL, VOD, Zip Me, and Prop4J) collected during different stages of comparison. We selected these projects randomly from the sample. All the SPL projects presented the same patterns. We identified that the precision regarding the variability identi- fication increased when we included more products in the compar- ison. This happens because, with more products, more configura- tions are analyzed. By analyzing all the feature combinations, it is possible to guarantee the detection of all the variable components and provide a reliable PLA. Moreover, after a certain number of comparisons, the value of the metrics became constant. For instance, we compared 18 products aiming to recover all the variability details of the Zip Me SPL. We observed the same pattern on other SPL projects. For example, it was necessary to compare 17 products to recover all the variability details of the VOD SPL. As the PLA recovery process examined and merged more prod- ucts, the set of components and relations that comprise the PLA and metrics values tends to stabilize. The set of products (after the metrics stabilization) had a common structure, with variations', 'ucts, the set of components and relations that comprise the PLA and metrics values tends to stabilize. The set of products (after the metrics stabilization) had a common structure, with variations present in low-level details. We also identified this pattern when we analyzed the CRR values with different combination of products in the recovery. Table 6 presents the CRR values from the Prop4J project. The CRRpair, CRR8 and CRRall columns present the CRR value for recov- ery based on two products, eight products, and all SPL products, respectively. As we raised the number of products in the compari- son, the CRR precision became higher. Table 6: CRR Measures for Prop4J Component CRRpair CRR8 CRRall And 100.0 50.0 54.5 AtLeast 50.0 37.5 36.4 AtMost 50.0 37.5 36.4 Choose 50.0 37.5 36.4 Equals 100.0 37.5 27.3 Implies 100.0 50.0 54.5 Literal 100.0 87.5 90.9 Node 100.0 100.0 100.0 NodeReader 100.0 62.5 54.5 NodeWriter 50.0 75.0 54.5 Not 50.0 50.0 36.4 Or 50.0 37.5 36.4 Prop4JTest 50.0 37.5 36.4 SatSolver 50.0 37.5 45.4 Moreover, to answer the RQ1, in the first stage of the analysis, we performed a correlation analysis among the variables of the exploratory study (see Figure 6). We identified a positive correlation between the number of products and the metrics addressing the variability (CO, OR, PLTV, SVC, and number of optional features) which means that with the increase of the number of optional features we have a increase in the variability identified on the PLA. We also identified a negative correlation between the number of optional features and the metrics related to commonality (SSC and RSSC) which means that with the increase of optional features', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil M. P. S. Cardoso et al. Figure 5: SSC and SVC metrics according to the number of products in the comparison that commonality tends to decrease. Table 7 shows the Spearman correlation test that rejected the null hypothesis. Figure 6: Correlation analysis Table 7: Comparisons that rejected the null hypothesis RQ1 Comparison p-value Products-CO 7.0e-03 Products-OR 7.0e-03 Products-PLTV 7.0e-03 Products-SVC 2.0e-02 Products-OF 2.0e-03 In the second research question, we investigated if there is a relation between the number of optional features in the SPL and the component reuse rate of the recovered PLA, we used the ANOVA (Analysis of Variance) to test the variables and the p-value was 1.3e-07. Such evidence allows to reject the null hypothesis (H 0a) of equal population means. Therefore, it is possible to conclude that at least one PLA has CRR values significantly different from the others. To identify the different means, we applied the Tukey test. We performed and analyzed 105 comparisons, and only 12 of them pre- sented statistically significant differences. Table 8 shows p-values of the comparisons that rejected the null hypothesis (the SPLs in- volved in the test, and the p-value). Based on such data, we identified that Prop4J (in eight comparisons) and DesktopSearcher (in four comparisons) yielded statistical difference in CRR values among the PLAs (see Figure 2). Table 8: Comparisons that rejected the null hypothesis RQ2 ID Comparison p-value 40 Elevator-DesktopSearcher 2.3e-03 43 GOL-DesktopSearcher 4.4e-02 49 VOD-DesktopSearcher 2.1e-03 50 ZipMe-DesktopSearcher 1.5e-03 57 Prop4J-Elevator 4.0e-05 74 Prop4J-ExamDB 1.9e-02 81 Propo4J-GOL 8.0e-04 92 Prop4J-PayCard 2.2e-02 96 Prop4J-PokerSPL 4.4e-02 100 UnionFind-Prop4J 1.9-e02 101 VOD-Prop4J 7.0e-05 102 ZipMe-Prop4J 4.0e-05 According to the correlation analysis performed (see Figure 6), the use of optional features impact the CRR values of the PLA by decreasing the commonality and increasing the variability. Config- uring a product with optional features implies in the appearance of new classes associated with these features. Meaning that there will be new variable components in the product architecture, that are often associated with specific product configurations. 5.2 General Findings Correlation between metrics. From the overall results for the fifteen SPL projects, we noticed that when the value of SSC was high, the PLA components presented high CRR values as well. This may be a preliminary evidence for a correlation between SSC and CRR metrics. Some metrics provided support for other metrics. The quantitative metrics ClassMandatory and ClassOptional counted the number of classes implementing mandatory features and optional features. They confirmed theSSC and SVC metrics values. Moreover,ClassVP and ComponentVariable indicated if a given class or component presented a variability.ClassMandatory, CLassOptional, ClassVP and ComponentVariable provided support for the SSC, SVC, and CRR metrics. The former validated and confirmed the values of the latter. Feature scattering and component reuse rate. In projects with met- rics high values (Zip Me, VOD, and GOL respectively), the classes', 'Investigating the Variability Impact on the Recovery of SPL Architectures SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil outnumbered the features, with feature scattering in a significant amount of classes. To perform this analysis, we verified how the feature selection to build each product was spread through the source code manually and compared to the metrics collected by the PLAR. The relation between feature scattering and high CRR deserves further investigation. 5.3 Threats to Validity The following threats to validity are discussed to reveal their po- tential interference with our study design. Internal Validity. PLAR tool limitations [3] may have impacted the results of the exploratory study. For instance, the input for PLAR tool is a MDG file created by STAN4J and Analizo. Currently, the extracted MDG only supports “call” dependencies between modules. Inheritance relationships are not extracted. External Validity. No industrial SPL projects were used in this exploratory study; only open source SPL projects created for educa- tional and research purposes were used. To minimize such a threat, we analyzed projects widely-accepted by SPL research community, which have been used as testbeds for empirical evaluations [12]. Construct Validity. The recovered PLA from the SPL projects were not verified by SPL developers. To minimize this threat, we performed a manual PLA extraction, which served as an oracle. Then, we compared the number of variable and common elements (including components and its relationships) obtained by PLAR tool against the oracle. The results indicated that there was no difference in the number of elements detected through the tool and manually. Conclusion Validity. We used statistical algorithms as recom- mended by Wohlin et al. [25] on experimentation in software en- gineering to computing the statistical significance and strength of the relationship between the metrics. These procedures aim to minimize issues regarding the conclusions we draw. Two authors checked the analysis to avoid missing data and prevent biases. The metrics SSC and CRR present an overview of the common and variable components of a PLA by showing the presence of the components among all the products. However, low-level granularity variability in the components, such as variable with different values and different methods implementation, are not covered by these metrics. This information can give a different perspective about the CRR. 6 RELATED WORK Wu et al. [26] presented a semi-automatic PLA recovery approach. The authors defined measures to detect similarity and variability points on software products source code of the same domain to migrate to the SPL paradigm. The study reports on a case study carried out with an industrial product line. The assumption is that legacy products of a same domain have similar designs and imple- mentation that can be used to build a SPL. In our approach, we build the PLA from the products generated by the SPL project. Losavio et al. [13] proposed a reactive refactoring bottom-up process to build a PLA from existing similar software product ar- chitectures of a domain. The main assets were expressed by UML logical views. Their work is focused on the construction and repre- sentation of a candidate PLA followed by an optimization process to obtain the final PLA. The refactoring process was applied to a case study in the robotics industry domain. The focus of our work is the assessment of recovered PLA based on metrics analysis. Torkamani [23] presents a novel SPL quality attribute for called Extractability. The attribute is calculated based on the weight of the reusable component over the weight of all components, a process very similar to the CRR metric calculation. Extractability effective- ness on six SPLs from a iranian telecommunication company was evaluated in practice. In our study, we analyzed SPLs from different domains. 7 CONCLUDING REMARKS', 'ness on six SPLs from a iranian telecommunication company was evaluated in practice. In our study, we analyzed SPLs from different domains. 7 CONCLUDING REMARKS Product Line Architecture recovery provides useful information for SPL developers and architects, to support maintenance, under- stand the implementation of SPL variability and foster reuse. The recovered PLA components and their relationships can serve as a basis for different types of visualization that expose variability, and for different types of analysis (e.g.identification of components that are more likely to be reused, propagation analysis during the implementation of reuse changes). In this paper, we presented the results of an exploratory study to assess the recovered PLAs from 15 open source SPL projects imple- mented in Java. The PLAR tool was used to support PLA recovery with the identification of commonality and variation points. Eleven out of fifteen recovered PLAs had high CRR values in- dicating high reuse of components during the SPL development phase. The results provided initial evidence regarding a correlation between the metrics values and the components reuse rate. We aim to replicate this study by including more SPL projects to strengthen the evidence base. One contribution of this paper was the PLA recovered for each SPL project, because none of them presented Product Line Archi- tecture documentation. The results of this exploratory study can be used to improve the design and execution of future empirical studies. As future work, we plan to evolve the PLAR tool to address existing problems and limitations. For instance, our main focus is to optimize the recovered PLA to address other types of variability not mapped, such as variability on the class attribute and method level. Also, we are working on improving the PLA visualization by mapping the SPL features on the PLA. Furthermore, the question about the minimum subset of prod- ucts that covers PLA recovery and results in an architecture that documents the SPL shall be investigated. ACKNOWLEDGMENTS. The authors would like to thank the anonymous reviewers for the thorough feedback. This work is partially supported by FAPESB grants BOL1564/2015, BOL2443/2016 and JCB0060/2016, and INES, grant CNPq/465614/2014-0. REFERENCES [1] Sven Apel, Don Batory, Christian Kstner, and Gunter Saake. 2013. Feature- Oriented Software Product Lines: Concepts and Implementation . Springer Publish- ing Company, Incorporated. [2] Jan Bosch and Rafael Capilla. 2013. Variability Implementation. Springer Berlin Heidelberg, 75–86. [3] Mateus Passos Soares Cardoso, Crescencio Lima, Christina von Flach Gar- cia Chavez, and Ivan do Carmo Machado. 2017. PLAR Tool - A Sofware Product', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil M. P. S. Cardoso et al. Line Architecture Recovery Tool. In 8th Brazilian Conference on Software: Theory and Practice - Tools Session. 18–22. [4] Paul Clements, David Garlan, Len Bass, Judith Stafford, Robert Nord, James Ivers, and Reed Little. 2002. Documenting Software Architectures: Views and Beyond. Pearson Education. [5] Thelma Elita Colanzi and Silvia Regina Vergilio. 2013. Representation of Software Product Line Architectures for search-based design. In 2013 1st International Workshop on Combining Modelling and Search-Based Software Engineering (CMS- BSE). 28–33. [6] W.W. Daniel. 1990. Applied nonparametric statistics. PWS-Kent Publ. [7] Stephane Ducasse and Damien Pollet. 2009. Software Architecture Reconstruc- tion: A Process-Oriented Taxonomy. IEEE Transactions on Software Engineering 35, 4 (July 2009), 573–591. [8] Martin Fowler. 1999. Refactoring: Improving the Design of Existing Code. Addison- Wesley, Boston, MA, USA. [9] Hassan Gomaa. 2004. Designing Software Product Lines with UML: From Use Cases to Pattern-Based Software Architectures. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA. [10] Christopher Henard, Mike Papadakis, Gilles Perrouin, Jacques Klein, Patrick Heymans, and Yves Le Traon. 2014. Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Soft- ware Product Lines. IEEE Transactions on Software Engineering 40, 7 (July 2014), 650–670. [11] Frank J. van der Linden, Klaus Schmid, and Eelco Rommes. 2007. Software Product Lines in Action: The Best Industrial Practice in Product Line Engineering. Springer-Verlag. [12] Lukas Linsbauer, Roberto Erick Lopez-Herrejon, and Alexander Egyed. 2016. Variability extraction and modeling for product variants. Software & Systems Modeling (Jan 2016). [13] Francisca Losavio, Oscar Ordaz, Nicole Levy, and Anthony Baiotto. 2013. Graph modelling of a refactoring process for Product Line Architecture design. In XXXIX Latin American Computing Conference (CLEI). 1–12. [14] Elisa Yumi Nakagawa, Pablo Oliveira Antonino, and Martin Becker. 2011. Refer- ence Architecture and Product Line Architecture: A Subtle but Critical Difference. Springer-Verlag, Essen, Germany, 207–211. [15] Mary Natrella. 2010. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH. [16] Crescencio Rodrigues Lima Neto, Mateus Passos Soares Cardoso, Christina von Flach Garcia Chavez, and Eduardo Santana de Almeida. 2015. Initial Evidence for Understanding the Relationship between Product Line Architecture and Software Architecture Recovery. In IX Brazilian Symposium on Components, Architectures and Reuse Software. 40–49. [17] Edson Alves Oliveira-Junior, I Gimenes, and J Maldonado. 2008. A metric suite to support software product line architecture evaluation. In XXXIV Conferencia Latinoamericana de Informatica. 489–498. [18] Klaus Pohl, Günter Böckle, and Frank J. van der Linden. 2005. Software Product Line Engineering: Foundations, Principles and Techniques. Springer-Verlag. [19] Julia Rubin and Marsha Chechik. 2012. Locating distinguishing features using diff sets. In 2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering. 242–245. [20] Anas Shatnawi, Abdelhak Seriai, and Houari Sahraoui. 2014. Recovering Archi- tectural Variability of a Family of Product Variants. InSoftware Reuse for Dynamic Systems in the Cloud and Beyond: 14th International Conference on Software Reuse. Springer, 17–33. [21] Mikael Svahnberg, Jilles van Gurp, and Jan Bosch. 2005. A Taxonomy of Vari- ability Realization Techniques: Research Articles. Softw. Pract. Exper. 35, 8 (2005), 705–754. [22] Thomas Thüm, Christian Kästner, Fabian Benduhn, Jens Meinicke, Gunter Saake, and Thomas Leich. 2014. FeatureIDE: An extensible framework for feature- oriented software development.Science of Computer Programming 79 (2014), 70–85.', 'and Thomas Leich. 2014. FeatureIDE: An extensible framework for feature- oriented software development.Science of Computer Programming 79 (2014), 70–85. [23] Mohammad Ali Torkamani. 2014. Extractability Effectiveness on Software Prod- uct Line. International Journal of Electrical and Computer Engineering 4, 1 (2014), 127. [24] Rini van Solingen, Vic Basili, Gianluigi Caldiera, and H. Dieter Rombach. 2002. Goal Question Metric (GQM) Approach. In Encyclopedia of Software Engineering. John Wiley & Sons, Inc. [25] Claes Wohlin, Per Runeson, Martin Hst, Magnus C. Ohlsson, Bjrn Regnell, and Anders Wessln. 2012. Experimentation in Software Engineering. Springer Publish- ing Company, Incorporated. [26] Yijian Wu, Yiming Yang, Xin Peng, Cheng Qiu, and Wenyun Zhao. 2011. Re- covering Object-oriented Framework for Software Product Line Reengineering. In Proceedings of the 12th International Conference on Top Productivity Through Software Reuse (ICSR’11). Springer-Verlag, Berlin, Heidelberg, 119–134. [27] Tao Zhang, Lei Deng, Jian Wu, Qiaoming Zhou, and Chunyan Ma. 2008. Some metrics for accessing quality of product line architecture. In International Con- ference on Computer Science and Software Engineering, Vol. 2. IEEE, 500–503.']","['THE IMPACT OF VARIBILITY ON SOFTWARE PRODUCT LINE ARCHITECTURE RECOVERY This  briefin  reports  evideice  of  ai exploratory  study  oi  the  recovery  of software  product  liie  architectures iivestinatiin  how  the  variability  cai affect the recovery processf FINDINGS \uf0b7 The  variability  identied  on  the Product  Line  Architecture  (PLA) reduced the metrics values related to component reuse rate.  \uf0b7 As a result of the study, we identied that  some  PLAs  did  not  provide variability  in  architectural  level.  In other words, all the Sofware Product Line  (SPL)  products  have  the  same architecture.  In  these  projects,  the variability is implemented in low-level granularity and did not afect the PLA structure.  \uf0b7 The metrics implemented in the study tend  to  be  more  precise  when  we included  more  products  in  the comparison to recover the PLA. \uf0b7 We  identied  a  correlaton  between the  metrics  Structure  Similarity Coefficient  (SSC)  and  Component Reuse Rate (CRR).  The higher the SSC value, the higher CRR values tends to be.  \uf0b7 We identied that projects with high values  of  CRR were the  ones  where the  number  of  system  classes outnumbered the features.  \uf0b7 Projects  with  no  detected  variability on architectural levels were also the one  where  the  number  of  classes outnumbered the features. \uf0b7 The  study  provided  the  PLA  of  15 open  source  SPL  projects  extracted from diferent repositories.  \uf0b7 The  T-Wise  method  can  be  used  to improve  the  recovery  process  by using  fewer  products,  this  must  be further investgated. \uf0b7 Some  PLA  showed  that  some  SPL projects could have bad code smells by  analyzing  the  Design  Structure Matrix (DSM).  \uf0b7 The DSM can be used to support the project for code smells identicaton and other code characteristcs. \uf0b7 Projects  with  the  higher  number  of optonal features were the ones with lower values of CRR.  Keywords:  Sofware Product Lines Product Line Architectures Variability Product Line Architecture Recovery Who is this briefin  or? Sofware engineering practtoners who want to make decisions about product  line architecture recovery based on  scientic evidence. Where the fidiins come  rom? All indings of this brieing were  extracted from exploratory study  conducted by Cardoso et. al. To access other evideice briefins  oi sofware einiieeriin: http:////www.lia.ufc.br//ccbsof2017//pro gramacao-sbcars// For additoial ii ormatoi about the study aid to view the recovered  PLA: https:////sites.google.com//view//sbcars20 17-mpassos//home For additoial ii ormatoi about  aSide @ UFBA - sofware desini aid evolutoi research nroup: http:////wiki.dcc.uba.br//Aside//']","**Title:** Enhancing Software Product Line Architectures through Effective Recovery Techniques

**Introduction:**  
This briefing summarizes findings from an exploratory study investigating the impact of variability on the recovery of Software Product Line Architectures (PLA). It aims to provide insights into how the recovery process can be optimized and the implications for reuse assessment in software product lines.

**Main Findings:**  
The study analyzed 15 open-source Software Product Line (SPL) projects to evaluate the effectiveness of the Product Line Architecture Recovery (PLAR) tool. Key findings include:

1. **Impact of Product Count on Variability Identification:** The number of products used in the recovery process significantly influences the precision of variability identification in the PLA. More products lead to better detection of variable components, which stabilizes the architecture's representation after analyzing a sufficient number of configurations. This suggests that selecting a minimum set of representative products is essential for effective PLA recovery.

2. **Correlation between Optional Features and Reuse Rate:** The study found a notable relationship between the number of optional features in an SPL and the component reuse rate. As the number of optional features increased, the component reuse rate (CRR) tended to decrease, indicating that optional features introduce variability that can complicate reuse. This insight is crucial for SPL architects aiming to balance flexibility and reusability.

3. **Reuse Assessment Metrics:** The research employed various metrics, such as Structure Similarity Coefficient (SSC), Structure Variability Coefficient (SVC), and Component Reuse Rate (CRR), to assess the recovered PLAs. The results showed that a high SSC often correlated with high CRR, suggesting that architectures with more common components tend to have better reuse characteristics.

4. **Recommendations for Practice:** The findings recommend that practitioners focus on identifying and selecting a representative sample of products for PLA recovery. Additionally, SPL architects should consider the implications of optional features on reuse rates when designing and managing SPLs.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, architects, and researchers involved in the development and management of Software Product Lines, particularly those interested in architecture recovery and reuse assessment.

**Where the findings come from?**  
All findings in this briefing are derived from an exploratory study conducted by Mateus Passos Soares Cardoso et al., which investigated the recovery of PLAs from 15 open-source SPL projects using the PLAR tool.

**What is included in this briefing?**  
This briefing includes insights on the impact of product count on variability identification, the relationship between optional features and reuse rates, and practical recommendations for effective PLA recovery and reuse assessment.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**Original Research Reference:**  
Mateus Passos Soares Cardoso, Crescencio Lima, Eduardo Santana de Almeida, Ivan do Carmo Machado, and Christina von Flach G. Chavez. (2017). Investigating the Variability Impact on the Recovery of Software Product Line Architectures: An Exploratory Study. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017. https://doi.org/10.1145/3132498.3133835"
"['A systematic review on the use of Definition of Done on agile so/f_tware development projects Ana Silva Federal Institute of Paraíba PB-264 Monteiro, Paraíba, Brazil 58500-000 anasilva.ifpb@gmail.com Thalles Araújo Federal Institute of Paraíba PB-264 Monteiro, Paraíba, Brazil 58500-000 thalleshenrique.na@gmail.com João Nunes Federal Institute of Paraíba PB-264 Monteiro, Paraíba, Brazil 58500-000 lockenunes@gmail.com Mirko Perkusich Federal University of Campina Grande 882 Aprigio Veloso street Campina Grande, Paraíba, Brazil 58429-900 mirko.perkusich@embedded.ufcg. edu.br Ednaldo Dilorenzo Federal Institute of Paraíba PB-264 Monteiro, Paraíba, Brazil 58500-000 ednaldo.dilorenzo@ifpb.edu.br Hyggo Almeida Federal University of Campina Grande 882 Aprigio Veloso street Campina Grande, Paraíba, Brazil 58429-900 hyggo@embedded.ufcg.edu.br Angelo Perkusich Federal University of Campina Grande 882 Aprigio Veloso street Campina Grande, Paraíba, Brazil 58429-900 perkusic@embedded.ufcg.edu.br ABSTRACT Background: De/f_inition of Done (DoD) is a Scrum practice that con- sists of a simple list of criteria that adds veri/f_iable or demonstrable value to the product. It is one of the most popular agile practices and assures a balance between short-term delivery of features and long-term product quality, but little is known of its actual use in Agile teams. Objective: To identify possible gaps in the literature and de/f_ine a starting point to de/f_ine DoD for practitioners through the identi- /f_ication and synthesis of the DoD criteria used in agile projects as presented in the scienti/f_ic literature. Method: We applied a Systematic Literature Review of studies published up to (and including) 2016 through database search and backward and forward snowballing. Results: In total, we evaluated 2326 papers, of which 8 included DoD criteria used in agile projects. We identi/f_ied that some studies presented up to 4 levels of DoD, which include story, sprint, re- lease or project. We identi/f_ied 62 done criteria, which are related to software veri/f_ication and validation, deploy, code inspection, test process quality, regulatory compliance, software architecture Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. EASE’17, Karlskrona, Sweden © 2017 ACM. 978-1-4503-4804-1/17/06. . . $15.00 DOI: http://dx.doi.org/10.1145/3084226.3084262 design, process management, con/f_iguration management and non- functional requirements. Conclusion: The main implication for research is a need for more and better empirical studies documenting and evaluating the use of the DoD in agile software development. For the industry, the review provides a map of how DoD is currently being used in the industry and can be used as a starting point to de/f_ine or compare with their own DoD de/f_inition. CCS CONCEPTS •Software and its engineering → Agile software development; KEYWORDS Agile Software Development, De/f_inition of Done, Systematic Liter- ature Review ACM Reference format: Ana Silva, Thalles Araújo, João Nunes, Mirko Perkusich, Ednaldo Dilorenzo, Hyggo Almeida, and Angelo Perkusich. 2017. A systematic review on the use of De/f_inition of Done on agile software development projects. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017, 10 pages. DOI: http://dx.doi.org/10.1145/3084226.3084262 1 INTRODUCTION Scrum is the most popular agile method [ 13]. It is a framework', 'Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017, 10 pages. DOI: http://dx.doi.org/10.1145/3084226.3084262 1 INTRODUCTION Scrum is the most popular agile method [ 13]. It is a framework to manage agile projects and de/f_ines artifacts, practices and roles. To promote continuous assessment of the quality of the product and guarantee incremental delivery, Scrum presents the practice De/f_inition of Done (DoD). It consists of a set of criteria to de/f_ine', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Silva et al. if a deliverable is done. In other words, it de/f_ines the minimal restrictions that must be ful/f_illed for a product to be released. According to Sutherland and Schwaber [ 11], DoD promotes transparency between the stakeholders on the meaning of com- pleting work and is a key concept in Scrum. It It can signi/f_icantly vary between diﬀerent teams, because it should be de/f_ined given their context. According to Williams [15], in which data from 326 practitioners were analyzed, DoD is the most popular agile practice along with short iterations and continuous integration. On the other hand, Williams [15] concluded that the principles in the Agile Manifesto are not enough to emphasize the need to produce high quality systems and the importance of non-functional requirements. As a consequence, the focus on fast delivery of customer value in a short period of time causes technical debt, which will, as time progresses, slow the progress. The problem was aggravated with the popularity of Scrum, which is not composed of quality management techniques such as test automation, continuous integration and pair programming. This is not a failure, but a characteristic: Scrum is a framework that must be complemented with technical and managerial processes. The use of Scrum with focus only on managerial activities is, as described by Martin Fowler, an in/f_luential Software Engineering practitioner, Flaccid Scrum . In a study performed with 18 teams of 11 companies, Eloranta et al. [ 6] identi/f_ied that one of the most common anti-pattern on Scrum projects is the lack of testing and test automation during the iteration in which the code was pro- duced. As a consequence, defects that were introduced into the code are not detected and the code evolves on top of code with a high probability of containing defects, which improves technical debt. Furthermore, delaying activities related to code quality is an obstacle for decision making in agile software development [4]. These issues can be avoided by an appropriate use of the DoD. Even though DoD is a key concept, to the best of our knowledge, there is no study that explored which criteria are being used in the context of agile software development. According to Dybå and Dingsøyr [5], there is a need of research to assess which agile practices proposed by practitioners are really useful. In this context, the main goal and contribution of this paper is to report the state of the are in the /f_ield of DoD in agile software development by means of a systematic literature review. We followed the guidelines proposed by Kitchenham and Charters [ 8] and Wohlin [ 16]. In this paper, we detail our study and also point the gaps and future directions for research in DoD for agile software development. This article is structured as follows. In Section 2, we present the protocol of our systematic review process. In Section 3, we present our /f_indings. In Section 4, we discuss the results. In Section 5, we present our conclusions and recommendations for future research. 2 REVIEW METHOD According to Kitchenham and Charters [8], systematic review is an evidence-based technique that uses a well-de/f_ined, unbiased and repeatable methodology to identify, analyze and interpret all the relevant papers related to a speci/f_ic research question, subject area, or phenomenon of interest. A key component of this technique is the protocol, which is a plan that describes the conduct of the systematic review. It includes the research questions, search process, selection process and data analysis procedures. A summary of the protocol used in this review is given in the following sub-sections. Database search Obtain primary studies Advanced criteria exclusion Snowballing (Backward and Forward) Basic criteria exclusion Figure 1: Overview of the search and selection process 2.1 Research questions The aims of this research are to identify possible gaps in the liter-', '(Backward and Forward) Basic criteria exclusion Figure 1: Overview of the search and selection process 2.1 Research questions The aims of this research are to identify possible gaps in the liter- ature and de/f_ine a starting point to de/f_ine DoD for practitioners through the identi/f_ication and synthesis of the DoD criteria used in agile projects as presented in the scienti/f_ic literature. Given this, we formulated the following research questions (RQs): RQ1 What are the done criteria used in agile software develop- ment projects? RQ2 What are the characteristics of the application domain of the papers that report the done criteria identi/f_ied in RQ1? RQ3 What types of studies are performed in the papers the report the done criteria identi/f_ied in RQ1? 2.2 Search strategy To minimize the probability of missing relevant papers, we used a hybrid search strategy, which is based on database search and snowballing (backward and forward). First, we de/f_ined a search string, which is presented in Section 2.2.1 and used it to search databases containing scienti/f_ic papers in the context of Computer Science, as shown in Section 2.2.2. After applying the basic criteria exclusion, shown in Section 2.3, the resulting papers were de/f_ined as the starting (i.e., seed) set for the snowballing. After executing the snowballing iterations, we applied the advanced criteria exclusion, which is related to the actual data extraction and quality assessment. We show an overview of the search and selection process in Figure 1. We decided to use this strategy to avoid missing papers due to limitations and inconsistencies of digital libraries. They have diﬀerent formats to handle the Boolean expressions, as discussed in Brereton et al. [3], and we were not sure how reliable is their ability to handle searches with long strings. Finally, there are evidences in the literature of the risks of missing papers using only one approach, as discussed in Badampudi et al. [2] and Felizardo et al. [7]. 2.2.1 Search terms. Starting with the research questions, suit- able keywords were identi/f_ied using synonyms and related terms. The following keywords were used to formulate the search string:', 'A systematic review on the use of Definition of Done on agile so/f_tware development projects EASE’17, June 15-16, 2017, Karlskrona, Sweden • Population: Agile software development. Alternative key- words: Scrum, Extreme Programming, Feature-Driven De- velopment, Dynamics Systems Development Method, Kan- ban, Crystal, Lean software development and DevOps. • Intervention: De/f_inition of done.Alternative keywords: Done criteria, DOD and de/f_inition of ready. • Context: Industry or academia. Our target population was papers performed in the industry or academy and we intended to capture papers in that context regardless of the type of research performed. Not used in the search string. To de/f_ine a /f_irst version of the search string, the keywords within a category were joined by using the Boolean operator ’OR’ and the two categories were joined using the Boolean operator ’AND’. This was done to target only papers in the context of agile software development related to de/f_inition of done. To simplify the strings and include additional synonyms and Portuguese translations, we de/f_ined the following search string: (“de/f_inition of ready” OR “de/f_inition of done” OR “done criteria” OR “dod” OR “de/f_inição de preparado” OR “de/f_inição de pronto”) AND (software AND (agile OR ágil) AND (scrum OR xp OR (crys- tal AND (clear OR orange OR red OR blue)) OR dsdm OR fdd OR “feature driven development” OR (lean AND (development OR de- senvolvimento)) OR Kanban OR “extreme programming” OR “pro- gramação extrema” OR devops)) 2.2.2 Data sources. Since our goal is to cover the literature published in Computer Science, we chose the following digital databases for data retrieval: • ACM Digital Library • Engineering Village • Science Direct • Scopus • Springer • Web of Science • Wiley Online Library We did not include IEEExplore because it could not handle our search string due to its size. On the other hand, Scopus, Engineering Village and Web of Science indexes IEEE papers. 2.3 Selection criteria Before applying the selection criteria given the topic of the review, we de/f_ined a generic exclusion criteria: • Published in non-peer reviewed publication channel such as books, thesis or dissertations, tutorials, keynotes, etc. OR • Not available in English or Portuguese language OR • A duplicate. We implemented the /f_irst two criteria in the search strings that were executed in the digital libraries, wherever possible. For the third criteria, since there are intersections of the coverage of journal and conferences by the libraries, we implemented a script to auto- matically remove papers with the same title, authors and abstract. Afterwards, the remaining papers were evaluated through two sets of selection criteria: basic and advanced. 2.3.1 Basic criteria. We applied the basic criteria to evaluate if papers are relevant to the aims of our paper by reading the titles and abstract. This criteria was applied on papers that passed the Reviewer 2 Relevant Uncertain Irrelevant Relevant Uncertain Irrelevant A B B C D D E E F Reviewer 1 Figure 2: Paper selection possibilities. generic exclusion criteria and were identi/f_ied through database search or snowballing. In this context, we included papers that: • Are related to agile software development AND • Are related to De/f_inition of Done. To evaluate if a paper was related to de/f_inition of done, we were not rigorous. Even if the term was not included in the title or abstract, we still included the paper if it presented a case (e.g., company or project) in which an agile process was applied due to the chance of the description present examples of done criteria. We excluded papers that: • Are not related to agile software development OR • Are not related to De/f_inition of Done. Following the procedure presented in Ali et al. [1], we decided that papers are classi/f_ied as: Relevant, Irrelevant or Uncertain (in the case the available information on the title and abstract is in-', 'that papers are classi/f_ied as: Relevant, Irrelevant or Uncertain (in the case the available information on the title and abstract is in- conclusive). Each paper was evaluated by two reviewers, which were chosen randomly. Given this, there are six possibilities of agreement or disagreement between the reviewers, as shown in Figure 2. Papers evaluated as A or B are selected for inclusion in the next step of the paper, because at least one reviewer evaluated them as relevant. Therefore, since we used a ”when in doubt, include"" approach, we include them. Papers evaluated as F are excluded, as both reviewers agreed on their irrelevance. Papers evaluated as C were further reviewed independently by both reviewers using the steps of adaptive reading, as described below. With this practice, we collect more details from the paper to assist on decision making. Papers evaluated as D and E show disagreement, in which cat- egory D is the worst, because one author considers the paper ir- relevant and the other considers it relevant. Papers in these two categories were discussed by the reviewers. As a result, through a consensus, the papers were classi/f_ied as category A, C or F. The adaptive reading process is composed of three steps: (1) read the Introduction, (2) if not having agreement in 1, read conclusion, (3) if not having agreement in 2, use the keywords to evaluate their usage to describe the context of the paper. 2.3.2 Advanced criteria. The advanced criteria is related to the actual data extraction, in which the full-text of the papers was read by two reviewers independently. We used the same criteria used in the previous steps (see Section 2.3.1), but reading the full-text. We excluded studies published in multiple papers, only including the', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Silva et al. extended version of the study. Additionally, we excluded papers that did not contain relevant information for RQ1. In other words, a paper was only included if it contained examples of done criteria. Following the advice presented in Brereton et al. [3] and Staples and Niazi [10], each paper was evaluated by one data extractor and one data checker. The data extractor completed the extraction form and the data checker con/f_irmed that the data on the extraction form were correct. Any disagreement were discussed and an agreed /f_inal data value recorded. Details regarding the quality assessment and data extraction process are presented, respectively, in Section 2.5 and Section 2.6. 2.4 Snowballing The snowballing approach was, /f_irst, performed on the set of papers identi/f_ied through the database search and included using the basic criteria. For each paper in the set, we applied the backward and forward snowballing. To execute the forward snowballing, we used Scopus and Google Scholar to identify the title and abstract of the papers citing our set of selected papers. We applied the basic criteria, shown in Section 2.3.1, to include these papers, in which each paper was evaluated by two reviewers. To execute the backward snowballing, /f_irst, we distributed the papers to be evaluated by only one reviewer. The reviewer was responsible to apply the generic exclusion criteria shown in Sec- tion 2.3. This was done by evaluating the title in the reference list and, if necessary, the place of reference in the text. Afterwards, the included studies were evaluated using the basic criteria, in which each paper was assessed by two reviewers. We decided not to follow the guidelines for snowballing pre- sented in Wohlin [16], in which it is recommended that the /f_inal inclusion of a paper should be done based on the full paper, because our research questions are too speci/f_ic. Therefore, it is possible that papers that do not present done criteria are cited by or refer- ence papers by relevant papers. Instead, we used the basic criteria (shown in Section 2.3). 2.5 Quality assessment The quality assessment of this research followed eleven criteria established by Dybå and Dingsøyr [ 5]. These criteria are listed below: 1. Is the paper based on research (or is it merely a “lessons learned” report based on expert opinion)? 2. Is there a clear statement of the aims of the research? 3. Is there an adequate description of the context in which the research was carried out? 4. Was the research design appropriate to address the aims of the research? 5. Was the recruitment strategy appropriate to the aims of the research? 6. Was there a control group with which to compare treatments? 7. Was the data collected in a way that addressed the research issue? 8. Was the data analysis suﬃciently rigorous? 9. Has the relationship between researcher and participants been considered to an adequate degree? 10. Is there a clear statement of /f_indings? 11. Is the paper of value for research or practice? To evaluate each of the criteria listed above, we used a dichoto- mous scale, receiving a score of “1” for “yes” and “0” for “no”. As discussed in Section 2.3.2, each paper was evaluated by a data ex- tractor and a data checker. 2.6 Data extraction We used a spreadsheet editor to record relevant information. With the spreadsheet, we were able to map each datum extracted with its source. From each paper, we extracted general information such as year and name of the publication channel and data related to the RQs. The following data were extracted from the papers: (i) type of article (journal, conference), (ii) name of the publication channel, (iii) year of publication, (iv) agile method, (v) product domain, (vi) application domain, (vii) team size, (viii) team distribution, (ix) number of cases, (x) research type, (xi) research question type, (xii) empirical research type, (xiii) research validation,', '(vi) application domain, (vii) team size, (viii) team distribution, (ix) number of cases, (x) research type, (xi) research question type, (xii) empirical research type, (xiii) research validation, (xiv) de/f_inition of done levels, (xv) done criteria. For question (x), we used the classi/f_ication presented by Wieringa et al. [14]: validation research, evaluation research, solution pro- posal, philosophical papers, opinion papers or experience papers. For (xi), we used the classi/f_ication presented by Shaw [9]: method or means of development; method for analysis or evaluation; design, evaluation, or analysis of a particular instance; generalization or characterization; or feasibility study or exploration. For (xii), we used the classi/f_ication presented by Tonella et al. [12]: experiment, observational study, experience report, case study or systematic review. For (xiii), we used the classi/f_ication scheme presented by Shaw [9]: analysis, evaluation, experience, example, persuasion or blatant assertion. The data collected for questions (xiv) and (xv) were synthesized to answer RQ1. 3 RESULTS In this section, we present the results for the systematic review process and for the research questions as well. In Figure 3, we present an overview of the number of studies passing through the diﬀerent stages of the study. We show details of the results of the database search in Figure 4 and of the results of the snowballing process, in which we iterated twice, in Figure 5 and Figure 6. We present the list of the included studies in Table 1. In Figure 7, we show the number of papers per year. In Fig- ure 8, we show the distribution of papers per type of publication channel. In Figure 9, we show the aggregated results of the quality assessment. By analyzing Figure 9, we concluded that the qual- ity of the studies from an evidence-based perspective is low. On', 'A systematic review on the use of Definition of Done on agile so/f_tware development projects EASE’17, June 15-16, 2017, Karlskrona, Sweden Table 1: Overview of the selected studies. Paper Number Authors Year Title Publication channel P1 Eloranta, V., Koskimies, K., Mikkonen, T. 2016 Exploring ScrumBut—An empirical paper of Scrum anti-patterns Information and Software Technology P2 Vlietland, J., Solingen R. V., Vilet, H. V. 2016 Aligning codependent Scrum teams to enable fast business value delivery: A governance framework and set of intervention actions The Journal of Systems and Software P3 Rindell, K., Hyrynsalmi, S., Leppänen, V. 2015 Securing Scrum for VAHTI Symposium on Programming Languages and Software Tools P4 Davis, N. 2013 Driving quality improvement and reducing technical debt with the de/f_inition of done Agile Conference P5 O’Connor, C. P. 2010 Letters from the edge of an agile transition ACM International Conference Companion on Object Orientad Programming Languages and Applications Companion P6 Igaki, H., Fukuyasu, N., Saiki, S., Matsumoto, S., Kusumoto, S. 2014 Quantitative assessment with using ticket driven development for teaching scrum framework International Conference on Software Engineering Companion P7 Gupta, R. K., Manikreddy, P. Naik, S., Arya, K. 2016 Pragmatic Approach for Managing Technical Debt in Legacy Software Project India Software Engineering Conference P8 Saddington, P. 2012 Scaling product ownership through team alignment and optimization Agile Conference    Database search Seed set: 16 Snowballing search (Backward and Forward) Preliminary selected studies: 20 Advanced criteria Final selected studies: 8 Figure 3: Number of papers in study selection. the other hand, due to the topic of the study, it is acceptable to include industrial experience reports with the risk of less reliable conclusions. ACM Digital Library: 258 Engineering Village: 8 Science Direct: 564 Scopus: 95 Springer: 272 Web of Science: 3 Wiley Online Library: 515 Total: 1715 Basic criteria 781 Duplicates 934  Remaining Seed set: 16 918 Excluded 16 Remaining Figure 4: Overview of the database search. 3.1 RQ1: Done criteria In this section, we show the done criteria identi/f_ied in the included papers. A total of 62 criteria were identi/f_ied. Three papers, out of the eight included papers, applied multilevel DoD, as shown in Table 2. In Table 2, P4 is presented twice, because it presents two cases with diﬀerent done criteria. Furthermore, in Table 2, we present the done criteria exactly as they are presented in the papers. By analyzing Table 2, it is possible to conclude that there are diﬀerent types of criteria such as activity (e.g., peer code review), metrics (e.g., localization defect density), targets (e.g., product com- mitted to VCS), standards (e.g. coding standards) and checklist (e.g.,', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Silva et al.    P1 20 Citations Cited by 0 19 excluded 1 included P2 60 Citations Cited by 3 62 excluded 1 included P3 71 Citations Cited by 0 71 excluded P4 99 Citations Cited by 5 104 excluded P5 18 Citations Cited by 0 16 excluded 2 included P6 20 Citations Cited by 0 20 excluded P7 0 Citations Cited by 7 7 excluded P8 99 Citations Cited by 5 104 excluded P9 3 Citations Cited by 0 3 excluded P10 1 Citations Cited by 3 4 excluded P11 37 Citations Cited by 86 123 excluded P12 0 Citations Cited by 1 1 excluded P13 0 Citations Cited by 0 P14 18 Citations Cited by 4 22 excluded P15 2Citations Cited by 0 2 excluded P16 7 Citations Cited by 0 7 excluded  (P17)  (P18)  (P19, P20) Figure 5: Results of the /f_irst snowballing iteration. P17 16 Citations Cited by 4 20 excluded P18 17 Citations Cited by 0 17 excluded P19 1 Citations Cited by 0 1 excluded P20 81 Citations Cited by 16 97 excluded Figure 6: Results of the second snowballing iteration. Figure 7: Number of papers per yer. design review checklist). To categorize the identi/f_ied criteria, we normalized them by relating them to activities. For instance, in study P7, the criteria No open static analysis errors , was associated 5 2 Journal Conference Conference Journal 2  6  Figure 8: Distribution of papers per type of publication chan- nel. Figure 9: Results of the quality assessment. with the activity Static code analysis . Afterwards, we categorized', 'A systematic review on the use of Definition of Done on agile so/f_tware development projects EASE’17, June 15-16, 2017, Karlskrona, Sweden Table 2: Done criteria identi/f_ied. Paper Number Number of levels Done criteria P1 1 Unit Tested, Integration Tested, Acceptance tested, Delivered and installed. P2 1 Functionality of a story worked in accordance with the feature stories, including the integration with the application connectivity. System tested, including interfacing and middleware testing P3 3 (basic, heightened and high) Basic: Security training, Additional security training, Application risk analysis, Test plan review, Threat modeling, threat modeling updates. Heightened: Goal and criticality de/f_inition, Business impact analysis, Documentation of security solutions, Architecture security requirement analysis, Security auditing, Security testing, Application security setting de/f_inition.High: Architectural and development guidelines, External interfaces review, Attack surface recognition and reduction, Architectural security requirements, Internal communication security, Security test cases review, Test phase code review, Use of automated testing tools, Security mechanism review, Development time auditing. P4 3 (story, sprint and release) Story: Design (according to design standards), Design peer review (using design review checklist), Code and unit Test (according to coding standards), Code peer review (using code review checklist), Acceptance test written (goal of 100% automation), Acceptance test peer review (using test review checklist). Sprint and Release: Localization Defect Density, Performance Test, System Test, Static Analysis (using organizational rules), Security Review P4 4 (story, sprint, release and project) Story: Development planning, Requirements analysis, Architectural design, Detailed design, Verify detailed design, Unit test implementation and veri/f_ication, Code veri/f_ication, Integration and integration testing and Systems testing. Sprint: Development planning, Integration and integration testing, System testing and regression testing Release: Development planning, Integration and integration testing, System testing and regression testing and release. Project: Development planning, Risk management planning, Requirements analysis, Risk control measures, Re-evaluate medical device risk analysis, Verify software requirements, Architectural design, Identify segregation for risk control, Verify software architecture. P5 1 Acceptance tests P6 1 Product (source code, test code or review reports) committed to VCS, Source code is reviewed by a diﬀerent person, source code in VCS must be able to be built normally after committing by developers, the number of assigned task for a developer should be within +-20% of the average of all the assigned task of the given type (source code, unit code, review), unit tests completed. P7 1 No open static analysis errors, No open memory leak violation, No degradation in performance compared to baseline, Reviewed by experts. P8 2 (sprint and release) Functionality complete, has been through technical design following a code quality checklist (not available), new service APIs de/f_ined, services integrated, meets all acceptance criteria for user story, UI conforms to approved wireframes and Visual Comps/Red Lines, committed to SVN against the Jira ID for the User Story, Build is successful after code committed, all errors generated within Flex PMD are removed, code is run through Flex Formatter to correct any formatting issues, Tech Design Checklist is reviewed and completed, Unit Testes written and veri/f_ied, Code conforms to Code Standards Guide (not available), Code Peer Reviewed following Code Review guidelines (not available), Functional testes written, Functional testes reviewed by PO, PO accepts story as complete, Jira is updated for story, Con/f_luence is updated, All DB scripts for story must be documented, packaged and reviewed by DBA.', 'written, Functional testes reviewed by PO, PO accepts story as complete, Jira is updated for story, Con/f_luence is updated, All DB scripts for story must be documented, packaged and reviewed by DBA. each activity given its end goal. For instance, Unit test was as- sociated with Software veri/f_ication and validationand Static code analysis with Code inspection . We de/f_ined a separate category for Non-functional requirements -related activities, because they require specialized testing. A special case is for standards compliance. In P3, the criteria were de/f_ined to comply with VAHTI, a Finish gov- ernmental security standard collection. In P4, case 2, the criteria were de/f_ined to comply with FDA IEC 62304 and TIR45, which are in the context of medical device software industry. In both cases, we associated their unique activities to External standards/regulatory compliance. In Figure 10, we present the distribution of the criteria given the identi/f_ied activities (i.e., categories). Notice that each criteria was only counted once, even if it was presented by several papers. In Figure 11, we present the frequency in which a criteria was presented in the papers. Notice that since P4 presents two cases, it is possible that a criteria was counted twice for it. Further- more, only criteria that were presented in at least two studies are presented in Figure 11. 3.2 RQ2: Characteristics of application domain of DoD Since the application of DoD depends on the context of the project, to analyze the done criteria reported in a given study, it is impor- tant to analyze its context. Therefore, we collected data regarding the agile method used, product domain, application domain (i.e., industry or academic), team size, team distribution and the number', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Silva et al. Software  verification and  validation Deploy Regulatory  Compliance Code  inspection Test process  quality Software  architecture  design Process  management Configuration  management Non-functional  requirements check Figure 10: Distribution of criteria per category.    Unit test Peer code review Acceptance test System test Integration test Static code analysis Security review Design Design Review Product committed Figure 11: Frequency of criteria. of cases. In Table 3, we show the collected data for each primary study. 3.3 RQ3: Characteristics of the studies performed This RQ is designed to identify the characteristics of the studies that reported done criteria. The design of the study along with the quality assessment data can be used to infer the scienti/f_ic relevance of the /f_indings. In Table 4, we show the collected data for each primary study, following the criteria presented in Section 2.6. 4 DISCUSSION The results of this study address three research questions (see Sec- tion 2.1), which explore the done criteria used in agile software development projects. We observed that the reporting of done criteria is recent. The /f_irst paper was published in 2010. As shown in Figure 9, the average quality of the studies, in the perspective of evidence-based science, is low. This is because most papers were written, apparently, by practitioners presenting an experience of applying the De/f_inition of Done, along with other practices, on agile software development projects. On the other hand, even though we only identi/f_ied 8 papers, we identi/f_ied 62 criteria, which are applied with diﬀerent goals, as shown in Figure 10. Since seven, out of eight, of the papers are in the context of Scrum, we will discuss the diﬀerent use of done criteria in this context. Since Scrum is a framework to manage agile projects, it requires to be complemented with technical and managerial practices. P6 and P8 de/f_ine done criteria to complement Scrum with con/f_igura- tion management practices. They de/f_ine a criteria to assess if the code is committed to VCS. P6 also veri/f_ies if, in the case of contin- uous integration, a successful build is generated by a developer’s commit. P8 presents criteria to assess the management process: update project management tools and documentation. P6 presents criteria to assist on task management and avoid unequal distribu- tion of eﬀort. This shows the potential of using DoD to, given the context of the project (e.g., team experience), de/f_ine a check- list of good process practices to be followed such as committing, branching and issue tracking. P4 and P7 de/f_ine criteria to check non-functional requirements. In the context of P4, criteria to check localization, performance and security were de/f_ined. In P7, they considered performance and memory leaks. In both cases, they present metrics and targets for each non-functional requirement. The targets are used to assist on deciding if a given story, sprint or release is done. In other contexts, we assume that it could be valuable to use DoD to assess other non-functional requirements such as battery consumption, for embedded systems, and interoperability, for web applications that must be accessed by others via APIs.', 'A systematic review on the use of Definition of Done on agile so/f_tware development projects EASE’17, June 15-16, 2017, Karlskrona, Sweden Table 3: Overview of the selected studies. Paper Number Agile method Product domain Application domain Team size Team distribution Number of cases P1 Scrum Not presented Industry Various Not de/f_ined Multiple P2 Scrum Banking Industry Multiple codependent teams Not de/f_ined 1 P3 Scrum Not presented None None None None P4 Scrum Not presented Industry 8 Collocated 2 P5 Non speci/f_ic Not presented Industry 62 Collocated 1 P6 Scrum Web application Academic 9 teams of 5-6 Collocated 1 P7 Scrum Web and mobile Industry Teams of 7-8 Distributed 1 P8 Scrum Not presented Industry Multiple Collocated 1 Table 4: Characteristics of the studies performed. Paper Number Research type Research question type Empirical Research type Research validation P1 Experience papers Generalization or characterization Observational study Evaluation P2 Solution proposal Method or means of development Case study Analysis P3 Experience papers None None Experience P4 Experience papers None None Blatant assertion P5 Solution proposal Method or means of development Case study Experience P6 Solution proposal Method or means of development None Experience P7 Solution proposal Method or means of development None Experience P8 Experience papers None None Experience P4 shows criteria for software architectural design. In their case, they noticed that they had to redo a lot of work due to poor design. Therefore, they de/f_ined a project design standard and a design review checklist to decrease the chance of wasting eﬀort redoing work. Depending on the experience of the team and the project complexity, this might be necessary. In any case, it is an example of how DoD can complement an agile process to be more proactive. P1, P2, P4, P5, P6, P7 and P8 present criteria to assist on quality management. P4, P6, P7 and P8 use peer code review as a gate that all code must pass before being considered done. They recommend using a checklist to provide a systematic way of reviewing code. Having a coding standard is also recommended. P4, P7 and P8 use targets regarding the number of open static analysis violations. P1, P2, P6 and P8 de/f_ines unit tests as an activity that must be ful/f_illed to consider a story as done. P1, P2, P4 and P5 considers that acceptance test must be passed to consider a story as done. Finally, P4 presents criteria to assess the quality of the test process, in which they measure the percentage of automated test, with a goal of 100% of automation. Therefore, there are several examples of how DoD can be used to complement the acceptance criteria of user stories and be used as a factor to consider them as done. P3 and P4 show how Scrum can be complemented to comply with external standards that were de/f_ined for traditional waterfall models. In P3, Scrum is complemented with 23 security-related activities to comply with VAHTI, which is a document-driven Fin- ish software security standard. In P4, Scrum is complemented to comply with FDA IEC 62304 and TIR45, which are in the context of medical device software industry. In this case, we can conclude that, given the context of the product being developed, agile processes can use the DoD to complement them and comply with external requirements such as for ISO 9001 and CMMI audits, for instance. Furthermore, P3, P4 and P8 show cases in which multiple levels of done were used. In P3, the levels were de/f_ined given the VAHTI documentation. In P4 and P8, the levels were de/f_ined given the main output of the project. A project is composed of releases. Within a release, sprints are executed. As a result of sprints, stories are delivered. The number of levels is a function of the complexity of the process required to deliver the project. For instance, in P4, they initially used three levels of done: story, sprint and release. On the', 'the process required to deliver the project. For instance, in P4, they initially used three levels of done: story, sprint and release. On the other hand, to adapt their process to be compliant with external standards, they added a new level: project. As shown in Figure 11, out of the 62 criteria, only one was present in 5 studies: unit test. Only two were present in 4 studies:', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Silva et al. peer code review and acceptance test. Only three was present in 3 studies: system test, integration test and static code review. This disagreement is expected, because, as claimed by the Scrum Guide, the DoD should be de/f_ined by the team according to its context. On the other hand, the data shows motivations to apply DoD. Regarding the consequences of using DoD, in P8, the authors claim that using the presented DoD, along with other interventions, improved the collaboration between the teams and they reached a 130% decrease in defects. P7 documents that the constraints in the process promoted by the DoD also promoted collaboration, as the members were forced to rotate between diﬀerent types of tasks (source code, unit tests and code review). P6 presents data on how the increased productivity and reduced technical debt. P4 reduced from 278 to 12 defects deferred, reduced the percentage of reopened defects from 20% to less than 1% and improved the team net promoter score from 63% to 70%. Even though the scienti/f_ic rigor of some of the studies is not high, we believe that these data can be used as an indicator that the correct use of DoD can help agile teams to work better. 5 THREATS TO VALIDITY A potential threat to validity is, as for any systematic literature review, if we were able to cover all primary studies. We aimed to construct a comprehensive string, covering key terms and their synonyms and applied the search strings to multiple databases that cover major Software Engineering conferences and journals. Furthermore, we complemented the database search with forward and backward snowballing. In regard to the quality of the selection of the study and data extraction, we used a systematic approach in which each paper was evaluated by at least two reviewers, to avoid reviewer bias and human errors. 6 CONCLUSION AND FUTURE WORK This study presents a systematic literature review on de/f_inition of done in agile software development. We used a hybrid approach composed of database search and snowballing. The primary search fetched 934 unique results, from which 16 papers were selected as a seed set for snowballing. In the snowballing, which was composed of 2 iterations, we evaluated an additional 611 papers, in which 4 were added to our set of selected papers. After quality assessment and data extraction, only 8 papers were included in the study. Data from these papers were analyzed to answer each research question. There is a variety of done criteria used in agile software devel- opment. Furthermore, some papers used a multilevel approach to manage DoD, which include story, sprint, release or project. We categorized the criteria into: software veri/f_ication and validation, deploy, code inspection, test process quality, regulatory compliance, software architecture design, process management, con/f_iguration management and non-functional requirements. There was not a high rate of agreement of the done criteria, which is expected since they should be de/f_ined given the context of the project. All papers presented activities that must be ful/f_illed to consider a story, sprint, release or project done. On the other hand, only three papers presented metrics and targets to assess that the activities are in conformance to the expected criteria. Finally, seven, out of the eight papers, are in the context of Scrum. Practitioners can use the results of this study as a guide for them to apply DoD on their projects or compare with their own DoD de/f_i- nition. Moreover, based on the results of this study, we recommend that there is strong need to publish more papers presenting how DoD is applied in agile projects and to conduct empirical studies to assess the results of applying this practice. For future work, we intend to execute a survey with practitioners and compare the collected data with our results in this study. Furthermore, we will', 'to assess the results of applying this practice. For future work, we intend to execute a survey with practitioners and compare the collected data with our results in this study. Furthermore, we will broaden the research aims by exploring the eﬀectiveness of the claimed bene/f_its, challenges on using DoD, factors that in/f_luence on de/f_ining the criteria and compare the state-of-practice with the authoritative guidelines and textbooks. REFERENCES [1] Nauman Bin Ali, Kai Petersen, and Claes Wohlin. 2014. A Systematic Literature Review on the Industrial Use of Software Process Simulation. J. Syst. Softw. 97, C (Oct. 2014), 65–85. DOI:http://dx.doi.org/10.1016/j.jss.2014.06.059 [2] Deepika Badampudi, Claes Wohlin, and Kai Petersen. 2015. Experiences from Using Snowballing and Database Searches in Systematic Literature Studies. In Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering (EASE ’15) . ACM, New York, NY, USA, Article 17, 10 pages. DOI:http://dx.doi.org/10.1145/2745802.2745818 [3] Pearl Brereton, Barbara A. Kitchenham, David Budgen, Mark Turner, and Mo- hamed Khalil. 2007. Lessons from applying the systematic literature review process within the software engineering domain. Journal of Systems and Soft- ware 80, 4 (2007), 571 – 583. DOI:http://dx.doi.org/10.1016/j.jss.2006.07.009 [4] Meghann Drury, Kieran Conboy, and Ken Power. 2012. Obstacles to Decision Making in Agile Software Development Teams. J. Syst. Softw. 85, 6 (June 2012), 1239–1254. DOI:http://dx.doi.org/10.1016/j.jss.2012.01.058 [5] Tore Dybå and Torgeir Dingsøyr. 2008. Empirical studies of agile software development: A systematic review. Information and Software Technology 50, 9–10 (2008), 833 – 859. DOI:http://dx.doi.org/10.1016/j.infsof.2008.01.006 [6] Veli-Pekka Eloranta, Kai Koskimies, and Tommi Mikkonen. 2016. Exploring ScrumBut—An empirical study of Scrum anti-patterns. Information and Software Technology 74 (2016), 194 – 203. DOI:http://dx.doi.org/10.1016/j.infsof.2015.12. 003 [7] Katia Romero Felizardo, Emilia Mendes, Marcos Kalinowski, Érica Ferreira Souza, and Nandamudi L. Vijaykumar. 2016. Using Forward Snowballing to Update Systematic Reviews in Software Engineering. InProceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM ’16). ACM, New York, NY, USA, Article 53, 6 pages. DOI:http://dx.doi. org/10.1145/2961111.2962630 [8] B. Kitchenham and S. Charters. 2007. Guidelines for Performing Systematic Literature Reviews in Software Engineering . Technical Report EBSE-2007-01. Schoolf of Computer Science and Mathematics, Keele University. [9] M. Shaw. 2003. Writing good software engineering research papers. In 25th International Conference on Software Engineering, 2003. Proceedings. 726–736. DOI:http://dx.doi.org/10.1109/ICSE.2003.1201262 [10] Mark Staples and Mahmood Niazi. 2007. Experiences Using Systematic Review Guidelines. J. Syst. Softw. 80, 9 (Sept. 2007), 1425–1437. DOI:http://dx.doi.org/10. 1016/j.jss.2006.09.046 [11] Jeﬀ Sutherland and Ken Schwaber. 2013. The Scrum Guide. http://www.scrumguides.org/docs/scrumguide/v1/Scrum-Guide-US.pdf. (2013). Accessed: 2016-03-02. [12] Paolo Tonella, Marco Torchiano, Bart Du Bois, and Tarja Systä. 2007. Empiri- cal studies in reverse engineering: state of the art and future trends. Empiri- cal Software Engineering 12, 5 (2007), 551–571. DOI:http://dx.doi.org/10.1007/ s10664-007-9037-5 [13] VersionOne. 2016. 10th Annual State of Agile Development Survey Results. (2016). [14] Roel Wieringa, Neil Maiden, Nancy Mead, and Colette Rolland. 2006. Re- quirements engineering paper classi/f_ication and evaluation criteria: a pro- posal and a discussion. Requirements Engineering 11, 1 (2006), 102–107. DOI: http://dx.doi.org/10.1007/s00766-005-0021-6 [15] Laurie Williams. 2012. What Agile Teams Think of Agile Principles. Commun. ACM 55, 4 (April 2012), 71–76. DOI:http://dx.doi.org/10.1145/2133806.2133823', '[15] Laurie Williams. 2012. What Agile Teams Think of Agile Principles. Commun. ACM 55, 4 (April 2012), 71–76. DOI:http://dx.doi.org/10.1145/2133806.2133823 [16] Claes Wohlin. 2014. Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering. InProceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering (EASE ’14) . ACM, New York, NY, USA, Article 38, 10 pages. DOI:http://dx.doi.org/10.1145/2601248. 2601268']","[""THE USE OF DEFINITION OF DONE ON AGILE PROJECTS This  briefing  reports  scientific  evidence on the Definition of Done criteria used in agile  projects  based  on  scientific evidence from a systematic review. FINDINGS The main findings of this paper considered that there are different types of Definition of Done (DoD) criteria, such as: \uf0b7 Activity (e.g., peer code review); \uf0b7 Metrics  (e.g.,  localization  defect density); \uf0b7 Targets  (e.g.,  product  committed  to CVS); \uf0b7 Standards (e.g., coding standards); \uf0b7 Checklist (e.g., design review checklist). The Done Criteria is categorized based on each  activity  given  its  end  goal.  The defined criteria was  software verification and  validation ,  deploy,  r egulatory compliance,  code inspection ,  test process quality,  software  architecture  design , process  management ,  configuration management,  and  Non-functional requirements check. After the process a total of 8 studies were selected. It shows that 3 studies applied multilevel DoD, meaning that to be done, a backlog  item, must be  checked during more than one development step, such as story, sprint, release, etc. Regarding  the  context  of  the  reported projects,  most  of  the  studies  (7  papers) used Scrum as the agile method, and were performed in the industry (6 papers). The  studies  indicate  they  were  done  as experience  papers  and solution proposal since most of them were used in industry. Based on quality assessment performed, we  concluded  that  the  quality  of  the studies  from  an  evidence-based perspective is low. Nevertheless, some studies show the use of DoD as a means of complementing the agile  process  and  comply  with  external requirements such as for ISO 9001, CMMI audits, as well as the addition of activities for assuring the product quality.  Only unit testing criteria was present in 5 studies,  while  peer  code  review  and acceptance test were present in 4 studies, and system test, integration test and static code review were found in 3 studies. This disagreement  is  expected,  because  the DoD  is defined by  team  according to its context. Studies indicate that the use of DoD bring some  benefits  to  the  projects.  However, those  benefits  are  not  clear,  since  they differ  among  studies.  The  main  benefits described are: \uf0b7 Improved  collaboration  between teams; \uf0b7 Increase productivity; \uf0b7 Reduce technical debt; \uf0b7 Reduce  defects  deferred  and  defects reopened; Who is this briefing for? Software engineering practitioners who want to make decisions about definition of  done  in  agile  projects  based  on scientific evidence. Where the findings come from? All  findings  of  this  briefing  were extracted  from  the  systematic  review conducted by Silva et al.   What is included in this briefing? The  main  findings  of  the  original systematic review. Evidence characteristics through a brief description  about  the  original systematic  review  and  the  studies  it analized. What is not included in this briefing? Details about the process performed to achieve  results  presented  in  this briefing report. To access other evidence briefings  on software engineering: http://ease2017.bth.se/ ORIGINAL RESEARCH REFERENCE Silva et al. A systematic review on the use of Definition of Done on agile software development projects. In Proceedings of the 21st International Conference on Evaluation  and Assessment in Software Engineering (EASE'17). 2017""]","**Title:** Understanding the Definition of Done in Agile Software Development

**Introduction:**  
This evidence briefing summarizes findings from a systematic literature review on the Definition of Done (DoD) in agile software development projects. The goal is to identify the criteria used in practice and provide insights for practitioners looking to implement or refine their own DoD.

**Main Findings:**  
The systematic review evaluated 2,326 papers, ultimately identifying 8 studies that provided insights into DoD criteria. A total of 62 distinct criteria were identified, categorized into various areas including software verification and validation, deployment, code inspection, test quality, regulatory compliance, software architecture design, process management, configuration management, and non-functional requirements. 

1. **Diverse Criteria:** The identified DoD criteria vary significantly, reflecting the unique contexts of different teams and projects. Some studies presented criteria at multiple levels, such as story, sprint, release, and project levels, indicating a layered approach to defining completion.
   
2. **Common Practices:** Key practices included unit testing, peer code reviews, acceptance testing, and performance assessments. Notably, unit testing was the most frequently cited criterion, appearing in five of the eight studies.

3. **Contextual Variability:** The application of DoD is highly context-dependent, influenced by factors such as team size, distribution, and the specific agile methodology employed (predominantly Scrum in the reviewed studies). This variability suggests that teams should tailor their DoD to align with their operational context.

4. **Quality Management:** The review highlighted that many teams use DoD to enhance quality management processes. For instance, criteria related to code review and automated testing were emphasized as essential for maintaining high-quality standards in deliverables.

5. **Need for Empirical Studies:** While the review provided a comprehensive overview of the existing literature, it underscored the need for more empirical studies to evaluate the effectiveness and practical application of DoD in agile projects.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, agile coaches, and project managers who are involved in agile software development and are looking to establish or refine their Definition of Done practices.

**Where the findings come from?**  
The findings are derived from a systematic literature review conducted by Ana Silva et al., focusing on studies published up to 2016. The review followed established guidelines for systematic reviews, ensuring a comprehensive and unbiased analysis of the literature.

**What is included in this briefing?**  
The briefing includes a summary of the DoD criteria identified in the literature, insights into their application in various contexts, and recommendations for practitioners on how to approach defining their own DoD.

**What is not included in this briefing?**  
The briefing does not provide detailed statistical analyses or exhaustive descriptions of each study's findings. It focuses on synthesizing core insights relevant for practitioners rather than presenting all empirical data.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research conducted:**  
Ana Silva et al. (2017). A systematic review on the use of Definition of Done on agile software development projects. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017. DOI: [http://dx.doi.org/10.1145/3084226.3084262](http://dx.doi.org/10.1145/3084226.3084262)"
"['Tweaking Association Rules to Optimize So/f_tware Change Recommendations Mairieli Santos Wessel Universidade Tecnol´ogica Federal do Paran´a (UTFPR) Campo Mour˜ao, Paran´a, Brasil mairieliw@alunos.utfpr.edu.br Maur´ıcio Finavaro Aniche Del/f_t University of Technology Del/f_t, South Holland, /T_he Netherlands m.f.aniche@tudel/f_t.nl Gustavo Ansaldi Oliva /Q_ueen’s University Kingston, Ontario, Canada gustavo@cs.queensu.ca Marco Aur´elio Gerosa Northern Arizona University Flagstaﬀ, Arizona, United States marco.gerosa@nau.edu Igor Scaliante Wiese Universidade Tecnol´ogica Federal do Paran´a (UTFPR) Campo Mour˜ao, Paran´a, Brasil igor@utfpr.edu.br ABSTRACT Past researchs have been trying to recommend artifacts that are likely to change together in a task to assist developers in making changes to a so/f_tware system, o/f_ten using techniques like associa- tion rules. Association rules learning is a data mining technique that has been frequently used to discover evolutionary couplings. /T_hese couplings constitute a fundamental piece of modern change prediction techniques. However, using association rules to detect evolutionary coupling requires a number of con/f_iguration parame- ters, such as measures of interest (e.g. support and con/f_idence), their cut-oﬀ values, and the portion of the commit history from which co-change relationships will be extracted. To accomplish this set up, researchers have to carry out empirical studies for each project, testing a few variations of the parameters before choosing a con/f_iguration. /T_his makes it diﬃcult to use association rules in practice, since developers would need to perform experiments before applying the technique and would end up choosing non- optimal solutions that lead to wrong predictions. In this paper, we propose a /f_itness function for a Genetic Algorithm that optimizes the co-change recommendations and evaluate it on /f_ive open source projects (CPython, Django, Laravel, Shiny and Gson). /T_he results indicate that our genetic algorithm is able to /f_ind optimized cut-oﬀ values for support and con/f_idence, as well as to determine which length of commit history yields the best recommendations. We also /f_ind that, for projects with less commit history (5k commits), our approach produced be/t_ter results than the regression function proposed in the literature. /T_his result is particularly encouraging, because repositories such as GitHub host many young projects. Our results can be used by researchers when conducting co-change prediction studies and by tool developers to produce automated support to be used by practitioners. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131163 CCS CONCEPTS •So/f_tware and its engineering→ So/f_tware evolution;Main- taining so/f_tware;•Information systems → Recommender sys- tems; Association rules; KEYWORDS Change Recommendation, Association Rules, Genetic Algorithm ACM Reference format: Mairieli Santos Wessel, Maur´ıcio Finavaro Aniche, Gustavo Ansaldi Oliva, Marco Aur´elio Gerosa, and Igor Scaliante Wiese. 2017. Tweaking Associa- tion Rules to Optimize So/f_tware Change Recommendations. InProceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. DOI: 10.1145/3131151.3131163 1 INTRODUC ¸ ˜AO De acordo com as leis de Lehman [15], `a medida que um sistema', 'CE, Brazil, September 20–22, 2017, 10 pages. DOI: 10.1145/3131151.3131163 1 INTRODUC ¸ ˜AO De acordo com as leis de Lehman [15], `a medida que um sistema evolui, sua estrutura torna-se mais complexa. A complexidade afeta diretamente como o sistema ´e modi/f_icado. Particularmente, essa complexidade afeta em grande parte desenvolvedores novatos e contribuidores casuais de projetos de so/f_tware livre, uma vez que a falta de conhecimento sobre a arquitetura do so/f_tware cria di/f_iculdades em encontrar quais artefatos devem ser modi/f_icados para resolver uma determinada tarefa [21, 26]. Para auxiliar os desenvolvedores, foram propostas diversas t´ecnicas de recomendac ¸˜ao de mudan c ¸as [3, 7, 12, 23, 29]. V ´arias dessas t´ecnicas se baseiam no algoritmo de regras de associac ¸˜ao, cuja premissa ´e a de que artefatos que mudam juntos frequente- mente no passado s˜ao propensos a mudar juntos no futuro. Mais especi/f_icamente, o resultado da execuc ¸˜ao desse algoritmo ´e uma lista de acoplamentos de mudanc ¸a (tamb´em chamados deacoplamen- tos evolucion´arios) [20]. Diz-se que h´a um acoplamento de mudanc ¸a de um artefato A para outro artefato B quando as mudan c ¸as em B frequentemente implicam em uma mudanc ¸a em A. As t´ecnicas baseadas em regras de associac ¸˜ao se valem desses acoplamentos para produzirem suas recomendac ¸˜oes. Entretanto, a acur ´acia das recomenda c ¸˜oes produzidas por t´ecnicas baseadas em regras de associac ¸˜ao ´e afetada por um con- junto de fatores, tais como: tamanho do hist ´orico de mudan c ¸as 94', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil M. S. Wessel et al. do projeto (tamb ´em chamado de conjunto de treinamento), con/f_igurac ¸˜ao de medidas de interesse (por exemplo, suporte e con/f_ianc ¸a) usadas para /f_iltrar os acoplamentos de mudan c ¸a e pela inclus ˜ao de novas mudan c ¸as ( commits) que ocorrem du- rante a evolu c ¸˜ao do so/f_tware. Embora os guias pr ´aticos atuais tenham investigado diversos fatores que in/f_luenciam a acur´acia das recomendac ¸˜oes [18, 19], eles n˜ao de/f_iniram valores recomendados para con/f_igurac ¸˜ao das medidas de interesse. Al´em disso, os valores ´otimos variam para cada projeto. O objetivo deste trabalho ´e investigar como determinar empi- ricamente limiares ´otimos de suporte, con/f_ianc ¸a e o tamanho do conjunto de treinamento que geram as melhores recomendac ¸˜oes de mudanc ¸a. A abordagem proposta de/f_ine uma fun c ¸˜ao de ap- tid˜ao (/f_itness function) para um algoritmo gen ´etico que visa en- contrar o conjunto de treinamento com base no hist ´orico de modi/f_icac ¸˜oes (commits) e limiares de suporte e con/f_ianc ¸a que oti- mizam as recomenda c ¸˜oes de mudan c ¸a para um projeto de so/f_t- ware. Para avalia c ¸˜ao da abordagem, comparamos a acur ´acia das recomendac ¸˜oes produzidas pelo modelo proposto com um modelo est´atico gerado a partir de uma func ¸˜ao de regress˜ao proposta por Moonen et al. [19]. O modelo est´atico retorna o valor ideal para o tamanho do conjunto de treinamento; no entanto, n˜ao de/f_ine valo- res ideais para os limiares de suporte e con/f_ianc ¸a. Para comparac ¸˜ao das abordagens foram adotados limiares de suporte e con/f_ianc ¸a usados na literatura [2, 29]. Para avaliar o modelo proposto, foram utilizados dados de cinco projetos de c ´odigo aberto (CPython, Django, Laravel, Shiny e Gson) para responder duas quest ˜oes de pesquisa: (QP1) Como a acur´acia do modelo de recomendac ¸˜ao de mudanc ¸as baseado em Algoritmo Gen´etico se compara `aquela do modelo est´atico proposto por Moonen et al.[ 19]? (QP2) Como a acur ´acia dos modelos de recomendac ¸˜ao de mudanc ¸a se comporta quando o conjunto de teste aumenta? Os resultados indicam que o modelo baseado em algoritmo gen´etico apresenta uma acur´acia similar `a abordagem de Moonen et al. [19]. Entretanto, o modelo proposto se ajustou a diferentes tamanhos de projetos, enquanto que o modelo de regress ˜ao n˜ao conseguiu identi/f_icar o tamanho do conjunto de treinamento ca- paz de ser usado na pr´atica para um dos projetos, uma vez que o tamanho do conjunto de treinamento recomendado foi maior do que a quantidade de hist ´orico dispon´ıvel do projeto. Esse resul- tado ´e particularmente interessante porque a maioria dos projetos hospedados em reposit´orios como o GitHub n˜ao tˆem hist´orico de mudanc ¸as grande [22]. Este trabalho est´a organizado da seguinte forma: a Sec ¸˜ao 2 dis- cute regras de associac ¸˜ao; a Sec ¸˜ao 3 apresenta o problema de pes- quisa; a Sec ¸˜ao 5 apresenta detalhes da abordagem de recomendac ¸˜ao de mudanc ¸as baseada em algoritmo gen´etico; a Sec ¸˜ao 6 apresenta os resultados e discuss˜ao das avaliac ¸˜oes realizadas; a Sec ¸˜ao 4 apre- senta os trabalhos relacionados; a Sec ¸˜ao 7 apresenta as ameac ¸as `a validade do estudo e a Se c ¸˜ao 8 apresenta as conclus ˜oes e planos para trabalhos futuros. 2 REGRAS DE ASSOCIAC ¸ ˜AO Uma regra de associac ¸˜ao [1] ´e um tipo especial de implicac ¸˜ao ex- pressa por X ⇒Y e que ´e lida da seguinte forma “quandoX ocorre, Y tende a ocorrer”(o contr ´ario n ˜ao ´e necessariamente verdade). Mais especi/f_icamente,X e Y s˜ao conjuntos disjuntos de itens, sendo X chamado de antecedente e Y chamado de consequente. Duas medidas de interesse s˜ao calculadas para /f_iltrar as regras re- levantes: suporte e con/f_ianc ¸a. O suporte ´e a medida que representa a frequˆencia de uma regra no conjunto de transac ¸˜oes. Um conjunto de regras que aparece em muitas transac ¸˜oes ´e dito ser frequente.', 'a frequˆencia de uma regra no conjunto de transac ¸˜oes. Um conjunto de regras que aparece em muitas transac ¸˜oes ´e dito ser frequente. O suporte da regra X ⇒Y , escrito como Suporte (X ⇒Y ), indica o n ´umero de transac ¸˜oes que cont ˆem tanto X quanto Y . A for c ¸a de uma regra, por sua vez, ´e medida pela con/f_ianc ¸a. A con/f_ianc ¸a, escrita como Conf ianc ¸a(X ⇒Y ), determina o qu ˜ao frequente Y aparece em transac ¸˜oes que contˆem X. Seja a frequˆencia de um conjunto de itens X em um conjunto de transac ¸˜oes θ de/f_inida comof reqθ (X)= |{T ∈θ, X ⊆T }|, as medi- das de interesse Suporte e Conf ianc ¸a s˜ao de/f_inidas formalmente como: Suporte (X ⇒Y )= f reqθ (X ∪Y ) Conf ianc ¸a(X ⇒Y )= Suporte (X ⇒Y ) Suporte (X) = f reqθ (X ∪Y ) f reqθ (X) Assim, dado um conjunto de transac ¸˜oes, a minerac ¸˜ao por regras de associac ¸˜ao gera um conjunto de regras que contenham suporte e con/f_ianc ¸a maiores ou iguais aos valores m´ınimos informados. O algoritmo Apriori [1] ´e frequentemente usado para se obter regras de associac ¸˜ao e/f_icientemente. 3 DESCRIC ¸ ˜AO DO PROBLEMA No contexto de desenvolvimento de so/f_tware, os sistemas de con- trole de vers˜ao armazenam o hist´orico de mudanc ¸as dos artefatos. Logo, um commit pode ser interpretado como uma transac ¸˜ao que contˆem um conjunto de arquivos modi/f_icados. Assim, minerando o sistema de controle de vers˜ao, ´e poss´ıvel extrair regras de associac ¸˜ao como {x}⇒{ /y.alt}, indicando que “quando um desenvolvedor modi- /f_ica o artefatox, ele tende a tamb´em modi/f_icar o artefato/y.alt”. Por- tanto, regras de associac ¸˜ao s˜ao uma forma natural de se identi/f_icar e expressar acoplamentos de mudanc ¸a entre artefatos de so/f_tware [2, 3, 20, 29]. Em particular, a regra {x}⇒{ /y.alt}aponta para um acoplamento de mudanc ¸a de /y.alt para x, indicando que a evoluc ¸˜ao de /y.alt est´a acoplada com a evoluc ¸˜ao de x. Acoplamentos de mudanc ¸a se tornaram um componente funda- mental de diversas t ´ecnicas e ferramentas de recomenda c ¸˜ao (ou predic ¸˜ao) de mudanc ¸as [5, 17]. Entretanto, existem quest ˜oes em aberto em relac ¸˜ao a aspectos pr´aticos da aplicac ¸˜ao desses acopla- mentos para a recomenda c ¸˜ao de mudan c ¸as conjuntas [ 20]. Em particular, o tamanho do hist´orico do projeto, assim como a escolha dos limiares para suporte e con/f_ianc ¸a, interferem na qualidade das regras geradas [19, 29]. Utilizar um hist´orico muito curto pode resultar em regras que n˜ao expressam conhecimento su/f_iciente sobre o sistema. No entanto, um hist´orico muito longo pode conter informac ¸˜oes desatualizadas e inserir ru´ıdos nas regras geradas. Segundo Zimmermman et al. [29], h´a um custo-benef´ıcio entre a quantidade de recomendac ¸˜oes e a qualidade dessas recomendac ¸˜oes. Usar valores baixos de suporte e 95', 'Tweaking Association Rules to Optimize So/f_tware Change Recommendations SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil con/f_ianc ¸a possibilita uma maior quantidade de regras de associac ¸˜ao, mas com menor acur´acia de recomendac ¸˜oes. Zimmermman et al. [ 29] sugerem um limiar de suporte igual a 3 e con/f_ianc ¸a de 90%. Valores similares podem ser encontrados nos trabalhos de DiPenta et al. [3] e Bavota et al. [2], que utilizam limiar de con/f_ianc ¸a de 80% e de suporte entre 2 e 3. Os trabalhos de Moonen et al. [18, 19], embora tenham avaliado empiricamente as regras de associac ¸˜ao, n˜ao de/f_inem limiares´otimos para a aplicac ¸˜ao em diferentes projetos. Determinar limiares ´otimos das medidas de interesse e o tamanho do hist´orico ainda ´e uma tarefa dif´ıcil. Deste modo, ´e necess´ario que novas t´ecnicas determinem a con/f_igurac ¸˜ao ideal para que os desenvolvedores possam se valer de acoplamentos evolucion´arios na pr´atica. 4 TRABALHOS RELACIONADOS A literatura de minera c ¸˜ao de reposit ´orio de so/f_tware frequente- mente menciona que a escolha do tamanho do hist ´orico usado interfere na acur´acia dos resultados [9, 11, 29]. Isso ocorre tanto ao escolher um hist´orico extremamente pequeno ou extremamente grande, seja porque n˜ao h´a informac ¸˜oes su/f_icientes para gerar co- nhecimento sobre o sistema, ou porque algumas informa c ¸˜oes j´a est˜ao desatualizadas. Moonen et al. [19] investigou o impacto des- ses efeitos e gerou uma func ¸˜ao de regress˜ao linear, que encontra o tamanho de hist ´orico ideal para ser usado como treinamento. Entretanto, foram considerados apenas projetos com mais de 5 k transac ¸˜oes no hist´orico. Pesquisadores de minera c ¸˜ao de regras de associa c ¸˜ao [16, 28] exp˜oem a necessidade de se investigar a in/f_luˆencia dos parˆametros dos algoritmos. Considerando a di/f_iculdade que usu´arios possuem em especi/f_icar um limiar apropriado para as regras de associac ¸˜ao, Selvi et al. [25] prop˜oem uma modi/f_icac ¸˜ao para o algoritmo Apriori de forma que limiares de suporte s˜ao de/f_inidos automaticamente para cada n´ıvel do processo de gerac ¸˜ao do conjunto de itens fre- quentes (”frequent itemset”). Moonen et al. [18] avaliaram, al´em de suporte e con/f_ianc ¸a, outras 38 medidas de interesse para recomendac ¸˜ao de so/f_tware baseada em acoplamento evolutivo. Segundo os autores, suporte e con/f_ianc ¸a est˜ao entre as melhores medidas de interesse para de/f_inir bons acoplamentos de mudanc ¸a. Os resultados de Moonen et al. tamb´em mostram que as recomendac ¸˜oes com melhores precis˜oes s˜ao obtidas usando transac ¸˜oes relativamente pequenas, contendo entre quatro e seis arquivos. No entanto, determinar os limiares ideais para que as regras se- jam su/f_icientemente relevantes´e uma tarefa complicada e depende das caracter´ısticas do projeto em quest˜ao. No trabalho de Zimmer- mann et al. [ 29], por exemplo, apenas regras de associa c ¸˜ao com suporte maior que 1 e con/f_ianc ¸a maior que 0.5 s˜ao consideradas relevantes. Por sua vez, Bavota et al. [2] consideraram relevante re- gras com limiar de con/f_ianc ¸a 0.8 e suporte 0.2. No entanto, nenhum trabalho anterior relata como escolher automaticamente um limiar de suporte e con/f_ianc ¸a, e tamb´em um conjunto de treinamento para um projeto de so/f_tware a /f_im de otimizar as recomendac ¸˜oes geradas. Pesquisadores de Engenharia de So/f_tware tˆem utilizado algo- ritmos gen´eticos em variados cen ´arios. Por exemplo, na ´area de testes, algoritmos gen´eticos foram utilizados para gerar e avaliar um conjunto de teste para linha de produto de so/f_tware a partir de parˆametros como operadores de mutac ¸˜ao [6]. Colares et al. [4] usaram um algoritmo gen´etico multiobjetivo para mostrar a apli- cabilidade de sua abordagem para o problema de planejamento de realease de so/f_tware. 5 METODOLOGIA Nesta sec ¸˜ao s˜ao apresentadas as quest˜oes de pesquisa, a abordagem', 'cabilidade de sua abordagem para o problema de planejamento de realease de so/f_tware. 5 METODOLOGIA Nesta sec ¸˜ao s˜ao apresentadas as quest˜oes de pesquisa, a abordagem proposta, os cen ´arios de avalia c ¸˜ao e os projetos utilizados para comparar a abordagem proposta com o trabalho de Moonen et al. [19]. 5.1 /Q_uest ˜oes de Pesquisa Este trabalho prop˜oe o uso do Algoritmo Gen´etico para otimizar a selec ¸˜ao do melhor conjunto de treinamento e dos limiares de suporte e con/f_ianc ¸a para um projeto de so/f_tware, conforme problematizado na Sec ¸˜ao 3. Para tal, investigamos empiricamente duas quest ˜oes de pesquisa: (QP1) Como a acur ´acia do modelo de recomendac ¸ ˜ao de mudanc ¸as baseado em Algoritmo Gen´etico se compara `aquela do modelo est´atico proposto por Moonen et al.? A primeira quest˜ao de pesquisa visa comparar a acur´acia do mo- delo de recomendac ¸˜ao de mudanc ¸as baseado em Algoritmo Gen´etico com o modelo est´atico proposto por Moonen et al. [19]. Este mo- delo utiliza uma func ¸˜ao de regress˜ao que de/f_ine o tamanho ideal do conjunto de treinamento baseado no n´umero de arquivos e na m´edia do tamanho dos commits do projeto. Como Moonen et al. [19] n˜ao indica quais foram os limiares de suporte e con/f_ianc ¸a usa- dos, n´os comparamos o modelo gen´etico proposto neste trabalho com o modelo de regress˜ao de Moonen et al. [19] usando limiares de suporte e con/f_ianc ¸a sugeridos na literatura [2, 29]. O objetivo desta quest ˜ao de pesquisa ´e veri/f_icar se o modelo de recomendac ¸˜ao proposto possui maior acur ´acia do que o mo- delo proposto por Moonen et al. [19]. Nesta QP foram testados a acur´acia dos modelos para prever as 5% mais recentes modi/f_icac ¸˜oes realizadas em cada um dos cinco projetos. (QP2) Como a acur ´acia dos modelos de recomendac ¸˜ao de mudanc ¸a se comporta quando o conjunto de teste aumenta? A mudanc ¸a de um so/f_tware´e um processo inevit´avel segundo a primeira lei de Lehman [15]. O ambiente muda constantemente, surgem novos requisitos e o so/f_tware deve ser modi/f_icado para n˜ao se tornar progressivamente menos satisfat´orio. Nesta quest˜ao de pesquisa, investigamos o quanto novas mudanc ¸as no sistema dete- rioram a estabilidade do modelo de recomendac ¸˜oes de mudanc ¸as. Para isso, testamos incrementalmente novas mudanc ¸as no per´ıodo de teste e analisamos o impacto na acur ´acia das recomendac ¸˜oes geradas. Para esta an´alise foram usados quatro per´ıodos de teste, sendo eles 5%, 10%, 20% e 30% mais recente das transac ¸˜oes do hist´orico de mudanc ¸as do projeto. O objetivo desta quest ˜ao de pesquisa ´e veri/f_icar se a variac ¸˜ao do tamanho do conjunto de teste de 5% at´e 30% afeta a estabilidade dos modelos fazendo com que eles percam acur´acia. Se isso ocorrer, podemos concluir que usar o modelo pro- posto neste trabalho ´e melhor, uma vez que ele conseguiria otimizar 96', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil M. S. Wessel et al. Figura 1: Vis ˜ao geral da abordagem de recomendac ¸˜ao de mudanc ¸as baseada em Algoritmo Gen´etico. as recomendac ¸˜oes em diferentes cen ´arios sem a necessidade de realizar um estudo emp´ırico para de/f_inir qual deveria ser o limiar de suporte e con/f_ianc ¸a que seria usado combinado com o tamanho do hist´orico de modi/f_icac ¸˜oes sugerido pela regress˜ao proposta por Moonen et al. [19]. 5.2 Projetos Estudados Para avaliar a abordagem proposta em v ´arias condic ¸˜oes, foram selecionados cinco projetos de c ´odigo aberto com diferentes ta- manhos de hist´orico de transac ¸˜oes. Al´em de possu´ırem hist´oricos de tamanhos distintos, esses projetos apresentam frequ ˆencia de commits, linguagem e dom´ınio diferentes. A Tabela 1 apresenta as caracter´ısticas relevantes dos projetos utilizados na avaliac ¸˜ao e sua diversidade. Entre os projetos selecionados, est˜ao trˆes framework web, Laravel, Django e Shiny, e tamb´em a biblioteca Gson criada pela Google. O quinto projeto escolhido ´e o Cpython, a principal implementac ¸˜ao da linguagem de programac ¸˜ao Python, escrita em C. Os cinco projetos tˆem seu c´odigo fonte dispon´ıvel no GitHub. A Tabela 1 mostra que o tempo de hist´orico dos projetos seleci- onados varia de pequeno a grande [18, 19]. Trabalhos anteriores consideram apenas sistemas com hist´orico que variam de m´edio a grande. Gson ´e o menor projeto, com aproximadamente nove anos de hist´orico, 516 arquivos ´unicos e 2533 commits at´e a coleta dos dados. 5.3 Abordagem Proposta Nesta se c ¸˜ao, apresentamos a abordagem de recomenda c ¸˜ao de mudanc ¸as baseada em Algoritmo Gen ´etico. Tal abordagem visa automatizar as recomendac ¸˜oes de limiares de suporte e con/f_ianc ¸a para um projeto de so/f_tware. A Figura 1 mostra uma vis˜ao geral da abordagem. Definic ¸˜ao da Abordagem. Para construir a abordagem pro- posta, utilizamos um Algoritmo Gen´etico. Algoritmo Gen´etico (AG) [14] ´e uma t´ecnica de busca e otimizac ¸˜ao inspirada nos princ´ıpios da evoluc ¸˜ao e selec ¸˜ao natural. Os AGs fornecem um mecanismo de busca que pode ser usado tanto em problemas de classi/f_icac ¸˜ao, quanto em problemas de otimizac ¸˜ao. O AG simula o processo de evoluc ¸˜ao natural, ou biol ´ogica, no qual os indiv´ıduos mais aptos dominam sobre os mais fracos imitando mecanismos biol´ogicos de evoluc ¸˜ao, tais como selec ¸˜ao natural, cruzamento e mutac ¸˜ao. Uma busca com AG come c ¸a com uma popula c ¸˜ao aleat ´oria de soluc ¸˜oes, na qual cada indiv ´ıduo representa uma soluc ¸˜ao para o problema. A populac ¸˜ao evolui para melhores soluc ¸˜oes por meio de gerac ¸˜oes subsequentes e, durante cada gerac ¸˜ao, os indiv´ıduos s˜ao avaliados com base em sua fun c ¸˜ao de aptid ˜ao, de modo que ape- nas os indiv´ıduos mais aptos se reproduzem. Para criar a pr´oxima gerac ¸˜ao, informac ¸˜oes gen´eticas s˜ao transferidas para os descenden- tes. Os novos indiv ´ıduos s˜ao gerados aplicando um operador de selec ¸˜ao com base na aptid˜ao dos indiv´ıduos a serem reproduzidos, recombinando com uma dada probabilidade, dois indiv ´ıduos da gerac ¸˜ao atual por meio de cruzamento e modi/f_icando, com uma determinada probabilidade, indiv´ıduos por meio de mutac ¸˜ao. O processo de evoluc ¸˜ao ´e encerrado com base em crit ´erios de con- vergˆencia, geralmente um n´umero m´aximo de gerac ¸˜oes. Alternati- vamente, o processo de evoluc ¸˜ao ´e interrompido quando um grande n´umero de gerac ¸˜oes n˜ao apresenta melhoria no melhor valor de aptid˜ao, ou quando um valor prede/f_inido´e atingido. Em um AG, a fun c ¸˜ao de aptid ˜ao confere uma nota para cada indiv´ıduo de acordo com a sua aptid˜ao. Assim, o objetivo ´e ma- ximizar o valor da fun c ¸˜ao de aptid ˜ao de modo que a cada nova gerac ¸˜ao, os indiv´ıduos estejam mais pr´oximos do ´otimo global, que ´e a melhor solu c ¸˜ao para o problema. Utilizamos AG para explo-', 'gerac ¸˜ao, os indiv´ıduos estejam mais pr´oximos do ´otimo global, que ´e a melhor solu c ¸˜ao para o problema. Utilizamos AG para explo- rar efetivamente o espac ¸o de busca de poss´ıveis combinac ¸˜oes de tempo de treinamento, suporte e con/f_ianc ¸a para selecionar as me- lhores recomendac ¸˜oes de mudanc ¸as esperadas para um projeto de so/f_tware. 5.3.1 Pr´e-processamento. O pr ´e-processamento dos dados consiste nas seguintes etapas. Extrac ¸˜ao dos dados . Nesta etapa, clonamos o reposit ´orio de c´odigo fonte e recuperamos as informac ¸˜oes de todo o hist´orico de commits, como os arquivos que foram modi/f_icados, o autor e data de cada commit. Essa etapa resulta em um arquivo CSV com os dados coletados do hist´orico de commits do projeto. Para desempenhar tal tarefa utilizamos o RepoDriller1, um framework Java que auxilia na minerac ¸˜ao de reposit´orios de so/f_tware e possibilita a extrac ¸˜ao de informac ¸˜oes de um reposit´orio Git e a exportac ¸˜ao para arquivos CSV. Filtragem dos dados. Removemos do hist´orico do projeto com- mits com mais de trinta arquivos, assim como no trabalho de Zim- mermann et al. [29], e tamb´em commits apenas com imagens. Essas /f_iltragens removem grandes transac ¸˜oes que n ˜ao apresentam re- levˆancia para se obter os acoplamentos de mudanc ¸a dos arquivos [18] e mudanc ¸as que n˜ao envolvem c´odigo-fonte. A /f_iltragem dos commit resulta em um conjunto de transac ¸˜oes T = {t1, t2, ··· , tn }, sendo n o n´umero de commits que permaneceram no hist´orico ap´os a /f_iltragem. 1h/t_tps://github.com/mauricioaniche/repodriller 97', 'Tweaking Association Rules to Optimize So/f_tware Change Recommendations SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Tabela 1: Caracter´ısticas dos projetos estudados Projeto URL do GitHub Linguagem Tamanho m ´edio Arquivos Hist ´orico de vers˜ao Tamanho do hist ´orico N ´umero de M ´edia de transac ¸˜oes usada das transac ¸ ˜oes ´unicos minerado (em meses) transac ¸ ˜oes por m ˆes Gson h/t_tps://github.com/google/gson Java 3.13 516 24-04-2008 – 16-07-2017 111 2533 22.81 Shiny h/t_tps://github.com/rstudio/shiny R 2.68 567 20-06-2012 – 13-07-2017 61 3789 62.11 Laravel h/t_tps://github.com/django/django PHP 1.91 492 09-06-2011 – 16-08-2016 62 4943 79.72 Django h/t_tps://github.com/laravel/laravel Python 2.52 2200 12-07-2005 – 16-08-2016 133 33532 252.12 CPython h/t_tps://github.com/python/cpython C 2.07 6703 09-08-1990 – 07-05-2016 309 87171 282.10 Separac ¸˜ao do conjunto de treinamento e teste . Dado o con- junto de transac ¸˜oes T , dividimos T em um conjunto de treinamento Treinamento = {tn1 , tn2 , ··· , tns }e um conjunto de teste Teste = {ts+1, ··· , tn }. O conjunto de teste ´e extra´ıdo das transac ¸˜oes mais recentes de T e ent˜ao geramos o conjunto de treinamento com a porcentagem restante. Aleatorizamos a ordem dos arquivos em cada uma das transac ¸˜oes dos conjuntos de treinamento gerados, pois n˜ao sabemos qual o primeiro arquivo modi/f_icado pelo desenvolvedor. 5.3.2 Execuc ¸˜ao do Algoritmo Gen´etico. Func ¸˜ao de Aptid ˜ao. Modelamos uma func ¸˜ao de aptid ˜ao para o AG para otimizar recomendac ¸˜oes de mudanc ¸as. Essa func ¸˜ao de aptid˜ao utiliza uma porcentagem do conjunto treinamento para construir o modelo, gerando regras de associac ¸˜ao, e utiliza tamb´em as transac ¸˜oes do conjunto de teste para para formar consultas e avaliar as recomendac ¸˜oes geradas pelas regras, similar `a avaliac ¸˜ao feita por outros pesquisadores [18, 19, 29]. A otimizac ¸˜ao realizada pelo AG ´e baseada no valor de Precis˜oes M´edias (AP), uma m´etrica que avalia o desempenho de uma recomendac ¸˜ao. A de/f_inic ¸˜ao da fun c ¸˜ao de aptid ˜ao ´e um passo importante da construc ¸˜ao do AG. Para o problema que estamos otimizando, as potenciais soluc ¸˜oes s ˜ao representadas por uma porcentagem do conjunto de treinamento, um limiar de suporte e um limiar de con/f_ianc ¸a chamados respectivamente deporcenta /afii10069.ital em treinamento , suporte e conf ianca. Al´em destes trˆes parˆametros, o conjuntoTeste e o conjuntoTreinamento s˜ao passados como parˆametros adicionais para a func ¸˜ao de aptid˜ao e s˜ao mantidos inalterados durante a busca. Como mostrado no Algoritmo 1 e na Figura 1, a func ¸˜ao de aptid˜ao abrange trˆes passos: gera c ¸˜ao das regras, execu c ¸˜ao das consultas e avaliac ¸˜ao das consultas. Cada um desses passos s ˜ao descritos a seguir. Gerac ¸˜ao das regras . O primeiro passo da fun c ¸˜ao de aptid ˜ao ´e extrair do conjunto Treinamento a porcentagem de transac ¸˜oes correspondente `a potencial soluc ¸˜ao porcenta /afii10069.ital em treinamento . Um novo conjunto de treinamento Treinamento n ´e formado com as transac ¸˜oes extra ´ıdas. A partir de Treinamento n, as regras de associac ¸˜ao s ˜ao geradas com as medidas de interesse suporte e conf ianc ¸a. Para minerar as regras de associa c ¸˜ao, utilizamos o algoritmo Apriori [1]. Esse algoritmo requer um valor de suporte e con/f_ianc ¸a m´ınimo como entrada, de modo que itens que n˜ao s˜ao frequentes s˜ao removidos preventivamente. Primeiro, o Apriori encontra todos os conjuntos de itens (itemsets) com suporte maior que o suporte m´ınimo. A partir de cada um desses itemsets obtidos s˜ao criadas regras. Todas as regras criadas a partir de um ´unico itemset tˆem o mesmo suporte, por ´em somente as regras que est ˜ao acima da con/f_ianc ¸a m´ınima s˜ao retornadas. Algorithm 1: F/u.sc/n.sc/c.sc/a.sc/o.scA/p.sc/t.sc/i.sc/d.sc/a.sc/o.sc Input: porcentagem treinamento, suporte, con/f_ianca Output: O valor de MAP correspondente as recomendac ¸˜oes', 'Algorithm 1: F/u.sc/n.sc/c.sc/a.sc/o.scA/p.sc/t.sc/i.sc/d.sc/a.sc/o.sc Input: porcentagem treinamento, suporte, con/f_ianca Output: O valor de MAP correspondente as recomendac ¸˜oes 1 {Gerac ¸˜ao das regras} 2 treinamento n ← extrai transacoes (treinamento , porcenta /afii10069.ital em treinamento ) 3 R ←arules (treinamento n, suporte , conf ianca) 4 for f t∈Teste do 5 {Execuc ¸˜ao da consulta} 6 Q ←f t[1] 7 E ←f t−Q 8 Rp ←aplica consulta (R, Q) 9 Rp ←ordena (Rp, 10){ordenac ¸˜ao usando suporte e k=10} 10 F ←consequente (Rp ) 11 {Avaliac ¸˜ao da consulta} 12 ap ←calcula ap(E, F) 13 end 14 map ←media(ap) 15 return map Seguindo Zimmermann [29], computamos um conjunto de regras de associac ¸˜ao R com um ´unico item em seu consequente X ⇒{e}. Tais regras s˜ao su/f_icientes, porque nessa abordagem consideramos a uni˜ao dos consequentes das regras com antecedente X. Execuc ¸˜ao das consultas. Ap´os a gerac ¸˜ao das regras, a func ¸˜ao de aptid˜ao veri/f_ica para cada uma das transac ¸˜oes de Teste se seus itens s˜ao previstos a partir de Treinamento n. Para desempenhar essa tarefa, geramos uma consulta q = (Q, E)para cada uma das transac ¸˜oes Ft . Sendo Ft = {f1, ··· , fn }uma transac ¸˜ao pertencente ao conjunto Teste , a particionamos em uma consulta Q = f1 e um resultado esperado E = Ft −Q. Ao aplicarmos uma consulta ao conjunto de regras de associac ¸˜ao R, obtemos o conjunto de regras Rp referente a Q. Isso signi/f_ica que Rp cont´em apenas regras nas quais a consulta ´e o antecessor: Q ⇒{e}. As regras Rp , no cen´ario pr´atico, representam uma lista de sugest˜ao que o desenvolvedor recebe ap´os selecionar o arquivo Q. Assumindo que listas muito grandes n˜ao s˜ao vi´aveis no cen´ario pr´atico por apresentarem ao desenvolvedor muitas sugest˜oes e o forc ¸arem a navegar por muitos arquivos, consideramos apenas as primeiras 10 regras. Selecionamos a partir de Rp as top −k regras, ordenadas por valor de con/f_ianc ¸a. Ap ´os essa /f_iltragem por valor de con/f_ianc ¸a,Rp apresenta tamanho menor ou igual a 10. Avaliac ¸˜ao das consultas. Aplicar a consulta q = (Q, E)resulta em uma lista de recomendac ¸˜ao ordenada F, a qual corresponde aos 98', '99', 'Tweaking Association Rules to Optimize So/f_tware Change Recommendations SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Tabela 2: Valores de Suporte m´ınimo e Suporte m´aximo Projeto Suporte m ´ınimo Suporte m ´aximo CPython 0.00001 0.0002 Django 0.00003 0.0006 Laravel 0.0002 0.004 Shiny 0.0002 0.005 Gson 0.0004 0.008 A Tabela 2 mostra os valores de suporte m´ınimo e m´aximo adota- dos para os projetos selecionados. O AG utiliza os limites m´ınimos e m´aximos para limitar seu espac ¸o de busca ao gerar novos indiv´ıduos. Avaliac ¸˜ao da acur ´acia da abordagem baseada em algo- ritmo gen´etico. Para avaliar o desempenho do AG em encontrar um conjunto de treinamento, suporte e con/f_ianc ¸a capazes de oti- mizar recomendac ¸˜oes de mudanc ¸as para um projeto de so/f_tware, utilizamos o trabalho de Moonen et al. [ 19]. Os autores de/f_inem uma func ¸˜ao de regress˜ao baseada no n´umero de arquivos e m´edia do tamanho dos commits do projetos para prever o valor do tamanho do hist´orico que deve ser usado como conjunto de treinamento. A func ¸˜ao de regress˜ao ´e dada por: Tamanho do treinamento= 33974.1 + 208.4 ×N ´umero de arqui/v.altos (em 1000) −3958.9 ×M ´edia do tamanho doscommits (1) Contudo, o trabalho de Moonen n˜ao apresenta quais s˜ao os limi- ares de suporte e con/f_ianc ¸a que devem ser usados. Por este motivo, comparamos nossa abordagem com um modelo est´atico gerado a partir do tamanho do hist´orico vindo do trabalho de Moonen et al. [18], variando os limiares de suporte entre 2 a 20 e os valores de con/f_ianc ¸a em 0.10, 0.5 e 0.9, de acordo com as recomendac ¸˜oes de Zimmermann et al. [29] e Bavota et al. [2]. A ideia-chave por detr ´as da avalia c ¸˜ao proposta para respon- der QP1 consistiu em executar os dois modelos, utilizando 5% do hist´orico mais recente para teste, e selecionar os melhores resulta- dos para an´alise. Desta forma, o algoritmo gen´etico utiliza os 95% restante das transac ¸˜oes para selecionar o treinamento necess´ario para otimizar as recomendac ¸˜oes, enquanto a func ¸˜ao de regress˜ao de/f_ine o tamanho do treinamento independente do tamanho do teste usado. Avaliac ¸˜ao da estabilidade dos modelos . Para investigar a QP2, avaliamos o quanto novas mudanc ¸as no sistema deterioram a estabilidade do modelo de recomendac ¸˜oes de mudanc ¸as. Foram executados os dois modelos de recomendac ¸˜ao de mudanc ¸as, usando quatro tamanhos diferentes para o conjunto de teste: 5%, 10%, 20% e 30% das transac ¸˜oes mais recentes do hist ´orico de mudanc ¸as do projeto. Cada uma das execuc ¸˜oes dos modelos considerava apenas um destes valores /f_ixos de teste. Assim, para um valor /f_ixo de teste o AG pode escolher entre a porcentagem restante de transa c ¸˜oes no hist´orico de mudanc ¸as (conjunto de treinamento) o tamanho do conjunto usado para gerar as recomenda c ¸˜oes. Da mesma forma, a regress ˜ao de/f_iniu a quantidade de transac ¸˜oes do conjunto de treinamento que seriam usadas. Ap´os a execuc ¸˜ao dos dois modelos, os melhores resultados para cada um dos per´ıodos de teste foram selecionados para an´alise. 6 RESULTADOS Nesta sec ¸˜ao, apresentamos os resultados da execuc ¸˜ao do AG para otimizar a recomendac ¸˜ao de mudanc ¸as com o comparativo reali- zado com a func ¸˜ao de regress˜ao proposta por Moonen et al. [19]. Tamb´em apresentamos a investigac ¸˜ao da avaliac ¸˜ao da deteriorac ¸˜ao dos modelos `a medida que o conjunto de treinamento diminui. (QP1) Como a acur ´acia do modelo de recomendac ¸ ˜ao de mudanc ¸as baseado em algoritmo gen ´etico se compara `aquela do modelo est´atico proposto por Moonen et al.? Para responder esta quest ˜ao de pesquisa foram executados os dois modelos para testar os 5% de modi/f_icac ¸˜oes mais recentes de cada um dos projetos analisados. O AG utilizou os 95% restante das transac ¸˜oes para selecionar o tamanho do hist´orico necess´ario para otimizar as recomendac ¸˜oes de mudanc ¸a. Durante a execuc ¸˜ao do AG, observamos que os conjuntos de trei-', 'transac ¸˜oes para selecionar o tamanho do hist´orico necess´ario para otimizar as recomendac ¸˜oes de mudanc ¸a. Durante a execuc ¸˜ao do AG, observamos que os conjuntos de trei- namento selecionados correspondem a um total de 35%, 41%, 13%, 13% e 79% das transac ¸˜oes dos projetos CPython, Django, Laravel, Shiny e Gson respectivamente, como pode ser visto na Tabela 3. A func ¸˜ao de regress ˜ao de Moonen et al. [ 19] n ˜ao ´e capaz de prever a quantidade de hist ´orico necess ´ario para gerar ´otimas recomendac ¸˜oes de mudanc ¸a em projetos pequenos. Nos projetos com menos de cinco mil transa c ¸˜oes, a quantidade de transa c ¸˜oes de treinamento recomendada pela regress ˜ao foi maior do que o tamanho do hist´orico dispon´ıvel. A Tabela 3 compara os resultados de tamanho de treinamento obtidos. Pode-se observar que o AG possui a vantagem de se adaptar ao tamanho do projeto e ao mesmo tempo selecionar um conjunto de treinamento capaz de otimizar as recomendac ¸˜oes de mudanc ¸as geradas a partir dele. A Figura 3 apresenta a variac ¸˜ao dos valores de MAP obtidos com as diferentes con/f_igurac ¸˜oes de suporte e con/f_ianc ¸a. ´E poss´ıvel perce- ber que, h´a uma variabilidade na acur´acia das recomendac ¸˜oes inde- pendente da quantidade decommits do projeto. ´E importante menci- onar que a con/f_igurac ¸˜ao manual dos valores de suporte e con/f_ianc ¸a podem levar a at´e 30% de diferenc ¸a na acur´acia dos modelos, como pode ser observado na variabilidade do valor de MAP, tendo em vista que todas as execuc ¸˜oes para um projeto usaram o mesmo tama- nho de treinamento indicado pela func ¸˜ao de regress˜ao. Essa grande diferenc ¸a de resultados reforc ¸a a necessidade de experimentac ¸˜ao de diferentes parˆametros, o que ´e tratado de modo autom´atico pela abordagem proposta. Comparando a acur´acia do modelo proposto neste trabalho com o trabalho de Moonen, observamos que a diferenc ¸a percentual da precis˜ao de acerto nas recomendac ¸˜oes entre as duas abordagens ´e muito pequena. No Projeto CPython, a abordagem proposta selecionou 29k commits para o treino enquanto a abordagem do Moonen selecionou 27k. O limiar de suporte escolhido pelas duas abordagens foi o mesmo, ocorrendo uma pequena variac ¸˜ao de 0.01 na con/f_ianc ¸a. Neste caso, ter adicionado mais hist ´orico para o treinamento n˜ao proporcionou melhora no valor de MAP quando comparamos as duas abordagens. Para o projeto Django, a abordagem proposta selecionou 11 k transac ¸˜oes a menos. Embora o limiar de suporte e con/f_ianc ¸a tenham sido os mesmos, o AG apresentou uma pequena melhora (0.013%) 100', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil M. S. Wessel et al. Tabela 3: Melhores resultados para 5% de teste. Projeto Modelo Teste (%) Transac ¸ ˜oes no Transac ¸ ˜oes usadas Transac ¸ ˜oes usadas Suporte Con/f_ianc ¸a MAP treinamento no treinamento no teste CPython Regress ˜ao 5 82813 27176 2083 2 0.10 0.298 CPython AG 5 82813 29622 2083 2 0.12 0.299 Django Regress ˜ao 5 31856 24456 845 2 0.10 0.183 Django AG 5 31856 13317 845 2 0.10 0.196 Laravel Regress ˜ao 5 4696 26515 * 54 2 0.10 0.134 Laravel AG 5 4696 637 54 10 0.12 0.157 Shiny Regres ˜ao 5 3600 23482 * 116 2 0.15 0.267 Shiny AG 5 3600 498 116 5 0.12 0.342 Gson Regres ˜ao 5 2407 21690 * 59 2 0.10 0.192 Gson AG 5 2407 1903 59 1 0.17 0.196 * O tamanho do treinamento previsto pela func ¸˜ao de regress˜ao ´e maior que o tamanho do hist´orico do projeto, ent˜ao foram usadas as transac ¸˜oes dispon´ıveis. Figura 3: Distribuic ¸˜ao do modelo est´atico para cada projeto. na acur´acia das recomendac ¸˜oes. Para os projetos Laravel, Shiny e Gson, a abordagem de Moonen et al. indicou que deveriam ser selecionadas respectivamente 27k, 24k e 22k transac ¸˜oes, todavia essas quantidades s˜ao maiores do que o hist´orico dispon´ıvel desses projetos. Neste caso, selecionamos a quantidade total restante do conjunto de treinamento para realizar a compara c ¸˜ao. Para o projeto Shiny o AG selecionou somente 498 transa c ¸˜oes e neste cen´ario veri/f_icamos a maior diferenc ¸a na acur´acia entre os modelos (0.075%). O AG proposto otimizou automaticamente o limiar de suporte e con/f_ianc ¸a e o tamanho de treinamento para projetos de diferentes tamanhos. A func ¸˜ao de regress˜ao proposta por Moonen et al. n˜ao foi capaz de prever a quantidade de hist ´orico para os projetos menores (Laravel, Shiny e Gson). Apesar da pouca diferen c ¸a no valor de MAP, a melhoria pode ser relevante, uma vez que um maior valor de MAP diminui o esforc ¸o do desenvolvedor em encontrar os arquivos para realizar uma mudanc ¸a. (QP2) Como a acur ´acia dos modelos de recomendac ¸˜ao de mudanc ¸a se comporta quando o conjunto de teste aumenta? Inserir novas mudanc ¸as no so/f_tware deteriora a estabilidade do modelo de recomendac ¸˜oes de mudanc ¸as. Nesta quest˜ao de pesquisa comparamos os modelos em quatro con/f_igurac ¸˜oes de teste distintas. A Tabela 3 apresenta os resultados para 5% de teste, e a Tabela 4 apresenta os resultados obtidos com 10%, 20% e 30% de teste para cada modelo. O maior projeto estudado, CPython, apresenta os melhores resul- tados de recomendac ¸˜ao de mudanc ¸as ao usar, nos dois modelos, 10% de seu hist´orico para teste. Aumentar o tamanho do teste de 10% para 20% e 30% fez a acur´acia das recomendac ¸˜oes geradas diminuir entre 0.03 a 0.05. Para o CPython, mesmo aumentando o conjunto de teste, os modelos praticamente n˜ao deterioraram. O projeto Django apresentou uma pequena diferen c ¸a em per- centual na acur´acia das recomendac ¸˜oes para as execuc ¸˜oes com os diferentes tamanhos de teste. A diferen c ¸a, para os dois modelos foi de aproximadamente 0.01. Django ´e considerado neste trabalho como um projeto de tamanho m´edio, possuindo 34k transac ¸˜oes em seu hist ´orico. Enquanto a abordagem de Moonen et al. selecio- nou 24k transac ¸˜oes de treinamento, valor esse insu/f_iciente para a quantidade de transac ¸˜oes dispon´ıveis ao usar 30% de teste. Nossa abordagem selecionou cerca de 10k transac ¸˜oes a menos. Para o projeto Laravel, aumentar o tamanho do teste de 10% para 20% fez com que a estabilidade das recomendac ¸˜oes tanto no modelo baseado em AG, quanto no est´atico se deteriorasse, diminuindo o valor de MAP em 0.077 nos dois modelos. O mesmo pode ser obser- vado para os projetos menores Shiny e Gson. Aumentar o tamanho do teste de 10% para 20% para o Shiny impactou na diminuic ¸˜ao de cerca de 0.089 no valor de MAP na abordagem proposta e 0.109 ao usar o modelo est´atico. Isto se deve ao fato de que projetos peque-', 'do teste de 10% para 20% para o Shiny impactou na diminuic ¸˜ao de cerca de 0.089 no valor de MAP na abordagem proposta e 0.109 ao usar o modelo est´atico. Isto se deve ao fato de que projetos peque- nos possuem pouco hist´orico de transac ¸˜oes dispon´ıvel, e aumentar o conjunto de teste signi/f_ica limitar consideravelmente o tamanho do conjunto de treinamento dispon´ıvel. Laravel possui apenas 492 arquivos ´unicos. Limitar o conjunto de treinamento possivelmente reduz a quantidade de mudan c ¸as conjuntas entre os arquivos, bem como a quantidade de arquivos que mudaram pelo menos uma vez no per´ıodo usado para criar o conjunto de treinamento. Apesar dos resultados apresentarem pouca diferenc ¸a na acur´acia (valor de MAP) das recomendac ¸˜oes, a reduc ¸˜ao no valor de MAP im- pacta o esforc ¸o que o desenvolvedor tem para encontrar os arquivos na lista de recomendac ¸˜oes, uma vez que o MAP est´a diretamente relacionado com a posi c ¸˜ao que os arquivos corretos ocupam na lista de recomendac ¸˜ao. Dessa forma, se o MAP diminuir muito, o 101', 'Tweaking Association Rules to Optimize So/f_tware Change Recommendations SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Tabela 4: Melhores resultados para diferentes porcentagens de teste. Projeto Modelo Teste (%) Transac ¸ ˜oes no Transac ¸ ˜oes usadas Transac ¸ ˜oes usadas Suporte Con/f_ianc ¸a MAP treinamento no treinamento no teste CPython Regress ˜ao 10 78454 27176 4188 2 0.10 0.306 CPython AG 10 78454 38627 4188 1 0.10 0.316 CPython Regress ˜ao 20 69737 27176 8286 2 0.10 0.271 CPython AG 20 69737 33081 8286 1 0.10 0.276 CPython Regress ˜ao 30 61019 27176 11977 2 0.10 0.258 CPython AG 30 61019 25222 11977 2 0.11 0.258 Django Regress ˜ao 10 30179 24456 1703 2 0.10 0.175 Django AG 10 30179 14351 1703 1 0.10 0.187 Django Regress ˜ao 20 26826 24456 3389 2 0.10 0.171 Django AG 20 26826 9926 3389 2 0.10 0.180 Django Regress ˜ao 30 23473 24456 * 5059 2 0.10 0.176 Django AG 30 23473 7613 5059 2 0.10 0.187 Laravel Regress ˜ao 10 4449 26515 * 102 2 0.10 0.111 Laravel AG 10 4449 659 102 7 0.13 0.120 Laravel Regress ˜ao 20 3955 26515 * 253 2 0.10 0.034 Laravel AG 20 3955 1296 253 5 0.10 0.043 Laravel Regress ˜ao 30 3461 26515 * 353 2 0.10 0.035 Laravel AG 30 3461 674 353 3 0.12 0.047 Shiny Regres ˜ao 10 3411 23482 * 227 2 0.10 0.322 Shiny AG 10 3411 3033 227 2 0.12 0.319 Shiny Regres ˜ao 20 3032 23482 * 464 2 0.10 0.213 Shiny AG 20 3032 1054 464 2 0.11 0.230 Shiny Regres ˜ao 30 2633 23482 * 683 2 0.10 0.209 Shiny AG 30 2633 872 683 2 0.10 0.225 Gson Regres ˜ao 10 2280 21690 * 114 2 0.10 0.184 Gson AG 10 2280 1771 114 2 0.10 0.181 Gson Regres ˜ao 20 2027 21690 * 219 2 0.10 0.145 Gson AG 20 2027 1231 219 2 0.12 0.155 Gson Regres ˜ao 30 1774 21690 * 326 2 0.10 0.180 Gson AG 30 1774 979 326 3 0.12 0.191 * O tamanho do treinamento previsto pela func ¸˜ao de regress˜ao ´e maior que o tamanho do hist´orico do projeto, ent˜ao foram usadas as transac ¸˜oes dispon´ıveis. esforc ¸o do desenvolvedor ser ´a muito maior, porque ele receber ´a muitos falsos positivos. Escolher valores arbitr´arios para as con/f_igurac ¸˜oes das medidas de interesse que otimizam as recomendac ¸˜oes geradas requer muito esforc ¸o. Como inserir novas mudanc ¸as no sistema deteriora a esta- bilidade do modelo de recomendac ¸˜oes, principalmente em projetos pequenos, o esfor c ¸o de encontrar essas medidas de interesse se torna constante se n˜ao houver uma t´ecnica que automatize essa es- colha. Por este motivo, usar um AG para automatizar a escolha dos valores das medidas de interesse que otimizam as recomendac ¸˜oes ´e importante. Inserir novas mudanc ¸as no sistema deteriora a estabilidade do modelo de recomendac ¸˜oes de mudanc ¸as. Portanto, usar o modelo proposto ´e melhor em relac ¸˜ao ao modelo est´atico, uma vez que os limiares de suporte e con/f_ianc ¸a tendem a mudar com o tamanho de treinamento dispon´ıvel. 7 AMEAC ¸AS `A VALIDADE H´abitos de Commit. A abordagem apresentada na Sec ¸˜ao 5.3 est´a baseada na minerac ¸˜ao de regras de associac ¸˜ao que utiliza o hist´orico de modi/f_icac ¸˜oes registradas no controle de vers ˜ao. Entretanto, desenvolvedores podem realizar commits que envolvam arquivos n˜ao relacionados e que portanto n ˜ao deveriam ser modi/f_icados conjuntamente [20]. Esse conceito ´e conhecido na literatura como tangled changes [13] e pode in/f_luenciar a gerac ¸˜ao das regras de associac ¸˜ao usadas pelos modelos de recomendac ¸˜oes de mudanc ¸a. Para evitar esse vi´es, removemos commits que continham mais de 30 arquivos [29], j ´a que eles poderiam se referir a opera c ¸˜oes de mudanc ¸a de ramos do controle de vers˜ao ou m´ultiplas mudanc ¸as. Amostragem aleat ´oria. O experimento realizou consultas a partir de um arquivo escolhido aleatoriamente. Embora tenhamos usado a mesma amostra para comparar os dois modelos, h´a a possi- bilidade de que se a consulta fosse realizada a partir de um outro arquivo selecionado aleatoriamente poderia apresentar outro valor de MAP. Essa escolha ´e feita porque n ˜ao se sabe qual foi o pri-', 'bilidade de que se a consulta fosse realizada a partir de um outro arquivo selecionado aleatoriamente poderia apresentar outro valor de MAP. Essa escolha ´e feita porque n ˜ao se sabe qual foi o pri- meiro arquivo modi/f_icado pelo desenvolvedor durante a realizac ¸˜ao do commit. Essa abordagem tamb´em foi utilizada na avaliac ¸˜ao da func ¸˜ao de regress˜ao proposta por Moonen et al. [19]. Generalizac ¸˜ao. Realizamos os experimentos em cinco proje- tos de c ´odigo aberto. Os cinco projetos selecionados variam em tempo, tamanho de hist´orico e frequˆencia de transac ¸˜oes. Apesar de sabermos que ´e necess´ario aumentar a quantidade de projetos para generalizar a comparac ¸˜ao entre as abordagens, o uso de um projeto com pequena quantidade de transac ¸˜oes no hist´orico de mudanc ¸as adicionou uma perspectiva diferente de avaliac ¸˜ao dos resultados 102', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil M. S. Wessel et al. que n ˜ao foi avaliada por Moonen et al. [ 19]. Tamb ´em nos pos- sibilitou realizar inspe c ¸˜oes e ter maior controle sobre o correto funcionamento da abordagem. Implementac ¸˜ao do Algoritmo Gen ´etico. As con/f_igurac ¸˜oes do algoritmo gen ´etico podem in/f_luenciar os resultados obtidos. Entretanto, veri/f_icou-se que todo o espac ¸o de busca foi explorado, bem como a melhor escolha da populac ¸˜ao foi obtida, uma vez que o algoritmo gen´etico convergiu rapidamente para a melhor soluc ¸˜ao de escolha do conjunto de treinamento e limiares de suporte e con/f_ianc ¸a. 8 CONCLUS ˜OES Neste trabalho, investigamos empiricamente como determinar va- lores ´otimos de suporte, con/f_ianc ¸a e conjunto de treinamento que geram as melhores recomendac ¸˜oes de mudanc ¸a. Para isto, de/f_ini- mos uma func ¸˜ao de aptid˜ao para um AG que seleciona o melhor conjunto de treinamento e valores de suporte e con/f_ianc ¸a que oti- mizam as recomendac ¸˜oes de mudanc ¸a para um projeto de so/f_tware. Ao comparar a acur´acia do modelo de recomendac ¸˜ao de mudanc ¸as baseado em AG com o modelo est´atico, gerado a partir do trabalho de Moonen et al., variando valores de suporte e con/f_ianc ¸a de acordo com valores usados na literatura [2, 29], foi poss´ıvel observar que a abordagem proposta ´e capaz de produzir resultados semelhantes ao trabalho de Moonen et al. para projetos grandes. Entretanto, o AG conseguiu otimizar valores de suporte, con/f_ianc ¸a e tamanho de treinamento para o projeto com menor quantidade de hist´orico, para o qual a regress˜ao n˜ao foi capaz de determinar o conjunto de treinamento. Na segunda quest˜ao de pesquisa, veri/f_icamos que a inserc ¸˜ao de novas mudanc ¸as no sistema deterioram a estabilidade dos modelos de recomendac ¸˜ao de mudanc ¸as. Especialmente no Laravel, um dos menores projetos, a acur´acia das recomendac ¸˜oes decresceu de 0.15 para 0.05 `a medida que novas mudanc ¸as eram consideradas para o teste dos modelos. Como trabalho futuro, pretendemos utilizar nossa abordagem com o algoritmo Targeted Association Rule Mining for All /Q_ueries (TARMAQ) [23] e comparar com o algoritmo Apriori, al´em de es- tender a an´alise para uma maior quantidade projetos. REFERˆENCIAS [1] R. Agrawal, T. Imieli´nski, and A. Swami. 1993. Mining Association Rules Between Sets of Items in Large Databases. SIGMOD Rec. 22, 2 (June 1993), 207–216. [2] G. Bavota, B. Dit, R. Oliveto, M. Di Penta, D. Poshyvanyk, and A. De Lucia. 2013. An Empirical Study on the Developers&#039; Perception of So/f_tware Coupling. In Proceedings of the 2013 International Conference on So/f_tware Engineering (ICSE ’13). IEEE Press, Piscataway, NJ, USA, 692–701. [3] G. Canfora, M. Ceccarelli, L. Cerulo, , and M. Di Penta. 2010. Using multivariate time series and association rules to detect logical change coupling: An empirical study. In IEEE International Conference on So/f_tware Maintenance, ICSM. [4] F. Colares, J. Souza, R. Carmo, C. P´adua, and G. R. Mateus. 2009. A New Approach to the So/f_tware Release Planning. In2009 XXIII Brazilian Symposium on So/f_tware Engineering. 207–215. [5] B. Dit, M. Wagner, S. Wen, W. Wang, M. Linares-V´asquez, D. Poshyvanyk, and H. Kagdi. 2014. ImpactMiner: A Tool for Change Impact Analysis. In Companion Proceedings of the 36th International Conference on So/f_tware Engineering (ICSE Companion 2014). ACM, New York, NY, USA, 540–543. [6] R. A. M. Filho and S. R. Vergilio. 2015. A Mutation and Multi-objective Test Data Generation Approach for Feature Testing of So/f_tware Product Lines. In2015 29th Brazilian Symposium on So/f_tware Engineering. 21–30. [7] H. Gall, K. Hajek, and M. Jazayeri. 1998. Detection of logical coupling based on product release history. In Proceedings. International Conference on So/f_tware Maintenance (Cat. No. 98CB36272). 190–198. [8] S. Gotshall and B. Rylander. 2002. Optimal population size and the genetic', 'Maintenance (Cat. No. 98CB36272). 190–198. [8] S. Gotshall and B. Rylander. 2002. Optimal population size and the genetic algorithm. Population 100, 400 (2002), 900. [9] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy. 2000. Predicting fault incidence using so/f_tware change history.IEEE Transactions on So/f_tware Engineering26, 7 (Jul 2000), 653–661. [10] Michael Hahsler, Sudheer Chelluboina, Kurt Hornik, and Christian Buchta. 2011. /T_he Arules R-Package Ecosystem: Analyzing Interesting Pa/t_terns from Large Transaction Data Sets. Journal of Machine Learning Research 12 (July 2011), 2021–2025. [11] A. E. Hassan. 2008. /T_he road ahead for Mining So/f_tware Repositories. In2008 Frontiers of So/f_tware Maintenance. 48–57. [12] A. E. Hassan and R. C. Holt. 2004. Predicting change propagation in so/f_tware systems. In 20th IEEE International Conference on So/f_tware Maintenance, 2004. Proceedings. 284–293. [13] K. Herzig and A. Zeller. 2013. /T_he Impact of Tangled Code Changes. InProceedings of the 10th Working Conference on Mining So/f_tware Repositories (MSR ’13). IEEE Press, Piscataway, NJ, USA, 121–130. [14] J. H. Holland. 1975. Adaptation in Natural and Arti/f_icial Systems. Vol. Ann Arbor. 183 pages. arXiv:0262082136 [15] M. M. Lehman. 1980. Programs, life cycles, and laws of so/f_tware evolution.Proc. IEEE 68, 9 (September 1980), 1060–1076. [16] O. Maimon and L. Rokach. 2010.Data Mining and Knowledge Discovery Handbook (2nd ed.). Springer Publishing Company, Incorporated. [17] H. Malik and A.E. Hassan. 2008. Supporting so/f_tware evolution using adaptive change propagation heuristics. In IEEE International Conference on So/f_tware Maintenance, 2008. ICSM 2008. 177–186. [18] L. Moonen, S. Di Alesio, D. Binkley, and T. Rolfsnes. 2016. Practical Guidelines for Change Recommendation using Association Rule Mining. In International Conference on Automated So/f_tware Engineering (ASE). ACM. [19] L. Moonen, S. Di Alesio, T. Rolfsnes, and D. Binkley. 2016. Exploring the Eﬀects of History Length and Age on Mining So/f_tware Change Impact. InInternational Working Conference on Source Code Analysis and Manipulation (SCAM). IEEE. [20] G. A. Oliva and M. A. Gerosa. 2015. Change Coupling Between So/f_tware Artifacts: Learning from Past Changes. In /T_he Art and Science of Analyzing So/f_tware Data, C. Bird, T. Menzies, and T. Zimmermann (Eds.). Morgan Kaufmann, 285–324. [21] G. Pinto, I. Steinmacher, and M. A. Gerosa. 2016. More Common /T_han You /T_hink: An In-depth Study of Casual Contributors. In IEEE 23rd International Conference on So/f_tware Analysis, Evolution, and Reengineering (SANER), Vol. 1. 112–123. [22] B. Ray, D. Posne/t_t, V. Filkov, and P. Devanbu. 2014. A Large Scale Study of Programming Languages and Code /Q_uality in Github. InProceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of So/f_tware Engineering (FSE 2014). ACM, New York, NY, USA, 155–165. [23] T. Rolfsnes, S. Di Alesio, R. Behjati, L. Moonen, and D. W. Binkley. 2016. Generali- zing the Analysis of Evolutionary Coupling for So/f_tware Change Impact Analysis. In International Conference on So/f_tware Analysis, Evolution, and Reengineering (SANER). IEEE, 201–212. [24] L. Scrucca. 2013. GA: A Package for Genetic Algorithms in R.Journal of Statistical So/f_tware53, 4 (2013), 1–37. h/t_tp://www.jstatso/f_t.org/v53/i04/ [25] CS. K. Selvi and A. Tamilarasi. 2009. An automated association rule mining technique with cumulative support thresholds. Int. J. Open Problems in Compt. Math 2, 3 (2009). [26] I. Steinmacher, T. Conte, M. A. Gerosa, and D. Redmiles. 2015. Social Barriers Faced by Newcomers Placing /T_heir First Contribution in Open Source So/f_tware Projects. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW ’15). ACM, New York, NY, USA, 1379–1392. [27] W. Su, Y. Yuan, and M. Zhu. 2015. A Relationship Between the Average Precision', 'Cooperative Work and Social Computing (CSCW ’15). ACM, New York, NY, USA, 1379–1392. [27] W. Su, Y. Yuan, and M. Zhu. 2015. A Relationship Between the Average Precision and the Area Under the ROC Curve. In Proceedings of the 2015 International Conference on /T_he /T_heory of Information Retrieval (ICTIR ’15). ACM, New York, NY, USA, 349–352. [28] Z. Zheng, R. Kohavi, and L. Mason. 2001. Real World Performance of Association Rule Algorithms. In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’01). ACM, New York, NY, USA, 401–406. [29] T. Zimmermann, P. Weißgerber, S. Diehl, and A. Zeller. 2005. Mining version histories to guide so/f_tware changes.IEEE Transactions on So/f_tware Engineering 31, 6 (2005), 429–445. 103']","[""OPTIMIZING CHANGE RECOMMENDATIONS This  briefin  reports  scieitfc  evideice oi  the  use  of  a  Geietc  Alnorithm  to optmizes  the  software  chaine recommeidatoiss FINDINGS \uf0b7 In  this  work  we  proposed  a  ftness function for a Genetic Algorith\x19 that opti\x19izes  the  set  of  reco\x19\x19ended changes by association rules.  \uf0b7 We  co\x19pared  our  approach  to  a prediction \x19echanis\x19 based on linear regression (Moonen et al. 2016) and evaluated  the  results  on  fve  open source  projects,  na\x19ely:  CPython, Django, Laravel, Shiny and Gson. Figure  1:  Regression  Model  distributon  for each project \uf0b7 Figure  1  contains  the  Mean  Average Precision (MAP)  values  obtained  by the  regression  \x19odel  using  diferent cut-of values  for  support  and confdence  in  each  studied  project. The boxplots show that choosing non- opti\x19al  solutions  can  lead  to  wrong reco\x19\x19endations. \uf0b7 Our  results  indicate  that  the  genetic algorith\x19  \x19odel  is  able  to  fnd opti\x19ized  cut-of values  for  support and  confdence,  as  well  as  to deter\x19ine  which  length  of  co\x19\x19it history  yields  the  best reco\x19\x19endations. \uf0b7 The  Genetic  Algorith\x19 \x19odel  adapts to the size of project co\x19\x19it history. \uf0b7 For projects with less co\x19\x19it history (less  than  5k  co\x19\x19its),  the  Genetic Algorith\x19 \x19odel  achieved  beter results  than  the  linear  regression \x19odel.  This  result  is  particularly encouraging,  because  repositories such  as  GitHub  host  \x19any  young projects with few co\x19\x19its. \uf0b7 I\x19proving  the  accuracy  of  change reco\x19\x19endations (MAP) reduces the developer's  efort  to  propagate changes in software syste\x19s. \uf0b7 Applying new changes to the software syste\x19 deteriorates the stability of the change reco\x19\x19endation \x19odel.  \uf0b7 The results can be used by researchers when  conducting  change  prediction studies.  Tool  developers  can  also leverage  our  approach  to  build tailored change reco\x19\x19endation tools that  do  not  require  \x19anually presetng  cut-of values  for  support and confdence.      Keywords Change Reco\x19\x19endation Association Rules Genetic Algorith\x19 Who is this briefng for? Software engineering practitioners who want to \x19ake decisions about the use  of software change reco\x19\x19endations  in practice based on scientifc evidence. Where the fndings come from? All fndings of this briefng were  obtained fro\x19 the e\x19pirical study  conducted by Wessel et al.   What is included in this briefng? The \x19ain fndings about the use of  Genetic Algorith\x19 \x19odel to opti\x19ize  software change reco\x19\x19endations. To access other evidence briefngs  on sofware engineering: htp:////www.lia.ufc.br//ccbsoft2017 ORIGINAL RESEARCH REFERENCE WESSEL, Mairieli Santos  et al. Tweaking Associaton Rules to Optiiie Sofwaae  Cange Recoiiendatons. 31st Brazilian Sy\x19posiu\x19 on Software Engineering, 2017.""]","**Optimizing Software Change Recommendations Using Genetic Algorithms**

This Evidence Briefing summarizes the research conducted by Wessel et al. (2017), which focuses on improving the accuracy of software change recommendations through the optimization of association rules using a genetic algorithm. The goal is to provide actionable insights for software engineers and practitioners interested in enhancing their software maintenance practices.

**Main Findings:**

1. **Association Rules and Change Recommendations**: The study emphasizes that association rules are commonly used to predict which software artifacts are likely to change together. However, the effectiveness of these predictions is heavily dependent on configuration parameters like support and confidence thresholds, as well as the length of commit history considered.

2. **Genetic Algorithm Approach**: The authors propose a novel fitness function for a genetic algorithm (GA) that automates the optimization of these parameters. This approach aims to identify the best support and confidence thresholds and the optimal length of commit history, leading to more accurate change recommendations.

3. **Empirical Evaluation**: The GA was tested on five open-source projects (CPython, Django, Laravel, Shiny, and Gson). Results showed that the GA could find optimal configurations that improved recommendation accuracy, especially for projects with fewer than 5,000 commits, where traditional regression models failed to provide suitable training sizes.

4. **Comparative Performance**: While the GA-based model produced comparable results to the static regression model proposed in previous literature, it demonstrated better adaptability to different project sizes and commit histories. This is particularly relevant for smaller projects, which are common in platforms like GitHub.

5. **Impact of New Changes on Model Stability**: The research found that introducing new changes to the software negatively impacted the stability of recommendation models. The GA approach maintained better accuracy across various testing scenarios compared to static models, highlighting its practical applicability in dynamic development environments.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and researchers interested in software maintenance and change management. It is particularly relevant for those working with open-source projects or in environments where software artifacts frequently change.

**Where the findings come from?**
The findings are derived from empirical studies conducted by Wessel et al. (2017), which analyzed the performance of a genetic algorithm in optimizing software change recommendations across five distinct open-source projects.

**What is included in this briefing?**
This briefing includes insights into the methodology of using genetic algorithms for optimizing association rules, the comparative performance of the proposed approach against traditional methods, and implications for practical software maintenance.

**To access the original research article:**
Wessel, M. S., Aniche, M. F., Oliva, G. A., Gerosa, M. A., & Wiese, I. S. (2017). Tweaking Association Rules to Optimize Software Change Recommendations. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017. DOI: [10.1145/3131151.3131163](https://doi.org/10.1145/3131151.3131163)."
"['Understanding Technical Debt at the Code Level from the Perspective of So/f_tware Developers Junior Cesar Rocha Universidade Federal do Rio Grande do Sul (UFRGS) Porto Alegre, Brazil junior.rocha@ufrgs.br Vanius Zapalowski Universidade Federal do Rio Grande do Sul (UFRGS) Porto Alegre, Brazil vzapalowski@inf.ufrgs.br Ingrid Nunes UFRGS, Porto Alegre, Brazil TU Dortmund, Dortmund, Germany ingridnunes@inf.ufrgs.br ABSTRACT Keeping the source code clean and organized throughout the soft- ware development and evolution is a challenging task. Due to many factors, design choices that cause the overall code structure to decay may be made and implemented, so that bene/f_its, such as reduced development time, can be obtained in the short term. In order to deal with these situations, the metaphor of technical debt emerged to allow such situations to be systematically managed. Although this concept is already known in academia, there are limited ev- idences that the industry widely adopts it. Therefore, this paper presents the results of a survey involving 74 participants that work in the Brazilian software industry, in order to understand why technical debt is introduced, eliminated and how it is managed in practice, with a focus on the code level. Our survey is not limited to the explicit management of technical debt but also includes the notion that the introduction of poor code without the awareness that it is a poor design choice can also become a debt. Such a code can be acknowledged as a debt to be paid as the software evolves. Our results show that overload of work and lack of time, together with pressure from the management, are the main reasons for the creation of technical debt. However, when participants evaluate other developers, they believe that inexperience also plays a key role. Moreover, the most eﬀective practice to avoid the creation of technical debt is code review, in the opinion of participants. CCS CONCEPTS •General and reference → Surveys and overviews ; •Social and professional topics → Systems development; •Software and its engineering → Maintaining software; KEYWORDS Technical Debt, Programming Best Practices, Survey ACM Reference format: Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes. 2017. Under- standing Technical Debt at the Code Level from the Perspective of Software Developers. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017, 10 pages. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. SBES’17, Fortaleza, CE, Brazil © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5326-7/17/09. . . $15.00 DOI: 10.1145/3131151.3131164 DOI: 10.1145/3131151.3131164 1 INTRODUCTION Writing and keeping the source code of software systems clean and organized, in accordance with best programming practices, is fundamental for reducing development costs and time as well as the likelihood of defect introduction. However, this is not a trivial task, and there are many factors that lead developers to choose poor or sub-optimal design and programming solutions, contributing to the degeneration of software systems and compromising their healthy evolution. Such factors range from the inexperience of developers to pressure to fast deliver running code. In order to cope with this scenario and motivate posterior software refactoring to improve its quality, the metaphor of technical debt (TD) [3] emerged, being not', 'scenario and motivate posterior software refactoring to improve its quality, the metaphor of technical debt (TD) [3] emerged, being not restricted to coding issues. It captures a wide variety of software quality problems that must be addressed. It is often the consequence of decisions that compromise quality in order to achieve short-time bene/f_its, such as higher productivity. The term technical debt was initially conceived to ease the com- munication with non-technical project stakeholders, so that they can understand the need for refactoring [9]. Nevertheless, such a term is limited to communication purposes if it is not explicitly and disciplined managed. Therefore, many approaches have been proposed to identify and manage TD, as shown in systematic stud- ies in the /f_ield [2, 6]. Despite all the academic work that has been done, there are limited evidences that the industry widely adopts it. Therefore, Ernst et al. [5] performed an extensive survey in order to better understand how the TD metaphor is used in practice. They investigated many aspects, such as the de/f_inition of TD, its source and how is is managed. However, there are still many issues that must be better understood. We in this paper extend the results obtained by Ernst et al. [5]. Similarly to their work, we conducted a survey to better compre- hend the use of TD in practice. However, our focus is exclusively on the code level, as opposed to the previous survey that aimed to understand more general issues regarding the management of TD, such as tools to support its management. Our focus on the code level allows us to have a deeper understanding of what makes developers to write poor code and also to refactor it. More speci/f_i- cally, we investigate: (i) developers’ willingness to adopt best pro- gramming practices; (ii) reasons to create TD at the code level; (iii) practices that would prevent its creation; and (iv) reasons for paying for it. Furthermore, we analyze the divergencies between developers’ opinions about themselves and other developers, which to our knowledge has not been investigated so far. Our study not 64', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes only allows us to extend the knowledge about TD in practice, but also compare results obtained in previous work—which focused on American and European companies—with our results obtained with 74 participants that work in the Brazilian software industry. Based on our survey, we concluded that overload of work and lack of time, together with pressure from the management, are the main rea- sons for creating TD. However, when participants evaluate other developers, they believe that inexperience also plays a key role. Moreover, the most eﬀective practice to avoid creating TD is code review, in the opinion of participants. Section 2 introduces work on TD and discusses results obtained in those related to ours. Sections 3 and 4 describe details of the procedure and participants of our survey, respectively. Our ob- tained results are presented and analyzed in Section 5, and further discussions are made in Section 6. Finally, Section 7 concludes. 2 RELATED WORK Since the idea of TD was conceived by Cunningham [4] in the con- text of software, much work has done in the /f_ield, with the proposal of techniques to support diﬀerent issues associated with TD. For example, SQALE is a method proposed by Letouzey and Ilkiewicz [11] that speci/f_ies models and tasks to be used to manage TD that exists in the source code of a software system. Zablah and Mur- phy [14] also focused on providing guidance for managing TD, by proposing a framework with a stronger connection to the /f_inancial metaphor. Their framework includes the idea of re/f_inancing TD, as TD can be paid with interests—e.g. the time saved by reducing code quality may be smaller than the time required to refactor it in the future. In another direction, Maldonado et al. [12] proposed an approach that mines the source code to search for self-admitted TD present in code comments. They used techniques in the area of natural language processing to do so. A signi/f_icant amount of the work on TD has been identi/f_ied and investigated in systematic mappings [2, 6]. The study performed by Fernández-Sánchez et al. [6] identi/f_ied which approaches focus on which types of TD (e.g. code, design, or testing debt) and which aspect (e.g. identi/f_ication, interest estimation, or management of the TD lifecycle). Alves et al. [2], in turn, did not consider a given set of TD types, but identi/f_ied those discussed in works covered by their study. They also investi- gated indicators (i.e. the source of information that indicated the presence of a debt) and management strategies as well as observed how approaches were evaluated by means of empirical studies. As discussed in the introduction, a previous survey was per- formed by Ernst et al. [5] with a goal similar to ours, i.e. to better understand the use of the TD metaphor in practice. Their study involved a large amount of participants of three large organizations, investigating three research questions. The /f_irst is focused on the de/f_inition of TD among software professionals, which allowed the authors to conclude that the metaphor is useful and commonly understood at an abstract level to indicate when accumulating soft- ware costs are urgent. The second research question aims to identify whether the software architecture was a source of debt, leading to the conclusion that it is. Finally, the last research question is associated with practices and tools adopted for managing TD. In summary, tools are not speci/f_ic enough to capture the accumulating problems of TD. As opposed to this previous survey, our survey investigates further issues focused on the code level. Moreover, our work targets an environment not covered by this previous survey, i.e. the Brazilian software industry. Because of our focus on the code level and the assumption that TD may be introduced without awareness (we later discuss the', 'i.e. the Brazilian software industry. Because of our focus on the code level and the assumption that TD may be introduced without awareness (we later discuss the de/f_inition of TD adopted in our survey), studies that investigate the origin of poor code written by developers are also related to our work. Lavallée and Robillard [10] conducted an in-depth ob- servational case study of the organization factors that impact on software quality. They observed a set of ten organizational issues, such as documenting complexity and under pressure, and suggested corrective actions for them. A large amount of open source projects were investigated by Tufano et al . [13] to understand when and why code smells [7] are introduced in the code. They concluded that code artifacts are often aﬀected by code smells since their cre- ation and also as a consequence of maintenance and evolution. Our survey—which, diﬀerently from these previous studies, captures the opinion of software professionals—allows us to verify whether there are similarities between the subjective view of professionals and the results of these studies. 3 STUDY SETTINGS We now focus on providing details about our survey. We /f_irst de- scribe our goal and derived research questions. Next, we give an overview of the questionnaire that participants had to answer. 3.1 Goal and Research Questions The goal of our survey is to have a deeper understanding of diﬀer- ent issues associated with the creation and payment of TD at the code level, from the developers’ perspective. However, we are not speci/f_ically interested in the explicit TD management but, more broadly, the roots of problems in the source code. Therefore, we used Hinsen’s [8] de/f_inition as a basis to ours, which was adapted to the code level and to take into account the unaware introduc- tion of TD. His de/f_inition is the following: “the term refers to future obligations that are the consequence of technical choices made for a short-term bene/f_it. ” Adapting his de/f_inition, we use the following de/f_inition of TD in our survey. Technical Debtis a metaphor used to refer to the need for rework in the future, which emerges from technical choices of low quality, in order to obtain short-term advantages. This debt is not necessarily identi/f_ied by who has made such choices. Therefore, poor design and programming choices, not considered as a debt since they were made, are also debts according to our de/f_inition. An example of such a situation is when a developer makes a poor design decision and introduces, e.g., a code smell in the code without being aware of it and, posteriorly, another developer identi/f_ies it and raises the issue, stating that there is a need for refactoring the code. There are four issues associated with TD that we are interested in investigating. This lead us to elaborate four research questions (RQs) that we aim to answer with our survey. They are listed next. RQ-1: Do developers believe that it is their duty to adopt best programming practices? 65', 'Understanding TD at the Code Level from the Perspective of Developers SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil RQ-2: What are the factors that lead developers to create tech- nical debt at the code level? RQ-3: What practices can prevent developers to create techni- cal debt at the code level? RQ-4: What are the reasons that lead developers to make tech- nical debt payments at the code level? Given that developers may have diﬀerent views of how they work and how other developers work, we aim to answer these questions considering developers’ opinion both about themselves and about other developers. 3.2 Questionnaire To answer these research questions, we elaborated a questionnaire1 that was answered by our study participants. Due to our focus on Brazilian participants, the questionnaire was provided in Por- tuguese. It is composed of /f_ive main parts, which are detailed next. Participant Data. After con/f_irming to have professional expe- rience in software development and consenting to par- ticipate, participants were asked to provide personal and professional details, in order for us to capture their pro/f_ile. More speci/f_ically, we asked their age, education, positions in which they worked (e.g. analyst or manager), years of ex- perience, number of companies where they worked, and a self-evaluation of their expertise in software development. Adoption of Best Programming Practices. Before investigating TD speci/f_ically, we veri/f_ied whether participants believe if developers should produce high-quality code by means of the adoption of best programming practices. Therefore, in this part of the questionnaire, participants were asked to state whether (i) they agree that it is an obligation of software developers to adopt best programming practices; and (ii) they adopt best programming practices, whenever possible. We also asked them to justify their answer. TD De/f_inition.Given that TD may be not widely known in indus- try, we asked participants to self-evaluate their knowledge about TD and how they de/f_ine it. Creation of TD. To better understand the creation of TD, we made three sets of questions to participants. First, we asked whether they agree with the following sentence: “I never create technical debt in the code I (or other developers) write. ” Second, we asked them to indicate the relevance of diﬀerent factors that lead them to create TD, using a 7-point Likert scale. These two questions were also asked with respect to other developers. Third, we asked them to indicate the relevance of diﬀerent practices that prevent the creation of TD using a 7-point Likert scale—practices that are adopted and should be adopted in their workplace. Management and Payment of TD. Finally, we made questions to understand what triggers TD payment. First, partici- pants were asked to state whether they agree with the following sentence: “I always make technical debt pay- ments, when I have the opportunity. ” Last, they indicated the relevance of diﬀerent factors that lead them to make TD payments (also with respect to other developers). 1Available at http://www.inf.ufrgs.br/prosoft/resources/2017/sbes-td-survey-form. pdf. In the end of the questionnaire, participants could also make additional comments, if they wanted to. In Table 1, we summarize the main questions of our questionnaire, split into categories. Some of the questions were asked from two perspectives, and we use ‘/’ to indicate the variation in these questions. The /f_irst column presents the labels of the questions, which are used to present our results. The alternatives provided in the questions of the questionnaire were selected based on an investigation of the literature and also on our experience in software development, i.e. whenever we had a source of information to list options, we adopted it; otherwise, we discussed which options should be made available. Moreover, to validate our questionnaire—both its questions as well as avail-', 'a source of information to list options, we adopted it; otherwise, we discussed which options should be made available. Moreover, to validate our questionnaire—both its questions as well as avail- able answers—we performed a pilot test. This test gave us feedback to improve our survey (questions, alternatives, and organization) before its release. Suggestions collected in the pilot test allowed us to clarify some questions and provide additional options to the questions. In the end of each question, we provided open-ended questions in which participants could list other options, if needed. These open-ended questions gave to the participants the opportu- nity of mentioning what was uncovered by our provided options. 4 PARTICIPANTS Volunteers were selected to participate in our study using conve- nience sampling. An invitation to participate in the study was sent to e-mail lists of undergraduate and graduate students at UFRGS, contacts of the researchers and published in posts in social networks. In order to participate in the study, participants had to con/f_irm that they have professional experience in software development. The questionnaire was available from February 24 to March 22, 2017. As result, we collected in total data from 74 Brazilian partic- ipants. The demographic data that characterize our participants are presented in Table 2. As can be seen, almost all participants have age between 20 and 39 years. The majority has a complete or incomplete undergraduate degree. With respect to professional experience, most of the participants have at least three years of ex- perience, having worked in up to four software companies—only 12 participants worked in /f_ive or more companies. Most participants (66%) evaluate themselves with (very) high expertise in software development. Almost all participants (96%) work or have worked as a developer, the majority also works or has worked as an analyst. Regarding knowledge about TD, more than a third (35%) of the participants has (very) low knowledge. This indicates that indeed this term is not widely known in the industry, at least in the Brazil- ian scenario. The highest amount of participants (36%) have medium knowledge about this metaphor. 5 RESULTS AND ANALYSIS Having described details of the procedure of our study and its participants, we now proceed to the presentation of our results and its analysis. The results are presented by research question. 5.1 RQ-1: Do developers believe that it is their duty to adopt best programming practices? Given that we also consider the unaware TD creation, we need /f_irst to understand whether developers believe they must write high- quality code and poor code is not only a consequence of the lack of 66', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes Table 1: Summary of the questionnaire. Adoption of Best Practices Q1: Obligation to adopt best practices It is an obligation of every software developer to adopt, whenever possible, best development practices in his/her work, such as following software engi- neering guidelines (e.g. modularity and legibility), adoption of design patterns, etc. Q2: I adopt best prac- tices Whenever possible, I adopt best soft- ware development practices in my work. Creation of Technical Debt Q3: I/developers create TD I/developers never create technical debt in the code I/they write. Q4: Reasons to create TD (I/DEV) Indicate the relevance of the following factors that make you/developers to create technical debt in the code. Q5: Practices to avoid TD (ARE/SHOULD BE) Indicate the relevance of the following practices, which are/should be adopted in work environment, to minimize the creation of technical debt in software development. Payment and Management of Technical Debt Q6: I make TD pay- ments I always make technical debt payments at the code level when I have the op- portunity. Q7: Reasons to make TD payments (I/DEV) Indicate the relevance of the follow- ing factors that make you/developers to make technical technical debt pay- ments in the code. developers’ willingness to not adopt best programming practices. A code that is in accordance with such practices tends to have less TD. Thus, we analyzed answers to Q1 and Q2 (Table 1), which are related to the obligation to adopt best practices. The answers to these questions as well as to Q3 and Q6 are shown in Figure 1. In this /f_igure, there is a statement in each line, and boxes show the distribution of answers given by participants. The percentages of disagreement, neutral and agreement answers are shown on the left, central and right hand sides, respectively, of the chart. As expected, the majority of the participants (97%) at least weakly agree that the adoption of best practices is mandatory during the software development. Only 3% of the participants reported that it is not a duty of developers to adopt them, thus disagreeing with the statement. Consequently, this result gives strong evidence that most of the developers believe that the adoption of best practices is an obligation. When questioned about whether they adopt best programming practices, the agreement with the statement was Table 2: Demographic data of participants (N = 74). Characteristic Answer # % Age < 20 years 1 1.4% 20–29 years 39 52.7% 30–39 years 32 43.2% > 39 years 2 2.7% Education High School 1 1.4% Undergraduate* 42 56.8% Master* 21 28.4% PhD* 3 4.1% Others 7 9.5% Professional Experience < 2 years 7 9.5% 3–5 years 20 27.0% 5–10 years 27 36.5% > 10 years 20 27.0% Employers 1–2 companies 29 39.2% 3–4 companies 33 44.6% 5–6 companies 9 12.2% > 6 companies 3 4.1% Expertise in Very Low 0 0.0% Software Development Low 4 5.4% Medium 21 28.4% High 33 44.6% Very High 16 21.6% Position Manager 10 13.5% (current and former) Development 71 95.9% Analyst 42 56.8% Support 17 23.0% Quality 12 16.2% Others 1 1.4% Knowledge about Very Low 7 9.5% Technical Debt Low 19 25.7% Medium 27 36.5% High 19 25.7% Very High 2 2.7% * Complete or Incomplete. unanimous. All participants answered that they adopt best pro- gramming practices (whenever possible). Even the few participants that stated that it is not mandatory to follow best practices said that they do so anyway. Furthermore, only few participants weakly agree with both statements, 3% and 4%, respectively. These results give evidence that developers believe that is their duty adopt best programming practices, and they (at least try to) do it . Complementary to these two questions, participants were asked about the reasons that lead developers to adopt or not programming best practices. On the one hand, as main reasons to adopt best', 'Complementary to these two questions, participants were asked about the reasons that lead developers to adopt or not programming best practices. On the one hand, as main reasons to adopt best practices, they mentioned the bene/f_its regarding source code with high quality that are gained by following them, such as reduction of maintenance costs and improvement of code legibility. On the other hand, participants pointed out as main reasons to not adopt best 67', 'Understanding TD at the Code Level from the Perspective of Developers SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil 0% 3% 15% 55% 93% 100% 97% 80% 24% 5% 0% 0% 5% 20% 1% I make TD payments Developers never create TD I never create TD I adopt best practices Obligation to adopt best practices 100 50 0 50 100 Percentage Response Strongly disagree Disagree Weakly disagree Neutral Weakly agree Agree Strongly agree Figure 1: Answers to agreement/disagreement questions. practices the lack of time due to inadequate deadlines and pressure to /f_inish tasks. There is also one issue mentioned that is pointed out as a reason to both adopt and not adopt best practices, which is the technical and project-speci/f_ic knowledge. Participants reported that the knowledge about the patterns and standards used in a given application would facilitate the adoption of best practices but, at the same time, they mentioned that most of the projects do not have adequate documentation, which contributes to the introduction of violations to the planned design of the systems. Despite that participants believe that it is their duty to adopt best programming practices, TD is commonly found in the source code. This scenario is contradicting because the adoption of programming best practices can potentially prevent TD creation. To comprehend how TD is created, we then asked their opinion about whether they and developers in general create TD. This was asked in Q3 both from the participant (self) and developers in general perspectives, as shown in the third and fourth lines of Figure 1. The answers to Q3 show that 55% participants at least weakly disagree, 20% have neutral opinions and 24% at least weakly agree that they never introduce TD. This means that, although all partici- pants agree with the importance of the adoption of programming best practices, many of them admit that they introduce TD. This means that the “whenever possible ” plays a key role in Q1, that is, it is not always possible to follow best practices, possibly creating TD. Even more contrasting to the questions related to best practices, the answers to the perspective of developers in general shows 93% of disagreement and only 5% of agreement. Thus, the reasons given to justify the not adoption of best practices have a signi/f_icant in/f_lu- ence on the development because most of the participants believe that adoption of best practice is mandatory, but also believe that TD is created by developers. Moreover, the answers diﬀer from the self perspective, when participants answered Q3 considering themselves, to the general perspective, when they answered it con- sidering developers in general. In the self perspective, 44% agree or are neutral that they never create TD, while only 6% have the same view regarding developers in general. A Wilcoxon Rank Sum and Signed Rank Test2 revealed that in fact the diﬀerence among 2The distribution of the data is not normal. the answers given by participants to the two perspectives of Q3 is signi/f_icant(p-value < 0.05). Thus, this gives evidence that the participants believe that they create less TD than other developers. Although TD may be created by developers, high-quality code may be also achieved by making TD payments. Therefore, we in- vestigated whether participants pay TD when they have the oppor- tunity (Q7 in Table 1). The results show that 80% of the developers at least weakly agree that they make TD payments and only 15% at least weakly disagree. This corroborates to the idea that developers at least have the intention of producing high-quality code. 5.2 RQ-2: What are the factors that lead developers to create TD at the code level? The questions discussed above show that participants admit that they create TD and they also believe that other developers do so. In RQ2, we aim to understand why it happens, i.e. identify the factors that lead developers to create TD. This was asked with the question', 'RQ2, we aim to understand why it happens, i.e. identify the factors that lead developers to create TD. This was asked with the question Q4 of Table 1, in which we provided 12 factors derived from our analysis of the main factors reported in recent work on TD. These 12 factors are presented in Table 3 with their label (used hereafter) and the alternatives presented to participants. The developer opin- ions of how much these factors in/f_luence the creation of TD are presented in Figure 2, which has both a self evaluation perspec- tive (Figure 2a) as well as an evaluation from the perspective of other developers (Figure 2b). In these presented boxplots, the black diamonds represent the average of each factor. Based on Figure 2a, we observe that the range of responses re- lated to the self perspective is from 1 to 7 in all factors. This wide variation indicate that the reasons to create TD, from a self perspec- tive, is developer-speci/f_ic. This can be justi/f_ied by the variability in the participants’ characteristics. Our study involved 74 participants with diﬀerent levels of experience, diﬀerent development environ- ments, and diﬀerent ages contributing to diﬀerent self evaluations. Despite the wide variation, there are four factors that have both median (MED) and mean (M) that indicate weakly agree or agree (5–6): tight schedule (MED = 6, M = 5.65); management pressure (MED = 6, M = 5.39); work overload (MED = 6, M= 5.28); and bad code (MED = 5, M = 5.11). These answers partly corroborate to the 68', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes Development Inexperience Technology Inexperience Immature Technology Personal Motivation Professional Motivation Poor Requirements Communication Management Pressure Tight Schedule Work Overload Bad Code Environment 1 2 3 4 5 6 7 (a) Self evaluation. Development Inexperience Technology Inexperience Immature Technology Personal Motivation Professional Motivation Poor Requirements Communication Management Pressure Tight Schedule Work Overload Bad Code Environment 1 2 3 4 5 6 7 (b) Evaluation of other developers. Figure 2: Reasons to create TD. reasons reported by Kruchten et al. [9] as major causes of TD. Their study presented a large number of reasons to create TD and the opinion of their participants diverged in a few points, such as their self-education. All the other factors have an average that ranges from 3 to 5, which may be considered as neutral. None of the factors have an average lower than weakly disagree (3). The factor with the lowest average is personal motivation with an average of 3.49. Given that this question is associated with more than two groups, we performed a Friedman’s test, which indicated that there is a signi/f_icantdiﬀerence among the diﬀerent factors evaluated (p-value < 0.05). Therefore, we performed a post-hoc analysis of the answers with the Wilcoxon-Nemenyi test. According to this test, there are signi/f_icant diﬀerences (p-values < 0.05) between:technology inexpe- rience and the two factors with the lowest averages; bad code and the four factors with the lowest averages; work overload and the six factors with the lowest averages; pressure and the seven factors with the lowest averages; andtime and the eight factors with lowest averages. Thus, these /f_ive factors are considered the most relevant according to the statistical test performed. The answers provided considering the perspective of other de- velopers, as can be seen in Figure 2b, tend to have a more restrict range of answers when compared to the self perspective. 8 of the 12 factors presented to the participants received no strongly disagree (1) answer. The answers are more concentrated on the right side of the boxplot, which correspond to agreement answers. The factors that have a median and mean at least agree (5) are: tight schedule (MED = 6.5, M = 6.14), management pressure (MED = 6, M = 6.07); work overload (MED = 6, M = 5.97); bad code (MED = 6, M =5.81); development inexperience (MED = 6, M = 5.58); and technology inex- perience (MED = 6, M = 5.43). These results are also in accordance with the work of Kruchten et al. [9], because they reported a lack of education and basic incompetence as major problems that lead to the creation of TD. The averages and medians of the remaining factors vary from neutral to agree (4–5). Moreover, none of them has median or average corresponding to the disagreement answers. The same statistical test performed above was done to perspective of other developers. It also resulted in asigni/f_icantdiﬀerence among the factors evaluated. The result of the post-hoc tests indicate that six factors have an average statistically diﬀerent from the other factors: technology inexperience and the two factors with the lowest averages; development inexperience and the /f_ive factors with the lowest averages (except professional);bad code, work overload, man- agement pressure , and tight schedule have averages signi/f_icantly diﬀerent from that of the six factors with the lowest averages. The most common comment in the open-ended question about the adoption of programming best practices is somewhat related to communication, e.g. lack of knowledge sharing about the project and documentation. However, when participants were asked about the factors that lead developers to create TD, it was reported as not so important—it has an average of 4.16 and 4.88 from the self 69', 'Understanding TD at the Code Level from the Perspective of Developers SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Table 3: Factors to create TD. Label Alternative Development Inexperience Inexperience or lack of knowledge about software development Technology Inexperience Inexperience or lack of knowledge about technologies adopted in the project Immature Technology Use of immature technology Personal Motivation Lack of motivation and interest by per- sonal reasons (e.g. stress) Professional Motivation Lack of motivation and interest by profes- sional reasons (e.g. uninteresting project) Poor Requirements Inadequately speci/f_ied requirements Communication Lack of communication among project stakeholders (e.g. users and developers) Management Pressure Pressure imposed by managements Tight Schedule Time limit lower than needed Work Overload Work overload Bad Code The current code state already has its structure compromised in terms of quality Environment Lack of adequate environment and infras- tructure and other developers perspectives, respectively. This may have occurred because in the former question about the adoption of best practices participants did not realize that there are other factors that are more relevant to the adoption of best practices and these practices were presented in the factors to avoid TD. Comparing the self and other developers perspectives, we no- ticed that answers associated with other developers have a higher average than those associated with the self perspective. These re- sults lead to the conclusion that developers believe that other devel- opers are more aﬀected by the mentioned factors than themselves. In both perspectives, developers highlighted the importance of management pressure, tight schedule, and work overload to the creation of TD, con/f_irmed by our statistical tests. Analyzing the diﬀerences in the factors with high averages, we noticed that devel- opers have a perception that other developers are less experienced than themselves. In the evaluation of other developers, the two factors related to inexperience have statistical signi/f_icant diﬀerence in their averages when compared to other factors and had a higher average and median when compared to self perspective. 5.3 RQ-3: What practices can prevent developers to create TD at the code level? We not only investigated the factors that lead developers to create TD, but also identi/f_ied the practices adopted in the developers’ work environment that prevent TD creation. Considering the practices to improve software quality, we selected those that could possibly help prevent the creation of TD. Using this information, we asked Table 4: Alternatives to prevent TD . Label Alternative Code Review Code review and inspection TDD Test-driven development Continuous Integration Continuous integration Automated Tests Automated tests Pair Programming Pair programming Code Tools Use of tools of static code analysis Backlog Keep a backlog with information about technical debt (estimation and actions) participants which of these practices are or should be used in their development environment in order to prevent this. We selected seven factors and additionally provided an open-ended question in which unlisted factors could provided. The seven provided factors are presented in Table 4, in which we detail their label and brief description. The participants were asked according to two perspec- tives, one asking about the practices that are adopted, and another asking the practices thatshould be adopted. The data collected based on the answers of participants are presented Figure 3. Analyzing the answers considering the practices that are adopted, reported in Figure 3a, all practices had a wide variation in their answers, i.e. all practices had answers from strongly disagree to strongly agree (1–7). This wide variation means that all practices are considered important by at least some of the participants. An-', 'answers, i.e. all practices had answers from strongly disagree to strongly agree (1–7). This wide variation means that all practices are considered important by at least some of the participants. An- other information about the adopted practices is that most of the practices had similar answers—5 of the 7 practice averages are be- tween disagree and neutral (3–4). Only code review and continuous integration had non-disagreement averages (higher than 4), being 4.81 and 4.02, respectively, but only code review had an agreement median. Based on these results, we observe that there is no adopted practice that is widely considered relevant to prevent the creation of TD. Even code review and continuous integration, which are the adopted practices considered most relevant, did not reach an agree- ment level of adoption. To verify whether the diﬀerence among the averages of the diﬀerent practices is statistical signi/f_icant, we performed the same statistical tests as above. The results show that there is a signi/f_icantdiﬀerence among the averages obtained by the practices—Friedman’s test with a p-value < 0.05—but this diﬀerence is only due to code review and the all other practices but continuous integration. This indicates that code review is an adopted practice that is potentially relevant to prevent TD creation. Given that participants know most of the practices but these are not necessarily adopted in their work environments, we also inves- tigated the answers provided by them regarding which practices should be adopted to prevent TD. The answers to this question are presented in Figure 3b. We observed that participants generally agree with the use of all practices. Only pair programming, code tools, and backlog, with averages 4.58, 4.88, and 4.96 respectively, were in a neutral range (3–5) of answers. However, these practices achieved a median of agreement (5). Again, the practice that is more 70', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes Code Review TDD Continuous Integration Automated Tests Pair Programming Code Tools Backlog 1 2 3 4 5 6 7 (a) Practices that are adopted. Code Review TDD Continuous Integration Automated Tests Pair Programming Code Tools Backlog 1 2 3 4 5 6 7 (b) Practices that should be adopted. Figure 3: Practices to avoid TD. relevant to prevent TD is code review , which had an average of 5.95 and median of 7. These results give evidence that developers are aware of the importance of such practices despite the fact that these practices are not adopted. We also performed a Friedman’s test with these answers, and the results presented a signi/f_icantdiﬀerence as well. The post-hoc tests showed a signi/f_icant diﬀerence between code review and the four practices with the worst averages. The practices that should be adopted show that participants see all programming practices as good methods to prevent TD given the high averages of each practice. However, they do not frequently adopt such practices—the averages and medians of practices that should be adopted are higher than those that are adopted. The only exception is code review, which appears as the most adopted and the most relevant to prevent TD creation. In both questions, participants answered that backlog is not one of the most important practices, diverging from the results of a previous study [9]. With respect to the answers of this research question, we can observe that the presence of outliers (see Figure 3b). We emphasize that, in our study, we did not have access to participants to further understand the reason underlying their answers. Consequently, further conclusions based on our data would be thus speculation. Note that our study is thus mostly quantitative, and was performed also to give insights for further qualitative investigations that can be done on a smaller scale. 5.4 RQ-4: What are the reasons that lead developers to make TD payments at the code level? The creation of TD is inevitable [1], likely due to the factors dis- cussed in our RQ-1. Therefore, developers must make TD pay- ments to keep the project maintainable in the long run. To /f_ind Table 5: Reasons to make TD payments: alternatives. Label Alternative Awareness Learn that there technical debt to be payed Time Have available time Co-change Need to change the code where the technical debt is located Request Request to make a technical debt payment out what leads developers to make TD payments, we asked partici- pants which reasons lead them to do it. Most of the work on TD focuses on the explicit management of TD. As our TD de/f_inition is broader, and we do not only consider the explicit TD manage- ment, we suggested four possible reasons we considered plausible, listed in Table 5. Participants could inform other possible reasons in an open-ended question. They answered the question associated with these speci/f_ied reasons using a 7-point Likert scale, as in the previous questions. We also investigated two perspectives in this question (self and other developers perspective). The answers to this question, from these two perspectives, are presented in Figure 4. From self perspective, participants at least weakly agreed that all alternatives are reasons that lead them to make TD payments. The four are relevant, but there is an order of importance, being the most relevant having available time. It is the only reason that had an average and a median greater than agree (5). The two least relevant reasons pointed out by the participants are awareness and request. These two reasons had similar answers with some variation, which caused the median of request one point higher than awareness. A Friedman’s test indicated that there is a statistical signi/f_icant 71', 'Understanding TD at the Code Level from the Perspective of Developers SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Awareness Time Co-change Request 1 2 3 4 5 6 7 (a) Self evaluation. Awareness Time Co-change Request 1 2 3 4 5 6 7 (b) Evaluation of other developers. Figure 4: Reasons to make TD payments. diﬀerence among the averages of the given reasons. The post-hoc tests showed that this is due to the diﬀerence between both time and awareness, and request. Analyzing the perspective of the other developers, we noticed that time, co-change and request have similar answers. These three reasons have a median of agree (6) and averages between weakly agree and agree (5–6). Awareness is the reason with the least impor- tance to make TD payments according to the participants. Similarly to the question from the self perspective, participants reinforced that having available time is an important issue. In this case, the conducted statistical test revealed that there is a signi/f_icantdif- ference among the reasons, and this is due to the diﬀerence from awareness from the remaining three reasons. The answers to both perspectives were similar considering the order of relevance of the reasons, but the level of agreement to the statements that these are reasons to make TD payments largely varies. Awareness received less importance in the other developers perspective and the opposite occurred with the other three reasons. Time is the reason with highest diﬀerence in the distributions of the two perspectives. In the self perspective, time had a restrict range of answers, which are mostly from agree to strongly agree (6–7), whereas, in the other developers perspective, the range is from neutral to strongly agree (4–7). 6 DISCUSSION Based on the obtained results, we derived insights related to the understanding of TD by developers. We /f_irst discuss what they consider a TD before participating in this study (Section 6.1) and then make further considerations regarding the adoption of devel- opment practices (Section 6.2). Finally, we discuss identi/f_ied threats to the validity of our study in Section 6.3. 6.1 Understanding of the De/f_inition of Technical Debt Our survey focused on understanding many issues related to TD. As said, our interpretation of TD in this paper includes the non- explicit management of TD. Consequently, participants—before this study—may not know TD as the metaphor as typically used. Before introducing a de/f_inition of TD to participants and making questions to answer our research questions, we asked them to make a self-evaluation of their knowledge regarding TD. The results are shown in Table 2 together with other characteristics of participants. We also asked them to tell us their de/f_inition of TD. Given that the majority of participants reported to have at most medium knowl- edge about the topic, it expected they may not be aware of the most used de/f_initions for the term. 89% of the participants de/f_ined TD using the following expres- sions: “maintenance cost, ” “bad code quality, ” “lack of time, ” “cost bene/f_it to deliver running code, ” “not the best implementation” and similar terms. These answers show that most of them have a general notion of the meaning of the term TD. However, six participants pointed out that they had no idea of what TD is about and one mentioned a de/f_inition found after searching the web. Moreover, although they understand that TD is associated with low quality software solutions, only a few expressed that it is the cost of ob- taining short time bene/f_its. Finally, only six expressed the need for making TD payments or refactorings. 6.2 Adoption of Development Practices As previously mentioned, the participants answered that code re- view is the most relevant practice—it is and should be adopted to prevent the creation of TD. These answers were based on their personal view and experience in software development. The occur-', 'view is the most relevant practice—it is and should be adopted to prevent the creation of TD. These answers were based on their personal view and experience in software development. The occur- rence of code review as the most adopted practice and the most important practice that should be adopted may be due to the limited experience with the other practices of the participants. The answers related to the adoption of practices were lower compared to the practices that should be adopted. Thus, these answers presented two possible scenarios to the adoption of development practices: (i) few development environments adopt alternative practices; and (ii) only code review is eﬀective to reduce the creation of TD. Based on the answers of the practices that should be adopted, we tend to believe that the more likely /f_irst scenario is the /f_irst, given that most participants indicated that all practices should be adopted. 72', 'SBES’17, September 20–22, 2017, Fortaleza, CE, Brazil Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes Therefore, it is important to reinforce the adoption of such practices, mainly code review. 6.3 Threats to Validity We identi/f_ied two main threats to the validity of our study. They are related to: (i) the participants of the survey; and (ii) the alterna- tives provided in the questions. We selected the participants of our study using convenience sampling. They were invited using social networks and e-mail lists. Moreover, we contacted software devel- opment companies and asked them to send invitations to their de- velopers (some declined the request due to the company’s policies). As result, we reached 74 developers with varying characteristics— from diﬀerent development environments, with diﬀerent expertise, age, education degree, and position—as presented in Table 2. This provided us with enough information to obtain insights and derive fruitful and statistically signi/f_icant conclusions about TD at the code level. However, if we had received input from more participants our result could have been improved. Note that all participants are Brazilians, which is a control variable given that our investigated scenario is the Brazilian IT industry. The second identi/f_ied threat is related to the alternatives pro- vided to participants. We run pilot tests with three people before executing the study (their answers were excluded), which allowed us to make improvements in the questionnaire. However, two partic- ipants reported that not applicable (N/A) should have been provided as an answer regarding the practices adopted and that should be adopted. Based on obtained answers, we concluded that participants gave a low score to the practices that are either not adopted or ir- relevant. This can be seen by analyzing the contrast with respect to the practices that should be adopted. Moreover, one of these two participants reported no experience in some of the practices, and thus marked a neutral answer, which might be valid as well. However, replications of this study should make N/A available to participants in these questions to avoid possible misinterpretation. 7 CONCLUSION Technical debt is considered a useful metaphor to express the trade- oﬀ between quality and productivity. It has been increasingly in- vestigated in academia, with a modest popularity in the industry. To understand how developers understand and deal with technical debt in the Brazilian IT industry, we performed a survey involving 74 developers with professional experience regarding this topic. We explored not only the scenario in which technical develop is explic- itly managed, but also considered the possibility of the unaware creation of technical debt. With an initial investigation on the adoption of best program- ming practices, we observed that the majority of participants believe that it is a duty of developers to follow such practices, and they do so whenever possible. However, they admit that they create technical debt, and also believe that other developers create it—and more than them. In a further investigation of what are the reasons that cause them to create technical debt, they pointed out manage- ment pressure, tight schedule, and work overload . Moreover, when considering other developers, inexperience was also pointed out as a main reason. To prevent the creation of technical debt, we con- cluded that: (i) few practices are adopted in the work environment of participants; and (ii) most of the typical practices to improve software quality are considered relevant, being the most relevant code review as a practice that is adopted and should be adopted. Finally, the key motivation, according to the participants from a self perspective, for making technical debt payments is to have available time . However, considering other developers, they also mentioned co-change and request as important reasons.', 'self perspective, for making technical debt payments is to have available time . However, considering other developers, they also mentioned co-change and request as important reasons. Our survey provided us with many useful knowledge regarding technical debt. As future work, we aim to use such knowledge to develop supporting tools to help the technical debt management. ACKNOWLEDGMENTS Ingrid Nunes would like to thank for research grants CNPq ref. 303232/2015-3, CAPES ref. 7619-15-4, and Alexander von Humboldt, ref. BRA 1184533 HFSTCAPES-P. REFERENCES [1] Eric Allman. 2012. Managing Technical Debt. ACM Commununication 55, 5 (May 2012), 50–55. DOI:http://dx.doi.org/10.1145/2160718.2160733 [2] Nicolli S.R. Alves, Thiago S. Mendes, Manoel G. de Mendonça, Rodrigo O. Spínola, Forrest Shull, and Carolyn Seaman. 2016. Identi/f_ication and Management of Technical Debt: A systematic mapping study.Inf. Softw. Technol. 70, C (Feb. 2016), 100–121. DOI:http://dx.doi.org/10.1016/j.infsof.2015.10.008 [3] F. Buschmann. 2011. To Pay or Not to Pay Technical Debt. IEEE Software 28, 6 (Nov 2011), 29–31. DOI:http://dx.doi.org/10.1109/MS.2011.150 [4] Ward Cunningham. 1992. The WyCash Portfolio Management System. SIGPLAN OOPS Mess. 4, 2 (Dec. 1992), 29–30. DOI:http://dx.doi.org/10.1145/157710.157715 [5] Neil A. Ernst, Stephany Bellomo, Ipek Ozkaya, Robert L. Nord, and Ian Gorton. 2015. Measure It? Manage It? Ignore It? Software Practitioners and Technical Debt. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2015) . ACM, New York, NY, USA, 50–60. DOI:http://dx. doi.org/10.1145/2786805.2786848 [6] C. Fernández-Sánchez, J. Garbajosa, C. Vidal, and A. Yagüe. 2015. An Analysis of Techniques and Methods for Technical Debt Management: A Re/f_lection from the Architecture Perspective. In 2015 IEEE/ACM 2nd International Workshop on Software Architecture and Metrics . 22–28. DOI:http://dx.doi.org/10.1109/SAM. 2015.11 [7] Martin Fowler. 1999. Refactoring: Improving the Design of Existing Code . Addison- Wesley Longman Publishing Co., Inc., Boston, MA, USA. [8] K. Hinsen. 2015. Technical Debt in Computational Science. Computing in Science Engineering 17, 6 (Nov 2015), 103–107. DOI:http://dx.doi.org/10.1109/MCSE. 2015.113 [9] P. Kruchten, R. L. Nord, and I. Ozkaya. 2012. Technical Debt: From Metaphor to Theory and Practice. IEEE Software 29, 6 (Nov 2012), 18–21. DOI:http://dx.doi. org/10.1109/MS.2012.167 [10] M. Lavallée and P. N. Robillard. 2015. Why Good Developers Write Bad Code: An Observational Case Study of the Impacts of Organizational Factors on Soft- ware Quality. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 1. 677–687. DOI:http://dx.doi.org/10.1109/ICSE.2015.83 [11] J. L. Letouzey and M. Ilkiewicz. 2012. Managing Technical Debt with the SQALE Method. IEEE Software 29, 6 (Nov 2012), 44–51. DOI:http://dx.doi.org/10.1109/ MS.2012.129 [12] E. Maldonado, E. Shihab, and N. Tsantalis. 2017. Using Natural Language Pro- cessing to Automatically Detect Self-Admitted Technical Debt.IEEE Transactions on Software Engineering PP, 99 (Jan 2017), 1–1. DOI:http://dx.doi.org/10.1109/ TSE.2017.2654244 [13] M. Tufano, F. Palomba, G. Bavota, R. Oliveto, M. Di Penta, A. De Lucia, and D. Poshyvanyk. 2015. When and Why Your Code Starts to Smell Bad. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering , Vol. 1. 403–414. DOI:http://dx.doi.org/10.1109/ICSE.2015.59 [14] R. Zablah and C. Murphy. 2015. Restructuring and re/f_inancing technical debt. In 2015 IEEE 7th International Workshop on Managing Technical Debt (MTD) . 77–80. DOI:http://dx.doi.org/10.1109/MTD.2015.7332629 73']","['TECHNICAL DEBT AT THE CODE LEVEL This  briefin  reports  scieitfc  evideice oi techiical debt at the code level based oi  a  survey  aiswered  by  Braziliai developers. We aialyzed techiical debt coisideriin  developers’  self-evaluatoi aid  their  opiiioi  about  other developers. FINDINGS \uf0b7 Developers believe that is their duty adopt  best programming practces and they at least  try to do it. \uf0b7 Developers believe that they create less  technical debt than other developers. \uf0b7 Developers have the intenton of producing  high-quality code. \uf0b7 Most developers (89% of the partcipants)  have a correct general noton of what is  technical debt. \uf0b7 Developers consider techiolony iiexperieice, bad code, work overload, pressure, aid tme  the main factors to create technical debt. \uf0b7 Developers believe that other developers are  more afected by the factors to create  technical debt than themselves. \uf0b7 All presented programming practces (code  review, TDD, coitiuous iitenratoi,  automated tests, pair pronrammiin, code  tools aid backlon) were considered good  practces to prevent technical debt by  developers, being code review the most  relevait oie. \uf0b7 Most of the presented programming practces  seem to not be adopted in practce, except  code review, which was pointed out as an  adopted practce to prevent technical debt. \uf0b7 Developers answered that all reasons  (awareness, available tme, co-change,  request) presented are relevant to make TD  payments. \uf0b7 Concerning self perspectve, developers  believe that tme is the most important  reason to make technical debt payments. \uf0b7 Concerning other developers, in additon to  tme, co-chaine aid rehuest were also  considered relevant to make technical debt  payments. \uf0b7 It is important to reinforce the adopton of  such practces, mainly code review. Who is this briefig for? Sofware engineering practtoners who want to make decisions about the  preventon and management of  technical debt at the code level based  on scientic evidence. Where the finiigs come from? All indings of this brieing were  extracted from the survey conducted  by Rocha et al. involving 74  partcipants. What is iiclunen ii this briefig? The main indings derived from a  survey that investgated the reasons to  create technical debt, prevent its  creaton and make technical debt  payments. For annitoial iiformatoi about the Prosof research group: http://www.inf.ufrgs.br/prosof ORIGINAL RESEARCH REFERENCE Rocha et al. Understanding Technical Debt at the Code Level from the Perspectve of Softare Developers. SBES, 2017.']","**Title: Understanding Technical Debt from the Perspective of Software Developers**

**Introduction:**
This Evidence Briefing summarizes the findings from a study conducted to explore the understanding and management of technical debt (TD) at the code level among software developers in the Brazilian industry. The study aims to shed light on the factors leading to the creation of TD, practices to prevent it, and the motivations for addressing it.

**Core Findings:**
1. **Perception of Technical Debt:** The majority of developers recognize the importance of adopting best programming practices. However, many admit to introducing TD due to various pressures, such as tight deadlines and management expectations. Notably, developers believe they create less TD than their peers, indicating a tendency to view their own work more favorably.

2. **Factors Contributing to Technical Debt:** Key reasons for the creation of TD include:
   - **Overwork and Management Pressure:** Developers report that heavy workloads and pressure from management are significant contributors to TD.
   - **Inexperience:** Many participants believe that inexperience, both personal and among peers, plays a crucial role in the introduction of poor coding practices.

3. **Effective Practices to Prevent Technical Debt:** Code review is identified as the most effective practice to mitigate the creation of TD. Other practices, such as Test-Driven Development (TDD) and Continuous Integration, are recognized as beneficial but are less frequently adopted in practice.

4. **Motivations for Paying Off Technical Debt:** The most significant motivator for developers to address TD is the availability of time. Other factors include the need to change code related to existing debt and formal requests to make payments.

5. **Awareness of Technical Debt:** Despite acknowledging the existence of TD, many developers have limited knowledge of the term and its implications. This suggests a gap in understanding that could hinder effective management of TD in practice.

**Who is this briefing for?**
This briefing is intended for software development managers, team leads, and practitioners who are interested in understanding the dynamics of technical debt in their projects and seeking strategies to improve code quality and team practices.

**Where the findings come from?**
The findings are based on a survey conducted with 74 software developers working in the Brazilian software industry, focusing on their perceptions, experiences, and practices related to technical debt.

**What is included in this briefing?**
This briefing includes insights into the reasons behind the creation of technical debt, effective practices for its prevention, and the motivations for addressing it, as well as the developers' overall understanding of the concept.

To access other evidence briefings on software engineering, visit: [http://ease2017.bth.se/](http://ease2017.bth.se/)

For additional information about the research group, please contact:
Junior Cesar Rocha: junior.rocha@ufrgs.br
Vanius Zapalowski: vzapalowski@inf.ufrgs.br
Ingrid Nunes: ingridnunes@inf.ufrgs.br

**ORIGINAL RESEARCH REFERENCE:**
Junior Cesar Rocha, Vanius Zapalowski, and Ingrid Nunes. 2017. Understanding Technical Debt at the Code Level from the Perspective of Software Developers. In Proceedings of SBES’17, Fortaleza, CE, Brazil, September 20–22, 2017. DOI: [10.1145/3131151.3131164](https://doi.org/10.1145/3131151.3131164)"
"['An Automated Refactoring Approach to Remove Unnecessary Complexity in Source CodeNathan Manera Magalhães Instituto Federal do Sudeste de Minas Gerais Juiz de Fora, Minas Gerais nathan.manera@gmail.com  Heleno de Souza Campos Junior Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais helenocampos@ice.ufjf.br Marco Antônio Pereira Araújo Instituto Federal do Sudeste de Minas Gerais Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais marco.araujo@ifsudestemg.edu.br  Vânia de Oliveira Neves Universidade Federal de Juiz de Fora Juiz de Fora, Minas Gerais vania@ice.ufjf.br ABSTRACT Programming apprentices may choose to prioritize the correct functioning of a source code without focusing on their quality, making them difficult to maintain and test. Based on that, a phenomenon called unnecessary structural complexity may occur, in which a program has a cyclomatic complexity value that can be reduced without affecting its external behavior. In a previous work, we developed an approach and tool to address this problem. The approach is able to identify the presence of unnecessary cyclomatic complexity and to show the developer a suggestion to restructure the source code, through a control flow graph. The goal of this paper is to automate the source code refactoring process to support the elimination of unnecessary cyclomatic complexity. We performed two experimental studies to evaluate the approach in the academic context. The evidences provided by these studies suggest that the approach is able to support unnecessary cyclomatic complexity removal. We could not find, however, evidences about the implications of such approach on unit tests development. CCS CONCEPTS D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement – restructuring, reverse engineering, and reengineering. KEYWORDS Cyclomatic Complexity; Software Quality; Source Code Refactoring; Software Testing; Control Flow Graph;', '1. INTRODUÇÃO Desenvolvedores de software, sobretudo iniciantes, podem optar por desenvolver sistemas de software se preocupando somente com que os mesmos sejam funcionais, deixando sua qualidade de lado. Segundo Lehman [1], conforme um software evolui, seu código fonte tende a tornar-se cada vez mais complexo e, de acordo com Yu e Zhou [2], um código fonte com alta complexidade é de difícil compreensão, podendo ocasionar custos elevados para sua manutenção. Uma forma de se medir o quanto complexo é um código fonte, é através da métrica complexidade ciclomática, proposta por McCabe [3]. Essa métrica se baseia na estrutura do fluxo de um código fonte para calcular a quantidade de fluxos de execução (caminhos únicos) presentes. A análise da estrutura do seu fluxo pode ser feita sobre Grafos de Fluxo de Controle (GFC) [4].  Conforme aumenta a complexidade ciclomática de um software e o desenvolvedor não se preocupa em manter sua qualidade, podem surgir condições redundantes no código fonte, que ao serem removidas, não alteram o seu comportamento externo. Este fenômeno, chamado de complexidade ciclomática desnecessária, foi observado por Campos Junior et al. [5] no ambiente acadêmico de ensino de programação, onde se manifestou em 16% de 482 tarefas de programação analisadas. Os autores desenvolveram uma ferramenta capaz de identificar complexidade ciclomática desnecessária e exibir, através do uso de GFCs, uma sugestão de reestruturação do código fonte analisado. Com base nessa sugestão, o desenvolvedor pode corrigir manualmente seu código fonte.  A interpretação dos GFCs pelo usuário e a correção manual do código fonte, são limitações que podem ser eliminadas para que o desenvolvedor despenda menos esforço ao melhorar a qualidade de seu software. Dessa forma, o presente trabalho tem como objetivo a evolução da ferramenta desenvolvida por Campos Junior et al. [5], através da adição de uma nova abordagem para automatizar a refatoração do código fonte, no intuito de eliminar a necessidade de se fazer a sua correção manual e também a de se interpretar os GFCs.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SAST, September 18–19, 2017, Fortaleza, Brazil  © 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.  ACM ISBN 978-1-4503-5302-1/17/09…$15.00  https://doi.org/10.1145/3128473.3128476', 'O trabalho segue estruturado da seguinte forma: Na Seção 2, é apresentado o referencial teórico, explicando conceitos sobre a abordagem já existente para identificação da complexidade desnecessária e trabalhos relacionados. Na Seção 3, é descrita a nova abordagem para automatizar a refatoração de código fonte que apresenta complexidade desnecessária. Um cenário de uso é apresentado na Seção 4, visando demonstrar o uso da abordagem proposta. Dois estudos experimentais são apresentados na Seção 5 e, por fim, na Seção 6, são apresentadas as considerações finais.  2. REFERENCIAL TEÓRICO E TRABALHOS RELACIONADOS Campos Junior et al. [6, 7], apresentam uma ferramenta que auxilia na criação de testes de unidade de software em linguagem de programação Java, a partir de análise estática do código fonte. Nela é gerado um GFC correspondente a cada método do código fonte analisado, sobre o qual é possível visualizar caminhos independentes do grafo. Estes caminhos podem ser usados pelo usuário na geração dos casos de teste de unidade para ser obtida a cobertura sobre as instruções condicionais existentes no método em questão. Uma instrução condicional é formada por expressões simples (binária ou unária) ou por expressões compostas. Expressão binária é aquela que possui um operador relacional que compara dois operandos (ex.:(valor < 100)). Já a expressão unária possui apenas um operando e, opcionalmente à sua esquerda um operador lógico de negação (!) (ex.:(!maiorIdade)). Expressões compostas são formadas de expressões simples que são combinadas por operadores lógicos de conjunção (&&) ou disjunção (||) (ex.:((valor < 100) && (!maiorIdade))), e podem ser simplificadas em expressões simples. O exemplo anterior da expressão composta pode ser decomposto nos respectivos exemplos anteriores de expressões simples. De acordo com Campos Junior et al. [5], expressões binárias são normalizadas na abordagem automatizada de identificação da complexidade desnecessária para formarem um padrão em que o operando da esquerda seja sempre uma variável e o operando da direita seja sempre uma constante, obedecendo assim à notação Variável / Operador / Constante. Caso não haja constantes nas condições, a abordagem não pode ser executada, uma vez que a mesma não considera que as variáveis analisadas mudem de valor ao longo da execução da abordagem. Instruções condicionais com variáveis de tipo numérico são avaliadas como verdadeiras quando o operando de valor variável assume valores que pertençam a um intervalo numérico delimitado. Esse intervalo é definido por dois valores numéricos que representam os limites mínimo e máximo que a variável pode assumir na condição em que se encontra. Os limites são definidos pelo operador relacional e pelo operando de valor constante presentes nessa mesma condição. Dependendo do operador presente, pode acontecer de um desses limites ser um valor infinito, que caso seja o valor da esquerda do intervalo, representa o menor valor possível para o tipo da variável numérica, e caso seja o valor da direita do intervalo, representa o maior valor possível para o tipo da variável numérica. Esses valores infinitos', 'são definidos conforme limitações de linguagem de programação e arquitetura do processador. Como exemplo, na Tabela 1 são apresentadas expressões binárias de condições com variável (n) de tipo numérico no operando de valor variável da expressão, que assume (de acordo com o operador relacional) valores maiores, menores, iguais ou diferentes do valor numérico (10) expresso no operando de valor constante, para que essa condição seja verdadeira. A condição (n < 10), por exemplo, é considerada como verdadeira se (n) for um valor numérico que esteja entre o menor valor possível (–infinito) para seu tipo, e o valor (10). Isso é representado por um intervalo aberto com início em (–infinito) e fim em (10). Tabela 1. Exemplo de definição dos intervalos para variáveis de tipo numérico. Expressão Intervalo definido (n < 10) ] –∞; +10 [ (n <= 10) ] –∞; +10 ] (n > 10) ] +10; +∞ [ (n >= 10) [ +10; +∞ [ (n == 10) [ +10; +10 ] (n != 10) ] –∞; +10 [ U ] +10; +∞ [ 2.1 Complexidade Ciclomática Desnecessária Vértices do GFC que possuem pelo menos uma aresta ligando diretamente um ao outro, e seguem padrões definidos em Campos Junior et al. [5], são agrupados em estruturas denominadas clusters de condições. Cada cluster forma um intervalo composto pela união dos intervalos formados por cada condição abrangida. Esse intervalo, ao ser analisado permite com que a complexidade ciclomática desnecessária seja identificada quando ele abrange todo o domínio do tipo da variável sendo usada, significando então que a última condição do cluster pode ser eliminada e substituída por um else.  No exemplo da Listagem 1, os intervalos das condições (var < 0) e (var >= 0), ao serem unidos, formam o intervalo com valor mínimo de (–infinito) e valor máximo de (+infinito), significando na prática que as condições presentes nesse cluster abrangem todo o domínio do tipo da variável presente (var). Essas instruções condicionais no mesmo cluster podem ter sua variável assumindo qualquer valor numérico entre o valor mínimo e o valor máximo para o seu tipo. E dessa forma, a complexidade ciclomática desnecessária é identificada, e esta é eliminada quando a última condição presente no cluster (neste caso, a condição var >= 0) é excluída do código fonte. Listagem 1. Exemplo de código fonte com complexidade ciclomática desnecessária. if (var < 0) {     System.out.println(""Negativo""); } else if (var >= 0) {     System.out.println(""Positivo""); }', 'Figura 1. GFC relativo ao código fonte da Listagem 1. A sugestão para reestruturação do código fonte é exibida através de um segundo GFC (Figura 2) na ferramenta, originado a partir da otimização do primeiro (Figura 1), ao ser eliminada a complexidade ciclomática desnecessária. Na abordagem existente, o desenvolvedor precisava refatorar manualmente o código fonte baseado na interpretação do novo GFC.  O GFC (Figura 2) do exemplo da Listagem 2 é a versão otimizada do GFC (Figura 1) do exemplo da Listagem 1. Listagem 2. Exemplo do código fonte da Listagem 1 sem a complexidade ciclomática desnecessária. if (var < 0) {     System.out.println(""Negativo""); } else {     System.out.println(""Positivo""); }   Figura 2. GFC relativo ao código fonte da Listagem 2. Para apoiar o desenvolvimento de uma abordagem que refatore código fonte eliminando complexidade ciclomática desnecessária, trabalhos e ferramentas com objetivos semelhantes foram buscados na literatura de forma ad-hoc.  Mendonça et al. [8] apresentam um framework para automatizar a refatoração de código fonte através do uso de padrões da linguagem XML. Esse framework serve de arcabouço para o desenvolvimento de ferramentas de refatoração que possam ser usadas em linguagens de programação similares à linguagem Java. Como exemplo, Maruyama e Shinichiro [9] apresentam uma', 'ferramenta que ajuda o desenvolvedor na compreensão de como a refatoração altera seu código fonte, através do uso de GFCs. O processo de refatoração corrige vários tipos de problemas encontrados em códigos fonte, e segundo Balazinska et al. [10], a ocorrência do código duplicado é causada pelo ato de programadores copiando e colando trechos de código fonte. Com isso, uma abordagem foi criada pelos autores para identificar informações detalhadas e relevantes acerca de códigos fonte duplicados em um software e também identificar suas diferenças. Seu objetivo é sugerir ao desenvolvedor, trechos do código fonte que podem ser eliminados por meio da refatoração. A ferramenta desenvolvida pelos autores serve como um complemento aos sistemas de refatoração existentes. Diferentemente da nossa abordagem, a ferramenta desse trabalho não tem por objetivo eliminar instruções condicionais redundantes, mas busca eliminar trechos repetidos de códigos fonte sem alterar o seu comportamento.  Uma consequência da refatoração é a possível diminuição do esforço necessário para criação de testes de software. Dessa forma, Elish e Alshayeb [11] investigam os efeitos da refatoração nos testes de software, onde observam que 3 de 5 métodos de refatoração analisados, podem resultar em maior esforço para criação de testes. Entretanto, nenhum deles tem relação com a diminuição de complexidade desnecessária. A hipótese avaliada neste trabalho é de que a diminuição da complexidade ciclomática resulta em menor esforço para se testar o software. 3. ABORDAGEM DESENVOLVIDA A análise dos intervalos de cada cluster da abordagem proposta por Campos Junior et al. [5] é restrita a valores provenientes de condições com variáveis de tipo numérico (Integer, double, float). Como oportunidade de aprimoramento, neste trabalho é adicionada a possibilidade de reconhecimento de variáveis do tipo não numérico (String, boolean) e também de condições com chamadas de métodos, conforme apresentado na Subseção 3.1. Note que a abordagem apresentada tem como base a linguagem de programação Java. Entretanto, uma vez que é baseada somente em estruturas condicionais, pode ser implementada para outras linguagens de programação.\tA Subseção 3.2 apresenta um processo que automatiza a refatoração do código fonte com complexidade desnecessária. 3.1 Variáveis de Tipo não Numérico Regras específicas são definidas para adaptar os tipos String e boolean ao uso da abordagem com um intervalo numérico. Nessas regras, o fato de o operando de cada um desses tipos não possuir valores numéricos, permite com que se possa utilizar valores fixos para definir os intervalos internamente. Estes intervalos, definidos pelas regras apresentadas nas subseções seguintes, diferentemente do tipo numérico, não representam um conjunto delimitado de valores que tornam a condição verdadeira. Apenas representam o valor (true ou false) análogo ao operador presente na expressão.  3.1.1 Tipo String. Na linguagem Java, a utilização de expressões binárias para comparação de Strings é considerada uma má prática. O ideal é a utilização de métodos tais como', '“equals()” ou “equalsIgnoreCase()” em uma expressão unária. Uma expressão unária com operando desse tipo tem seu intervalo definido com base na aparição do operador lógico de negação (!). A ausência desse operador na expressão é representada por um intervalo fechado com início em (–1) e fim em (+1). Outra expressão que faz parte do mesmo cluster que a anterior e com a presença desse operador, é representada por um intervalo aberto de uma união entre dois intervalos, cujo primeiro tem início em (–infinito) e fim em (–1), e o outro tem início em (+1) e fim em (+infinito). A Tabela 2 apresenta exemplos de expressões com uma variável (p) de tipo String que se diferenciam pela presença do operador (!), formando assim os intervalos pré-definidos.  3.1.2 Tipo Boolean. Expressões unárias com operando do tipo boolean tem seu intervalo definido da mesma forma que o do tipo String. Porém o uso desse tipo em uma expressão binária admite apenas dois valores: true ou false. Os operadores lógicos (==) e (!=) são análogos a esses valores do operando. Com isso, o intervalo é definido através da combinação do valor do operando com o do operador. Em uma expressão com operando de valor igual ao do operador, é usado o intervalo fechado com início em (–1) e fim em (+1). Outra expressão que faz parte do mesmo cluster que a anterior, e com operando de valor oposto ao do operador, é usado o intervalo aberto de uma união entre dois intervalos, cujo primeiro tem início em (–infinito) e fim em (–1), e o outro tem início em (+1) e fim em (+infinito). Um exemplo de uso dessa regra está na Tabela 2, com expressões de uma variável (b) do tipo boolean, e que se diferenciam pela combinação do operador com o valor do operando constante.  3.1.3 Método em expressão binária. Expressões binárias que possuam chamadas de métodos em seu operando variável, usam o intervalo respectivo ao tipo de retorno do método (numérico ou não). Na Tabela 2, é demonstrado um exemplo de expressão com o método getNumero(), retornando um valor de tipo numérico. Tabela 2. Definição de intervalos para tipos de variáveis não numéricas. Tipo Expressão Intervalo definido String (p.equals(""valor"")) [ –1; +1 ] (!p.equals(""valor"")) ] –∞; –1 [ U ] +1; +∞ [ Boolean (b == true) ou  (b != false) [ –1; +1 ] (b != true) ou  (b == false) ] –∞; –1 [ U ] +1; +∞ [ Método (getNumero() < 10) ] –∞; +10 [ 3.2 Abordagem Automatizada para Refatorar Código Fonte O processo de eliminação da complexidade ciclomática desnecessária dependia do usuário para interpretar os GFCs e corrigir manualmente o código fonte. Uma nova abordagem foi desenvolvida neste trabalho, com o intuito de permitir com que a ferramenta proponha ao desenvolvedor uma sugestão direta de', 'refatoração do código fonte analisado. Isso é feito através da comparação dos dois GFCs, o original e a sua versão otimizada, resultando assim em uma possível diminuição do esforço do desenvolvedor para refatorar o código fonte. O processo desta abordagem é apresentado por um fluxograma na Figura 3 e é descrito em três passos nas subseções seguintes.  3.2.1 Passo 1 – Vértices excluídos. A abordagem de identificação da complexidade desnecessária é executada, gerando dois GFCs, um representando o código fonte original e um otimizado, a partir de agora chamado de refatorado, eliminando a complexidade desnecessária. São feitas listagens de todos os vértices do GFC original (GFCO) e do GFC refatorado (GFCR), a fim de que sejam determinados quais vértices foram excluídos do GFCO para formar o GFCR. Os vértices identificados são inseridos em uma lista denominada arranjo. Um laço de repetição é iniciado com a leitura do primeiro vértice do arranjo de vértices excluídos, cuja instrução condicional é denominada condição atual. 3.2.2 Passo 2 – Expressão composta. Ao iniciar esse laço, é verificado primeiro se a condição atual se encontra no interior de uma expressão composta (com operadores lógicos de conjunção (&&) ou de disjunção (||)), e caso esteja, é feita a decomposição para o formato de expressão simples (binária ou unária), separando a condição atual das outras condições, a fim de preparar o código fonte para a eliminação da condição atual. A Listagem 3 mostra dois exemplos de expressões compostas sendo transformadas em expressões simples, para que no próximo passo sejam refatoradas ao ser eliminado o trecho com a condição atual.  Listagem 3. Decomposição de expressões compostas (esquerda) em expressões binárias simples (direita).\t if (a >= 0 && a < 10) {    System.out.println(""A""); }  if (a >= 0){     if (a < 10) {     System.out.println(""A"");     } }   if (a < 0 || a >= 10) {    System.out.println(""B""); }   if (a < 0) {     System.out.println(""B""); } else if (a >= 10) {     System.out.println(""B""); } 3.2.3 Passo 3 – Condição inalcançável ou redundante. Em seguida, é procurado no GFCR, um vértice representando uma condição que, através de uma aresta, se liga diretamente com o vértice da estrutura interna da condição atual. Caso não haja algum vértice condicional com essas características, considera-se então que a condição atual é inalcançável, sendo esta eliminada do texto do código fonte junto com a sua estrutura interna através da refatoração. Na Listagem 4.1 e Figura 4.1, a condição (a >= 30) foi considerada como uma condição inalcançável porque não foi encontrado nenhum vértice ligando com a sua estrutura interna, e assim, tanto a condição atual quanto a estrutura interna foram eliminadas. O resultado é exibido na Listagem 4.2 e Figura 4.2.', 'Figura 3. Fluxograma do processo de refatoração do código fonte.  Listagem 4.1. Exemplo com uma condição inalcançável. if (a < 20) {     System.out.println(""A""); } else if (a >= 20) {     System.out.println(""B""); } else if (a >= 30) {     System.out.println(""C""); }  Listagem 4.2. Exemplo sem a condição inalcançável. if (a < 20) {     System.out.println(""A""); } else {     System.out.println(""B""); }    Figura 4.1. GFC original relativo ao código fonte da Listagem 4.1.', 'Figura 4.2. GFC refatorado relativo ao código fonte da Listagem 4.2. Caso o vértice condicional com as características tenha sido encontrado, então a condição atual é considerada como redundante. O laço atual continua e esse mesmo vértice é buscado na lista do GFCO para que seja feita uma comparação entre ele e o seu equivalente da lista do GFCR, a fim de verificar se este possuía um else em sua estrutura. Se esse vértice na primeira lista não possui um else e na segunda lista possui um else, faz-se então a refatoração do código fonte excluindo o trecho correspondente à condição atual e adicionando em seu lugar um comando else. Caso contrário, deve-se apenas excluir a condição atual. Quando a condição atual é considerada como redundante, a sua estrutura interna permanece preservada. No exemplo da Listagem 4.1, a condição (a >= 20) era redundante. A Listagem 5 e Figura 5 mostram um exemplo de código fonte com condição redundante sem else, enquanto que a Listagem 6 e Figura 6 mostram um exemplo de condição redundante com else. A Listagem 7 e Figura 7 mostram a versão refatorada desses exemplos.  Listagem 5. Exemplo de condição redundante sem else.\tif (a < 40) {    System.out.println(""A""); }  if (a >= 40) {    System.out.println(""B""); }   Figura 5. GFC relativo ao código fonte da Listagem 5.  Listagem 6. Exemplo de condição redundante com else.\tif (a < 40) {    System.out.println(""A""); } else if (a >= 40) {    System.out.println(""B""); }   Figura 6. GFC relativo ao código fonte da Listagem 6. Listagem 7. Exemplo sem condição redundante.\tif (a < 40) {     System.out.println(""A""); } else {     System.out.println(""B""); }   Figura 7. GFC refatorado relativo ao código fonte da Listagem 7. 3.2.4 Finalização. Após os passos apresentados, o laço atual é encerrado para que se inicie um novo, caso ainda reste algum vértice condicional a ser lido do arranjo. O processo é encerrado quando todos os vértices do arranjo são lidos e assim, a sugestão de refatoração do código fonte lido é retornada à tela do usuário. 4. CENÁRIO DE USO Nesta seção, um cenário hipotético é apresentado com o objetivo de exemplificar o uso da abordagem desenvolvida.  Suponha que um desenvolvedor precise desenvolver um método com um algoritmo para calcular o Índice de Massa', 'Corporal (IMC) de uma pessoa através da fórmula IMC = !""#$(!""#$%!)!. O método deve receber como parâmetros o peso e a altura da pessoa, ambos de tipo float. O resultado correspondente a cada valor calculado é mostrado na Tabela 3. Seguindo o enunciado, o desenvolvedor então desenvolve o código fonte conforme a Listagem 8. Tabela 3. Regras para classificação do IMC. Mensagem IMC  Abaixo do peso IMC < 18.5 Peso normal 18.5 <= IMC <= 24.9 Pré-obesidade 24.9 < IMC <= 29.9 Obesidade 29.9 < IMC <= 34.9 Obesidade mórbida IMC > 34.9 Listagem 8. Código fonte desenvolvido conforme a tarefa apresentada. public String classificaIMC (float peso, float altura) {     float imc = peso / (altura * altura);     if (imc < 18.5) {         return ""Abaixo do peso"";     } else if (18.5 <= imc && imc <= 24.9) {         return ""Peso normal"";     } else if (24.9 < imc && imc <= 29.9) {         return ""Pré-obesidade"";     } else if (29.9 < imc && imc <= 34.9) {         return ""Obesidade"";     } else if (imc > 34.9) {         return ""Obesidade mórbida"";     }     return """"; } Com a tarefa concluída, o desenvolvedor decide usar a ferramenta para apoiar a construção dos testes de unidade. Para isso, carrega na ferramenta que implementa a abordagem, o arquivo que contém o código fonte a ser analisado. A ferramenta então apresenta o seu GFC, conforme a Figura 8, e na aba Source Code é visualizado o código fonte lido, conforme a Figura 9.  Na aba Analysis, é informado ao usuário que o código fonte lido possui uma complexidade ciclomática de valor 9, sugerindo assim que devem ser criados até 9 casos de teste de unidade para cobrir todos os vértices do GFC. Esse valor é também referente à quantidade de caminhos únicos desse código fonte, onde cada caminho único é destacado em azul no GFC. Como exemplo, um dos caminhos é mostrado na Figura 8. Cada caminho único do programa representa um caso de teste de unidade a ser construído para se obter cobertura de todos os nós desse método. O critério de teste todos-nós garante que todos os nós do GFC são visitados pelo menos uma vez durante a execução dos testes de unidade.  É informado também que o código fonte apresenta complexidade ciclomática desnecessária, e é sugerida uma complexidade ciclomática ideal de valor 5 para esse código fonte.  O GFC refatorado é exibido conforme a Figura 10 e a sugestão de alteração do código fonte, baseada no GFC refatorado é exibida na aba Refactored Code, conforme Figura 11.   Figura 8. GFC referente ao método classificaIMC().   Figura 9. Exibição do código fonte analisado.   Figura 10. GFC refatorado do método classificaIMC().', 'Figura 11. Exibição da sugestão de refatoração. O desenvolvedor então analisa a sugestão junto com o GFC refatorado e decide se a aceita ou não. Caso aceite, a sugestão pode ser selecionada e copiada para a área de transferência, a fim de substituir o código fonte original. Neste exemplo, o valor da complexidade ciclomática foi reduzido de 9 para 5, significando na prática que ao invés de 9 casos de testes de unidade, o desenvolvedor vai precisar de no máximo 5 para cobrir todas as condições do método desenvolvido, com as condições redundantes já ausentes. Além disso, caso precise alterar o código fonte no futuro, terá que despender menor esforço, uma vez que o mesmo será menos complexo. 5. ESTUDO EXPERIMENTAL Com o objetivo de avaliar as funcionalidades providas pela ferramenta desenvolvida, foi executado um estudo experimental com acadêmicos do curso de Bacharelado em Sistemas de Informação do IF Sudeste MG – Campus Juiz de Fora. O estudo foi replicado em duas turmas, uma de sétimo período, denominado experimento 1 e outra de quinto, denominado experimento 2. Os participantes do experimento 1 já haviam estudado sobre testes de software, enquanto os do experimento 2 aprenderam o conteúdo durante treinamento ministrado antes do experimento. De acordo com o método GQM [12], o objetivo do experimento foi definido como analisar a ferramenta desenvolvida, com o objetivo de avaliar suas funcionalidades em relação a suporte na criação de casos de teste de unidade e remoção da complexidade ciclomática desnecessária, do ponto de vista de pesquisadores no contexto de aprendizes de programação. Dessa forma, como questões de pesquisa é investigado se 1) o uso da ferramenta proposta impacta no esforço para se criar testes de unidade para um método e 2) o uso da ferramenta proposta resulta em código fonte com menor complexidade ciclomática (CC). Para medir o esforço para se criar casos de teste de unidade, mediu-se o tempo de desenvolvimento dos casos de testes de unidade, até se atingir 100% de cobertura das instruções.  No total, 27 pessoas diferentes participaram dos experimentos, sendo 10 no experimento 1 e 17 no experimento 2. Os participantes foram divididos em dois grupos, onde um grupo  poderia utilizar a ferramenta desenvolvida como apoio e o outro grupo, de controle, não poderia.  Foram dadas duas tarefas para os participantes. A primeira tarefa consiste no desenvolvimento de um método em linguagem Java, igual ao apresentado na Seção 4 deste trabalho. Finalizada esta tarefa, os participantes deveriam construir casos de teste de unidade para o método desenvolvido, até que fosse atingido 100% de cobertura das instruções do mesmo. Foram utilizados a IDE Eclipse para o desenvolvimento das atividades, o plug-in Eclemma para medição da cobertura dos testes de unidade e a ferramenta apresentada neste trabalho. O design do experimento é de um fator, a ferramenta desenvolvida e dois tratamentos, sendo o seu uso e não uso. O grupo 1 utiliza a ferramenta desenvolvida, enquanto o grupo 2 não utiliza. Os dados coletados no experimento 1 e 2 são exibidos nas Tabelas 4 e 5, respectivamente. Tabela 4. Dados coletados no experimento 1. ID Grupo Tempo (min) CC 1 1 8 5 2 1 25 8 3 1 23 5 4 1 32 7 5 1 32 5 6 2 26 8 7 2 15 8 8 2 7 8 9 2 39 8 10 2 14 9 Tabela 5. Dados coletados no experimento 2. ID Grupo Tempo (min) CC 1 1 30 8 2 1 30 5 3 1 19 8 4 1 15 8 5 1 6 6 6 1 35 6 7 1 31 8 8 1 19 9 9 1 15 9 10 2 6 8 11 2 48 8 12 2 16 8 13 2 19 8 14 2 25 8 15 2 31 8 16 2 41 5 17 2 17 5 5.1 Esforço para desenvolvimento de testes de unidade A primeira hipótese avaliada tem relação com a implicação do uso das abordagens implementadas pela ferramenta desenvolvida. Acreditamos que ao diminuir a complexidade ciclomática de um método, diminui-se também o esforço necessário para se criar casos de teste de unidade para esse método, uma vez que um critério possível de teste é diretamente ligado ao valor da', 'complexidade ciclomática. Dessa forma, a hipótese avaliada é de que o uso da abordagem proposta resulta em menor esforço para se criar casos de teste de unidade para um método. O esforço para se criar casos de teste de unidade foi medido utilizando-se o tempo para se criar casos de teste de unidade até que 100% de cobertura sobre as instruções do método fosse atingida.  Para testar a normalidade da distribuição dos dados coletados nos experimentos, utilizou-se o teste de Shapiro-Wilk, uma vez que a quantidade de participantes é menor que 30. Com p-value > 0,100, verificou-se que as amostras possuem distribuição normal. Testou-se ainda, a homocedasticidade das amostras, utilizando o teste de Levene. Com p-values de 0,41 para o experimento 1 e 0,685 para o experimento 2, verificou-se que as amostras são homocedásticas. Uma vez que as amostras possuem distribuições normais e homocedásticas, pode-se utilizar um teste de hipóteses paramétrico. Utilizou-se o Test T, cujos resultados para os experimentos 1 e 2 são exibidos nas Listagens 9 e 10, respectivamente. Listagem 9. Test T para a hipótese 1 do experimento 1. Two-Sample T-Test and CI: Grupo 1 (com ferramenta); Grupo 2 (sem ferramenta)   Two-sample T for Grupo 1 (com ferramenta) vs Grupo 2 (sem ferramenta)                                           N   Mean  StDev  SE Mean Grupo 1 (com ferramenta)  5  24,00   9,82      4,4 Grupo 2 (sem ferramenta)  5   20,2   12,5      5,6  Difference = µ (Grupo 1 (com ferramenta)) - µ (Grupo 2 (sem ferramenta)) Estimate for difference:  3,80 95% CI for difference:  (-13,03; 20,63) T-Test of difference = 0 (vs ≠): T-Value = 0,53  P-Value = 0,610  DF = 7  Listagem 10. Test T para a hipótese 1 do experimento 2. Two-Sample T-Test and CI: Grupo 1 (com ferramenta); Grupo 2 (sem ferramenta)   Two-sample T for Grupo 1 (com ferramenta) vs Grupo 2 (sem ferramenta)                                            N   Mean  StDev  SE Mean Grupo 1 (com ferramenta)  9  22,22   9,68      3,2 Grupo 2 (sem ferramenta)  8   25,4   13,9      4,9  Difference = µ (Grupo 1 (com ferramenta)) - µ (Grupo 2 (sem ferramenta)) Estimate for difference:  -3,15 95% CI for difference:  (-15,99; 9,69) T-Test of difference = 0 (vs ≠): T-Value = -0,53  P-Value = 0,602  DF = 12  Analisando os resultados obtidos, com p-values de 0,610 e 0,602 para os experimentos 1 e 2, respectivamente, que são maiores que o nível de significância estabelecido de 0,05, não é possível rejeitar a hipótese nula de que as médias são iguais. Assim, a partir dos dados coletados, não é possível dizer que as médias entre os grupos são diferentes e que a abordagem de eliminação da complexidade desnecessária impacta no esforço para criação de casos de teste.  5.2 Eliminação da complexidade ciclomática desnecessária A segunda hipótese considerada é ligada à eliminação da complexidade ciclomática desnecessária, objetivo da abordagem apresentada neste trabalho. Dessa forma, a hipótese é de que o uso da abordagem proposta resulta em código fonte com menor complexidade ciclomática do que quando de seu não uso. Para', 'avaliar essa hipótese, comparou-se as médias de complexidade ciclomática dos métodos desenvolvidos pelos participantes que utilizaram a ferramenta com as dos que não utilizaram.  Assim como na primeira hipótese, utilizou-se o teste de Shapiro-Wilk para testar a normalidade da distribuição das amostras. Com p-values > 0,100, verificou-se que são normais. Ainda, foi testada a homocedasticidade com o teste de Levene, onde foram encontrados p-values de 0,262 e 0,687 para os experimentos 1 e 2, respectivamente, sugerindo que as amostras são homocedásticas. Dessa forma, é possível a utilização de um teste de hipóteses paramétrico. Utilizou-se o Test T, cujos resultados são exibidos nas Listagens 11 e 12, para os experimentos 1 e 2, respectivamente. Listagem 11. Test T para a hipótese 2 do experimento 1. Two-Sample T-Test and CI: Grupo 1 (com ferramenta); Grupo 2 (sem ferramenta)   Two-sample T for Grupo 1 (com ferramenta) vs Grupo 2 (sem ferramenta)                                            N   Mean  StDev  SE Mean Grupo 1 (com ferramenta)  5   6,00   1,41     0,63 Grupo 2 (sem ferramenta)  5  8,200  0,447     0,20  Difference = µ (Grupo 1 (com ferramenta)) - µ (Grupo 2 (sem ferramenta)) Estimate for difference:  -2,200 95% CI for difference:  (-4,042; -0,358) T-Test of difference = 0 (vs ≠): T-Value = -3,32  P-Value = 0,029  DF = 4  Listagem 12. Test T para a hipótese 2 do experimento 2. Two-Sample T-Test and CI: Grupo 1 (com ferramenta); Grupo 2 (sem ferramenta)   Two-sample T for Grupo 1 (com ferramenta) vs Grupo 2 (sem ferramenta)                                            N  Mean  StDev  SE Mean Grupo 1 (com ferramenta)  9  7,44   1,42     0,47 Grupo 2 (sem ferramenta)  8  7,25   1,39     0,49  Difference = µ (Grupo 1 (com ferramenta)) - µ (Grupo 2 (sem ferramenta)) Estimate for difference:  0,194 95% CI for difference:  (-1,270; 1,659) T-Test of difference = 0 (vs ≠): T-Value = 0,28  P-Value = 0,780  DF = 14  Analisando os resultados para o experimento 1, foi encontrado p-value igual a 0,029, que é menor que o nível de significância estabelecido de 0,05. Esse resultado sugere a rejeição da hipótese nula de que as médias dos dois grupos são iguais, e dá indícios para aceitação da hipótese alternativa de que são diferentes. Dessa forma, pode-se dizer que para o experimento 1, o uso da ferramenta proposta impactou na complexidade ciclomática dos métodos desenvolvidos pelos participantes. Ainda, comparando as médias de complexidade ciclomática, é possível observar que os participantes que utilizaram a ferramenta desenvolveram métodos com uma complexidade ciclomática média de 6, enquanto que os que não utilizaram, desenvolveram métodos com complexidade ciclomática média de 8,2. Esse resultado evidencia a aceitação da hipótese avaliada.  No experimento 2, foi encontrado p-value igual a 0,78, que é maior que o nível de significância estabelecido de 0,05. Esse resultado sugere que não existem evidências suficientes para rejeitar a hipótese nula de que os dados são iguais. Assim, para o experimento 2, não é possível dizer que a ferramenta proposta tenha impactado na complexidade ciclomática dos métodos desenvolvidos pelos participantes.', '5.3  Discussão dos resultados Para a primeira hipótese, que diz respeito ao esforço para se criar casos de testes de unidade utilizando e não utilizando a ferramenta proposta, não foram encontradas evidências para sua aceitação. Um possível fator que pode ter influenciado este resultado é a experiência dos participantes. Talvez por não estarem habituados a criar casos de teste de unidade, tenham tido dificuldade na sua criação e isso pode ter impactado no tempo de desenvolvimento dos casos de teste. A segunda hipótese avaliada é referente à diminuição da complexidade ciclomática resultante do uso da abordagem proposta. Após análise dos resultados, foram encontradas evidências que sugerem aceitá-la para o experimento 1, mas não foram encontradas evidências suficientes para aceitá-la no experimento 2. Esse resultado é compatível com experimentos anteriores [5], onde essa hipótese também foi validada com estudantes menos experientes em programação. Durante a execução dos experimentos e análise dos resultados, foi observada uma oportunidade de melhoria da abordagem proposta neste trabalho. A substituição automatizada do código fonte com possibilidade de refatoração poderia ser útil, uma vez que alguns dos participantes não utilizaram a funcionalidade de sugestão de refatoração do código fonte provida pela ferramenta, principalmente pelo fato de ter que copiar e colar a sugestão provida. 6. CONSIDERAÇÕES FINAIS Neste trabalho foi apresentada uma abordagem implementada em uma ferramenta preexistente, cujo objetivo é sugerir para o desenvolvedor uma refatoração do código fonte, visando a eliminação da complexidade ciclomática desnecessária. Ainda, foi feito o desenvolvimento de suporte para tipos de variáveis não tratadas anteriormente, ampliando a possiblidade de identificação da complexidade ciclomática desnecessária. Um cenário de uso hipotético foi apresentado, exemplificando o uso das novas abordagens. Ainda, foram planejados e executados dois estudos experimentais, com o objetivo de avaliar as funcionalidades providas pela ferramenta no contexto de aprendizagem de programação.  Como resultado dos estudos experimentais, foram encontradas evidências que sugerem que o uso da abordagem proposta resulta em código fonte com menor complexidade ciclomática do que quando de seu não uso. Não foram encontradas, entretanto, evidências que sugiram a diminuição do esforço para criação de casos de teste unidade quando os participantes utilizam a abordagem proposta. Estes resultados são limitados ao contexto no qual foram obtidos, necessitando novos estudos para que seja possível generalizá-los.', 'Em trabalhos futuros, pretende-se evoluir a abordagem no sentido de automatizar a substituição do código fonte original pelo código fonte refatorado e integrar a ferramenta desenvolvida com Ambientes de Desenvolvimento Integrado (IDEs) através de plug-ins. Também pretende-se realizar experimentos e estudos de caso com usuários profissionais e com sistemas de software de maior porte. AGRADECIMENTOS Os autores agradecem ao IF Sudeste MG, à FAPEMIG e ao CNPq pelo financiamento deste trabalho. REFERÊNCIAS [1] Lehman, M. M. “Programs, Life Cycles and Laws of Software Evolution”, Proceedings of the IEEE, vol. 68, no. 9, p.1060–1076, Set. 1980. [2] Yu, S. e Zhou, S. 2010. “A Survey on Metric of Software Complexity”. In: IEEE INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT AND ENGINEERING. 2nd, 2010, Chengdu. Proceedings of the 2nd IEEE International Conference on Information Management and Engineering. IEEE, p. 352-356, 2010. [3] McCabe, T. J. “A complexity measure”. In: IEEE Trans. Software Eng. Vol. SE-2, N. 4, p. 308-320, 1976. [4] Allen, F. E. “Control flow analysis”, Proceedings of a symposium on Compiler optimization, Urbana-Champaign, Illinois, p. 1-19, 1970. [5] Campos Junior, H. S.; Martins Filho, L. R. V.; Araújo, M. A. P. “An Approach for Detecting Unnecessary Cyclomatic Complexity on Source Code”. IEEE Latin America Transactions, vol. 14, no. 8, 2016. [6] Campos Junior, H. S., Martins Filho, L. R. V.; Araújo, M. A. P. “Uma ferramenta interativa para visualização de código fonte no apoio à construção de casos de teste de unidade”. In: BRAZILIAN WORKSHOP ON SYSTEMATIC AND AUTOMATED SOFTWARE TESTING, 9th, 2015, Belo Horizonte. Proceedings of the 9th Brazilian Workshop on Systematic and Automated Software Testing. p. 31-40, 2015. [7] Campos Junior, H. S.; Prado, A. F.; Araújo, M. A. P. “Complexity Tool: uma ferramenta para medir complexidade ciclomática de métodos Java”. Revista Multiverso. v. 1, n.1, p. 66-76, 2016. [8] Mendonça, N. C.; Maia, P.H.M.; Fonseca, L. A.; Andrade, R. M. C. “RefaX: A Refactoring Framework Based on XML”, in: Software Maintenance, 2004. Proceedings. 20th IEEE International Conference on. IEEE, p. 147-156. 2004. [9] Maruyama, K. e Shinichiro, Y.  ""Design and implementation of an extensible and modifiable refactoring tool."" Program Comprehension, 2005. IWPC 2005. Proceedings. 13th International Workshop on. IEEE, p. 195-204. 2005. [10] Balazinska, M.; Merlo, E.; Dagenais, M.; Lague, B.; Kontogiannis, K. “Advanced Clone-Analysis to Support ObjectOriented System Refactoring.” Proceedings of Seventh Working Conference on Reverse Engineering (WCRE’00). IEEE, p. 98-107, Nov. 2000. [11] Elish K.O. e Alshayeb, M. ""Investigating the Effects of Refactoring on Software Testing Effort"", Software Engineering Conference, 2009. APSEC\'09. Asia-Pacific. IEEE, p. 29-34. 2009. [12] Basili, V. R. e Rombach, H. D. 1988. The TAME Project: Towards Improvement Oriented Software Environments. IEEE Transactions on Software Engineering. Vol.SE-14, no.6, pp. 758-773.']","['ORIGINAL RESEARCH REFERENCE Magalhães, N. M.; Campos Junior, H. S.; Araújo, M. A. P.; Neves, V. O.; An Automated Refactoring Approach to Remove Unnecessary Complexity in Source Code.   Proceedings of the 2nd Brazilian Symposium on Systematc and Automated Softare  estng. 2017.  htpss::doi.org:10.1145:3128473.3128476                    UNNECESSARY COMPLEXITY REMOVAL This briefin reports scieitfc evideice oi the  use  of  ai  automated  refactoriin approach  to  remove  uiiecessary complexity ii source code. FINDINGS \uf0b7  he fndings presented in this briefng are from an experimental study conducted to evaluate the functonalites of a tool (complexity tool) that  measures  cyclomatc  complexity  from source code, and suggests a refactored version of it to eliminate unnecessary complexity in its structure. \uf0b7 It is believed that less cyclomatc complexity equals less efort to develop unit tests. \uf0b7 In a previous study, 482 students’ programming assignments tere analyzed. 16% of them had unnecessary conditonal expressions and could be  improved  in  terms  of  their  cyclomatc complexity. \uf0b7 A study tas conducted tith subjects to evaluate hot complex (cyclomatc complexity) tas their source code and hot long (in minutes) they tould take to develop unit tests. \uf0b7 Subjects tere  Informaton  Systems students from 4th and 3rd year, those results are shotn in Figures 1 and 2 respectvely. Finure 1:  Graphical results from experimental study applied to the 4th year students. Finure 2:  Graphical results from experimental study applied to the 3rd year students.   \uf0b7 In the context of the study, students of the 4 th year developed the source code tith an average complexity of 6 then using the unnecessary complexity eliminaton tool (Group 1), thile for those tho did not use, they had an average complexity of 8 (Group 2). \uf0b7  hey took, on average, 24 minutes to develop unit test cases then using the complexity tool (Group 1), thile those tho did not use took about 20 minutes (Group 2), on average. \uf0b7 Students of the 3 rd year developed the source code  tith  an  average  complexity  of approximately 7 then using and not using the unnecessary  complexity  eliminaton  tool (Groups 1 and 2). \uf0b7  hey  took,  on  average,  approximately  22 minutes to develop unit test cases then using the  complexity  tool  (Group  1),  and approximately 25 minutes, on average, for those tho did not use (Group 2). \uf0b7  he tool successfully helped 4 th year students in developing source code tithout unnecessary cyclomatc complexity. \uf0b7 Hotever, it tas not possible to confrm that the tool helped decreasing unit tests develop tme. Keywords Cyclomatc Complexity Softare Quality Source Code Refactoring Softare  estng Control Flot Graph Who is this briefin  or? Softare engineering practtoners tho tant to use a tool to remove unnecessary cyclomatc complexity in source code. Where the fidiins come  rom? All fndings of this briefng tere extracted  from  an  experimental study conducted by Magalhães et al.   What is iicluded ii this briefin? Results  obtained  in  the experimental study. What is iot iicluded ii this  briefin? Statstcal  analysis  about  the experimental  study  and functonalites  descripton  of complexity  tool  are  both  not included in this evidence briefng.']","**Title: Automating Code Quality Improvement by Reducing Cyclomatic Complexity**

**Introduction:**
This Evidence Briefing summarizes the findings from a study that explores an automated approach to refactor source code, specifically targeting unnecessary cyclomatic complexity. The goal is to enhance code quality, making it easier to maintain and test, particularly for programming novices who often overlook code quality in favor of functionality.

**Core Findings:**
The study presents a novel approach and tool designed to automate the refactoring process aimed at reducing unnecessary cyclomatic complexity in source code. Key findings include:

1. **Identification of Complexity:** The tool effectively identifies unnecessary cyclomatic complexity in code, which can occur when redundant conditions do not alter the program's external behavior. This complexity can hinder maintainability and increase testing difficulty.

2. **Automated Refactoring Process:** The proposed approach automates the refactoring of code by generating a control flow graph (CFG) that illustrates both the original and optimized versions of the code. This helps developers visualize changes and reduces the manual effort required for refactoring.

3. **Impact on Code Quality:** Experimental studies conducted with programming students demonstrated that the tool could successfully reduce cyclomatic complexity. In one experiment, students using the tool developed methods with an average complexity of 6, compared to 8.2 for those who did not use the tool, indicating a significant reduction in unnecessary complexity.

4. **Effort in Unit Testing:** However, the studies did not provide conclusive evidence that using the tool significantly decreased the effort required to create unit tests. Participants' experiences varied, suggesting that while the tool aids in complexity reduction, its impact on testing effort may depend on user familiarity with testing practices.

5. **Future Improvements:** The authors suggest that further enhancements could automate the replacement of original code with refactored code directly, potentially improving user adoption and efficiency.

**Who is this briefing for?**
This briefing is intended for software developers, educators, and practitioners interested in improving code quality and maintainability, particularly in educational settings where students are learning programming.

**Where the findings come from?**
The findings are based on experimental studies conducted with students from the Instituto Federal do Sudeste de Minas Gerais and the Universidade Federal de Juiz de Fora, as detailed in the paper by Magalhães et al.

**What is included in this briefing?**
This briefing includes insights into the automated refactoring approach, its effectiveness in reducing cyclomatic complexity, and the implications for unit testing effort, along with potential areas for future research.

**To access other evidence briefings on software engineering:**
[http://cin.ufpe.br/eseg/evidence-briefings](http://cin.ufpe.br/eseg/evidence-briefings)

**For additional information about the research group:**
[http://cin.ufpe.br/eseg](http://cin.ufpe.br/eseg)

**Original Research Reference:**
Magalhães, N. M., Campos Junior, H. de S., Araújo, M. A. P., & Neves, V. de O. (2017). An Automated Refactoring Approach to Remove Unnecessary Complexity in Source Code. In Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering (EASE 2017). DOI: [https://doi.org/10.1145/3128473.3128476](https://doi.org/10.1145/3128473.3128476)"
"['How are Conceptual Models used in Industrial Software Development? A Descriptive Survey Harald Störrle QAware GmbH, Aschauer Str. 32, 81549 München, Germany HStorrle@acm.org ABSTRACT Background: There is a controversy about the relevance, role, and utility of models, modeling, and modeling languages in industry. For instance, while some consider UML as the “lingua franca of software engineering” , others claim that “the majority [of industry practitioners] simply do not use UML. ” Objective: We aspire to evolve this debate to differentiate the cir- cumstances of modeling, and the degrees of formality of models. Method: We have conducted an online survey among industry practitioners and asked them how and for what purposes they use models. The raw (anonymized) survey data is published online. Results: We find that models are widely used in industry, and UML is indeed the leading language. Three distinct usage modes of models are reported, the most frequent of which is informal usage for communication and cognition. MDE-style usage is rare, but does occur. Software architects are believed to benefit most from modeling. Conclusions: Our study contrasts and complements existing stud- ies, and offers explanations for some of the seeming contradictions of previous results. There might be cultural differences in modeling usage that are worth exploring in the future. ACM Reference format: Harald Störrle. 2017. How are Conceptual Models used in Industrial Software Development? A Descriptive Survey. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017, 10 pages. https://doi.org/http://dx.doi.org/10.1145/3084226.3084256 1 INTRODUCTION There has been considerable controversy regarding the extent to which industrial software engineering benefits from conceptual modeling. Those in favor maintain that “Model-based approaches [...] hold out the promise of significantly improving the productivity of software developers and the quality of the products they generate” [38, p. 525]. They particularly promote using the Unified Model- ing Language (UML) as the “lingua franca of software engineering” [18, p. v] in the context of the Model Driven Engineering (MDE)1 1There are several formulations of essentially the same paradigm, e. g., Model Driven Architecture (MDA), and Model Driven Development (MDD). We use these terms interchangeably. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. EASE’17, June 15-16, 2017, Karlskrona, Sweden © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4804-1/17/06. . . $15.00 https://doi.org/http://dx.doi.org/10.1145/3084226.3084256 paradigm which, they believe, “has the potential to greatly reduce development time ”, and “will help us develop better software systems faster. ”[6, p. 8]. Furthermore, they claim that “MDA is not a vision of some future [...] it has already proven itself many times over in diverse application domains ” (ibid). Note that this quote is from 2004. However, Mohagheghi and Dehlen famously asked “Where is the proof? ” in their 2008 landmark paper [29]. Even MDA-supporters had to admit that “adoption of this approach has been surprisingly slow” [38, p. 513]. Various contributions have since studied success cases and failure cases of MDA adoption in an attempt to explain ex- actly when and why MDA projects fail or succeed [20–22; 41; 42]). Others have outright denied that UML is used to any degree in', 'cases and failure cases of MDA adoption in an attempt to explain ex- actly when and why MDA projects fail or succeed [20–22; 41; 42]). Others have outright denied that UML is used to any degree in industry [31; 32]. Where UML is used, Petre claims, it is used only informally, and “if models end up merely as documentation, they are of limited value [...]. Consequently, a key premise behind MDD is that programs are automatically generated from their corresponding mod- els” [37, p. 20]. So, in order to assess the status and claim of MDD, it is quite relevant to understand whether conceptual modeling is used in industry, and what languages are actually in use. Thus, our first research question is: RQ 1: Are conceptual modeling languages like UML or BPMN (widely) used in the (software) industry at all? Biased by personal experience, the author believes this is true. If indeed it is, we ask our second research question: RQ 2: When, for what purposes, and by whom are models used in industrial software development? This question may yield a simple enumeration of distinct pur- poses, or we may discover a structure in this set. For instance, Fowler [17] postulates the three model-roles “sketch”, “blueprint”, and “program” with increasing degrees of formality: fully formal models that are system representations, i. e., the kind of models MDD purposes (see quote above), semi-formal models serve to plan or document a system, andinformal models are ephemeral sketches on whiteboards or napkins serve to support conception of and con- versation about systems. While this distinction has a strong intuitive appeal, there is no direct evidence for it. After all, there could be other usage modes, too. Thus our third question is: RQ 3: Are there distinct usage modes for models, and if so, how many can be distinguished? The first three questions may appear trivial, yet there is disagree- ment about them. Answering the first three questions establishes a base line to ask the fourth, decisive question: RQ 4: What is the relative frequency of the usage modes of models? The answer to this question will be indicative of the state of MDE, and can inform tool builders as well as researchers.', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Harald Störrle 2 RESEARCH METHODOLOGY In this section we present the study design, describe the study execution and data analysis, and discuss threats to validity. An overview of the setup and major parameters of this survey is pre- sented in Fig. 1. The live survey is available at http://tinyurl.com/ MU-survey-2014, a dump of the full questionnaire as well as the (anonymized) results are available as [40]. 2.1 Study design Guidelines to create surveys are presented in [23] and [39], which our study conforms to. As our research goal is of primarily descrip- tive nature, a cross-sectional design is most appropriate. Given that Software Engineering is a discipline practiced all over the world, and informed by our hypothesis H1 that there are cultural and regional differences, we concluded that an online survey is the only viable implementation of our survey. Of course, it is also a very cost-effective way of conducting a survey. 2.2 Questionnaire construction Due to space restrictions, we cannot include the complete question- naire in this paper. They can be found in an archival link, however [40]. We designed the questionnaire based on our research ques- tions and evaluated it in two rounds. The first round served to identify and resolve issues in the question wording. To this end, we presented the survey questions to three graduate students and asked them to paraphrase the questions. Whenever discrepancies between interpretations arose, we rephrased or clarified the ques- tions. We also added explanatory texts to all questions as a result. In the second round, we aimed at ensuring the relevance and completeness of the questionnaire. To this end, we presented the questionnaire to two acquainted industry professionals and asked for feedback that was duly processed. As an ongoing control for questionnaire validity, we encouraged participants to also provide meta comments about the questionnaire as such, that is, whether they considered it completeness and meaningful. Participants did actually use these answer fields, but with one exception, no prob- lematic issues were discovered. The questionnaire consists of an instruction and five main parts. In the introduction we explained our research objective, provided instructions, offered to leave a feedback address, and requested explicit consent to participating. In the first section we asked cul- tural background and participants demographic details such as their education, occupation, and current affiliation. In the second part, participants were asked about their professional experience. Based on the self-reported kind of affiliation, participants from academia and industry were asked slightly different questions. The third and fourth parts of the questionnaire constitutes the main part of the survey and investigates experiences with modeling and modeling languages in general (ppart 3) and in an industrial context (part 4, again only presented to people from industry). The fifth and last part of the questionnaire asked the degree of agree- ment or disagreement to a set of opinions, and offered participants with free text fields for adding any other comments or opinions. Participants were encouraged to also reflect on the questionnaire itself, e.g., we asked “Are there any questions that we haven’t asked but that you expected? ”. 2.3 Study execution This study is conceived as a probe into the practice of modeling, and how opinions and experiences differ depending on profes- sional maturity and type of affiliation, in particular industry and academia. Attracting academics and junior practitioners is fairly simple. However, attracting a sizable number of senior and very senior practitioners all over the world is quite a challenge: this pop- ulation is in high demand and very likely does not respond to cold calling or anonymous campaigns on social media. Our recruiting strategy, thus, consisted of three elements.', 'ulation is in high demand and very likely does not respond to cold calling or anonymous campaigns on social media. Our recruiting strategy, thus, consisted of three elements. • First, we used all the conventional advertisement channels like the social media (particularly, Twitter and LinkedIn), included the link to the survey in our email signature, and advertised it on our web page. We also distributed advertisement material at over 15 scientific workshops and conferences like MODELS. • Second, we contacted our personal network in industry with the request to pass on word about the survey. We also asked our students to spread the word at their jobs, and distributed fliers at trade fairs like Cebit and industry- oriented conferences like OOP and EclipseWorld. • Third, we advertised the survey in person when giving more than ten talks at evening functions arranged by in- dustrial special interest groups, regional chapters of the ACM, professional bodies, and similar organizations. This strategy proved to be particularly effective in attracting contributions: spikes in the participant numbers inevitably followed after a presentation. All of these activities were conducted in a 18 month period, from September 2014 to April 2016. We chose Google Forms as a platform to implement our questionnaire. We attracted a total of 96 completed answers (Google Forms does not store incomplete answers). Many of the arguments against online questionnaires do not apply to the audience addressed in this survey (e.g., limited computer literacy, restricted internet access), and we agree that“the advantages of Internet surveys outweigh many of the disadvantages. ” [39, p. 153]. We then copied the data over to a local spreadsheet, and manually separated and regrouped the data columns. The data was then encoded, where necessary (e. g., free-form questions, “other” choices of closed questions), and exported to R [33]. We used the Likert package to analyze the data [10]. No coding occurred for the prose answers. 2.4 Threats to validity Since our data analysis does neither aspire to uncover causal re- lationships nor use inferential statistics, internal validity is of no concern to us [36]. For a survey like this, presenting primarily qual- itative results, the primary concerns are thus external and construct validity. 2.4.1 External validity. Obviously, there are significant threats to external validity, that is, whether the results obtained in this survey allow generalization and replication. First, the population studied in this survey is not necessarily representative for the global community of practitioners in the software industry. First, we have', 'How are Conceptual Models used in Industrial Software Development? EASE’17, June 15-16, 2017, Karlskrona, Sweden ????? ﬁve closed questions two open questions senior practitioners other participants Ques�onnaire Flow Ques�on Types ?? Sampling  - convenience  - snowballing Sampling Important info on Surveys: * sampling (snowball, convenience,  * survey kind (descrip�ve, representa�ve, ...) * self-administered, online, Google Forms * cross-sec�onal, cohort, case control Consent Instruc�on Feedback 0 1 Cultural Background ????? ? 2 Industrial Experience ????? 3 Modeling Experience ???? 4 Modeling in So�ware Developm. ????? ????? ????? 5 Modeling Opinions  ????? ? Respondents Ques�ons Online Survey (descrip�ve, cross sec�onal) Ques�onnaire Answers senior, industry Demographics age, education, experience,  cultural background 38 Modes of Modeling Modeling languages used ways and scenarios of modeling Beneﬁts & Beneﬁciaries Beneﬁts gained from and beneﬁciaries of modeling Opinions on Modeling MDE endorsement, meaning of modeling junior/academia 58 65 Figure 1: The setup and major parameters of the survey reported here: the different types of arrow on the left indicate different sets of questions administered to practitioners (solid blue arrows) and academics (dashed black arrows). The shapes on the right indicate the kinds of answers discussed in Section Section 3. used convenience sampling rather than probability sampling re- sulting in uneven representations of, in particular, Germany and the US. So while it perfectly possible that our findings are an ef- fect of cultural differences (as some people in the community will maintain), we have no proof. Second, with self-administered online questionnaires such as Google Forms, we have no control over the completion rate and so this survey may suffer from response rate bias – only completed forms are saved. Since recruitment depended to a notable degree of the author’s personal appearances, there is also a risk of experimenter bias. In defense of this research, we would like to highlight that this is a general open methodological problem [24, p. 105] and no comparable survey in the area has used probability sampling. Clearly, our study suffers from selection bias in the sense that only those people are likely to answer our survey about modeling that are interested in modeling. Therefore, we can not support a conclusions about IT professionals that do not model. We can, however, claim that there is a significant number of people that do model, and those that do exhibit certain usage patterns. Thus, our conclusions are not compromised by the validity threats explained here. 2.4.2 Construct validity. While this questionnaire was constructed with great care and was piloted repeatedly, there is a risk of bias- ing the participants by the questions and the answer options. For instance, in the list of usage profiles of models, we might have omitted some, or misrepresented others. To mitigate this problem, we asked participants to describe further scenarios in prose. Many participants took advantage of such text fields, and we could indeed identify one usage profile we had omitted. Also, we specifically asked whether participants felt that any im- portant questions were missing. While the respondents consistently offered opinions in other text fields, there was little feedback in this category. The questions that were suggested pointed to aspects beyond the scope of this study, such as tooling, particular modeling languages, and visual vs. textual notations for modeling. Some par- ticipants also suggested to drill deeper into application conditions (system size, availability of formal training), and causes of modeling failures. There was no comment suggesting that the questions were wrong, irrelevant, or biased, though there were two participants that understood “model” in the sense of mathematical model rather than conceptual model in software engineering. We take this as a', 'wrong, irrelevant, or biased, though there were two participants that understood “model” in the sense of mathematical model rather than conceptual model in software engineering. We take this as a confirmation that the questions were fair and balanced. 3 OBSERVATIONS This whole study hinges on attracting the “right” participants: we need to survey a broad spectrum of suitably qualified and experi- enced respondents in order to achieve ecological validity. Thus, we first analyze the demographic structure of the participants in greater than usual depth. As we go along we shall point out potential biases that will have to be considered when deriving conclusions from our observations. 3.1 Participant demographics For the given purpose of this survey, we believe the answers of ex- perienced professionals to be more valuable than those of students, academics, or junior developers, say. Thus, our first goal in recruit- ing was to attract senior people with more overview and insight. Fig. 2 shows the demographic structure of the participants of our survey. Obviously, the vast majority has an education in Computer Science, a significant portion up to the level of PhD. Similarly, a majority of the participants come from industry (49 of 96), and here, most are senior people (38 of 49). The survey also attracted a number of academics, most of them junior (23 of 43). While this is not our targeted audience, it allows us to contrast the opinions held in academia and industry as well as differences between junior and senior people in the area. The gender distribution is largely representative of the field. Most participants are in their 30s, with a self-assessed median modeling proficiency 7 out of 10.', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Harald Störrle PhD MSc BSc CS, Informa�cs, Comp. Engineering Maths, Sciences,  Engineering Other Region Germany India Scandinavia Europe (other) Americas Asia (other) 38 23 20 11 3 Occupa�on Senior IT Professional Junior Scien�st Senior Scien�st Junior IT Professional Other 82 14 Gender male female 40 14 13 17 8 4 same sequence of sub-popula�ons in chart and legend 24 45 13 6 4 21 1 Educa�on 20 30 40 50 60 Age Figure 2: Demographic structure of the participant population: level and area of education, occupation, region, gender, age, and years of professional experience (left to right). Gender is reported for completeness only. We also tried to include practitioners from all over the world, assuming that regional differences reflect cultural differences, which we suspect might have an impact too—this sentiment has been repeatedly expressed by renowned figures in the MDE community (e. g., “people in the US just do it in a less structured way ”, Bran Selic, personal communication). Clearly, the regional distribution of the study participants strongly favors Europe, in particular Germany and, to a lesser degree, Scandinavia. This is clearly a consequence of the recruitment process, and the author’s personal network and affiliation. Unlike previous surveys, however, there is also a sizable number of participants from Asia, mostly from India. About half way through the survey we observed the absence of participants from the UK and North America. We reached out to contacts in those regions, and specifically tried to attract respondents from these regions, but without much success. It is not clear whether this is a weakness of this study or a finding, indicating less model usage in those regions. From the 38 senior respondents, we asked additional details about the kind of projects on which they have worked, see Fig. 3. Given the conventional career path in industry, we expected a clear dominance of managers, consultants, and software architects, but among the respondents of our survey, there is also a large number of developers and testers. As expected, the vast majority of senior professionals work in fairly large commercial corporations, and for even larger client organizations. We have also asked the senior respondents to indicate the size of projects they have witnessed, and the absolute frequency in which these occur. Unsurprisingly, very few participants report having been part of very large projects. Interestingly, though, a fair number of the participants report that they have never or rarely been part of small (less than 10 people) and very small projects (less than 5 people). The results of this survey are thus biased towards large projects. We conclude that overall, the participant population matches our expectations, and allows us to draw conclusions about the opinions held by both junior and senior IT professionals in the software development industry. Note that our analysis exhibits a likely source of bias: large organizations and large projects are known to tend towards more formalized development processes, which may favor the use of modeling over more ad-hoc, non-modeling software development methods. 3.2 Usages modes of modeling We asked participants to indicate in which contexts they use mod- eling, what modeling languages they use, and how they assess their level of modeling expertise (see Fig. 5). Clearly, using models in the context of software development is the dominating usage context (76%)(Question C1), in line with the intended audience of this survey. To a lesser degree (18%), participants stated more domain-oriented contexts, such as business process or enterprise architecture mod- eling. Clearly, the UML is the most used modeling language among the respondents (83 out of 96), with Domain Specific Languages (DSLs) coming in second (31 of 96). Then, we asked for specific usages of models and offered 16', 'the respondents (83 out of 96), with Domain Specific Languages (DSLs) coming in second (31 of 96). Then, we asked for specific usages of models and offered 16 predefined items with a five-point Likert scale each to indicate the frequency of these usages. Despite our best efforts in constructing this questionnaire, there is a risk of biasing the participants by these 16 items. To mitigate this problem, we asked participants to describe further scenarios in prose, and 16 (17%) of them used this option. Seven of these alternate scenarios were minor variations of the categories initially offered, six referred to testing and verification, and three showed a different understanding of “model” altogether. So, we conclude that apart from the usage category “Testing and Verification” the suggested set of model usages is adequate. The results are presented in Fig. 6. The most popular usage scenarios by far (scenarios 1-3) center around communicative and cognitive processes: between 70 and 79% of all participants use models for such activities “often” or “always”, while only between 4 and 8% do this “rarely” or “never”. In contrast, about half the population in our survey used models “rarely” or “never” for generating code or a DSL, while only a quar- ter to a third of the population did this “often” or “always”. Also,', 'How are Conceptual Models used in Industrial Software Development? EASE’17, June 15-16, 2017, Karlskrona, Sweden 10 9 6 4 2 2 Programmer,  Developer,Tester Consultant SWA Line/Project Manager Domain Expert Other Current Position 26 3 3 1 Corporation Agency Freelance Academia Organization Type (provider) Organization size (provider) <10 10..100 101..500 501..5000 >5000 3 6 4 11 9 1 6 2 10 14 Organization size (client) Figure 3: Details about the professional experience of study participants: Current position, type and size of the organization they are affiliated with, and years of professional experience (left to right). Only the results from senior professionals are reported here (n=38), with some participants skipping these questions. 39% 52% 58% 82% 85% 88% 91% 61% 48% 42% 18% 15% 12% 9% 5 - 10 <5 11-25 26-50 51-100 101-500 >500 100 50 0 50 100 Percentage Response Never Once or twice Three to ten times more than 10 times Project Size [people] How often have you experienced projects of the respective sizes? Figure 4: Few participants have witnessed ultra large projects, but many have also only been part in mid-sized or large projects. Only the results from senior professionals are reported here (n=38). 47% 17% 8% 7% 7% 7% U M L DSL ECOR E ERD, IE, etc. BPM N Ma tlab/Sim ulink Language Usage 1 2 5 5 12 12 13 14 31 83 0 20 40 60 80 IDEF proprietary ARIS/EPC SysML BPM N Ma tlab/Sim ulink ERD, IE, etc. ECOR E DSL U M L 0 20 40 60 80 Other EAM,  Do m ain M odeling Business Process Mo deling Databases, On tologies H W /SW -System s Developm ent Softw are D evelopm ent absolute relative 6 3 17 2 3 77 76% 18% 6% Sw -Related Business Related Other Usage Context absolute relative Figure 5: More details about the modeling experience of study participants: Usage context of models in absolute and relative numbers (top), modeling languages used in absolute and relative numbers (bottom). using models for domain- and requirements-oriented tasks (sce- narios 4, 6, 9) is apparently more common than using models for technology-specific tasks (scenarios 5, 7, 10, 11). This is in line with the findings of [31] who concludes that UML is mostly used as a “thought tool” and to facilitate communication with stakeholders. Usage scenario 2 (“Visualize an idea or concept”) is the one that is used at least “sometimes” by most participants (96%). This supports the commonly found sentiment that the visual nature of modeling languages like UML or BPMN is of prime importance. We find it interesting that while 45% answered they use models to document software “often” or “always”, only 20% answered they use models to look up information “often” or “always”. A plausible explanation could be that models are in principle useful to find information about a given system, but may often be out of date or contain not the right level of detail to be of use. It could also mean that it is difficult to find the appropriate model, or relevant information inside a model. All of these phenomena have previously been described by [25]. As we have seen, models are widely used in eliciting knowledge in early life-cycle phases. Models are also mentioned as a versatile tool in “Software Archeology” [19], that is, (re)discovering of knowl- edge in the (late) maintenance stage. Judging by our results, though, this is not a common practice as most participants reported doing this never or rarely (55%), and only a fifth did it often or always. This is consistent with the finding that using models to look up information about a product is a relatively uncommon practice, as mentioned above. The only model-related less common than these two is using models as (part of) contractual documents: only 16%', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Harald Störrle Capture domain knowledge Capture client requirements Capture technical requirements Communicate with clients Negotiate consensus Deﬁne contract Discuss with colleagues Look up product details Visualize and idea or concept Help me think, sketch a thought Design system or code Generate prototype code Generate production code Document a system or code Reconstruct knowledgefrom source code etc. Create a DSL 100 50 0 50 100 Percentage Response Never Rarely Sometimes Often Always C4: For which activities do you use models in your software development activities? 7% 4% 8% 16% 22% 33% 29% 28% 28% 44% 49% 41% 59% 54% 55% 66% 79% 75% 70% 57% 55% 47% 45% 44% 43% 35% 28% 26% 26% 20% 20% 18% 15% 21% 22% 26% 24% 20% 27% 28% 28% 20% 22% 32% 15% 25% 26% 16% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Figure 6: Frequency of model usage scenarios sorted by decreasing frequency. The numbers behind the model usage scenario names are used as shorthands to identify them. Answers from all respondents are reported here. use models this way often or always, while two thirds never or rarely do this. Based on our own experience, we distinguish the following three kinds of models with specific usage scenarios. • Informal models support communication and cognition, utilizing rich information implicit in the situational con- text. • Semi-formal models support design and documentation activities. • Fully formal models are to be taken literal and binding, so as to allow the analysis of system properties, simulation, and generation of code and test cases. Fully formal models can also be used like legal documents such as contracts, or other formalized agreements. Observe that this grouping aligns with Fowler’s distinction of models as sketches, blueprints, and programs [17]. It turns out in terms of these categories, the sub-populations are in agreement. Furthermore, there is a clear priority among the three usage sce- nario types: the more formality is required, the less frequently the usage occurs. This is in line with the results reported previously by Baltes and Diehl [3]. Interestingly, when splitting the answers by sub-populations, we see differences between the usage profiles of senior profession- als and other participants: observe how the relative ranks of the practices shuffle for senior practitioners (see Fig. 7, left) while the overall shape of the graphs stays more or less the same. When looking at the results from all other survey participants sorted by the same activities, differences become visible (see Fig. 7, right). However, these occur almost exclusively within the three major usage modes. That is to say, while the frequency of individual activ- ities varies a bit by population, the overall usage modes do not: it seems, that there is surprisingly little difference between industrial and academic usages of models. 3.3 Benefits and beneficiaries of modeling Next we asked who benefits from modeling (see Fig. 8). We pre- sented seven kinds of stakeholders and roles that are involved in the process of software development and asked how much they benefit from modeling (Q 25). Unsurprisingly, participants believed that Software Architects have the greatest benefit: 91% of the respon- dents believed that Software Architects benefit of from modeling “a lot” or “crucially”, and only one believed that their benefit is “little”. All other roles are ascribed less benefit from modeling, in par- ticular, the percentage asserting “crucial” benefit more than halves from Software Architects to the next category Developer/Tester (18 down from 40). The following categories of Domain Expert and Requirements Analyst get very similar assessments with 17/24 and 16/26 respondents asserting “a lot of” and “crucial” benefit, respec- tively. The next two categories of Project Managers and Clients are again rated very similar. Interestingly, even the last category of End', 'tively. The next two categories of Project Managers and Clients are again rated very similar. Interestingly, even the last category of End Users ere said to gain at least some benefit, according to 39% of the respondents. In order to correct for question bias, we also asked respondents to add other stakeholder that we may not have asked for (Q26), and eight respondents mentioned substantially different roles. Partly, these were technical roles such as UX/UI designers, product man- agers, and technical writers. One respondent said that technical risk analysts have crucial benefit from modeling. The other answers referred to other, less obvious stakeholders, such as the media and regulatory bodies, or roles related to administrative roles bordering on software development such as enterprise modelers, database administrators, and contract managers. All of these roles are not directly inside the core of software development, highlighting the communicative function of models. Or, as one respondent put it “I think modeling is beneficial in terms of knowledge transfer, and since software development is more and more a social spectacle, it’s borderline crucial. ”', 'How are Conceptual Models used in Industrial Software Development? EASE’17, June 15-16, 2017, Karlskrona, Sweden 8% 11% 8% 23% 28% 25% 26% 24% 24% 36% 54% 53% 51% 65% 70% 52% 76% 68% 62% 57% 56% 53% 51% 46% 43% 30% 29% 28% 24% 24% 18% 15% 30% 19% 100 50 0 50 100 Percentage 2% 6% 6% 11% 21% 31% 31% 37% 38% 31% 51% 46% 57% 45% 57% 67% 83% 80% 71% 60% 53% 44% 42% 40% 40% 40% 31% 28% 24% 23% 16% 15% 15% 14% 24% 29% 26% 25% 27% 23% 21% 29% 18% 26% 20% 32% 27% 19% Capture domain knowledge Capture client requirements Capture technical requirements Communicate with clients Negotiate consensus Deﬁne contract Discuss with colleagues Look up product details Visualize and idea or concept Help me think, sketch a thought Design system or code Generate prototype code Generate production code Document a system or code Reconstruct knowledge from source code etc. Create a DSL 100 50 0 50 100 Percentage Response Never Rarely Sometimes Often Always Senior IT professionals (n=38) Other (n=58) 16% 21% 30% 20% 17% 22% 23% 32% 33% 17% 24% 12% 12% 33% 1 3 2 5 6 4 7 8 9 12 11 10 15 16 13 14 Figure 7: Frequency of model usage scenarios split by senior professionals (left) and all other participants (right). 1% 7% 9% 14% 24% 29% 60% 91% 77% 63% 62% 39% 36% 13% 7% 16% 28% 24% 37% 35% 26% De velopers & T esters Software Architects Project Managers Requirements Analysts Domain Expe rts Clients End Users 100 50 0 50 100 P ercentage Response None Little Some A lot Cr ucial Who beneﬁts how much from modeling? Figure 8: Beneficiaries of modeling according to respondents 3.4 Opinions about modeling As the last part of our questionnaire, we asked respondents for their opinions on modeling in general. We first checked the degree of endorsement of seven statements, and asked for additional free-text comments. Regarding the predefined items (see Fig. 9), 97% of the respondents agree that modeling helps them deliver software with higher quality. Much lower, though still large numbers agree that modeling also helps with reducing effort, speeding up delivery, and increased market agility (72%, 66%, 61% agreement, respectively). The opinion item with the most divided response was about the MDE vision of generating complete applications from models. Only half the respondents shared that vision, while 37% disagreed with it. This is a clear indication that modeling as such is doubtlessly useful, but MDE-style code generation is not, at least not today, or not to the same degree as modeling. This resonates with the usage modes reported above where almost all respondents use informal modeling frequently, but formal modeling seems to be used a lot less often. One respondent wrote that “Modeling for code generation can (only) be useful in certain domains, e. g., smaller systems with safety critical applications ”. Another respondent opined that “Modeling for communication between humans and modeling for generating code can not be done using the same models [when considering] large, real world applications. Models for code generation are [...] too detailed, too technical for easy understanding. On the other hand, models for communication require “strong” abstractions which make them inappropriate for code generation. ” The existence of different modes of modeling was highlighted in- dependently by several other respondents with remarks like “’mod- eling’ [...] can range from information sharing at a whiteboard to construction of entire applications using modeling tools and code gen- eration” and “there are no projects without modeling, but there are different kinds of modeling ”. Another respondent wrote “Not in every scenario, I will use a dedicated modeling tool to create something that is a model in the strict sense of the word. For instance, to communicate with my colleagues, I will often draw sketches that might incorporate some modeling notation. ” Another ubiquitous aspect is that of tooling. It is worth stating', 'with my colleagues, I will often draw sketches that might incorporate some modeling notation. ” Another ubiquitous aspect is that of tooling. It is worth stating here that the modeling tools used most commonly in industrial software development are not, as one might think, UML tools like', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Harald Störrle Figure 9: General opinions about modeling: degree of endorsement of a number of opinions MagicDraw or EnterpriseArchitect, but generic drawing tools like PowerPoint and Visio. Respondents expressed dissatisfaction with existing tools, e. g. “the available tools, such as UMLet and Microsoft Visio, are often frustrating since they either don’t have the capability to create every model or are annoying to use (erroneous resize, move, ...). [...] They also do not perform semantic checks, such as duplicate detection, inefficient pattern usage etc. ”. Another respondent wrote “Many “great modeling tools/methods” are only applicable [to] small systems, e.g. student projects or “hello world examples”. ” Finally, “Most modeling tools are either awkward to use, lack key features (such as good code generation) or are too expensive ” and “there is no really good tooling available when it comes to modeling plus requirements engineering plus development plus testing plus bug tracking plus traceability”. Lastly, it appears that there are diverging ideas about the na- ture of modeling. Three participants indicated that they have a fundamentally different understanding of the term “model”: “ In my domain, modeling implies mathematical modeling. ” With a simi- larly global understanding of the term “model”, another participant wrote “programming ought to be considered just a special case of modeling”. One respondent offers a point of view often heard in the modeling community: “modeling will allow software develop- ment to evolve from craftsmanship to a engineering discipline in the traditional way.” 4 RELATED WORK The related work is concerned with three closely related topics: (1) the adoption and usage of UML in industry, (2) modeling as a practice in industry, and (3) the adoption and benefit of MDE in industry. Historically, many people would consider these three topics to be a single one: • If UML is the “lingua franca” of modeling, then (mostly) all modeling is UML modeling. • If MDE is the prime purpose of using models, then (mostly) all modeling is MDE. We believe this is an unhelpful confusion of separate aspects, and we aspire to help tease them apart with the present survey. Since its advent in the early 2000’s, practices like Model Driven Engineering (MDE) or Model Driven Architecture (MDA) have attracted a great deal of academic and industry attention, with several dedicated conference series and journals. There was a stream of industrial success stories like [4; 7; 34] and informal experience reports [2; 5] of individual cases. However, it took some time until researchers asked whether MDE really lived up to its promise with Mohagheghi and Dehlen’s landmark paper [29]. Since then, there has been substantial research into the success factors and application conditions of MDE. With the breadth of the field, most studies focused on specific types of systems and/or specific industries, for instance [ 9; 27] study embedded systems in the automotive industry. Others focus on quasi-experiments to compare MDE with model-based and code-based development [28] or report industrial case studies [1]. Yet others explore industrial applications of particular MDE technologies, such as the multi- case study by Brambilla and Fraternali [ 8], that report repeated industrial applications in different companies over several years. Such studies have been useful in elucidating factors at play, but, of course, lack ecologic validity due to the very narrow application conditions necessarily applied. More generic studies in industry have revealed more factors, and yielded more generalizable results [20–22; 41; 42]. A second stream of related work is concerned with UML as a modeling language and how it is used in industry. This stream goes back to the studies of Dobing & Parsons who demonstrated that different parts of the UML are used for different purposes and by', 'modeling language and how it is used in industry. This stream goes back to the studies of Dobing & Parsons who demonstrated that different parts of the UML are used for different purposes and by different people [14; 15]. These findings have been confirmed and refined by later analyzes of available models and documents, cf. [26; 35]. Probably the only piece of secondary research regarding the UML and its usage in practice is [11], which dates back to 2010. A third stream of related work looks at how modeling actually takes place in industry. Davies at al. studied the usage of various modeling languages (including UML) in Australian Companies and', 'How are Conceptual Models used in Industrial Software Development? EASE’17, June 15-16, 2017, Karlskrona, Sweden already found a wide spectrum of purposes for modeling of which communication between stakeholders was the most important one [12]. Forward and Lethbridge surveyed software professionals for problems and opportunities for MDE as opposed to Code-based software development [ 16]. Chaudron and colleagues surveyed software architects in industry [25; 30] and asked them how they used UML. They found evidence for different degrees of formality, with most using it in a rather loose sense. Later, Baltes and Diehl investigated the usage of informal models in practice [3]. They also concluded that most purposes were related to communication and understanding. Recently, Petre reported that “the majority of those interviewed simply do not use UML, and those who do use it tend to do so selectively and often informally” [31, p. 731], see also [32]. This contrasts our results to some degree: While we also find that the more informal usage modes of models are much more frequent than the more formal usage modes, we do find that UML is widely used. This usage may not be “universal”, but there is a sizable sub-population that uses UML in industry in fully formal usage modes, such as MDE. As one respondent put it: “there are no projects without modeling. But there are different kinds of modeling. ” The differences between Petre’s findings and ours are likely due to three factors. First, we asked for models and modeling where Petre asked for UML. Second, we considered a broad spectrum of users and usages of models where Petre seems to have focused on the very formal usages of UML, in particular, code generation, which our research confirms is the least common usage scenario. Third, there is a different regional distribution of the participants of Petre’s study and ours: Petre seems to have focused on North America and the UK, two regions that are not well represented in our study. On the other hand, we have significant numbers of participants from Germany, Scandinavia, and India. It is possible, that this is the reason for the different findings. In other words, it is possible that there are regional, and indeed cultural differences in software development practices. 5 CONCLUSIONS Based on an online survey with 96 participants from all over the world, we answer our research questions as follows. RQ 1: Are conceptual modeling languages like UML or BPMN (widely) used in the (software) industry at all? We can assert that models are used in industry, at least in some re- gions of the world. UML is, by far, the most used modeling language in the population of our participants. Even if this study allows only limited generalizations, there is no hint that any other language enjoys a similar degree of adoption. RQ 2: When, for what purposes, and by whom are models used in industrial practice? Models are used for a great variety of purposes by diverse stake- holders in the context software development. Interestingly, every single group of stakeholders benefits from modeling, at least to some degree, even End Users are said to benefit from modeling. At the other end of the spectrum, 91% of the respondents believed that Software Architects benefit of from modeling “a lot” or “cru- cially”. Various roles on the fringes of software are mentioned as beneficiaries, too. RQ 3: Are there distinct usage modes for models, and if so, how many can be distinguished? There are three distinct modes of modeling: informal modeling for cognition and communication, semi-formal modeling for plan- ning and documentation, and formal modeling for generation and contracts. This finding aligns with previous accounts, such as the one by Martin Fowler. RQ 4: What is the relative frequency of the usage modes of models? Informal modeling clearly dominates in terms of frequency (used by 70 to 79% of the respondents), followed by semi-formal modeling', 'RQ 4: What is the relative frequency of the usage modes of models? Informal modeling clearly dominates in terms of frequency (used by 70 to 79% of the respondents), followed by semi-formal modeling (43 to 57% of the respondents). However, even formal modeling does occur in relevant quantities—between 18 and 35% of the re- spondents use models in formal ways. So while it is correct to say that modeling in industry is mostly informal, it is wrong to say that formal modeling does not take place in industry. There is no indication that the usage profile in academia is different than that in industry. Our findings align with much of the previous research, but con- trast with Petre’s findings [31; 32]. This raises the question of the validity of the previous studies (as well as the present one), and their conclusions. Obviously, any study like ours suffers from se- lection bias: only those people are likely to answer a survey who are interested in the topic, and have something to say about it. So, people who do not model (and possibly do not like modeling) will likely not have taken part in this survey to begin with. If this questionnaire would have been answered by the participants of Petre’s study, say, our findings would probably have been different. However, the only conclusion that we derive from our observations is an existential one, while Petre’s study seeks to support auniversal statement, which clearly requires much stronger support. It is remarkable, though, that the populations sampled by Petre and us have different regional distribution (Petre is unfortunately a little bit vague on this and other methodological details of her study). This could indicate regional differences in software develop- ment practices, which are likely the result of more general cultural differences, and will possibly also impact other aspects of software development practice. This is a factor that is largely ignored in the software engineering research community today—we believe, how- ever, that this is a very interesting result of our study and merits further investigation. It is important to notice that the biases we discuss above do not compromise our main conclusions, namely, (1) that modeling is indeed used in industry, (2) that UML is the dominating modeling language, (3) that there are several distinct modes of modeling that serve different purposes, and (4) the less formal the usage mode the more frequently it is found in practice. Observe, however, that our study does not determine the absolute degree of adoption of either UML or modeling as a practice in industry. For such a claim, a representative study would have to be conducted. While some readers may our findings trivial and self- evident, we believe that the differences between modeling, UML, and MDE are not sufficiently appreciated, which is highlighted by recent publications such as [31; 32].', 'EASE’17, June 15-16, 2017, Karlskrona, Sweden Harald Störrle REFERENCES [1] Bente Anda, Kai Hansen, Ingolf Gullesen, and Hanne Kristin Thorsen. Experi- ences from introducing UML-based development in a large safety-critical project. Empir. Software Eng. , 11:555–581, 2006. [2] H. Andersson, E. Herzog, G. Johansson, and O. Johansson. Experience from introducing Unified Modeling Language/Systems Modeling Language at Saab Aerosystems. Systems Engineering , 13:369–380, 2010. [3] Sebastian Baltes and Stephan Diehl. Sketches and Diagrams in Practice. In Proc. 22nd ACM SIGSOFT Intl. Symp. Foundations of Software Engineering (FSE) , pages 530–541. ACM, 2014. [4] Ian Barnard. Increase MDD Efficiency by Customizing Tau Automatic Error Checking. Technical report, Telelogic, May 2007. White Paper. [5] M. Belaunde, G. Dupe, and B. Nicolas. France Telecom ROI, Assessment, and Feedback. Technical report, MODELWARE Project, 2006. [6] Grady Booch, Alan Brown, Sridhar Iyengar, James Rumbaugh, and Bran Selic. An MDA Manifesto. MDA Journal, 5:2–9, May 2004. [7] M. Born, J. Hössler, O. Kath, M. Soden, and S. Saito. Significant Productivity Enhancement through Model Driven Techniques: A Success Story. In Proc. 10th IEEE Intl. Conf. Enterprise Distributed Object Computing Conference (EDOC) , pages 367–373. IEEE, 2006. [8] M. Brambilla and P. P. Fraternali. Large-scale model-driven engineering of web user interaction: The WebML and WebRatio experience. Science of Computer Programming, Part B: Special issue on Success Stories in Model Driven Engineering , 11(89):71–84, 2014. [9] M. Broy, S. Kirstan, H. Krcmar, B. Schätz, and J. Zimmermann.What is the Benefit of a Model-Based Design of Embedded Software Systems in the Car Industry? , pages 410–443. IGI Global, 2011. [10] Jason Bryer and Kimberly Speerschneider. likert: Analysis and Visualization Likert Items , 2016. R package version 1.3.5. [11] D. Budgen, A. J. Burn, O.P. Brereton, B.A. Kitchenham, and R. Pretorius. Empirical Evidence about the UML: a systematic literature review. Journal of Software: Practice and Experience , 41:363–392, April 2011. [12] Islay Davies, Peter Green, Michael Rosemann, Marta Indulska, and Stan Gallo. How do practitioners use conceptual modeling in practice? Data & Knowledge Engineering, 58:358–380, 2006. [13] Jürgen et al. Dingel. 17th Intl. Conf. Model Driven Engineering Languages and Systems (MoDELS’14). In Jürgen et al. Dingel, editor, 17th Intl. Conf. Model Driven Engineering Languages and Systems (MoDELS’14) , number 8767 in LNCS. Springer, 2014. [14] Brian Dobing and Jeffrey Parsons. How UML is used.Comm. ACM, 49(5):109–113, 2006. [15] Brian Dobing and Jeffrey Parsons. Dimensions of UML Diagram Use: Practitioner Survey and Research Agenda. Principle advancements in database management technologies: new applications and frameworks , pages 271–290, 2010. [16] Andrew Forward and Timothy Lethbridge. Problems and Opportunities for Model-Centric Versus Code-Centric Software Development: A Survey of Soft- ware Professionals. In Proc. Intl. Ws. Models in Software Engineering (MiSE) , pages 27–32, 2008. [17] Martin Fowler. UML Distilled . Addison-Wesley, 3rd edition, 2003. [18] Robert France and Bernhard Rumpe, editors.Proc. 2nd Intl. Conf. Unified Modeling Language (UML) . Number 1723 in LNCS. Springer, 1999. [19] Andy Hunt and David Thomas. Software Archaeology. IEEE Software , 19(2):20– 22, 2002. [20] John Hutchinson, Mark Rouncefield, and Jon Whittle. Model-Driven Engineering Practices in Industry. In Proc. Intl. Conf. Software Engineering (ICSE) , pages 633– 642. IEEE Press, 2011. [21] John Hutchinson, Jon Whittle, and Mark Rouncefield. Empirical Assessment of MDE in Industry. In Proc. Intl. Conf. Software Engineering (ICSE) , pages 471–480. IEEE Press, 2011. [22] John Hutchinson, Jon Whittle, and Mark Rouncefield. Model-driven engineering practices in industry: Social, organizational and managerial factors that lead to', 'IEEE Press, 2011. [22] John Hutchinson, Jon Whittle, and Mark Rouncefield. Model-driven engineering practices in industry: Social, organizational and managerial factors that lead to success or failure. Science of Computer Programming , 89:144–161, 2014. [23] Barbara A. Kitchenham and Shari L. Pfleeger. Personal Opinion Surveys. In Forrest Shull, Singer Janice, and I. K. Sjøberg, Dag, editors, Guide to Advanced Empirical Software Engineering , chapter 3, pages 63–92. Springer, 2008. [24] Robert Kraut, Judith Olson, Mahzarin R. Banaji, Amy Bruckman, Jeffrey Cohen, and Mick Couper. Psychological research online: Report of Board of Scientific AffairsŠ Advisory Group on the conduct of research on the Internet. American Psychologist, 59:105–117, 2004. [25] Christian F.J. Lange, Michel R.V. Chaudron, and Johan Musken. In Practice: UML Software Architecture and Design Description. IEEE Software, pages 40–47, March/April 2006. [26] P. Langer, T. Mayerhofer, M. Wimmer, and G. Kappel. On the Usage of UML: Initial Results of Analyzing Open UML Models. In H.-G. Fill, D. Karagiannis, and U. Reimer, editors,Proc. Modellierung, pages 289–304. Gesellschaft für Informatik, 2014. [27] Grischa Liebel, Nadja Marko, Matthias Tichy, Andrea Leitner, and Jörgen Hans- son. Assessing the State-of-Practice of Model-Based Engineering in the Embed- ded Systems Domain. In Dingel [13], pages 166–182. [28] Yulkeidi Martínez, Cristina Cachero, and Santiago Meliá. MDD vs. traditional software development: A practitionerŠs subjective perspective. Information and Software Technology, 55:189–200, FEB 2012. [29] Parastoo Mohagheghi and Vegard Dehlen. Where is the proof?-A review of experiences from applying MDE in industry. In Proc. Eur. Conf. Model Driven Architecture–Foundations and Applications , pages 432–443. Springer, 2008. [30] A. Nugroho and M.R.V. Chaudron. A Survey into the Rigor of UML Use and its Perceived Impact on Quality and Productivity. In Dieter Rombach, Sebastian Elbaum, and Jürgen Münch, editors, Proc. 2nd ACM/IEEE Intl. Symp. Empirical SE and Measurement (ESEM) , 2008. [31] Marian Petre. UML in Practice. In Proc. Intl. Conf. Software Engineering (ICSE) , pages 722–731. IEEE Press, 2013. [32] Marian Petre. “No shit” or “Oh, shit!”: responses to observations on the use of UML in professional practice. Software & Systems Modeling , 13(4):1225–1235, 2014. [33] R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2011. [34] G. Bhaskar Rao and Keerthi Timmaraju. Model-Driven Development of HA Net- work elements: Experience Summary with UML 2.0. Technical report, Telelogic, October 2005. White Paper. [35] G. Reggio, M. Leotta, and F. Ricca. Who Knows/Uses What of the UML: A Personal Opinion Survey. In Dingel [13], pages 149–165. [36] Per Runeson, Martin Höst, Austen Rainer, and Björn Regnell.Case Study Research in Software Engineering . Wiley, 2012. [37] Bran Selic. The pragmatics of Model-Driven Development. IEEE Software , 20(5):19–25, September/October 2003. [38] Bran Selic. What will it take? A view on adoption of model-based methods in practice. Software & Systems Modeling , 11:513–526, 2012. [39] Shaughnessy, J. and Zechmeister, E. and Zechmeister, J. Research methods in psychology. McGraw Hill, 9th ed. edition, 2011. [40] Harald Störrle. Survey questionnaire and resulting raw data for the DMP4-study, 2017. available at https://figshare.com/s/86d797cc3b816e972f1b, DOI:10.6084/m9. figshare.4810600. [41] J. Whittle, J. Hutchinson, and M. Rouncefield. The State of Practice in Model- Driven Engineering. IEEE Softw., 31(3):79–85, May 2014. [42] J. Whittle, J. Hutchinson, M. Rouncefield, H. Burden, and R. Heldal. Industrial Adoption of Model-Driven Engineering: Are the Tools Really the Problem? In A. Moreira and others, editor,16th Intl. Conf. Model Driven Engineering Languages and Systems (MoDELS’13) , number 8107 in LNCS, pages 1–17. Springer, 2013.']","['ORIGINAL RESEARCH REFERENCE Harald Störrle How are Conceptual Models used in Industrial Sofware Development? A Descriptve Survey.   Proc. Intl. Conf. Evaluaton and Assessments in Softare Engineering, 2017, ACM, DOI: 10.1145/3084226.3084256 USAGE OF CONCEPTUAL MODELS IN INDUSTRY This briefin reports scieitfc evideice oi the usane  of  coiceptual  models  for  sof ware developmeit purposes ii iidustry. Ii partcular, FINDINGS Contrary  to  popular  opinions,  the  usage  of conceptual  models  in  the  context  of  industrial softare  development  is  neither  a  self-evident fact of life, nor an academic myth tithout any support in reality.  It turns out, that there are several distnct modes of  modeling  occurring  in  diferent  relatve  fre- quencies,  and  tith  specifc  applicaton  condit- ions. We fnd three such modes, co rresponding to three types of models: \uf0b7 Iiformal  models are  used  very  tidely  to support  communic aton  and  cog nition. Typical  tools  are  thite boards,  pen-and- paper, and ad-hoc media. Models of this kind tend to be very sketchy, though some have a surprisingly  long  lifespan.  Informal  models are strongly ted to the context in thich they arise. \uf0b7 Partally formal models  support design and documentaton  actvites.  In  order  to  stand for itself outside a given situaton al context, such  models  de mand  more  contextual informaton and notatonal precision,  \uf0b7 Fully formal models are intended to be taken literal and binding,  to allot the analysis of system  propertes,  simulaton,  and  gener- aton  of  code  and  test  cases.  Fully  formal models can also be used like legal documents such as con tracts, or other formalized agree - ments. While  informal  models  are  used  most  by  far, partally formal models are stll fairly frequently used, and even fully formal models are used in signifcant amounts among respondents. Model Type Never Or rarely Some tmes Ofei or Always Sketchy/ Iiformal 4-8% 15-22% 70-79% Semi- formal 16-33% 20-28% 43-57% Fully formal 54-66% 16-32% 18-28% There  is  tidespread  agreement  that  softare architects  beneft  most  from  modeling,  but Developers,  Testers,  Domain  Experts  and  Re- quirements Analysists are also frequently men - toned as ben eftng (bby more than 60% of study partcipants). Modelers  agree  that  modeling  is  benefcial  for delivering softare tith higher quality, using less development efort, and re duced delivery tme (b93%,  72%,  and  66%  of  study  partcipants,  re- spectvely).  Any fndings of this survey critcally depend on the populaton partcipatng in it, so te describe the populaton characteristcs in some detail. \uf0b7 Educatoi  level Partcipants  of  our  survey have  an  above-average  level  of  formal educaton: 45% of them have a MSc, another 24%  have  a  PhD  as  their  highest  degree. Among the senior partcipants from industry, te  also  see  32%  and  55%  MSc  and  PhD degrees, respectvely.  \uf0b7 Renioial  Orinii Most  of  the  study  part- cipants are from Europe, in partcular, Ger - many and Scandinavia.  India, the Americas, UK, and China are under-represented. \uf0b7 Geieral Opiiioi  Half the study partcipants share the MDE vision of generatng complete applicatons from models, thile 37% disagree tith this opinion. \uf0b7 Afliatoi  Type  While  this  survey  tas targeted  at  senior  industry  practton ers there  tas  a  large  number  of  junior  prac- ttoners  and  academics  that  also  partci- pated.  There  tere  litle  diferences  among the  populatons,  including  endorsement  of the MDE vision. There may be difer ences by industry.  We  hypothesize,  that  cultural  diferences  may account or the variaton of fndings betteen this and previous studies, at least in part. Keywords Model-based Softare Development Conceptual Modeling Modeling Modes Modeling Benefciaries Who is this briefnn for? Softare engineering practtoners tho tant to make decisions about thy, then, hot to use  conceptual models based on scientfc evidence.', 'Modeling Modes Modeling Benefciaries Who is this briefnn for? Softare engineering practtoners tho tant to make decisions about thy, then, hot to use  conceptual models based on scientfc evidence. Where do the fndinns come from? All fndings of this briefng tere extracted from  the descriptve survey conducted by H. Störrle.  The survey may be accessed online through the  folloting link. What is included in this briefnn? The main fndings of the original survey.  Evidence characteristcs through a brief  descripton about the original survey and the  populaton it covered. What is not included in this briefnn? Additonal informaton not presented in the  original survey. Detailed descriptons about the  studies analised in the original survey. For additonal informaton about QAware: ttt.qatare.de For additonal informaton about the  author: ttt.about.me/stoerrle']","**Title: Understanding the Role of Conceptual Models in Software Development**

**Introduction:**
This Evidence Briefing summarizes findings from a survey conducted by Harald Störrle that explores how conceptual models, particularly the Unified Modeling Language (UML), are utilized in industrial software development. The objective is to clarify the relevance and practical applications of modeling in the industry, addressing the ongoing debate regarding its utility and effectiveness.

**Core Findings:**
1. **Widespread Use of Models:** The survey indicates that conceptual models are widely employed in the software industry, with UML being the predominant modeling language used by practitioners. Approximately 83% of respondents reported using UML.

2. **Usage Modes of Models:** Three distinct modes of modeling usage were identified:
   - **Informal Models:** Primarily used for communication and cognitive processes, these models are the most frequently employed, with 70-79% of respondents using them ""often"" or ""always.""
   - **Semi-Formal Models:** Utilized for planning and documentation, these models are less common, with 43-57% of respondents indicating regular use.
   - **Formal Models:** These models, which are binding and can be used for code generation and contracts, are used by 18-35% of respondents. 

3. **Benefits Across Roles:** Software architects were identified as the primary beneficiaries of modeling, with 91% of respondents believing they gain significant advantages. Other roles, including developers, testers, and even end-users, also reported varying degrees of benefit from modeling practices.

4. **Challenges in Tooling:** Respondents expressed dissatisfaction with existing modeling tools, indicating that generic drawing tools like PowerPoint and Visio are more commonly used than specialized UML tools. This suggests a gap in the availability and effectiveness of modeling tools in the industry.

5. **Cultural Differences:** The survey results hint at potential cultural and regional differences in the adoption and use of modeling practices, particularly between Europe and North America. This aspect warrants further investigation to understand how cultural contexts influence modeling practices.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, software architects, and educators who are interested in understanding the practical implications of modeling in software development and its impact on communication, documentation, and overall project success.

**Where the findings come from:**
The findings are derived from an online survey conducted by Harald Störrle, which gathered responses from 96 industry practitioners over an 18-month period. The survey aimed to capture a broad spectrum of experiences and opinions regarding the use of conceptual models in software development.

**What is included in this briefing?**
This briefing includes an overview of the primary findings related to the usage modes of models, the benefits perceived by various stakeholders, and insights into the challenges associated with modeling tools.

**What is NOT included in this briefing?**
This briefing does not include detailed statistical analyses or raw data from the survey. It also does not cover specific case studies or anecdotal evidence beyond the scope of the survey findings.

**To access other evidence briefings on software engineering:**
[https://dl.acm.org/citation.cfm?doid=3131151.3131160](https://dl.acm.org/citation.cfm?doid=3131151.3131160)

**Original Research Reference:**
Störrle, H. (2017). How are Conceptual Models used in Industrial Software Development? A Descriptive Survey. In Proceedings of EASE’17, Karlskrona, Sweden, June 15-16, 2017. https://doi.org/http://dx.doi.org/10.1145/3084226.3084256"
"['A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform Casimiro Conde Marco Neto Universidade Federal do Estado do Rio de Janeiro Av. Pasteur, 458 – Urca Rio de Janeiro, RJ casimiro.neto@uniriotec.br M´arcio de O. Barros Universidade Federal do Estado do Rio de Janeiro Av. Pasteur, 458 – Urca Rio de Janeiro, RJ marcio.barros@uniriotec.br ABSTRACT Issue tracking systems help so/f_tware development teams in iden- tifying problems to be solved and new features to be added to a so/f_tware system. In this paper, we replicate and extend a study carried out in 2013 on the usage of the issue tracking system provi- ded by the GitHub platform. /T_he replication aims at determining whether the results observed four years ago are still valid. /T_he extension seeks to analyze how o/f_ten issues are terminated by com- mits to the version control system and understand whether this feature allows developers to relate an issue to the source code mo- dules that were changed to resolve it. We conclude that the results of the previous study remain valid and that issues closed by com- mits are uncommon (about 4% of our sample) and o/f_ten linked to technical aspects of the project. CCS CONCEPTS •So/f_tware and its engineering→So/f_tware evolution; KEYWORDS GitHub,issues, issue management, issue tracking systems ACM Reference format: Casimiro Conde Marco Neto and M´arcio de O. Barros. 2017. A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017,10 pages. DOI: 10.1145/3132498.3134110 1 INTRODUC ¸ ˜AO O desenvolvimento de so/f_tware´e uma atividade complexa e ´e co- mum que as partes envolvidas tenham quest ˜oes a serem analisadas e respondidas sobre o so/f_tware em construc ¸˜ao. Essas quest ˜oes s˜ao genericamente chamadas de issues e indicam a existˆencia de defei- tos, problemas, riscos, oportunidades de melhoria, pendˆencias ou d´uvidas que devem ser avaliadas pela equipe de desenvolvimento. Desenvolvedores e usu ´arios do so/f_tware identi/f_icamissues com frequˆencia e s ˜ao encorajados a registr´a-los em um issue tracking system para facilitar a comunicac ¸˜ao entre os grupos respons´aveis pelo desenvolvimento, operac ¸˜ao e uso do sistema [1]. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or aﬃliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SBCARS 2017, Fortaleza, CE, Brazil © 2017 ACM. 978-1-4503-5325-0/17/09. . . $15.00 DOI: 10.1145/3132498.3134110 Os estudos sobre issue tracking systemsanalisam como este tipo de ferramenta pode apoiar a equipe durante o desenvolvimento de um projeto de so/f_tware. O maior estudo sobre o assunto at´e o mo- mento foi realizado em 2013 por BISSYANDE et al. [1]. Os autores identi/f_icaram que poucos projetos de c´odigo aberto usam issue trac- king systemse que estas ferramentas s˜ao geralmente utilizadas por projetos mais antigos, com maior base de c´odigo-fonte, com equipes maiores e com l´ıderes mais populares. Tamb´em identi/f_icaram que n˜ao h´a uma diferenc ¸a clara entre colaboradores que participam em um projeto apenas registrando issues e outros que apenas desen- volvem o c ´odigo-fonte, mas que grande parte dos colaboradores cumprem os dois pap´eis nos projetos. Um t´opico pouco abordado nos estudos anteriores ´e a relac ¸˜ao entre os issues e o c´odigo-fonte envolvido em sua resoluc ¸˜ao. Esta relac ¸˜ao permite evidenciar que alterac ¸˜oes foram realizadas em um projeto para encerrar umissue, possibilitando a rastreabilidade entre os m´odulos de c´odigo-fonte e osissues e serve de base para a criac ¸˜ao de modelos de predic ¸˜ao que permitam identi/f_icar os m´odulos de c´odigo-fonte que precisar˜ao ser alterados para resolver os novos', 'de modelos de predic ¸˜ao que permitam identi/f_icar os m´odulos de c´odigo-fonte que precisar˜ao ser alterados para resolver os novos issues que ser˜ao registrados no futuro. Por outro lado, esta relac ¸˜ao n˜ao ´e identi/f_icada de forma expl´ıcita no GitHub, visto que n˜ao existe uma funcionalidade para relacionar um issue com os trechos do c´odigo-fonte que ele afeta. A funcionalidade de encerramento de issues por commits oferecida pelo issue tracking systemda plata- forma GitHub pode ser utilizada para analisar esta relac ¸˜ao de forma indireta. Atrav´es desta funcionalidade, sempre que uma mensagem com o padr ˜ao “Closed #X”´e encontrada em um commit enviado para o sistema de controle de vers˜oes, a plataforma marca oissue de n´umero X como fechado, indicando que o problema nele descrito foi resolvido pelas alterac ¸˜oes realizadas nos m´odulos de c´odigo-fonte que participam do commit. Em decorrˆencia da ausˆencia de estudos sobre o assunto, n˜ao sabemos se esta funcionalidade ´e utilizada com frequˆencia e para resolver que tipo de issue. O objetivo desse artigo ´e compreender as caracter´ısticas dosis- sues de projetos de so/f_tware e determinar com que frequˆencia estes issues podem ser relacionados com o c´odigo-fonte utilizado para re- solvˆe-los atrav´es o recurso de encerramento porcommit. Para tanto, projetamos um estudo dividido em duas partes e realizado sobre uma amostra de 76.909 projetos resultantes de um /f_iltro aplicado sobre 220.000 projetos da plataforma GitHub. Na primeira parte do estudo foram replicadas as an´alises realizadas por BISSYANDE et al. [1] com o intuito de veri/f_icar se as conclus˜oes do estudo original se mantˆem passados quatro anos desde a sua divulgac ¸˜ao. A replicac ¸˜ao de estudos ´e uma tarefa importante para garantir que os resul- tados encontrados anteriormente continuam observ´aveis, se eles se mant´em idˆenticos ou seu comportamento apresentou alguma', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros mudanc ¸a com os novos dados dispon ´ıveis. Por´em, a replica c ¸˜ao ´e pouco disseminada devido a di/f_iculdades de realizac ¸˜ao: muitos artigos n˜ao apresentam todos os dados necess´arios para que seus es- tudos sejam replicados, muitas replicac ¸˜oes n˜ao chegam a resultados semelhantes aos estudos originais e seus autores tˆem di/f_iculdade para publicar resultados negativos e dissonantes [9]. Na segunda parte do estudo foram introduzidas novas quest˜oes de pesquisa que analisam a rela c ¸˜ao entre issues e o c ´odigo-fonte alterado para resolvˆe-los. Consideramos que um issue est´a relacio- nado aos m´odulos de c´odigo-fonte afetados por um commit quando o issue for encerrado por este commit. Se os issues fossem fre- quentemente encerrados por commits e os mesmos m ´odulos de c´odigo-fonte fossem alterados para tratar issues similares, talvez fosse poss´ıvel usar oscommits para prever os m´odulos de c´odigo- fonte que ser˜ao afetados por novosissues. No entanto, identi/f_icamos que a funcionalidade de encerramento de issues por commits n˜ao ´e amplamente difundida e buscamos caracterizar osissues encerrados desta forma de acordo com os desenvolvedores envolvidos, o uso de r´otulos e seu tempo de correc ¸˜ao. Enquanto a primeira parte do estudo pretende adicionar evid ˆencias atualizadas aos resultados encontrados por BISSYANDE et al. [1], a segunda parte pretende agregar novos conhecimentos e trazer informac ¸˜oes sobre o uso de issue tracking systemse como suas informac ¸˜oes podem ser utilizadas em pesquisas na ´area de Engenharia de So/f_tware. Este artigo est´a organizado em seis sec ¸˜oes, comec ¸ando por esta introduc ¸˜ao. A sec ¸˜ao 2 apresenta uma revis˜ao bibliogr´a/f_ica compre- endendo trabalhos que abordam o uso de issue tracking systems. A sec ¸˜ao 3 apresenta a metodologia utilizada para coleta de dados e os dados utilizados no estudo proposto. A se c ¸˜ao 4 apresenta a an´alise de dados e as conclus˜oes alcanc ¸adas. A sec ¸˜ao 5 consolida os resultados da se c ¸˜ao anterior e os discute de forma conjunta. Finalmente, a sec ¸˜ao 6 apresenta as conclus ˜oes ap´os a an ´alise, as limitac ¸˜oes, ameac ¸as`a validade e propostas para trabalhos futuros. 2 TRABALHOS RELACIONADOS O uso de issue tracking system´e um assunto abordado de diver- sos pontos de vista, principalmente buscando identi/f_icar qual a importˆancia do registro e correc ¸˜ao de issues para o ciclo de vida e sucesso de um projeto ou so/f_tware. Nesta sec ¸˜ao, concentraremos a discuss˜ao nos trabalhos relacionados com a plataforma GitHub. KALLIAMVAKOU et al. [5] descrevem o GitHub como um ambi- ente colaborativo de suporte ao desenvolvimento de c´odigo, cons- tru´ıdo sobre o sistema de controle de vers˜ao GIT. Em maio de 2016 a plataforma possu´ıa mais de 12 milh˜oes de usu´arios e 38 milh˜oes de projetos, sendo assim o maior reposit´orio de c´odigo-fonte online do mundo. A plataforma integra recursos de redes sociais `as funci- onalidades de desenvolvimento, permitindo que desenvolvedores “sigam” outros desenvolvedores ou projetos [6]. O GitHub tem sido alvo de estudos para entender o comportamento dos seus usu´arios e como ele pode in/f_luenciar no sucesso dos projetos. DABBISH et al. [4] identi/f_icam que a transparˆencia no ambiente colaborativo permite que os usu´arios do GitHub conhec ¸am outros desenvolvedores, possibilitando o reconhecimento, aproximac ¸˜ao e feedback frequente. LEE et al. [6] identi/f_icam uma grande in/f_luˆencia dos usu´arios muito seguidos no comportamento dos demais, apon- tando que as funcionalidades sociais oferecidas pelo GitHub criam diversas maneiras da comunidade colaborar e in/f_luenciar nas ativi- dades dos seus membros. XAVIER et al. [8] analisam as diferenc ¸as entre os usu´arios que registram issues e aqueles que os corrigem. Os autores identi/f_icaram', 'dades dos seus membros. XAVIER et al. [8] analisam as diferenc ¸as entre os usu´arios que registram issues e aqueles que os corrigem. Os autores identi/f_icaram que os primeiros tendem a n ˜ao participar na equipe de nenhum projeto, enquanto muitos usu´arios que corrigem issues participam na equipe de desenvolvimento de pelo menos um projeto. WEICHENG et al. [7] exploram padr˜oes nos commits feitos pelos desenvolvedores no GitHub buscando uma relac ¸˜ao entre os padr˜oes e a evoluc ¸˜ao do so/f_tware. Os autores identi/f_icaram dois padr˜oes na evoluc ¸˜ao do so/f_tware: (i) um arquivo alterado em uma vers˜ao tende a afetar um arquivo dependente dele em uma vers˜ao futura; e (ii) o tempo m´edio para a ocorrˆencia de um “grandecommit”, de/f_inido como um commit em que mais de cinco arquivos tiveram mais de cinco linhas alteradas, ´e trˆes vezes maior que o tempo entrecommits normais. Usando estes padr ˜oes, os autores a/f_irmam poder prever o pr´oximo commit do projeto e que arquivos ser˜ao alterados nesse commit, gerando assim informac ¸˜oes para apoiar o planejamento da evoluc ¸˜ao dos so/f_twares hospedados na plataforma. CABOT et al. [2] conclu´ıram que apenas 3,25% de trˆes milh˜oes de projetos avaliados no GitHub possu´ıam algumissue classi/f_icado em algum grupo. Os principais grupos utilizados foram melho- ria, defeito, quest˜ao, funcionalidade e documentac ¸˜ao. Os autores identi/f_icaram que os projetos que classi/f_icavam seusissues apresen- tavam um maior envolvimento de pessoas na soluc ¸˜ao e discuss˜ao dos mesmos, al´em de um aumento do n´umero de issues fechados. BISSYANDE et al. [1] analisaram a ferramenta de issue tracking da plataforma GitHub buscando identi/f_icar as caracter´ısticas que di- ferenciam os projetos que adotaram issue tracking systemsdaqueles que n˜ao utilizam essa ferramenta. Os autores utilizaram uma amos- tra de 100.000 projetos de c´odigo aberto, dispon´ıveis na plataforma. De posse dessa amostra, realizaram um estudo visando responder diversas quest˜oes de pesquisa, conforme ser´a detalhado na pr´oxima sec ¸˜ao. Identi/f_icaram que poucos projetos de c´odigo aberto utilizam issue tracking systemse que esta ferramenta ´e mais utilizada por projetos mais antigos, com maior base de c´odigo-fonte, com equipes de desenvolvimento maiores e com l´ıderes mais populares. 3 METODOLOGIA E COLETA DE DADOS Para colher os dados utilizados em nosso estudo da plataforma GitHub utilizamos o framework Egit, dispon´ıvel para o ambiente Eclipse, que possibilita a captura de dados do sistema de controle de vers˜oes GIT por programas escritos na linguagem de programac ¸˜ao Java. Foram utilizadas duas m´aquinas para a coleta de dados, que demorou 75 dias. Ap´os a coleta e o tratamento dos dados, o pacote estat´ıstico R v3.3 foi utilizado para realizar as an´alises estat´ısticas. 3.1 Entidades e dados coletados As an´alises desse estudo foram executadas sobre uma amostra de 220.010 projetos coletados entre os 38 milh˜oes de projetos registra- dos no GitHub. Para isso, coletamos os identi/f_icadores de todos os projetos ativos em maio/2016 e sorteamos aleatoriamente aqueles que iriam fazer parte da amostra. Em seguida, coletamos dados detalhados sobre esses projetos. Visando compreender a utilizac ¸˜ao da ferramenta de issue tracking, analisamos as caracter´ısticas de trˆes entidades: o projeto, o issue e o colaborador.', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Para cada projeto, foram coletadas a sua idade (medida em sema- nas desde a data de criac ¸˜ao do projeto at´e a data de in´ıcio da coleta, em maio de 2016), o l´ıder do projeto (usu´ario que registrou o pro- jeto na plataforma GitHub), a principal linguagem de programac ¸˜ao usada no projeto (conforme retornado pelo GitHub quando solicita- mos a linguagem em que o projeto foi desenvolvido), seu tamanho (em KBytes), seu n´umero de linhas de c´odigo, seu hist´orico de com- mits no sistema de controle de vers˜oes e um indicador que mostra se o projeto possui a funcionalidade de issue trackingativa. Capturamos tamb´em informac ¸˜oes sobre a popularidade dos pro- jetos. Um fork representa a “c´opia” de um projeto, aqui representada entre aspas porque ´e mantida uma relac ¸˜ao que permite submeter as alterac ¸˜oes realizadas em um fork para o projeto original. Assu- mimos que projetos com mais forks s˜ao mais populares e usamos o n´umero de forks como medida de popularidade do projeto. Para cada projeto da amostra, veri/f_icamos se ele´e um fork e capturamos o n´umero de forks que tiveram origem no projeto. Uma segunda medida de popularidade est ´a associada aos re- cursos de redes sociais oferecidos pela plataforma GitHub, que permitem que os usu´arios da plataforma demonstrem interesse por determinados projetos e por outros usu´arios. Do ponto de vista dos projetos, este interesse ´e demostrado “seguindo” as ac ¸˜oes que s˜ao realizadas nos projetos. Assim, os usu´arios se inscrevem nos pro- jetos e passam a ser noti/f_icados das ac ¸˜oes que os desenvolvedores realizam sobre estes. Como uma segunda medida de popularidade dos projetos, capturamos o n´umero de usu´arios da plataforma que “seguem” cada projeto da amostra. Tamb´em foram coletadas informac ¸˜oes sobre os issues registrados em cada projeto da amostra, incluindo o seustatus na data da coleta (aberto ou fechado), o usu´ario respons´avel pelo registro do issue, os r´otulos usados para classi/f_icar oissue, seu tempo de corre c ¸˜ao (medido em dias entre a data de registro e a data de encerramento, apenas para issues fechados) e seu modo de encerramento (por commit ou atrav´es da interface com o usu´ario da plataforma GitHub, apenas para issues fechados). Adicionalmente, coletamos informac ¸˜oes sobre os colaboradores, que s ˜ao os usu ´arios da plataforma GitHub que interagiram com algum projeto da amostra. Para cada colaborador, identi/f_icamos seu nome, seu login, seu e-mail e o n´umero de usu´arios da plataforma que “seguem” o colaborador. O n´umero de seguidores ´e tomado aqui como uma medida de popularidade do colaborador, de forma similar ao descrito para os projetos acima. Tamb´em classi/f_icamos os colaboradores como reporters, quando estes registram issues, e desenvolvedores, quando estes realizam commits no sistema de controle de vers˜oes. Importante notar que a classi/f_icac ¸˜ao ´e feita por projeto, ou seja, um determinado colaborador pode ser reporter em um projeto, desenvolvedor em outro ou mesmo desempenhar os dois pap´eis em um terceiro projeto. Ao /f_inal da coleta de dados, aplicamos uma curadoria sobre a amostra que se baseou em dois crit´erios de exclus˜ao. O primeiro, proposto no estudo original, sugere eliminar todos os projetos em que a ferramenta deissue trackingestiver desativada, visando retirar os casos em que os issues foram mantidos em ferramentas externas ao GitHub. Assim, foram retirados 97.286 projetos dos 220.010 cole- tados, restando 122.724 projetos na amostra. O segundo crit´erio de exclus˜ao foi aplicado para garantir que os projetos analisados ar- mazenavam o c´odigo-fonte de um so/f_tware. Assim, foram retirados 45.815 projetos em que n ˜ao foi identi/f_icada nenhuma linguagem de programac ¸˜ao, restando 76.909 projetos (35% do total de projetos inicialmente coletados).', '45.815 projetos em que n ˜ao foi identi/f_icada nenhuma linguagem de programac ¸˜ao, restando 76.909 projetos (35% do total de projetos inicialmente coletados). 3.2 /Q_uest ˜oes de pesquisa As quest˜oes de pesquisa a seguir s˜ao avaliadas em nossa an´alise. As quest˜oes de n´umero 1 a 6 foram propostas no estudo original [1] e sua an´alise ser´a repetida neste estudo. Na s´etima e oitava quest˜oes caracterizamos o uso do recurso de encerramento de issues por commits, que permite relacionar os m ´odulos de c ´odigo-fonte do projeto com os seus issues. RQ1. /Q_ual a proporc ¸˜ao de projetos que recebem registros de issues e que caracter´ısticas diferenciam estes projetos dos demais? RQ2. /Q_uantosissues, em m´edia, s˜ao registrados em projetos que utilizam issue tracking systems? RQ3. /Q_ual a popularidade da utilizac ¸˜ao de r´otulos para classi/f_icar issues? /Q_uais s˜ao os principais r´otulos utilizados? RQ4. /Q_uem cadastraissues? /Q_uantos destes colaboradores fazem parte da equipe de desenvolvimento? RQ5. ´E poss´ıvel identi/f_icar alguma relac ¸˜ao entre a utilizac ¸˜ao de issue tracking systemse a popularidade do projeto? RQ6. /Q_ue caracter´ısticas do projeto, dos colaboradores ou dos issues afetam o tempo de correc ¸˜ao dos issues? RQ7. /Q_uais s˜ao as caracter´ısticas dos colaboradores que registram issues? RQ8. /Q_uais s˜ao as caracter´ısticas dosissues que s˜ao encerrados por commit? 3.3 Di/f_iculdades na coleta de dados O GitHub restringe a quantidade de requisi c ¸˜oes feitas por um usu´ario a 5.000 por hora. Essa limita c ¸˜ao impediu que a coleta de dados fosse realizada de forma cont ´ınua, pois a coleta era inter- rompida sempre que o limite de requisic ¸˜oes era alcanc ¸ado. Essa foi a principal di/f_iculdade enfrentada durante a coleta de dados, pois tornou o processo mais lento devido ao tempo de espera quando o limite de requisic ¸˜oes era atingido. Outro obst´aculo foi a estrat´egia de coleta. O artigo original utili- zou os primeiros 100.000 projetos retornados pela API do GitHub, coletando os projetos mais antigos e executando a an´alise sobre eles. A primeira tentativa de coleta seguiu a mesma estrat´egia, coletando os primeiros 200.000 projetos retornados pela API. Por ´em, iden- ti/f_icamos que essa amostra n˜ao era representativa para a an´alise pretendida, visto que se concentrava em projetos antigos, enquanto a nossa intenc ¸˜ao era saber se projetos mais recentes mantinham o padr˜ao de uso de issue trackingdos projetos do artigo original. Por esse motivo, foi executada uma segunda coleta, gerando a amostra utilizada nesse estudo, com projetos cadastrados entre 2008 e 2016, garantindo que essa replicac ¸˜ao ter´a uma amostra com projetos mais recentes do que o estudo original. Outra ´ultima di/f_iculdade foi a inexistˆencia de padr˜oes de escrita nos campos de texto coletados dos commits e issues, onde foram identi/f_icados diversos caracteres de formatac ¸˜ao, tags de HTML e', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros pontuac ¸˜oes que atrapalhavam o processo de an´alise. Visando supe- rar essa di/f_iculdade, foram constru´ıdos mecanismos para tratamento pr´evio desses campos, retirando a “poluic ¸˜ao”. 3.4 T ´ecnicas de an´alise de dados Nas an´alises relatadas a seguir compararemos dois conjuntos de dados (por exemplo, o conjunto com as idades dos projetos que utilizam issue tracking systemscom o conjunto com as idades dos projetos que n˜ao utilizam este recurso). Neste contexto, interessa saber se existe diferenc ¸a entre estes conjuntos e se esta diferenc ¸a pode ser atribu´ıda a fatores aleat´orios ou ru´ıdo nos dados. Para tanto, utilizaremos o teste de Wilcoxon-Mann-Whitney, um teste n˜ao-param´etrico que veri/f_ica se existe diferenc ¸a estatisticamente signi/f_icativa entre as medianas de dois conjuntos de dados. Este teste exige que se estabelec ¸a um n´ıvel m´ınimo de con/f_ianc ¸a, que em nossas an´alises ser´a de 99% (ou seja, a diferenc ¸a entre as medianas dos valores ser´a considerada signi/f_icativa se p-value< 0, 01). No entanto, o teste estat´ıstico n˜ao ´e su/f_iciente quando temos um grande volume de dados, pois o volume tende a reduzir a varia- bilidade nos dados e a encontrar diferenc ¸as signi/f_icativas mesmo quando a diferen c ¸a nominal ´e muito pequena. Nestes cen ´arios, o teste estat´ıstico deve ser complementado com uma m´etrica de tamanho de efeito padronizado, que mede a frequ ˆencia com que um dado selecionado aleatoriamente de um conjunto de dados ser´a melhor que um dado selecionado aleatoriamente do outro conjunto segundo um crit´erio previamente de/f_inido (por exemplo, identi/f_icar o maior valor nominal). A m´etrica A12 de Vargha & Delaney serve a este prop´osito. Esta m´etrica varia de 0% a 100%. /Q_uando assume o valor de 50%, ela indica que os dois grupos tˆem chances iguais de produzir o melhor resultado. Valores acima de 50% indicam que o primeiro grupo tem chances maiores de apresentar o melhor resul- tado. Neste trabalho utilizaremos a m´etrica A12 para complementar o teste de Wilcoxon-Mann-Whitney, utilizando a tabela proposta por COHEN [ 3] para caracterizar tamanhos de efeito pequenos (A12 ≤60%), m´edios (A12 ≤75%) e grandes (A12 > 75%)1. Finalmente, algumas an´alises exigir˜ao o c´alculo da intensidade de dependˆencia entre dois conjuntos de dados. Nestes casos, uti- lizaremos o ´ındice de correlac ¸˜ao de Spearman ( ρ), uma medida n˜ao-param´etrica de depend ˆencia, que varia de -1 a +1. Valores pr´oximos de zero indicam que os conjuntos de dados n˜ao s˜ao rela- cionados, enquanto valores pr´oximos de +1 indicam que os valores do primeiro conjunto tendem a crescer junto com os valores do segundo e valores pr´oximos a -1 indicam que os valores do primeiro conjunto tendem a diminuir a medida que os valores do segundo crescem e vice-versa. Assim como na medida de tamanho de efeito, lanc ¸amos m˜ao de uma tabela proposta por COHEN [3] para carac- terizar relac ¸˜oes fracas (ρ ≥0.1), moderadas ( ρ ≥0.3) ou fortes (ρ ≥0.5) entre os dados analisados. 4 AN ´ALISE DE DADOS 4.1 Frequ ˆencia de projetos com issues A primeira quest ˜ao de pesquisa investiga a distribui c ¸˜ao do uso de issue tracking systemsnos projetos da amostra. Nas an ´alises abaixo utilizamos a amostra de 76.909 projetos que restaram ap´os 1Interpretado a partir dos limites da m´etrica d para tamanho de efeito. curadoria. Passamos ent ˜ao a relacionar o n ´umero de issues com caracter´ısticas do projeto, como sua idade, tamanho, tamanho de equipe e a popularidade do l´ıder. 4.1.1 Idade do projeto. A Figura 1 apresenta a distribuic ¸˜ao da idade dos projetos de acordo com o uso de issue tracking systems. Foi identi/f_icado que a m´edia de idade para projetos que possuem issues ´e de 94,6 semanas, com mediana de 71 semanas. Para os projetos que n˜ao possuem issues a m´edia ´e de 82,2 semanas, com', 'Foi identi/f_icado que a m´edia de idade para projetos que possuem issues ´e de 94,6 semanas, com mediana de 71 semanas. Para os projetos que n˜ao possuem issues a m´edia ´e de 82,2 semanas, com mediana de 62 semanas. Os testes estat ´ısticos indicam diferenc ¸a signi/f_icativa entre as idades dos projetos com e semissues, com pequeno tamanho de efeito pequeno (55%). Figura 1: Issues e a idade dos projetos 4.1.2 Tamanho do projeto. Projetos com issues registrados pos- suem uma m ´edia de 24.272,7 linhas de c ´odigo (LOC), com uma mediana de 957 LOC. Por sua vez, projetos sem issues possuem uma m´edia de 22.080,4 LOC e uma mediana de 515 LOC. Os testes estat´ısticos con/f_irmam que as duas distribuic ¸˜oes s˜ao signi/f_icativa- mente diferentes com tamanho de efeito pequeno (55%). 4.1.3 Tamanho da equipe. Identi/f_icamos uma m´edia de 12 desen- volvedores e uma mediana de trˆes desenvolvedores para projetos com issues registrados, enquanto que para projetos sem issues a m´edia ´e de 4,2 desenvolvedores e a mediana de apenas um desen- volvedor (67% dos projetos deste conjunto possuem apenas um desenvolvedor). Os testes estat´ısticos indicam diferenc ¸a signi/f_ica- tiva entre os dois conjuntos com tamanho de efeito grande (81%). 4.1.4 Popularidade do l ´ıder do projeto. A m´edia de seguidores de l´ıderes de projetos comissues ´e de 44,7 usu´arios, com uma medi- ana de apenas dois usu´arios. Para projetos sem issues, a m´edia de seguidores ´e 15 usu´arios, com mediana de um usu ´ario. Os testes estat´ısticos indicam diferenc ¸a signi/f_icativa entre os conjuntos de dados, com tamanho de efeito pequeno (54%). 4.1.5 Sum ´ario. Os t´opicos abordados nesta quest˜ao de pesquisa apresentam comportamento semelhante ao observado no estudo original. Assim, con/f_irmamos os resultados de BISSYANDE et al. que a/f_irmam que “projetos comissues reportados tendem a ser mais velhos, ter mais linhas de c´odigo, maior n´umero de desenvolvedores e l´ıderes mais populares que projetos semissues” [1].', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil 4.2 N ´umero de issues por projeto A segunda quest˜ao de pesquisa investiga a distribuic ¸˜ao do n´umero de issues por projeto. A partir desta quest˜ao de pesquisa descarta- mos os projetos que n ˜ao possuem issues. Assim, esta quest ˜ao foi respondida utilizando uma base de 226.340 issues reportados em 11.662 projetos. A Tabela 1 apresenta a distribuic ¸˜ao de issues por projeto. Observamos que 93,1% dos projetos possuem menos de 50 issues reportados, contra 86% apresentados no estudo original [1]. Tabela 1: Distribuic ¸˜ao do n ´umero de issues por projeto Nosso estudo Estudo original [1] # Issues # Proj % Total # Proj % Total 1-9 8.803 75,48% 11.602 57,89% 10-49 2.054 17,61% 5.526 27,57% 50-99 355 3,04% 1.411 7,04% 100-249 272 2,33% 976 4,87% 250-499 108 0,93% 290 1,44% 500-999 58 0,50% 163 0,81% 1000-4999 12 0,10% 69 0,34% 5000-9999 0 0,00% 2 0,01% 10000+ 0 0,00% 2 0,01% Investigamos tamb´em o n´umero de issues a cada 1.000 LOC (ou 1 kLOC) nos projetos. Observamos uma m´edia de 13,5 issues/kLOC, com uma mediana de 1,54 issues/kLOC. O estudo original reporta uma mediana de 6,32 issues/kLOC. A Figura 2 apresenta um scat- terplot em escala logar´ıtmica do n´umero de issues contra o n´umero de LOC do projeto. Observamos uma correla c ¸˜ao fraca entre as vari´aveis (ρ=0,199), ligeiramente inferior ao apresentado no estudo original (ρ=0,341). Figura 2: N ´umero de issues por tamanho de projeto, medido em LOC A Figura 3 apresenta a distribuic ¸˜ao de issues entre os projetos que tˆem como linguagem de programac ¸˜ao principal alguma das dez linguagens com maior n´umero de issues em nossa amostra. 194.081 issues (86%) est˜ao distribu´ıdos em projetos que usam uma destas linguagens (60.902 projetos, ou 79%). A maior m´edia foi encontrada para a linguagem C, com 5,27issues por projeto. No estudo original, a linguagem com maior m ´edia foi Ruby. A Tabela 2 apresenta a m´edia de issues por projeto para cada uma dessas linguagens. Como BISSYANDE et al. [1], observamos uma forte utilizac ¸˜ao de issue tracking systemsem projetos desenvolvidos para web. Em linha com o estudo original, conclu´ımos que a maioria dos projetos tˆem poucos ou nenhum issue: apenas 6,9% dos projetos da amostra possuem mais de 50 issues. Por /f_im, observamos uma relac ¸˜ao fraca entre o n´umero de LOC e o n´umero de issues, variando pouco entre as principais linguagens de programac ¸˜ao. Figura 3: N ´umero de issues em projetos de acordo com as linguagens de programac ¸˜ao mais utilizadas Tabela 2: N ´umero m´edio de issues por projeto de acordo com a sua linguagem de programac ¸˜ao principal Linguagem Projetos Issues Issues/Projeto C 2.998 15.807 5,27 C++ 3.565 18.536 5,20 C# 3.004 13.673 4,55 Java 12.111 43.211 3,57 Python 6.668 21.440 3,22 Java Script 14.023 39.358 2,81 Objective-C 2.053 5.647 2,75 PHP 5.147 14.112 2,74 HTML 4.960 11.214 2,26 Ruby 6.373 11.083 1,74 4.3 Uso de r ´otulos em issues Na terceira quest˜ao de pesquisa avaliamos a utilizac ¸˜ao de r´otulos para classi/f_icarissues. No issue tracking system oferecido pelo GitHub ´e poss´ıvel criar r´otulos espec´ı/f_icos para cada projeto ou utilizar os r´otulos disponibilizados pela plataforma. A plataforma n˜ao atribui signi/f_icado aos r´otulos: ao criar um novo r ´otulo, o cola- borador deve se assegurar de que seu nome seja su/f_icientemente descritivo para que um desenvolvedor ou reporter identi/f_ique o ob- jetivo do r´otulo ou documentos a parte devem especi/f_icar o objetivo de cada r´otulo usado no projeto. Foram encontrados 304.033 r ´otulos nos 226.340 issues identi/f_i- cados na nossa amostra, sendo 2.596 r´otulos distintos distribu´ıdos em 118.596 issues. V ´arios destes r´otulos possu´ıam signi/f_icado se- melhante e foram agrupados manualmente em 301 grupos. Os', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros trˆes r´otulos identi/f_icados como mais utilizados no estudo original (bug, feature e enhacement) tamb´em /f_iguram entre os dez grupos de r´otulos mais utilizados em nossa amostra. Por ´em, eles representam apenas 12% dos r´otulos identi/f_icados, contra 34% no estudo original. Os trˆes grupos mais utilizados em nossa amostra (auto-migrated, priority e defect) representam 72% do total de r ´otulos utilizados, tendo sido encontrados em 82.782 issues distintos, sendo que 60.874 issues possuem os trˆes r´otulos ao mesmo tempo. A m´edia de r ´otulos identi/f_icados porissue ´e de 2,6, com uma mediana de trˆes r´otulos e um m´aximo de 10 r´otulos em um issue. 52% dos issues da amostra possuem r´otulos, contra 30% no estudo original. 26% destes issues possuem apenas um r´otulo, 54% possuem dois r´otulos, 8% possuem trˆes r´otulos, 8% possuem quatro r´otulos, e 3% possuem cinco ou mais r´otulos. A recorrˆencia de issues com dois ou mais r´otulos indica que estes s˜ao utilizados com diversos /f_ins, como apontamento de tipo de problema, vers˜ao do so/f_tware, componente do so/f_tware, entre outros. 4.4 Per/f_il do colaborador que cadastra issues A quarta quest ˜ao de pesquisa discute se issue tracking systems s˜ao utilizados exclusivamente por reporters ou se desenvolvedo- res tamb´em registram issues. Identi/f_icamos 110.946 desenvolvedo- res nos projetos que possuem issues (581.856 no estudo original), com uma m´edia de 9,5 desenvolvedores por projeto e uma medi- ana de dois desenvolvedores. Consideramos como desenvolvedor todo usu´ario que contribuiu com ao menos um commit no projeto. Tamb´em foram identi/f_icados 44.522reporters (239.629 no estudo original), com uma m ´edia de 3,8 por projeto e uma mediana de apenas um. Consideramos como reporter todo usu´ario que apenas registra issues em um projeto. Observamos que em m´edia 38% dos desenvolvedores atuam como reporter em outros projetos, com uma mediana de 33% (31% no estudo original). Por sua vez, em m´edia 53% dosreporters participam de alguma atividade de desenvolvimento em algum projeto, com uma mediana de 50% (42% no estudo original). A Figura 4 mostra a distribuic ¸˜ao de desenvolvedores e reporters nos projetos. Como BISSYANDE et al. [ 1], veri/f_icamos que muitos desenvolvedores registram issues, enquanto grande parte dosreporters contribui para o c´odigo-fonte. Conclu´ımos ent˜ao que n˜ao existem grupos distintos de pessoas que apenas desenvolvem ou apenas registram issues, mas que diversos colaboradores executam os dois pap´eis. 4.5 Issues e a popularidade do projeto Para analisar a quinta quest˜ao de pesquisa utilizaremos o n´umero de forks e o seguidores para indicar a popularidade de um projeto e o interesse despertado na comunidade. Em seguida, analisaremos a relac ¸˜ao entre a popularidade do projeto e o registro de issues. 4.5.1 N ´umero de seguidores. A Figura 5 apresenta umsca/t_terplot da relac ¸˜ao entre o n´umero de issues e o n´umero de seguidores nos projetos que comp ˜oem a amostra. Observamos uma correla c ¸˜ao moderada (ρ=0,311) entre as vari´aveis, inferior ao encontrado no estudo original (ρ=0,628). Analisamos tamb ´em a relac ¸˜ao entre o n´umero de seguidores e o n´umero de reporters. Mais uma vez encon- tramos uma dependˆencia moderada entre estas vari´aveis (ρ=0,323), tamb´em inferior ao estudo original (ρ=0,789). Figura 4: Proporc ¸˜ao de desenvolvedores entre os issues que s˜ao reporters e vice-versa Figura 5: A relac ¸˜ao entre o n´umero de seguidores e n´umero de issues 4.5.2 N ´umero de forks. Observamos uma correlac ¸˜ao moderada entre o n´umero de issues e o n´umero de forks (ρ=0,426) nos projetos da amostra, inferior ao observado no estudo original ( ρ=0,669). Observamos tamb´em uma correlac ¸˜ao moderada entre o n ´umero de reporters e o n ´umero de forks (ρ=0,442), tamb ´em inferior ao reportado no estudo original (ρ=0,829).', 'Observamos tamb´em uma correlac ¸˜ao moderada entre o n ´umero de reporters e o n ´umero de forks (ρ=0,442), tamb ´em inferior ao reportado no estudo original (ρ=0,829). 4.5.3 Sum ´ario. Conclu´ımos que o n´umero de seguidores e forks possuem relac ¸˜ao moderada com a utilizac ¸˜ao de issue tracking sys- tems e com o n´umero de reporters em um projeto. Observamos que projetos com mais seguidores e forks tendem a ter mais issues e um maior n´umero de colaboradores reportando issues, con/f_irmando os resultados de BISSYANDE et al. [1], por´em com menor intensidade. 4.6 Tempo de correc ¸˜ao dos issues Na sexta quest˜ao de pesquisa investigamos aspectos relacionados com a velocidade com que issues s˜ao corrigidos.', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Figura 6: A relac ¸˜ao entre o n´umero de reporters e o tempo para correc ¸˜ao de issues 4.6.1 N ´umero de reporters. A Figura 6 apresenta um sca/t_terplot relacionando o n´umero de reporters e o tempo m´edio de correc ¸˜ao dos issues nos projetos. O gr´a/f_ico n˜ao apresenta relac ¸˜ao linear entre as duas vari´aveis, como no estudo original. Observamos correlac ¸˜ao quase nula entre o n ´umero de reporters e o tempo de corre c ¸˜ao (ρ=0,062), pr´oximo ao reportado no estudo original (ρ=0,161). 4.6.2 Uso de r ´otulos. Nesse ponto encerramos a replicac ¸˜ao do estudo original e passamos a analisar outras caracter ´ısticas que podem afetar o tempo de correc ¸˜ao dosissues, comec ¸ando pelo uso de r´otulos. O tempo m´edio de correc ¸˜ao de issues que possuem r´otulos ´e 22,52 dias, com mediana de zero dias, visto que 73% deles foram encerrados no mesmo dia em que foram registrados. Para issues que n˜ao possuem r´otulos, o tempo m´edio de correc ¸˜ao ´e 25,17 dias, com uma mediana de zero dias (62% dos issues foram encerrados em menos de um dia). Os testes estat ´ısticos indicam diferenc ¸a signi/f_icativa entre os dados com tamanho de efeito pequeno (56%). 4.6.3 Uso dos r ´otulos mais frequentes. Analisamos a rela c ¸˜ao entre o tempo de correc ¸˜ao de um issue e se ele possui algum r´otulos dos dez grupos mais utilizados. A m´edia do tempo de correc ¸˜ao para os issues que possuem algum r´otulo destes grupos´e de 17,3 dias, com mediana de zero dias (80% dosissues foram encerrados em menos de um dia). J´a para os issues que n˜ao possuem r´otulos ou usam r´otulos menos frequentes, a m´edia do tempo de correc ¸˜ao ´e de 28,8 dias, com mediana de zero dias (60% dos issues foram encerrados em menos de um dia). Os testes estat ´ısticos indicam diferenc ¸a signi/f_icativa entre os dados com tamanho de efeito pequeno (60%). 4.6.4 N ´umero de coment´arios. Investigamos a relac ¸˜ao entre o tempo de correc ¸˜ao de umissue e o n´umero de coment´arios recebidos. Encontramos correlac ¸˜ao quase nula ( ρ=0,065), indicando que o n´umero de coment´arios n˜ao in/f_luencia o tempo de correc ¸˜ao. 4.6.5 Sum ´ario. Conclu´ımos queissues com r´otulos tendem a ser corrigidos mais rapidamente e que issues que possuem r´otulos pre- sentes nos dez grupos mais utilizados tˆem essa tendˆencia acentuada. Al´em disso, veri/f_icamos que n˜ao h´a relac ¸˜ao entre a quantidade de coment´arios recebidos por um issue ou o n´umero de reporters em um projeto e o tempo m´edio de correc ¸˜ao dos issues. Figura 7: A relac ¸˜ao entre issues registrados e o tempo desde a criac ¸˜ao da conta do colaborador 4.7 Caracter ´ısticas dosreporters Na s´etima quest˜ao de pesquisa investigamos as caracter´ısticas dos colaboradores e a sua rela c ¸˜ao com a utiliza c ¸˜ao de issue tracking systems. Para tal, retiramos da amostra os colaboradores com contas canceladas ou privativas, restando 40.885 colaboradores dos quais 28.177 registraram pelo menos um issue nos projetos da amostra. 4.7.1 Tempo de cria c ¸˜ao da conta. Analisamos a relac ¸˜ao entre o n ´umero de issues registrados por um colaborador e o tempo, em dias, desde a cria c ¸˜ao da sua conta. A Figura 7 apresenta o sca/t_terplotdestes dados. Encontramos uma correlac ¸˜ao negativa e fraca entre as vari´aveis (ρ=-0,101), indicando que n˜ao existe uma relac ¸˜ao evidente entre o n ´umero de issues registrados e a idade da conta de um colaborador. Identi/f_icamos que a idade m´edia da conta dos colaboradores que registraramissues ´e de 1597,7 dias, com mediana de 1601 dias. Para os colaboradores que n˜ao registraram issues, a m´edia ´e de 1827,7 dias com mediana de 1836 dias. Os testes estat´ısticos indicam diferenc ¸as signi/f_icativas entre os dados, com tamanho de efeito pequeno (59%). 4.7.2 N ´umero de commits registrados. Avaliamos a relac ¸˜ao entre', 'estat´ısticos indicam diferenc ¸as signi/f_icativas entre os dados, com tamanho de efeito pequeno (59%). 4.7.2 N ´umero de commits registrados. Avaliamos a relac ¸˜ao entre o n´umero de issues registrados e o n´umero de commits realizados pelos colaboradores. Identi/f_icamos uma correlac ¸˜ao fraca e nega- tiva (ρ=-0,185) entre estas vari´aveis, indicando que existe uma leve tendˆencia de que reporters registrem mais issues do que desenvol- vedores. Os reporters realizaram em m ´edia 30,83 commits, com mediana de zero (56% dos reporters n˜ao registraram commits). J ´a para colaboradores que n ˜ao registraram issues a m´edia ´e de 99,6 commits, com mediana de cinco commits. Os testes estat ´ısticos indicam distribuic ¸˜oes signi/f_icativamente diferentes entre os grupos de colaboradores, com tamanho de efeito m´edio (74%). Conclu´ımos que colaboradores que registram issues tendem a realizar menos commits do que aqueles que n˜ao registram. 4.7.3 N ´umero de seguidores. Nesse t´opico avaliamos a relac ¸˜ao entre o n´umero de issues registrados e a popularidade de um cola- borador, medindo essa popularidade pelo n´umero de seguidores do colaborador em quest˜ao. Encontramos uma correlac ¸˜ao negativa e', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros quase nula (ρ=–0,063) na amostra, indicando que a popularidade n˜ao ´e um bom preditor do n´umero de issues registrados. Veri/f_icamos que a m´edia de seguidores para os colaboradores que registraramis- sues ´e de 144,43, com mediana de sete seguidores. Foi retirado dessa amostra um outlier que possu´ıa mais de 20 milh˜oes de seguidores; com ele, a m´edia seria de 1057,27 seguidores. Para colaboradores que n˜ao registraram issues, a m´edia ´e de 157,04, com mediana de 15 seguidores. Os testes estat´ısticos indicam diferenc ¸as signi/f_icativas entre os dados, com tamanho de efeito pequeno (59%). Portanto, colaboradores que registram issues tendem a possuir menos segui- dores do que aqueles que n˜ao registram issues. Supomos que esse comportamento pode estar ligado ao fato de que os desenvolvedores recebam mais atenc ¸˜ao da comunidade do que reporters. 4.7.4 Projetos p ´ublicos. Encerrando essa quest˜ao de pesquisa, avaliamos a relac ¸˜ao entre o n´umero de issues registrados por um co- laborador e o n´umero de projetos p´ublicos que ele possui. Mais uma vez, encontramos uma correlac ¸˜ao negativa e quase nula (ρ=-0,024) entre esses dados. A m ´edia de projetos p ´ublicos por colaborado- res que registraram issues ´e de 38,6, com mediana de 19 projetos. Para colaboradores que n˜ao registram issues, a m´edia ´e de 38,9 com mediana de 25. Os testes estat ´ısticos indicam distribuic ¸˜oes signi- /f_icativamente diferentes, com tamanho de efeito pequeno (55%). Conclu´ımos que os colaboradores que registramissues tendem a possuir um n´umero ligeiramente menor de projetos p´ublicos do que aqueles que n˜ao registraram issues. Esse comportamento pode estar ligado ao fato de que os l´ıderes de projetos geralmente participam como desenvolvedores, enquanto os reporters n˜ao precisam possuir projetos para registrar issues, realizando essa ac ¸˜ao em projetos de outros usu´arios da plataforma. 4.7.5 Sum ´ario. Conclu´ımos que as caracter´ısticas de umrepor- ter, como a idade da sua conta, o n´umero de seguidores e o n´umero de projetos p´ublicos tˆem pouca correlac ¸˜ao com o n´umero de issues registrados por ele. J´a o n´umero de commits realizados pelo colabo- rador tem uma correlac ¸˜ao fraca com o n´umero de issues registrados por ele: colaboradores que registramissues tendem a realizar menos commits que os demais colaboradores. 4.8 Issues encerrados por commit Na oitava quest˜ao de pesquisa investigamos as caracter´ısticas dos issues encerrados por commit em comparac ¸˜ao com issues encerra- dos atrav´es da interface com o usu ´ario do issue tracking system. Para isso, utilizamos os 159.118 issues marcados como fechados na amostra. Desses, 7.000 issues foram encerrados por commits. O n´umero m´edio de issues encerrados por commits para projetos com pelo menos um issue fechado ´e de 0,8 issue por projeto, indicando que em m´edia apenas 4% dos issues de um projeto s˜ao encerrados por commits. Para projetos com pelo menos um issue encerrado por commit, a m´edia de issues encerrados desta forma ´e de 6,5 issues, indicando que em m´edia 28% dos issues do projeto s˜ao encerrados por commit, com mediana de dois issues. Estes resultados apontam para uma concentrac ¸˜ao da pr´atica de encerramento de issues por commits em poucos projetos. Pesqui- sadores que busquem o desenvolvimento de modelos de predic ¸˜ao de m´odulos mais sujeitos a alterac ¸˜oes, explorando a relac ¸˜ao entre os issues e os m´odulos de c´odigo-fonte alterados para resolvˆe-los, devem concentrar seus esforc ¸os nestes poucos projetos que usam encerramentos de issues por commits. N ˜ao ´e recomendado o uso de uma ampla base de projetos, pois os issues encerrados por com- mits ser˜ao poucos na maior parte dos projetos componentes desta base, provendo assim pouco material para relacionar os issues e os m´odulos de c´odigo-fonte.', 'mits ser˜ao poucos na maior parte dos projetos componentes desta base, provendo assim pouco material para relacionar os issues e os m´odulos de c´odigo-fonte. 4.8.1 Linguagem de programa c ¸˜ao. As cinco linguagens que mais tiveram issues encerrados por commits s˜ao JavaScript (com 1.509 issues, 4% dos issues registrados em projetos baseados nessa lingua- gem), Java (com 1.377 issues, 3% dos issues registrados em projetos baseados nessa linguagem), C++ (com 778 issues, 4% dos issues em projetos com essa linguagem), Python (com 722issues, 3% dos issues em projetos com essa linguagem) e PHP (com 490 issues, 3% dos issues em projetos com essa linguagem). Todas estas linguagens est˜ao entre as dez que mais possuem issues registrados. 4.8.2 Tempo de corre c ¸˜ao. O tempo m´edio de correc ¸˜ao para issues encerrados por commits ´e de 27,61 dias, com mediana de dois dias. Para issues que n˜ao foram encerrados por commit, a m´edia ´e de 23,8 dias, com mediana de zero dias (70% foram encerrados no mesmo dia). Os testes estat´ısticos indicam uma diferenc ¸a signi/f_icativa entre os grupos, com tamanho de efeito m´edio (65%). 4.8.3 Utiliza c ¸˜ao de r´otulos. Dos 7.000 issues encerrados por com- mit, apenas 4.219 possuem r´otulos. O teste chi-quadrado, utilizado para saber se os dados de dois grupos s ˜ao estatisticamente dife- rentes, indica que o percentual de issues encerrados por commit ´e signi/f_icativamente maior entre osissues que possuem r´otulos e os issues que n˜ao possuem r´otulos, com tamanho de efeito pequeno (x2 = 278,1, p-value < 0,01, Cramer-V (tamanho de efeito) = 4,18%). Apenas 4% dos issues que n˜ao possuem r´otulos ou possuem r´otulos fora do grupo dos dez r´otulos mais utilizados foram encerrados por commits, enquanto 6% dos issues que possuem r´otulos entre os dez grupos mais utilizados foram encerrados desta forma. Utilizando o teste chi-quadrado veri/f_icamos que existe diferenc ¸a signi/f_icativa do porcentual de issues encerrados por commit entre os issues que possuem algum r´otulo entre os dez grupos mais utilizados e aqueles que possuem r ´otulos fora destes grupos, com tamanho de efeito pequeno (x2 = 114,34, p-value < 0,001, Cramer-V = 2,6%). Os cinco r´otulos mais utilizados em issues encerrados por com- mits pertencem aos grupos bug (1.768 issues encerrados por commit ou 19% dos issues registrados com r´otulos deste grupo); enhance- ment (1.392 issues encerrados por commit ou 6% dos issues com r´otulos deste grupo); feature (335 issues encerrados por commit ou 13% dos issues com r´otulos deste grupo); priority (211 issues encer- rados por commit ou 0,3% dos issues com r´otulos deste grupo); e user interface(92 issues encerrados por commit ou 20% dos issues com r´otulos deste grupo). Encontramos uma correlac ¸˜ao fraca ( ρ=0,158) entre o n ´umero de commits registrados por um colaborador e o n´umero de issues com r´otulos encerrados por commits. Veri/f_icamos que a m´edia de commits realizados por colaboradores que encerraram issues com r´otulos por commits ´e de 369,49 commits por projeto, com mediana de 53 commits. J ´a para aqueles que n ˜ao encerraram issues com r´otulos por commits a m´edia ´e de 45,98 commits, com mediana de', 'A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB PlatformSBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil um commit. Os testes estat´ısticos identi/f_icaram distribuic ¸˜oes signi- /f_icativamente diferentes, com tamanho de efeito grande (82%). Ve- ri/f_icamos, conforme o esperado, que os colaboradores que utilizam a funcionalidade de encerramento de issues por commit executam mais commits do que os colaboradores que n˜ao a utilizam. 4.8.4 N ´umero de coment´arios por issue. Investigamos o n´umero de coment´arios recebidos por issues fechados por commit. A m´edia de coment´arios recebidos por issues encerrados por commits ´e de 1,7 coment´arios/issue, com mediana de zero (56% dos issues deste tipo n˜ao receberam coment´arios). Para issues que n˜ao foram encerrados por commits essa m´edia ´e de 2,4 coment´arios/issue, com mediana de um coment´ario. Os testes estat´ısticos indicam diferenc ¸as signi/f_i- cativas entre os grupos, com tamanho de efeito m´edio (62%). 4.8.5 N ´umero de commits. Encontramos uma correla c ¸˜ao pe- quena (ρ=0,211) entre o n´umero de issues encerrados por commit e o n´umero de commits registrados por um colaborador. A Figura 8 apresenta um sca/t_terplotos dados utilizados nesta an ´alise. A m´edia de commits registrados pelos colaboradores que encerraram ao menos um issue por commit ´e de 270,41, com mediana de 42 commits. Para aqueles que n ˜ao encerraram issues por commits a m´edia ´e de 44,39, com mediana de um commit. Os teste estat´ısticos identi/f_icaram diferenc ¸as signi/f_icativas entre os grupos, com tama- nho de efeito grande (82%). Conclu´ımos que o n´umero de commits realizados tem correlac ¸˜ao pequena com a quantidade deissues que o colaborador encerrou por commits. Veri/f_icamos tamb´em, conforme o esperado, que os desenvolvedores tendem a utilizar a funcionali- dade de encerramento de issues por commit com mais frequˆencia. Figura 8: A relac ¸˜ao entre o n´umero de issues encerrados por commit e o n´umero de commits realizados pelo colaborador 4.8.6 Issues externos encerrados por commit. Issues externos s˜ao issues recebidos por um projeto que foi criado a partir de um fork de outro projeto. Os issues externos s˜ao issues do projeto original, que podem ter sido resolvidos no fork. Identi/f_icamos 14.951issues externos encerrados por commit em nossa amostra de projetos. Issues externos n˜ao tiveram seus dados coletados para an´alise deta- lhada. Somando os issues presentes nos projetos coletados (7.000) aos externos (14.951), temos um total de 21.951 issues encerrados por commit. Identi/f_icamos uma correlac ¸˜ao fraca (ρ=0,155) entre o n´umero de issues externos encerrados por commits e o n´umero de commits realizados nos projetos. Veri/f_icamos que a m´edia de commits realizados por colaboradores que encerraram issues exter- nos por commits ´e de 521,08 commits por projeto, com mediana de 19 commits. J ´a para aqueles que n ˜ao encerraram issues externos atrav´es de commits a m´edia ´e de 39,07 commits, com mediana de um commit. Os testes estat ´ısticos identi/f_icam diferenc ¸a signi/f_icativa entre os grupos, com tamanho de efeito grande (77%). 4.8.7 Sum ´ario. Observamos que issues encerrados por commits tendem a consumir mais tempo para a sua correc ¸˜ao. Tamb´em ve- mos uma pequena predominˆancia de encerramento por commits para issues que possuem r´otulos. Os r ´otulos mais comuns indicam que a funcionalidade de encerramento de issues por commits ´e mais utilizada para issues referentes `a correc ¸˜ao de defeitos ou melhorias de c ´odigo, enquanto que issues em maior n´ıvel de abstrac ¸˜ao s ˜ao encerrados manualmente. Por /f_im, veri/f_icamos que o n´umero de coment´arios recebidos por issues encerrados por commit ´e ligeira- mente menor do que para os demais issues. Uma poss ´ıvel raz˜ao para essa diferenc ¸a´e a necessidade de acessar o issue tracking sys- tem quando se fecha um issue manualmente, o que pode motivar', 'mente menor do que para os demais issues. Uma poss ´ıvel raz˜ao para essa diferenc ¸a´e a necessidade de acessar o issue tracking sys- tem quando se fecha um issue manualmente, o que pode motivar os colaboradores a escreverem coment´arios, pois essa opc ¸˜ao /f_ica muito pr´oxima do comando de encerramento manual de issue. J´a nos issues encerrados por commit, o colaborador n˜ao tem a opc ¸˜ao de escrever coment´arios para o issue no momento do commit. 5 DISCUSS ˜AO E SUM ´ARIO DOS RESULTADOS Para as quest˜oes de pesquisa que diziam respeito `a replicac ¸˜ao do estudo de BISSYANDE et al. [1] (RQ1 a RQ6) foram encontrados resultados que corroboram aqueles reportados no estudo original, ainda que alguns destes resultados tenham variado em sua inten- sidade. Conclu´ımos que (i) os projetosopen-source do GitHub n˜ao recebem um grande n ´umero de issues, visto que apenas 15% da amostra possui issues registrados; (ii) n˜ao existem grupos bem de/f_i- nidos de desenvolvedores e reporters, pois grande parte daqueles que atuam em um papel tamb´em executa atividades relacionadas ao outro papel; (iii) que existe uma correla c ¸˜ao moderada entre o n´umero de seguidores eforks de um projeto e a quantidade derepor- ters envolvidos e issues registrados, evidenciando que o ambiente distribu´ıdo e colaborativo tem uma in/f_luˆencia positiva na utilizac ¸˜ao da ferramenta de issue tracking; e (iv) o n ´umero de reporters e o tempo de correc ¸˜ao dos issues possuem correlac ¸˜ao quase nula. O per/f_il de projetos que mais utilizamissue tracking systemsse manteve o mesmo daquele observado por BISSYANDE et al. [ 1], sendo eles projetos mais antigos, com maior base de c ´odigo, que possuem uma grande equipe de desenvolvimento e cujos l ´ıderes s˜ao populares. Con/f_irmamos que existe uma correlac ¸˜ao fraca entre o n´umero de issues e o n´umero de linhas de c´odigo de um projeto. A m´edia de issues por linha de c´odigo varia pouco entre as linguagens de programac ¸˜ao que servem de base para o projeto e o registro de issues ´e mais comum em projetos voltados para a Web. Tamb´em elencamos os r´otulos mais utilizados para classi/f_icac ¸˜ao de issues e veri/f_icamos a grande utilizac ¸˜ao de r´otulos como defect, relacionado `a correc ¸˜ao de erros, e auto-migrated, relacionado `a migrac ¸˜ao de c´odigo-fonte e issues vindo de outros sistemas de controle de vers˜ao e outros issue tracking systems.', 'SBCARS 2017, September 18–19, 2017, Fortaleza, CE, Brazil Casimiro Conde Marco Neto and M ´arcio de O. Barros Na s´etima e oitava quest˜oes de pesquisa, o estudo buscou analisar novos pontos de vista referentes a utilizac ¸˜ao da ferramenta de issue tracking. Com a an´alise dessas quest˜oes, chegamos a algumas con- clus˜oes, sendo uma delas a tendˆencia de issues que possuem r´otulos serem encerrados mais rapidamente do que aqueles que n˜ao pos- suem. Identi/f_icamos tamb´em que os colaboradores que registram issues tendem a realizar um menor n´umero de commits no c´odigo- fonte, ainda que previamente n˜ao tenhamos encontrado diferenc ¸a signi/f_icativa entre os pap´eis de desenvolvedor e reporter. Por outro lado, n˜ao foram identi/f_icados resultados relevantes em relac ¸˜ao `as caracter´ısticas de tempo de criac ¸˜ao da conta do colaborador, seu n´umero de seguidores e seu n´umero de projetos p´ublicos. Conclu´ımos tamb´em que o n´umero de commits realizados por um colaborador tem relac ¸˜ao com o n´umero de issues que encerrou por commits. Assim, veri/f_icamos que os colaboradores que mais registram commits tendem a utilizar a funcionalidade de encerra- mento de issues atrav´es de commits com maior frequˆencia. J´a em relac ¸˜ao `as caracter´ısticas dosissues, veri/f_icamos que aqueles encer- rados por commits tendem a possuir um tempo de vida maior do que os encerrados atrav ´es da interface com o usu ´ario e recebem um n ´umero menor de coment ´arios. Tamb ´em veri/f_icamos que a funcionalidade de encerramento por commit ´e bastante utilizada para fechar issues que possuem r´otulos e que os principais r´otulos de issues encerrados desta forma possuem rela c ¸˜ao direta com o c´odigo-fonte, como bug, feature, enhancement e user interface. Por /f_im, identi/f_icamos uma concentrac ¸˜ao da pr´atica de encerra- mento de issues por commits em poucos projetos, onde a pr´atica ´e estabelecida e utilizada por grande parte dos desenvolvedores. Pes- quisas que busquem relacionar issues com m´odulos de c´odigo-fonte usando informac ¸˜oes da plataforma GitHub devem se concentrar nestes projetos, ao inv´es de tomar por base uma grande quantidade de projetos onde este recurso n˜ao ´e utilizado com frequˆencia. 5.1 Ameac ¸as `a validade Durante o planejamento e execuc ¸˜ao deste estudo identi/f_icamos al- guns pontos que devem ser citados como ameac ¸as `a validade dos seus resultados. A primeira ameac ¸a se refere ao n´umero de commits existentes em cada projeto analisado. Ao selecionarmos os projetos, n˜ao /f_iltramos a base dispon´ıvel pelo n´umero de commits. Sendo assim, em nossa amostra existem projetos com apenas um ou ne- nhum commit, quando possivelmente seria melhor para a an´alise que todos possu´ıssem muitoscommits, demonstrando maior ativi- dade nos projetos e maior probabilidade desses projetos possu´ırem issues corrigidos atrav´es de commits. Outra ameac ¸a est´a no m´etodo escolhido para identi/f_icar os cola- boradores distintos. Todos os registros de colaborador que possu´ıssem o mesmo nome e e-mail foram considerados como apenas um cola- borador em cada projeto. Pessoas que possuem mais de uma conta, cada qual com um e-mail distinto, foram contabilizadas duas vezes e suas contribuic ¸˜oes foram divididas entre as contas. Finalmente, n˜ao foi poss´ıvel realizar a replicac ¸˜ao exata do m´etodo utilizado por BISSYANDE et al. [ 1], pois os detalhes t ´ecnicos do estudo original n˜ao foram disponibilizados. Por exemplo, n˜ao sa- bemos qual ´e a intersecc ¸˜ao entre os projetos selecionados para as nossas an´alises e os projetos utilizados no artigo original. Replica- mos os conceitos, curadoria e quest˜oes de pesquisa apresentados no estudo original, por´em utilizando m´etodos constru´ıdos para esse estudo. Por falta de maiores detalhes, tivemos que aplicar algumas interpretac ¸˜oes do que foi feito no estudo original e n˜ao podemos garantir que essas interpretac ¸˜oes n˜ao in/f_luenciaram nos resultados', 'interpretac ¸˜oes do que foi feito no estudo original e n˜ao podemos garantir que essas interpretac ¸˜oes n˜ao in/f_luenciaram nos resultados encontrados na replicac ¸˜ao. 6 CONCLUS ˜AO Este artigo apresentou uma replicac ¸˜ao da pesquisa realizada por BISSYANDE et al. [1] para veri/f_icar se as conclus˜oes apresentadas pela pesquisa continuam v´alidas ap´os quatro anos da sua publicac ¸˜ao e uma an´alise das caracter´ısticas presentes emissues encerrados por commits. Como principais contribuic ¸˜oes do estudo aqui reportado, podemos citar: (i) a con/f_irmac ¸˜ao dos resultados do estudo replicado, adicionando evidˆencias que con/f_irmam as suas conclus˜oes; (ii) a realizac ¸˜ao de um estudo focado na utilizac ¸˜ao da funcionalidade de encerramento de issues por commit, que concluiu que este recurso ´e utilizado em projetos com muitos commits, que os principais issues encerrados desta forma s˜ao reportados por desenvolvedores, envolvem quest˜oes relacionadas ao c´odigo-fonte e seu tempo m´edio de correc ¸˜ao n˜ao ´e muito diferente dos demais issues. Identi/f_icamos como limitac ¸˜ao do presente trabalho o fato de n˜ao termos avaliado a relac ¸˜ao entre os m´odulos de c´odigo-fonte afetados por commits e outras caracter´ısticas dosissues, como por exemplo se determinados m´odulos est˜ao ligados ao mesmo desenvolvedor, aos mesmos r´otulos, entre outros. Assim, propomos como trabalho futuro estudos que investiguem a relac ¸˜ao dos issues, r´otulos e cola- boradores com os m´odulos de c´odigo-fonte afetados por commits que encerrem issues. Assim, poderemos estudar meios de identi/f_icar os colaboradores mais aptos a solucionar umissue relacionado a um m´odulo do so/f_tware ou encontrar os m´odulos que provavelmente precisar˜ao ser alterados para resolver umissue. Outras propostas de trabalho futuro incluem a realizac ¸˜ao de um estudo qualitativo sobre o uso deissue tracking systeme a replicac ¸˜ao desse estudo levando em considerac ¸˜ao projetos privativos, avaliando se as conclus˜oes apre- sentadas para projetos de c´odigo aberto s˜ao semelhantes quando analisadas para projetos que n˜ao sejam p´ublicos. REFERˆENCIAS [1] T.F. Bissyande, D. Lo, J. Lingxiao, L. Reveillere, and Y. Le Traon. 2013. Got Issues? Who Cares About It? A Large Scale Investigation of Issue Trackers from GitHub. In IEEE 24th Intl Symposium on So/f_tware Reliability Engineering. 188–197. [2] J. Cabot, J.L.C. Izquierdo, and V. Cosentino. 2015. Exploring the Use of Labels to Categorize Issues in Open-Source So/f_tware Projects. InSo/f_tware Analysis, Evolution and Reengineering (SANER’15). 550–554. [3] J. Cohen. 1992. A Power Primer. Psychological Bulletin(1992), 155–159. [4] L. Dabbish, C. Stuart, J. Tasy, and J. Herbsleb. 2012. Social Coding in GitHub: Transparency and Collaboration in an Open So/f_tware Repository. InACM 2012 Conference on Computer Supported Cooperative Work. 1277–1286. [5] E. Kalliamvakou, K. Blincoe, L. Singer, and D. Damian. 2014. /T_he Promises and Perils of Mining GitHub. In11th Working Conference on Minig So/f_tware Repositories (MSR’14). 92–101. [6] M.J. Lee, B. Ferwerda, J. Choi, J. Hahn, J. Moon, and J. Kim. 2013. GitHub developers use rockstars to overcome over/f_low of News. InHuman Factors in Computing System. 233. [7] Y. Weicheng, S. Beijun, and X. Ben. 2013. Mining GitHub: Why commit stops – Exploring the relationship between developer’s commit pa/t_tern and /f_ile version evolution. In Proc. of the Asia-Paci/f_ic So/f_tware Engineering Conference. 165–169. [8] J. Xavier, A. Macedo, and M. A. Maia. 2014. Understanding the popularity of reporters and assignees in the Github. In26th International Conference on So/f_tware Engineering and Knowledge Engineering. 484–489. [9] E. Yong. 2012. Bad Copy. Nature 495 (2012), 298–300.']","['USAGE OF GITHUB ’S ISSUE TRACKING SYSTEM     This briefing reports scientific evidence on  how GitHub’s issue tracking system  can  support a team during the development  of a software project.      FINDINGS    Studies addressing issue tracking systems (ITS)  analyze how this tool can support the team  during the development of a software  project. We have replicated the analyses  reported in a former study addressing the  ITS offered by the GitHub platform.  Following are some results we obtained:  \uf0b7 Few open -source p rojects use ITS  (15% of our sample).    \uf0b7 Even taking only those projects with at  least one issue, 86% of our sample  have recorded less than 50 issues.    \uf0b7 Larger projects (both in LOC and  number of developers) tend to use ITS  more extensively. The correlations i n  the original paper were stronger than  what we have found now.    \uf0b7 Projects led by popular developers  tend to use ITS more extensively.  Correlations were stronger in the  former paper.    \uf0b7 Labels are not frequently used and the  most common labels refer to  automated actions, defects, and task  prioritization.    \uf0b7 Issue reporters are typically  developers.    \uf0b7 There is a moderate correlation  between the number of forks created  from a project or the number of  developers following its leader and the  number of issues reported  for the  project. This correlation was also  stronger in the former paper.                \uf0b7 There are only weak or moderate  correlations between the number of  reporters in a project, the use of  labels, the use of frequently -used  labels or the number of comments and  the average time required to resolve  an issue. These correlations were also  stronger in the former paper.    \uf0b7 Developers tend to mark issues as  resolved by mentioning them on  commits to the version control  system. This allows some traceability  between the chan ges made to the  code and the issue they resolve.  However, few issues are resolved  through these means and most of  them are related to bugs, new  features, and feature enhancements –  that is, they are closely related to the  source-code!    ENOUGH TALK… SHOW ME  THE  NUMBERS!    \uf0b7 Approximately 38.000.000 projects on  the platform    \uf0b7 220.000 selected projects    \uf0b7 76.909 analyzed projects    \uf0b7 11.662 projects with issues    \uf0b7 226.340 issues    \uf0b7 159.118 resolved issues    \uf0b7 1,54 issues/KLOC    \uf0b7 304.033 labels    \uf0b7 2.596 unique labels    \uf0b7 52% issues with labels    \uf0b7 2 or 3 labels/issue    \uf0b7 7.000 issues closed by commit            Who is this briefing for?    Software engineering practitioners  who want to make decisions about  the use of GitHub issue tracking  system based on scientific  evidence.      Where the findings come from?    All findings of this briefing were  extracted from the experiment  replication conducted by Neto et al.        What is included in this briefing?    Evidence from a experiment  replication.      What is not included in this briefing?    Results from the original experiment.      To access other evidence briefings  on software engineering:    http://cin.ufpe.br/eseg/evidence- briefings      ORIGINAL RESEARCH REFERENCE  Neto et al. A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform. SBCARS, 2017.']","**Title: Understanding the Use of GitHub's Issue Tracking System**

**Introduction:**
This Evidence Briefing summarizes key findings from a study that replicates and extends research on the issue tracking system provided by GitHub, evaluating whether previous results remain valid and analyzing the relationship between issues and source code modifications.

**Main Findings:**
1. **Continued Relevance of Previous Findings:** The study confirms that the trends observed in the original 2013 research still hold today. Projects utilizing issue tracking systems are typically older, larger, and led by more popular developers.

2. **Low Usage of Commit-Based Issue Closure:** Only about 4% of issues are closed through commits, indicating that developers often do not link issues directly to the code changes that resolve them. This practice is concentrated in a few active projects, suggesting limited predictive modeling opportunities for future issues based on past commits.

3. **Characteristics of Projects with Issues:** Projects that report issues tend to have more lines of code, larger teams, and greater popularity, as indicated by the number of followers and forks. These projects also have a higher average age compared to those without reported issues.

4. **Label Usage in Issue Tracking:** The study found that 52% of issues have labels, indicating a significant increase from the previous study. However, the most common labels (e.g., bug, feature) represent a smaller fraction of total labels used, suggesting a shift in how issues are categorized.

5. **Role of Contributors:** Many developers also report issues, with approximately 38% of developers acting as reporters in addition to their coding roles. This blurs the lines between the roles of developers and reporters, reinforcing the collaborative nature of GitHub.

6. **Impact of Labels on Issue Resolution Time:** Issues that are labeled tend to be resolved faster than those without labels. Notably, issues closed by commits take longer to resolve than those closed via the GitHub interface.

7. **Correlation with Popularity Metrics:** There is a moderate correlation between the number of issues and the popularity of a project, measured by followers and forks. Projects with more followers tend to have more reported issues.

**Who is this briefing for?**
This briefing is intended for software development teams, project managers, and researchers interested in understanding the dynamics of issue tracking systems within open-source projects on GitHub.

**What is included in this briefing?**
The briefing includes insights into the continued relevance of previous research findings, the characteristics of projects using issue tracking systems, the role of contributors, and the impact of labeling on issue resolution.

**What is NOT included in this briefing?**
This briefing does not delve into technical details of the methodologies used for data collection or statistical analysis, nor does it provide a comprehensive overview of the original study's data.

**To access other evidence briefings on software engineering:**
[Link to other evidence briefings]

**Original Research Reference:**
Casimiro Conde Marco Neto and Mário de O. Barros. 2017. A Structured Survey on the Usage of the Issue Tracking System provided by the GITHUB Platform. In Proceedings of SBCARS 2017, Fortaleza, CE, Brazil, September 18–19, 2017. DOI: 10.1145/3132498.3134110"
"['Knowledge management in software engineering: A systematic review of studied concepts, ﬁndings and research methods used Finn Olav Bjørnsona,*, Torgeir Dingsøyra,b,1 a Norwegian University of Science and Technology, Department of Computer and Information Science, Sem S/C26landsvei 7-9, 7491 Trondheim, Norway b SINTEF Information and Communication Technology, SP Andersens vei 15b, 7465 Trondheim, Norway article info Article history: Received 9 August 2007 Received in revised form 19 February 2008 Accepted 11 March 2008 Available online 30 March 2008 Keywords: Software engineering Knowledge management Learning software organization Software process improvement Systematic review abstract Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identiﬁes empirical studies of knowledge manage- ment initiatives in software engineering, and discusses the concepts studied, the major ﬁndings, and the research methods used. Seven hundred and sixty-two articles were identiﬁed, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies.The majority of empirical studies relate to technocratic and behavioural aspects of knowledge management, while there are few studies relating to economic, spatial and cartographic approaches. A ﬁnding reported across multiple papers was the need to not focus exclu- sively on explicit knowledge, but also consider tacit knowledge. We also describe implications for research and for practice. /C2112008 Elsevier B.V. All rights reserved. Contents 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1056 2. Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1056 2.1. Knowledge management ........................................................................................ 1056 2.2. Theories of organizational learning . . . . . . . ......................................................................... 1057 2.3. Knowledge management in software engineering . . . . . . . . . . . . . . ...................................................... 1058 3. Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1058 3.1. Planning the review . . . ......................................................................................... 1058 3.2. Identification of research ........................................................................................ 1059 3.3. Selection of primary studies . . . . . . . . . . . . . ......................................................................... 1059 3.4. Quality assessment and classification . . . . . ......................................................................... 1059 3.5. Synthesis . . . . . . . . . . . . ......................................................................................... 1059 4. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1059 4.1. Technocratic schools . . . ......................................................................................... 1060 4.1.1. Systems ..................................................................................................... 1060', '4.1.1. Systems ..................................................................................................... 1060 4.1.2. Cartographic ................................................................................................. 1061 4.1.3. Engineering .................................................................................................. 1061 4.2. Behavioural schools. . . . . ........................................................................................ 1062 4.2.1. Organizational. . . ............................................................................................. 1062 4.2.2. Strategic ..................................................................................................... 1062 4.3. Knowledge management in general. . . . . . . ......................................................................... 1063 4.3.1. The impact of knowledge management initiatives . ................................................................ 1063 4.3.2. Knowledge management per se ................................................................................. 1063 5. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1063 5.1. Major knowledge management concepts and findings. . . . . . . . . . . ...................................................... 1063 5.1.1. Technocratic schools .......................................................................................... 1064 5.1.2. Behavioural schools ........................................................................................... 1064 0950-5849/$ - see front matter/C2112008 Elsevier B.V. All rights reserved. doi:10.1016/j.infsof.2008.03.006 * Corresponding author. Tel.: +47 73 59 34 40; fax: +47 73 59 44 66. E-mail addresses: bjornson@idi.ntnu.no (F.O. Bjørnson),torgeir.dingsoyr@sintef.no (T. Dingsøyr). 1 Tel.: +47 73 59 29 79; fax: +47 73 59 29 77. Information and Software Technology 50 (2008) 1055–1068 Contents lists available atScienceDirect Information and Software Technology journal homepage: www.elsevier.com/locate/infsof', '5.2. Research methods .............................................................................................. 1064 5.3. Implications for research and practice. . ............................................................................ 1065 5.3.1. Implications for research ....................................................................................... 1065 5.3.2. Implications for practice ....................................................................................... 1065 5.4. Limitations. . . . . . .............................................................................................. 1065 6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1066 Appendix ................................................................................................................... 1066 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1066 1. Introduction Software engineering is a knowledge-intensive activity. For software organizations, the main assets are not manufacturing plants, buildings, and machines, but the knowledge held by the employees. Software engineering has long recognized the need for managing knowledge and the community could learn much from the knowledge management community, which bases its the- ories on well-established disciplines such as cognitive science, ergonomics, and management. As the ﬁeld of software engineering matures, there is an in- creased demand for empirically-validated results and not just the testing of technology, which seems to have dominated the ﬁeld so far. A recent trend in software engineering is an increased focus on evidence-based software engineering, (EBSE)[41,65]. Since the volume of research in the ﬁeld is expanding constantly, it is becoming more and more difﬁcult to evaluate critically and to syn- thesise the material in any given area. This has lead to an increased interest in systematic reviews (SR)[64] within the ﬁeld of software engineering. In this article, we report on a systematic review of empirical studies of knowledge management in software engineering. Our goal is to provide an overview of empirical studies within this ﬁeld, what kinds of concepts have been explored, what the main ﬁndings are, and what research methods are used. More speciﬁcally we ask the following research questions: 1. What are the major knowledge management concepts that have been investigated in software engineering? 2. What are the major ﬁndings on knowledge management in software engineering? 3. What research methods have been used within the area so far? Our target readership is three groups that we think will be interested in an overview of empirical research on knowledge management in software engineering: (1) researchers from soft- ware engineering who would like to design studies to address important research gaps, and identify relevant research methods; (2) researchers on knowledge management in general, who would be interested in comparing work in the software engineering ﬁeld to other knowledge-intensive ﬁelds; and (3) reﬂective practitioners in software engineering, who will be interested in knowing what knowledge management initiatives have been made in software companies, or quickly identifying relevant studies, and the major ﬁndings and implications from these. The remainder of this article is structured as follows. Section2 presents the background and general theories on knowledge man- agement. Section3 describes the research method that we used to select and review the data material for our research, and presents our chosen framework for analysis. Section4 presents the results', 'agement. Section3 describes the research method that we used to select and review the data material for our research, and presents our chosen framework for analysis. Section4 presents the results of the systematic review according to our chosen framework. In Section 5, we discuss the ﬁndings and their implications. For re- search, we identify what we belive are the most important re- search gaps. For practitioners, we provide advice on how to use the results in practice. Section6 concludes. 2. Background In this section, we ﬁrst give a brief background on knowledge management, then give an overview of theories often referred to in the knowledge management literature. Finally, we give an over- view of existing work on knowledge management in software engineering. 2.1. Knowledge management Knowledge management is a large interdisciplinary ﬁeld. There is, as a consequence, an ongoing debate as to what constitutes knowledge management. However, it is beyond the scope of this article to engage in that debate. For our purposes, it is sufﬁcient to cite some deﬁnitions that are in common use. Davenport has de- ﬁned knowledge managementas ‘‘a method that simpliﬁes the pro- cess of sharing, distributing, creating, capturing and understanding of a company’s knowledge”[26]. A related term isorganizational learning. What does it mean to say that an organization as a whole learns? According to Stata, this differs from individual learning in two respects [110]: ﬁrst, it occurs through shared insight, knowl- edge and shared models; second, it is based not only on the mem- ory of the participants in the organization, but also on ‘‘institutional mechanisms” such as policies, strategies, explicit models and deﬁned processes (we can call this the ‘‘culture” of the organization). These mechanisms may change over time, what we can say is a form of learning. Knowledge management has received much attention in vari- ous ﬁelds, which is demonstrated by the publication of two ‘‘hand- books” [31,43], one encyclopaedia [104], and numerous books [23,26,107]. Hanssen et al.[53] refer to two main strategies for knowledge management: /C15Codiﬁcation – to systematize and store information that consti- tutes the knowledge of the company, and to make this available to the people in the company. /C15Personalization – to support the ﬂow of information in a com- pany by having a centralised store of information about knowl- edge sources, like a ‘‘yellow pages” of who knows what in a company. Earl [42] has further classiﬁed work in knowledge management into schools (seeTable 1). The schools are broadly categorized as ‘‘technocratic”, ‘‘economic” and ‘‘behavioural”. The technocratic schools are: (1) the systems school, which focuses on technology for knowledge sharing, using knowledge repositories; (2) the car- tographic school, which focuses on knowledge maps and creating knowledge directories; and (3) the engineering school, which fo- cuses on processes and knowledge ﬂows in organizations. The economic school focuses on how knowledge assets relates to income in organizations. The behavioural school consists of three subschools: (1) the organizational school, which focuses on networks for sharing knowledge; (2) the spatial school, which focuses on how ofﬁce 1056 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068', 'space can be designed to promote knowledge sharing; and (3) the strategic school, which focuses on how knowledge can be seen as the essence of a company’s strategy. There are a number of overview articles of the knowledge man- agement ﬁeld in the literature. In the following we describe over- view articles from management science and information systems. In the introduction to the bookChallenges and Issues in Knowl- edge Management [20], in the ﬁeld of management consulting, Buono and Poulfelt claim that the ﬁeld is moving from ﬁrst to sec- ond generation knowledge management. In ﬁrst generation knowl- edge management, knowledge was considered a possession, something that could be captured, thus knowledge management was largely a technical issue on how to capture and spread the knowledge through tools like management information systems, data repositories and mechanistic support structures. The second generation of knowledge management is characterized by know- ing-in-action. Knowledge is though of as a socially embedded phe- nomenon, and solutions have to consider complex human systems, communities of practice, knowledge zones, and organic support structures. The change in knowledge management initiatives is seen to go from a planned change approach to a more guided changing approach. Coming from the ﬁeld of management consulting, Christensen [24] performed a literature review focusing on special journal is- sues on knowledge management from 1995 to 2003. He performed a content analysis of 50 identiﬁed papers focusing on knowledge management context, knowledge management outcomes, empiri- cal setting and the key drivers for knowledge management. The ﬁnding was that KM writings seem to focus on how to create knowledge and to a lesser degree, how to transfer knowledge. The categories that did not receive adequate coverage were inte- gration, production, measurement, retention and reﬂection. A sec- ond ﬁnding was that the drivers for both knowledge creation and knowledge transfer were generic and to a large degree overlapping. He goes on to explore knowledge management in practice through 10 managers from industry and compares his results to the results of the theoretic study. The main conclusion is that KM theory does reﬂect, in generic terms, the practices that support KM activities, but the challenge is to observe this practical application of generic drivers, which often is difﬁcult to observe in practice. In the information systems ﬁeld, Alavi and Leidner[3] summa- rize literature from different ﬁelds, which is relevant to research on knowledge management systems. One of the major challenges in KM according to them is to facilitate the ﬂow of knowledge be- tween individuals so that the maximum amount of transfer occurs. They also conclude that no single or optimal solution to organiza- tional knowledge management can be developed. Instead a variety of approaches and systems needs to be employed to deal with the diversity of knowledge types. Knowledge management is not a monolithic but a dynamic and continuous phenomenon. Liao gives an overview of technology and applications for knowledge management in a review of the literature from 1995 to 2002 [72]. The review covers knowledge-based systems, data mining, ICT applications, expert systems, database technology and modeling technology. Argote et al.[7] conclude a special issue of Management Science with an article that provides a framework for organizing the liter- ature on knowledge management, identiﬁes emerging themes, and suggests directions for further research. Many have been critical to the concept of knowledge manage- ment, and in particular to the use of information technology in knowledge management. Hislop[54] questions the distinction be- tween tacit and explicit knowledge. If explicit knowledge cannot be managed independently, this means that information technol- ogy will have a smaller part in knowledge management. This cri-', 'tween tacit and explicit knowledge. If explicit knowledge cannot be managed independently, this means that information technol- ogy will have a smaller part in knowledge management. This cri- tique is also supported by McDermott[83], who argues that ‘‘if people working in a group don’t already share knowledge, don’t al- ready have plenty of contact, don’t already understand what in- sights and information will be useful to each other, information technology is not likely to create it”. In addition, Swan et al. [112] criticize the knowledge management ﬁeld for being too occu- pied with tools and techniques. They claim that researchers tend to overstate the codiﬁability of knowledge and to overemphasize the utility of IT to give organizational performance improvement. They also warn that ‘‘codiﬁcation of tacit knowledge into formal systems may generate its own pathology: the informal and locally situated practices that allow the ﬁrm to cope with uncertainty may become rigidiﬁed by the system”. Schultze and Leidner [103] studied discourses of knowledge management in information systems research, and warn that knowledge can be a double-edged sword: too little can result in expensive mistakes, while too much can lead to unwanted account- ability. In a study of research on information systems, they found that most existing research is optimistic on the role of knowledge management in organizations, and they urge researchers to give more attention to the critique of knowledge management. 2.2. Theories of organizational learning In cognitive and organization science, we ﬁnd many models on how knowledge is transferred or learned at an individual and orga- nizational level. We present four theories that are referred to widely: Kolb’s model of experiential learning, the double-loop learning the- ory of Argyris and Schön, Wenger’s theory of communities of prac- tice, and Nonaka and Takeuchi’s theory of knowledge creation. Kolb describes learning from experience (‘‘experiential learn- ing”, see [68]) as four different learning modes that we can place in two dimensions. One dimension is how people take hold of experience, with two modes, either relying on symbolic represen- tation – which he calls comprehension, or through ‘‘tangible, felt qualities of immediate experience”, which he calls apprehension. The other dimension is how people transform experience, with two modes, either through internal reﬂection, which he refers to as intention, or through ‘‘active external manipulation of the exter- nal world”, which he calls extension. Kolb argues that people need to take advantage of all four modes of learning to be effective, they ‘‘must be able to involve themselves fully, openly, and without bias in new experiences; re- ﬂect on and observe these experiences from many perspectives; create concepts that integrate their observations into logically sound theories; and use these theories to make decisions and solve problems” [69]. Argyris and Schön distinguish between what they call single and double-loop learning [9] in organizations. In single-loop Table 1 Earl’s schools of knowledge management Technocratic Economic commercial Behavioural Systems Cartographic Engineering Organizational Spatial Strategic Focus Technology Maps Processes Income Networks Space Mindset Aim Knowledge bases Knowledge directories Knowledge ﬂows Knowledge assets Knowledge pooling Knowledge exchange Knowledge capabilities Unit Domain Enterprise Activity Know-how Communities Place Business F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068 1057', 'learning, one receives feedback in the form of observed effects and then acts on the basis solely of these observations to change and improve the process or causal chain of events that generated them. In double-loop learning, one not only observes the effects of a pro- cess or causal chain of events, but also understands the factors that inﬂuence the effects[8]. One traditional view of learning is that it is most effective when it takes place in a setting where you isolate and abstract knowledge and then ‘‘teach” it to ‘‘students” in rooms free of context. Wenger describes this as a view of learning as an individual process where, for example, collaboration is considered a kind of cheating[118].I n his book about communities of practice, he describes a completely different view: learning as asocial phenomenon. A community of practice develops its own ‘‘practices, routines, rituals, artefacts, symbols, conventions, stories and histories”. This is often different from what you ﬁnd in work instructions, manuals and the like. Wenger deﬁnes learning in communities of practice as follows: For individuals: learning takes place in the course of engaging in, and contributing to, a community. For communities: learning is to reﬁne the practice. For organizations: learning is to sustain interconnected com- munities of practice. Nonaka and Takeuchi[88] claim that knowledge is constantly converted from tacit to explicit and back again as it passes through an organization. By tacit knowledge[92] we mean knowledge that a human is not able to express explicitly, but is guiding the behav- iour of the human. Explicit knowledge is knowledge that we can represent in textual or symbolic form. They say that knowledge can be converted from tacit to tacit, from tacit to explicit, or from explicit to either tacit or explicit knowledge. These modes of con- version are described as follows. Socializationmeans to transfer tacit knowledge to another person through observation, imitation and practice, what has been referred to as ‘‘on the job” training.Externalisation means to go from tacit knowledge to explicit. Explicit knowledge can ‘‘take the shapes of met- aphors, analogies, concepts, hypotheses or models”.Internalization means to take externalised knowledge and make it into individual ta- cit knowledge in the form of mental models or technical know-how. Combination means to go from explicit to explicit knowledge, by taking knowledge from different sources such as documents, meet- ings, telephone conferences, or bulletin boards and aggregating and systematizing it. According to Nonaka and Takeuchi, knowledge passes through different modes of conversion, which makes the knowledge more reﬁned and spreads it across different layers in an organization. 2.3. Knowledge management in software engineering Companies developing information systems have failed to learn effective means for problem solving to such an extent that they have learned to fail, according to an article by Lyytinen and Robey [79]. One suggested mean to overcome this problem is an in- creased focus on knowledge management. There are many approaches to how software should be devel- oped, which also affect how knowledge is managed. A main differ- ence between methods here is if they are plan-based or traditional, which rely primarily on managing explicit knowledge, or agile methods, which primarily rely on managing tacit knowledge[86]. In software engineering, there has been much discussion about how to manage knowledge, or foster ‘‘learning software organiza- tions”. In this context, Feldmann and Althoff have deﬁned a ‘‘learn- ing software organization” as an organization that has to ‘‘create a culture that promotes continuous learning and fosters the ex- change of experience”[48]. Dybå places more emphasis on action in his deﬁnition: ‘‘A software organization that promotes improved actions through better knowledge and understanding”[39].', 'change of experience”[48]. Dybå places more emphasis on action in his deﬁnition: ‘‘A software organization that promotes improved actions through better knowledge and understanding”[39]. In software engineering, reusing life cycle experience, processes and products for software development is often referred to as hav- ing an ‘‘Experience Factory”[13]. In this framework, experience is collected from software development projects, and are packaged and stored in anexperience base. By packing, we mean generalising, tailoring, and formalising experience so that it is easy to reuse. In 1999, the ﬁrst workshop on ‘‘learning software organiza- tions” was organized in conjunction with the SEKE conference. This workshop has been one of the main arenas for empirical studies as well as technological development related to knowledge manage- ment in software engineering. The May 2002 issue of IEEE Software [75] was devoted to knowledge management in software engineering, giving several examples of knowledge management applications in software companies. In 2003, the book ‘‘Managing Software Engineering Knowledge” [38] was published, focusing on a range of topics, from identifying why knowledge management is important in software engineering [76], to supporting structures for knowledge manage- ment applications in software engineering, to offering practical guidelines for managing knowledge. However, Edwards notes in an overview chapter in the book on Managing Software Engineering Knowledge[45] that knowledge management in software engineering is somewhat distanced from mainstream knowledge management. Several PhD thesis have also been published on aspects of knowledge management that are related to software engineering [15,17,34,115]. In addition, a number of overviews of work on knowledge man- agement in software engineering have previously been published. Rus et al.[98] present an overview of knowledge management in software engineering. The review focuses on motivations for knowledge management, approaches to knowledge management, and factors that are important when implementing knowledge management strategies in software companies. Lindvall et al. [78] describe types of software tools that are relevant for knowl- edge management, including tools for managing documents and content, tools for managing competence, and tools for collabora- tion. Dingsøyr and Conradi[35] surveyed the literature for studies of knowledge management initiatives in software engineering. They found eight reports on lessons learned, which are formulated with respect to what actions companies took, what the effects of the actions were, what beneﬁts are reported, and what kinds of strategy for managing knowledge were used. Despite of the previously published overviews of the ﬁeld, there is still a lack of broad overviews of knowledge management in soft- ware engineering. Our motivation for this study was thus, to give a more thorough and broader overview in the form of a systematic review. This study also covers recent work, and assesses the quality of the research in the ﬁeld. 3. Method The research method used is a systematic review[64], with de- mands placed on research questions, identiﬁcation of research, selection process, appraisal, synthesis, and inferences. We now ad- dress each of these in turn. 3.1. Planning the review We started by developing a protocol for the systematic review, specifying in advance the process and methods that we would ap- ply. The protocol speciﬁed the research questions, the search strat- egy, criteria for inclusion and exclusion, and method of synthesis. 1058 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068', 'The aim of the study was to provide an overview of the empir- ically studied methods for knowledge management in software engineering, answering the research questions listed in Section1. 3.2. Identiﬁcation of research A comprehensive, unbiased search is a fundamental factor that distinguishes a systematic review from a traditional review of the literature. Our systematic search started with the identiﬁcation of keywords and search terms. We used general keywords in the search in order to identify as many relevant papers as possible (see Table 2). All possible permutations of the software engineering and knowledge management concepts were tried in the search con- ducted. The following electronic bases were those we considered most relevant[40]: ISI Web of Science, Compendex, IEEE Xplore and the ACM Digital Library. In addition, we identiﬁed two arenas that, to our knowledge, are the only ones that pertain speciﬁcally to knowledge management in software engineering: the workshop series on Learning Software Organizations (LSO) from 1999 until 2006, and the book Managing Software Engineering Knowledge[10]. We searched all proceedings from the workshop series and included all chapters from the book. We performed the search in August 2006, which means that publications up to and including the ﬁrst quarter of 2006 are in- cluded, but some studies in the second quarter might not have been indexed in the databases. The identiﬁcation process yielded 2102 articles. This formed the basis for the next step in our selection process. 3.3. Selection of primary studies The ﬁrst step after the articles had been identiﬁed was to elim- inate duplicate titles, and titles clearly not related to the review. One researcher (the ﬁrst author) read through the 2102 titles and removed duplicates and those clearly not related to the ﬁeld of software engineering. This yielded a result of 762 articles. After this we obtained the abstract of these articles and both authors read through all abstracts, with the following exclusion criterion. /C15Exclude if the focus of the paper is clearly not on software engineering. /C15Exclude if the focus of the paper is clearly not on knowledge management. /C15Exclude if the method, tool or theory described is not tested in industry. To narrow the search further we also decided to focus on tech- nical and process knowledge (thus, ‘‘software engineering knowl- edge”). Hence, we also used the criterion /C15Exclude if the focus of the paper is on domain knowledge. After each researcher had gone through the papers we com- pared results. Where we disagreed as to whether to keep or re- move a paper, we discussed the matter until we reached agreement. This process reduced the number of articles to 133, and agree- ment between researchers was ‘good’ (Kappa value of 0.655). The full text for all 133 papers was obtained and both research- ers read through all the papers with the same criteria for exclusion in mind. The ﬁnal number of papers selected for the review was 68. The agreement between researchers at this stage was ‘‘moderate” (Kappa value: 0.523). 3.4. Quality assessment and classiﬁcation We chose to classify the 68 papers identiﬁed along two axes. (1) We wanted to examine what kinds of concept had been tested. To aid us with this we chose the framework for classifying strategies for managing knowledge presented by Earl in[42]. Each researcher classiﬁed the 68 papers individually according to the framework, before comparing the results. Disagreements were discussed until a consensus was reached on the classiﬁcation. (2) We also wanted to examine the scientiﬁc rigor of the studies. Here we settled on a simpler classiﬁcation. All studies included so far had results taken from industry. We further assessed the quality of the selected pa- pers by categorizing these into empirical studies and lessons learned reports. The criterion for being accepted as an empirical', 'from industry. We further assessed the quality of the selected pa- pers by categorizing these into empirical studies and lessons learned reports. The criterion for being accepted as an empirical study and not a report of lessons learned was that the article had a section describing the research method and context. Again, each study was classiﬁed individually by the two researchers before comparing the results and discussing problem cases in order to reach agreement. After the quality assessment, we had 29 empiri- cal studies and 39 reports of lessons learned. 3.5. Synthesis For the synthesis, we chose to only use the papers classiﬁed as empirical studies in our framework, in order to avoid problems associated with lessons learned reports stemming from their lack of scientiﬁc rigor. We extracted concepts covered, main ﬁndings and the research method for each article. One researcher (the ﬁrst author) focused on the studies in the technocratic schools, while the other researcher (the second author) focused on the behav- ioural schools. 4. Results Using the framework outlined in Section3.4, we categorized the 29 empirical studies and 39 reports of lessons learned inTable 3. For a complete listing of papers in each category, see theAppendix. Within Earl’s framework, we found a heavy concentration on the technocratic schools and a fair mention of the behavioural school. We did not ﬁnd any papers relating to the economic school with our search criterion. Within the technocratic schools, systems and engineering stand out as areas that have received much atten- tion. Within the behavioural schools, organizational and strategic have received the most attention. Table 2 Keywords for our search Software engineering keywords Knowledge management keywords /C15Software engineering /C15Software process /C15Learning software organization /C15Knowledge management /C15Tacit knowledge /C15Explicit knowledge /C15Knowledge creation /C15Knowledge acquisition /C15Knowledge sharing /C15Knowledge retention /C15Knowledge valuation /C15Knowledge use /C15Knowledge application /C15Knowledge discovery /C15Knowledge integration /C15Knowledge theory /C15Organization knowledge /C15Knowledge engineering /C15Experience transfer /C15Technology transfer F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068 1059', 'Four of the empirical studies did not ﬁt into Earl’s framework. These were classiﬁed as studies on the impact of knowledge man- agement initiatives and on knowledge management per se. Thus, we ended up with 25 studies classiﬁed as empirical within the framework. Of the 39 reports of lessons learned, two belonged to two categories, which is why we ended up with a sum of 41 for the reports of lessons learned in the table. Looking at the papers by year of publication, presented inFig. 1, we notice an increasing interest in the area from 1999 onwards. We also notice a shift from more papers on lessons learned to empirical papers from 2003 onwards. The apparent decrease in attention in 2006 is due to our covering only the ﬁrst third of this year, since our search was conducted in August. To obtain an overview of the research methods used within this ﬁeld, we used the classiﬁcation presented in Glass et al.[50]. This was carried out on the 25 papers classiﬁed as empirical studies. The result is presented inTable 4. See theAppendix for a complete listing of which paper was classiﬁed in which category. In the following subsections, we present the concepts and main ﬁndings from the empirical studies within the main knowledge management schools. 4.1. Technocratic schools The technocratic schools are based on information or manage- ment technologies, which largely support and, to different degrees, condition employees in their everyday tasks. We identiﬁed a total of 19 empirical studies and 29 papers on lessons learned in this category. The main focus is on the engineering and systems schools. 4.1.1. Systems As deﬁned by Earl, the systems school is built on the underlying principle that knowledge should be codiﬁed in knowledge bases. This is what Hansen et al. refer to as the ‘‘codiﬁcation strategy”, and what Nonaka and Takeuchi refer to as externalisation. This school is the longest established school of knowledge man- agement, and it is in this category we found the oldest papers in our search. Most of the papers that were excluded would have been placed in this category, if they had contained empirical re- sults from industry. They could mainly be classiﬁed as conceptual analysis and concept implementation, according to Glass’s deﬁni- tion. In total, we classiﬁed six papers as empirical in this school, and 20 as lessons learned. The empirical papers in this category can broadly be deﬁned as either dealing with the development or use of knowledge repositories. In what follows, we brieﬂy present the major concepts studied in the empirical papers. An overview of concepts and ﬁndings can be found inTable 5. In [22], Chewar and McCrickard present their conclusions from three case studies investigating the use of their knowledge repos- itory. On the basis of their case studies, they present general guide- lines and tradeoffs for developing a knowledge repository. In[18], Bjørnson and Stålhane follow a small consulting company that wanted to introduce an experience repository. On the basis of interviews with the employees, they draw conclusions about atti- tudes towards the new experience repository, and the content and functionality preferred by the employees. Barros et al.[11] investigate how risk archetypes and scenario models can be used to codify reusable knowledge about project management. They test their approach by an observational analysis in industry. They also describe a feasibility study within an academic environment. Concerning the actual usage of experience repositories or knowledge bases, Dingsøyr and Røyrvik[33] investigate the prac- tices in a medium-sized software consulting company where knowledge repositories are used in concrete work situations. They Table 5 Concepts and main ﬁndings for the systems school School Concepts Main ﬁndings Reference Systems Development of knowledge repositories and initial use Approach to supporting risk in project management [11] Users should be involved in development [18]', 'Systems Development of knowledge repositories and initial use Approach to supporting risk in project management [11] Users should be involved in development [18] Approach to support design activities [22,109] Use of knowledge repositories over time Beneﬁts can be realized quickly, tool remains useful over time, and more beneﬁts accrue over time [70] Tool can be used for different kinds of knowledge than originally intended [33] Table 3 Articles categorized by type and knowledge management school Systems Cartographic Engineering Commercial Organizational Spatial Strategic SUM Empirical studies 6 1 12 0 3 0 3 25 % Distribution, empirical studies 24 4 48 0 12 0 12 100 Lessons learned reports 20 0 9 0 2 1 9 41 % Distribution, lessons learned reports49 0 22 0 5 2 22 100 0 2 4 6 8 10 12 14 16 199219931994199519961997199819992000200120022003200420052006 Lessons Learned  Empirical studies Fig. 1. Publications by year. Table 4 Overview of research methods Action research Case study Ethnography Experiment Field study Sum Systems 1 3 1 1 6 Cartographic 1 1 Engineering 1 8 1 2 12 Organizational 3 3 Strategic 1 2 3 Sum 3 14 2 1 5 25 % 12 56 8 4 20 100 1060 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068', 'found several distinct ways of using the tool and highlight the importance of informal organization and the social integration of the tool in daily work practices. A more formal approach to knowl- edge management tools is found in[109], where Skuce describes experiences from applying a knowledge management tool in the design of a large commercial software system. Concerning long- term effects of experience repositories, Kurniawati and Jeffery [70] followed the usage of a combined electronic process guide and experience repository in a small-to-medium-sized software development company for 21 weeks, starting a year after the tool was introduced. They conclude that tangible beneﬁts can be real- ized quickly and that the tool remains useful with more beneﬁts accruing over time. 4.1.2. Cartographic The principal idea of the cartographic school is to make sure that knowledgeable people in an organization are accessible to each other for advice, consultation, or knowledge exchange. This is often achieved through knowledge directories, or so-called ‘‘yel- low pages”, that can be searched for information as required. We found only one empirical paper within this school and no papers on lessons learned. In[32], Dingsøyr et al. examine a skills management tool at a medium-sized consulting company. They identify four major usages of the tool and point out implications of their ﬁndings for future or other existing tools in this category, see Table 6. 4.1.3. Engineering The engineering school of knowledge management is a deriva- tive or outgrowth of business process reengineering. Consequently it focuses on processes. According to our classiﬁcation, the largest amount of empirical papers came from this school. Two major cat- egories can be identiﬁed. The ﬁrst contains work done by research- ers who investigate the entire software process with respect to knowledge management. The second contains work done by researchers who focus more on speciﬁc activities and how the pro- cess can be improved within this activity.Table 7 gives an over- view of concepts and ﬁndings for this school. Baskerville and Pries-Heje[14] used knowledge management as the underlying theory to develop a set of key process areas to sup- plement the Capability Maturity Model (CMM)[91] in a Small- and Medium-sized Enterprise (SME) software development company. Realizing that the CMM did not ﬁt well with an SME company, they helped their case companies to develop new key process areas that focused on managing their knowledge capability. Arent et al.[6] address the challenge of creating organizational knowledge during software process improvement. They argue for the importance of creating organizational knowledge in Software Process Improve- ment (SPI) efforts and claim that its creation is a major factor for success. On the basis of an examination of several cases, they claim that both explicit and tacit knowledge are required, no matter what approach is pursued. Segal[106] investigates organizational learning in software process improvement. Using a case to initiate and implement a manual of best practice as a basis, she observed that the ideal and actual scenarios of use differed and identiﬁed possible reasons for the difference. In[49], Folkestad et al. studied the effect of using the rational uniﬁed process as a tool for organi- zational change. In this case, it was used to introduce development staff to a new technology and methodology. Folkestad et al. con- cluded that the iterative approach of the uniﬁed process had obvi- ous effects on organizational and individual learning. The uniﬁed process also resulted in new patterns of communication and a new division of labour being instituted, which had a signiﬁcant Table 6 Concepts and main ﬁndings for the cartographic school School Concepts Main ﬁndings Reference Cartographic Use of cartographic system Tool was used for: allocating resources, searching for competence, identifying project opportunities and', 'School Concepts Main ﬁndings Reference Cartographic Use of cartographic system Tool was used for: allocating resources, searching for competence, identifying project opportunities and upgrading skills [32] Tool enabled learning practice at both individual and company level. [32] Table 7 Concepts and main ﬁndings for the engineering school School Concepts Main ﬁndings Reference Engineering Managing knowledge on the software development process It is feasible to use knowledge management as underlying theory to develop key process areas to supplement the CMM [14] No matter what knowledge management approach you pursue in SPI, you need to create both tacit and explicit knowledge. Tacit is necessary to change practice, explicit is necessary to create an organizational memory [6] A techno-centric approach to SPI may impose unnatural work practices on an organization and fails to take account of how process improvements might occur spontaneously within a community of practice [106] The iterative approach of Uniﬁed Process ensures large effects in terms of learning, but Uniﬁed Process also improves on communication and work distribution in the company [49] It is possible to deﬁne and implement software process in a beneﬁcial and cost-efﬁcient manner in small software organizations. Special considerations must be given to their speciﬁc business goals, models, characteristics, and resource limitations [116] Managing knowledge through formal routines Formal routines must be supplemented by collaborative, social processes to promote effective dissemination and organizational learning [25] Mapping of knowledge ﬂows Knowledge mapping can successfully help an organization to select relevant focus areas for planning future improvement initiatives [52] Casual maps for risk modeling contributes to organizational learning [2] Process for conducting project reviews to extract knowledge Creating a suitable environment for reﬂection, dialogue, criticism, and interaction is salient to the conducting of a postmortem [30] The organizational level can only beneﬁt from the learning of project teams if the knowledge and reasoning behind the process improvements is converted into such an explicit format that it can be utilized for learning in organizational level also [99] Implications of social interaction on knowledge sharing The focus on the pure codiﬁed approach is the critical reason of Tayloristic team failure to effectively share knowledge among all stakeholders of a software project [84] Increasing the level of reﬂection in mentor programmes can result in more double-looped learning [16] F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068 1061', 'effect on the company. Wangenheim et al.[116] report on their experiences of deﬁning and implementing software processes. They conﬁrm what others have experienced, that it is possible to deﬁne and implement software processes in the context of small companies in a beneﬁcial and cost-effective way. In the papers that focused on speciﬁc activities within the pro- cess, we identiﬁed four major areas: formal routines, mapping of knowledge ﬂows, project reviews, and social interaction. Many of these processes are aimed at stimulating several ways of learning, as, for example, Kolb suggests. In [25], Conradi and Dybå report on a survey that investigated the utility of formal routines for transferring knowledge and expe- rience. Their main observation was that developers were rather sceptical about using written routines, while quality and technical managers took this for granted. Given this conﬂict of attitudes, they describe three implications for research on this topic. Hansen and Kautz[52] argue that if software companies are to survive, it is critical that they improve continuously the services that they provide. Such improvement depends, to a great extent, on the organization’s capability to share knowledge and thus on the way knowledge ﬂows in an organization. To investigate knowl- edge ﬂow, they introduced a tool to map the ﬂows of organiza- tional knowledge in a software development company. Using their new method, they identify potential threats to knowledge ﬂows in an organization. Also using ﬂow diagrams, Al-Shehab et al. [2] describe how learning from analyses of past projects and from the issues that contributed to their failure is becoming a major stage in the risk management process. They introduce cau- sal mapping as a method to visualise cause and effect in risk net- works. They claim that their method is useful for organizational learning, because it helps people to visualise differences in perceptions. In [30], Desouza et al. describe two ways of conducting project postmortems. They stress that learning through postmortems must occur at three levels: individual, team, and organization. The paper describes guidelines for when to select different kinds of postmor- tem, depending on the context and the knowledge that is to be shared. The authors also argue that postmortems must be woven into the fabric of current project management practices. Salo[99] also studies postmortem techniques and concludes that existing techniques lack a systematic approach to validating iteratively the implementation and effectiveness of action taken to improve software processes. Salo studies the implementation of a method to remedy this and observes that the organizational level can only beneﬁt from the learning of project teams if the knowledge and reasoning behind the improvements to processes are converted into an explicit format such that it can be utilized for learning at the organizational level. In [84], Melnik and Maurer discuss the role of conversation and social interaction effective knowledge sharing in an agile process. Their main ﬁnding suggests that the focus on pure codiﬁcation is the principal reason that Tailoristic teams fail to share knowledge effectively. Moving the focus from codiﬁcation to socialization, Bjørnson and Dingsøyr [16] investigated knowledge sharing through a mentor programme in a small software consultancy company. They describe how mentor programmes could be chan- ged to improve the learning in the organization. They also identify several unofﬁcial learning schemes that could be improved. 4.2. Behavioural schools The behavioural aspects of knowledge management are covered in three schools in Earl’s framework: the organizational, spatial, and strategic schools. In our review, we found three empirical studies and two reports of lessons learned in the organizational school, no empirical study and one report of lessons learned in the spatial school, and three empirical studies and nine reports', 'studies and two reports of lessons learned in the organizational school, no empirical study and one report of lessons learned in the spatial school, and three empirical studies and nine reports of lessons learned in the strategic school. We present the main con- cepts and ﬁndings from the organizational and strategic schools. 4.2.1. Organizational The organizational school focuses on describing the use of orga- nizational structures (networks) to share or pool knowledge. These structures are often referred to as ‘‘knowledge communities”. Work on knowledge communities is related to work on communi- ties of practice as described in Section2.2. An overview of our ﬁnd- ings from this school is presented inTable 8. The role of networking as an approach to knowledge manage- ment has been investigated in three settings where software is developed. Grabher and Ibert[51] discuss what types of network exist in companies, where one case is a software company based in Germany. Mathiassen and Vogelsang[82] discuss how to imple- ment software methods in practice and use two concepts from knowledge management: networks and networking. The network perspective emphasizes the use of technology for sharing knowl- edge, while networking focuses on trust and collaboration among practitioners involved in software development. The authors stress that knowledge management is highly relevant to understand challenges when introducing new methods for software engineer- ing, and that every company have to ﬁnd a suitable balance be- tween strategies. In the case company, the emphasis on networks and networking changed considerably during the project. Nörbjerg et al.[89] discuss the advantages and limitations of knowledge net- works. They base their discussion on an analysis of two networks related to software process improvement in a medium-sized soft- ware company in Europe. Their main ﬁnding is that building a net- work on existing informal networks gave the highest value to the organization. 4.2.2. Strategic In the strategic school, knowledge management is seen as a dimension of competitive strategy. Skandia’s views are a prime example [111]. Developing conceptual models of the purpose and nature of intellectual capital has been a central issue. An overview of our ﬁndings from this school is presented inTable 9. One important issue in the literature on knowledge manage- ment has been to identify the factors that lead to the successful management of knowledge. Feher and Gabor [47] developed a model of the factors that support knowledge management. The model includes technological, organizational and human resource factors, and was developed on the basis of data on 72 software development organizations that are contained in the European database for the improvement of software processes. Another issue of strategic importance is the processes that are in place to facilitate learning. Arent and Nørbjerg [5] analysed three industrial projects for the improvement of software pro- cesses, in order to identify the learning processes used. They found that both tacit and explicit knowledge were important for improv- Table 8 Concepts and main ﬁndings for the organizational school School Concepts Main ﬁndings Reference Organizational How networks are used in software engineering Networks should be used in addition to other activities when introducing new software engineering methods [82] Description of the role of networks [51] Networks built on existing informal networks are more likely to be successful [89] 1062 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068', 'ing practice, and that improvement requires ongoing interaction between different learning processes. Trittmann [114] distinguish between two types of strategy for managing knowledge: ‘‘mechanistic” and ‘‘organic”. Organic knowledge management pertains to activities that seek to foster innovation, while mechanistic knowledge management aims at using existing knowledge. A survey of 28 software companies in Germany supported the existence of two such strategies. This work parallels the works of Hansen et al. on codiﬁcation and personali- zation as important strategies for managing knowledge in the ﬁeld of management science. 4.3. Knowledge management in general Some studies could not be classiﬁed using Earl’s framework. These studies can be placed in a broad category that encompasses works that seek to identify the impact of knowledge management initiatives (two empirical studies), and works that investigate knowledge management per se (two empirical studies). An over- view of these are presented inTable 10. 4.3.1. The impact of knowledge management initiatives Ajila and Sun [1] investigated two approaches to delivering knowledge to software development projects: ‘‘push” and ‘‘pull”. ‘‘Push” means using tools to identify and provide knowledge to po- tential users. ‘‘Pull” means that users themselves have to use repositories and other tools to identify relevant knowledge. On the basis of a survey of 41 software companies in North America, the authors claim that pulling leads to more effective software development. Ravichandran and Rai [95] studied two models for how the embedding and creation of knowledge inﬂuence software process capability. Embedding refers to the process of employing knowl- edge in standard practices, for example through making work rou- tines, methods and procedures. They found support for a model where knowledge creation has an effect on process capability when the knowledge is embedded after it is created. This means that knowledge has to be internalized before it can be used to im- prove processes. The study was done as a survey of 103 Fortune 1000 companies and federal and state government agencies in the US. 4.3.2. Knowledge management per se Ward and Aurum[117] describe current practices for managing knowledge in two Australian software companies and explain how leadership, technology, culture, and measurements enable knowl- edge to be managed effectively and efﬁciently. They found leader- ship to be the most signiﬁcant positive factor for the management of knowledge, but that the tools, techniques, and methodologies that the companies were using were not adequate for managing knowledge effectively. Desouza et al.[29] examined what factors contribute to the use of knowledge artefacts in a survey of 175 employees in a software engineering organization. They speciﬁcally looked at factors that govern the use of explicit knowledge. They found that the follow- ing factors relate to the use of explicit knowledge: perceived com- plexity, perceived relative advantage, and perceived risk. 5. Discussion In this study, we have identiﬁed far more studies, particularly empirical studies, than have been reported in previous assess- ments by Rus et al.[98], Lindvall et al.[78] and Dingsøyr and Con- radi [35]. We have shown that although there are not many empirical studies, except for in the systems and engineering schools, there are either empirical studies or reports of lessons learned in all schools except the economic school. Thus, research on knowledge management in software engineering seems to be slowly gaining a broader focus, although research on knowledge management in software engineering is still somewhat distanced from mainstream research on knowledge management. If we compare the studies found in software engineering to the research directions suggested by Alavi Leidner[3], we see that soft- ware engineering has primarily addressed the storage and retrieval', 'If we compare the studies found in software engineering to the research directions suggested by Alavi Leidner[3], we see that soft- ware engineering has primarily addressed the storage and retrieval of knowledge, while topics such as knowledge creation, the trans- fer and application of knowledge still needs more attention. We now discuss our ﬁndings. We begin with a discussion con- cerning our ﬁrst two research questions, then the third, outline implications for research and practice, and end with a discussion of the validity of our study. 5.1. Major knowledge management concepts and ﬁndings To answer our two ﬁrst research questions, we organize the dis- cussion according to Earl’s framework, answering ‘‘what are the major knowledge management concepts that have been investi- gated in software engineering?” and ‘‘what are the major ﬁndings on knowledge management in software engineering?” In this discussion of what we found, we will also include a dis- cussion of how relevant we think these knowledge management schools are for software engineering. Software engineering is a large ﬁeld with several disciplines relevant to knowledge manage- ment, for example software process improvement. One recent development in software engineering, which has implications for knowledge management activities is whether a company seeks to have agile development processes in place, or rely on traditional development methods such as the waterfall process [86]. Agile Table 10 Concepts and main ﬁndings for studies of knowledge management in general School Concepts Main ﬁndings Reference Knowledge management in general The impact of knowledge management initiatives Knowledge pull leads to more effective knowledge management than knowledge push[1] Knowledge needs to be internalized to improve processes [95] Factors that enable effective knowledge management Leadership is the most important enabler for knowledge management [117] Factors that contribute to use of knowledge artefacts Perceived complexity, perceived advantage and perceived risk contribute to the use of knowledge management artefacts [29] Table 9 Concepts and main ﬁndings for the strategic school School Concepts Main ﬁndings Reference Strategic What factors contribute to successful knowledge management Suggested model, including technological, organizational and human resource factors [47] What learning processes are used in practice Ongoing interaction between different learning processes important to improve practice [5] What strategies exist for managing software engineering knowledge Found evidence of strategies for codiﬁcation and personalization in software companies [114] F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068 1063', 'software development will focus mainly on knowledge manage- ment activities related to tacit knowledge, while the traditional development processes will need activities related to explicit knowledge. In the following, we will discuss the concepts identi- ﬁed in research, and give our opinion on what we think are the most relevant research areas to support agile and traditional soft- ware development. The ﬁnal selection of papers was divided between the techno- cratic and behavioural schools, with an emphasis on the techno- cratic side. This was not surprising, given the general focus of software engineering on the construction of tools and processes. We did not ﬁnd any examples of what Earl considers economic schools. The reason for this can be twofold, either few software companies track their intellectual capital, or there is little interest in reporting ﬁndings from such activities in software engineering. 5.1.1. Technocratic schools The technocratic schools applied in software engineering can be interesting for other knowledge-intensive disciplines as software engineers are likely to easily adopt new information technology. Looking closer at these schools, we saw a heavy focus on the sys- tems and engineering schools, with barely any mention of the car- tographic school. The heavy focus on the systems school can be explained by the software engineering ﬁeld’s focus on implement- ing new tools[35]. For this school, there is a greater number of les- sons learned reports than empirical studies. The main concepts we identiﬁed in this school were the development and use of knowl- edge repositories. There was, however, little to no overlap between the identiﬁed papers. As for ﬁndings in this school, there are two studies of the use of knowledge repositories over time, which shows that such tools are actually in use, and have more beneﬁts than the obvious. In Section 2.1, we referred to critique of the codiﬁcation strategy, and espe- cially a belief that knowledge repositories easily can generate information junkyards. There is not any evidence to support such a claim in software engineering, but we believe there is a heavy publication bias towards success stories. But the cases described in this review shows that it is possible to successfully implement knowledge repositories to work in software companies. The engineering school is the school that received the most empirical attention, according to our review. Again, we identiﬁed two main areas within this school: those focusing on the entire software process and those focusing on particular activities within the process. Within the papers focusing on speciﬁc activities, we identiﬁed four main areas: formal routines, mapping of knowledge ﬂows, project reviews, and social interaction. As with the systems school, there is little or no overlap between the empirical studies. A possible explanation for the heavy empirical focus within this school is the close ﬁt with work on the improvement of software development processes. For the ﬁndings on whole development process, we see that having an established development process can both improve communication and learning, but we also see that it is important to focus also on sharing tacit knowledge in order to change practice. In relation to development processes for software, the systems and engineering schools support sharing of explicit knowledge, which is important in traditional software development. Both of these schools require a technical infrastructure in order to facilitate knowledge sharing. However, a ﬁnding both from studies in other ﬁelds of the systems school[60] and studies of a speciﬁc engineer- ing activities, electronic process guides, is that it is difﬁcult to get such technology in actual use [37]. However, many companies have invested in such infrastructure, and this indicates that we need a better understanding of the factors that lead to effective knowledge sharing within these two schools.', 'have invested in such infrastructure, and this indicates that we need a better understanding of the factors that lead to effective knowledge sharing within these two schools. That there are so few papers in the cartographic school is inter- esting. One possible explanation is that the ‘‘yellow pages” systems are considered ‘‘simple” and undeserving of attention. Earl refers to a number of consulting companies using this school, including McKin- sey and Bain (see Ref.[53]). However, as the lone study in software engineering shows, such tools have uses other than the obvious, and can stimulate learning both at individual and organizational le- vel. One argument for this school is that although it requires a tech- nical infrastructure, the investment is low because there is no need to codify knowledge. This is a school which is relevant for agile soft- ware development, and because of the growing number of such development practices as well as the low cost, we think this is a school which requires further research. A counter-argument could be that tacit knowledge is not as relevant for software development as explicit knowledge, but we see from research on agile develop- ment that it is possible to develop high-quality software without making much use of explicit knowledge management[108]. 5.1.2. Behavioural schools In the behavioural schools, we found a limited number of pa- pers focusing on organizational and strategic aspects, and no pa- pers focusing on spatial aspects. The three studies in the organizational school discuss the use of people networks in software organizations. Two of the studies investigated the improvement of software development processes. In Earl’s taxonomy, both intra- and interorganizational communi- ties are mentioned as examples. In the software engineering liter- ature, we only ﬁnd studies made in single organizations. Also, a much debated topic in general knowledge management is what ac- tions management can take in order to support this type of knowl- edge sharing, what some refer to as knowledge governance. How much should be formal, and what should be left to employees to organize themselves? As for relevance for software engineering, we believe that this school has the potential to deliver inexpensive solutions for com- panies, although as the studies in software engineering indicate, there is a debate on whether such initiatives are best left to grow by themselves or if the management should have an active involve- ment. For software engineering, it could be useful with studies that address this strategy in relation to speciﬁc challenges for software development, like challenges with new technology, process improvement or understanding customer needs. This school is rel- evant for organizations that run multidisciplinary projects, which we believe is the case for most software companies, whether they do agile or traditional development. As for the spatial school, no empirical studies on software engi- neering were found in this category. The question is then: Is this something that could be relevant in a software engineering setting? The role of open-plan ofﬁces has been studied in other ﬁelds, and this is something that also should have an impact on how knowledge is shared in software teams. Many of the agile development methods recommend open-plan ofﬁces, and knowing more about what spe- ciﬁc effects this has on software development would be valuable. The empirical studies in the strategic school focus on factors pertaining to successful knowledge management, learning pro- cesses, and types of strategy for managing knowledge. It was, per- haps, to be expected that there would not be many articles discussing the strategic importance of knowledge in software engi- neering supported by empirical ﬁndings, because its importance is assumed in most published works on knowledge management in software engineering. 5.2. Research methods Our third research question addressed research methods used:', 'assumed in most published works on knowledge management in software engineering. 5.2. Research methods Our third research question addressed research methods used: ‘‘What research methods have been used within the area so far?” 1064 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068', 'Of the 68 studies identiﬁed, 39 were reports of lessons learned and 29 were empirical studies. Case studies constituted the largest number of empirical studies (seeTable 4), followed by ﬁeld studies and action research. It is positive that the emphasis on empirical studies has increased (see Fig. 1). The apparent dip in 2006 is due to the time at which the search was conducted. We searched the databases in August and most compilers of databases take some months to index their papers; hence, we can only claim to have covered the ﬁrst third of 2006 fully. The research methods in the studies that we selected are dom- inated by case studies, both single and multiple. This is not surpris- ing, considering our limitation on only including studies that performed tests in industry. We found one experiment, and it is not surprising that there are few experiments. Knowledge manage- ment is a broad ﬁeld, and it is difﬁcult to isolate factors for exper- iments without making the experiment irrelevant. An important question is then: Is it the right mixture of research methods that are applied to study knowledge management in soft- ware engineering? Given the broad nature of knowledge manage- ment, we believe it is right to have a large number of case studies. But as the ﬁeld matures, and we would like to see more studies of the effects of knowledge management, we think we need more in- depth studies in companies, which call for more studies oriented towards ethnography. Glass et al.[50] found that empirical studies constitute about 5% of published research in software engineering as a whole. Compar- ing our ﬁnal ﬁndings to the results from our ﬁrst rough sorting of papers, our ﬁnal selection constituted about 3% of the initially se- lected papers. If we assume that Glass’s data are representative for the area that we studied within software engineering, we could extrapolate that about 70% of those papers would be conceptual analysis and concept implementation. Most of the papers dis- carded were indeed conceptual analysis and concept implementa- tion without empirical testing, our results do however, not show a discard number on the empirical criterion as high as 70%. Many studies were also excluded because they were not relevant to either software engineering or knowledge management. Therefore it seems that empirical studies constitute a larger part of the stud- ies on knowledge management in software engineering than in software engineering in general. 5.3. Implications for research and practice This systematic review has implications both for researchers planning new studies of knowledge management initiatives in software companies, and for practitioners working in software companies who would like to design knowledge management ini- tiatives to meet local needs. 5.3.1. Implications for research For research, we think it is important to have in mind that what kind of knowledge management activities a company should en- gage in should be determined by how the company develops soft- ware. We have distinguished between two types of development which has implications for strategy for knowledge management, namely traditional and agile development. In this systematic review, we have seen that the knowledge management schools associated with traditional software develop- ment so far has received the most attention, namely the systems and engineering schools. This is in line with the observations of Buono and Poulfelt[20], indicating that knowledge management in software engineering is mainly focusing on ﬁrst generation knowledge management in Section2.1. We believe the schools that are relevant to agile software devel- opment should be given further attention in the future, as this trend seems to have much inﬂuence on industry practice today. Another issue in deciding on priorities for research is the cost of implementing activities in the schools. In general, the schools which do not require codiﬁcation and a technical infrastructure', 'Another issue in deciding on priorities for research is the cost of implementing activities in the schools. In general, the schools which do not require codiﬁcation and a technical infrastructure will be less expensive than the others. Therefore, we argue that in particular the organizational school should be further re- searched as this school is both relevant for agile and traditional software development, and is inexpensive. Also, the cartographic and spatial schools are good candidates for further research. As for research methods applied, we think there should be a larger fo- cus on in-depth studies, shown through a larger use of ethno- graphic methods. 5.3.2. Implications for practice As we indicated in implications for research, the technocratic schools are closely related to traditional software development while the behavioural schools are more related to the agile approach to development. The main consideration for practitioners is thus that organizations developing software through a tradi- tional approach will probably beneﬁt more from the technocratic schools, while agile teams would beneﬁt more from behavioural schools. Practitioners following a traditional approach can ﬁnd some empirical papers and several lessons learned reports on how to build a knowledge repository. Even though all papers we identiﬁed within the systems school are positive it is important to remember the objections to following a pure codiﬁcation strategy we men- tioned in Section2.1. We believe there is potential bias in the num- ber of positive reports from this school versus those who report negative results. Our ﬁndings from the engineering school also support this view, where several papers underline the importance of not focusing exclusively on codiﬁcation. An advantage of follow- ing the technocratic approach to knowledge management is that there is more material available within this ‘‘classical” school. A disadvantage is the cost of implementing strategies relying heavily on codiﬁcation. The most important ﬁnding from the behavioural schools with implications for practitioners developing in an agile environment would be that network building is more likely to be successful if they are built on already existing networks. Also, the need for diversity in both learning processes and strategies are stressed as important in order to improve practice. An advantage of the behav- ioural approach to knowledge management is the reduced cost compared to implementing the more application heavy solutions in the technocratic school. However, it has its disadvantage in the relatively few publications on this theme to learn from. 5.4. Limitations The main threats to validity in this systematic review are three- fold: our selection of the studies to be included, correct classiﬁca- tion of studies according to Earl’s framework of schools in knowledge management, and potential author bias. As for the selection of studies, only one researcher read through and discarded the ﬁrst results on the basis of the papers’ titles. However, in cases where there was doubt, the papers were in- cluded in the next stage. The second and third selection stages, which were based on abstracts and full papers, were carried out by both researchers and we observed a ‘good’ degree of consensus. In cases where there was disagreement, the issue was discussed until consensus was reached. Concerning the classiﬁcation of studies, both researchers classi- ﬁed all papers individually before comparing the results. As before, in cases where there was disagreement, the issue was discussed until consensus was reached. Finally, there is a potential bias in that both authors have writ- ten papers that were included in the review. Where only one F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068 1065', 'author had participated in the primary study, the other author decided whether or not to include it. 6. Conclusion This systematic review has addressed the following research questions. (1) What are the major knowledge management con- cepts that have been investigated in software engineering? (2) What are the major ﬁndings on knowledge management in soft- ware engineering? (3) What research methods have been used within the area so far? For the ﬁrst research question, our main ﬁndings are: /C15The majority of studies of knowledge management in software engineering relate to technocratic and behavioural aspects of knowledge management. /C15The studies that report on concepts within the ﬁelds of techno- cratic and behavioural aspects have very little overlap. /C15There are few studies relating to economic, spatial and carto- graphic approaches to knowledge management. For the second research question, we found that: /C15As for the concepts, the ﬁndings are also divided and have very little overlap. /C15The major ﬁnding, which is repeated over several papers and across several schools is the need to not focus exclusively on explicit knowledge but also on tacit knowledge. For the third research question, we found that: /C15The majority of reports of applications of knowledge manage- ment in the software engineering industry are reports of lessons learned, not scientiﬁc studies. /C15Of the reports categorized as empirical studies, more than half of the reports are case studies. /C15Our search returned ﬁeld studies, action research, ethnographic studies, and one laboratory experiment. The main implication for research is to focus more on the orga- nizational school, while we believe practitioners should focus also on activities to manage tacit knowledge when working on knowl- edge management initiatives. Acknowledgements We are grateful to Reidar Conradi at the Department of Com- puter and Information Science, Norwegian University of Science and Technology, and Tore Dybå at SINTEF ICT for comments on an earlier version of this article. We also thank Chris Wright for proofreading and useful comments. This work was partially funded by the Research Council of Norway through the project Evidence- Based Software Engineering (181685/I30). Appendix See Tables 11 and 12. References [1] S.A. Ajila, Z. Sun, Knowledge management: impact of knowledge delivery factors on software product development efﬁciency, in: Proceedings of the IEEE International Conference on Information Reuse and Integration, Las Vegas, NV, United States, 2004, pp. 320–325. [2] A.J. Al-Shehab, R.T. Hughes, G. Winstanley, Facilitating organisational learning through causal mapping, in: Proceedings of the Seventh International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2005, pp. 145–154. [3] M. Alavi, D.E. Leidner, Review: knowledge management and knowledge management systems: conceptual foundations and research issues, MIS Quarterly 25 (1) (2001) 107–136. [4] N. Angkasaputra, D. Pfahl, E. Ras, S. Trapp, The collaborative learning methodology CORONET-train: implementation and guidance, in: Proceedings of the Fourth International Workshop on Learning Software Organizations, Springer Verlag, Chicago, IL, USA, 2002, pp. 13–24. [5] J. Arent, J. Nørbjerg, Software process improvement as organizational knowledge creation: a multiple case analysis, in: Proceedings of the Hawaii International Conference on System Sciences, Maui, USA, 2000, p. 105. [6] J. Arent, J. Nørbjerg, M.H. Pedersen, Creating organizational knowledge in software process improvement, in: Proceedings of the 2nd Workshop on Learning Software Organizations, Oulu, Finland, 2000, pp. 81–92. [7] L. Argote, B. McEvily, R. Reagans, Managing knowledge in organizations: an integrative framework and review of emerging themes, Management Science 49 (4) (2003) 571–582. [8] C. Argyris, Overcoming Organizational Defences: Facilitating Organizational', 'integrative framework and review of emerging themes, Management Science 49 (4) (2003) 571–582. [8] C. Argyris, Overcoming Organizational Defences: Facilitating Organizational Learning, Prentice Hall, Boston, 1990. [9] C. Argyris, D.A. Schön, Organizational learning II: theory, method and practise, Organization Development Series, Addison Wesley, Reading, MA, USA, 1996. [10] A. Aurum, R. Jeffrey, C. Wohlin, M. Handzic, Managing Software Engineering Knowledge, Springer Verlag, Berlin, 2003. [11] M.d.O. Barros, C.M.L. Werner, G.H. Travassos, Supporting risks in software project management, Journal of Systems and Software 70 (1–2) (2004) 21–35. [12] V.R. Basili, G. Caldiera, F. McGarry, R. Pajerski, G. Page, The software engineering laboratory – an operational software experience factory, in: Proceedings of the 14th International Conference on Software Engineering, 1992, pp. 370–381. [13] V.R. Basili, G. Caldiera, H.D. Rombach, The experience factory, in: J.J. Marciniak (Ed.), Encyclopedia of Software Engineering, 1, John Wiley, New York, 1994, pp. 469–476. [14] R. Baskerville, J. Pries-Heje, Knowledge capability and maturity in software management, (1999). [15] A. Birk, A Knowledge Management Infrastructure for Systematic Improvement in Software Engineering, Dr. Ing thesis, University of Kaiserslautern, Department of Informatics, 2000. [16] F.O. Bjørnson, T. Dingsøyr, A study of a mentoring program for knowledge transfer in a small software consultancy company, in: Lecture Notes in Computer Science 3547, Springer Verlag, Heidelberg, 2005, pp. 245–256. [17] F.O. Bjørnson, Knowledge Management in Software Process Improvement, PhD thesis, Norwegian University of Science and Technology, Department of Computer and Information Science, 2007. [18] F.O. Bjørnson, T. Stålhane, Harvesting knowledge through a method framework in an electronic process guide, in: Proceedings of the Seventh International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2005, pp. 86–90. [19] P. Brössler, Knowledge management at a software engineering company – an experience report, in: Proceedings of the 1st Workshop on Learning Software Organizations, Kaiserslautern, Germany, 1999, pp. 77–86. [20] A.F. Buono, F. Poulfelt, Challenges and Issues in Knowledge Management, Information Age Publishing, Greenwich, CT, USA, 2005. [21] B. Chatters, Implementing an experience factory: maintenance and evolution of the software and systems development process, in: Proceedings of IEEE Table 11 Categorized articles, extended Systems Cartographic Engineering Economic Organizational Spatial Strategic Emp [11,18,22,33,70,109] [32] [2,6,14,16,25,30,49,52,84,99,106,116] [51,82,89] [5,47,114] LL [4,12,21,27,55–57,66,67,71,74,77,80,85,87,94, 97,100,102,105] [4,36,46,62,63,81,96,105,113] [58,59] [28] [19,38,44,61,73,90,93,101,119] Table 12 Overview of research methods, extended Research method KM/SE Action research [5,15,18] Case study [2,6,14,22,30,49,51,70,82,89,99,106,109,116] Ethnography [32,33] Laboratory experiment [84] Field study [11,25,47,52,114] 1066 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068', 'International Conference of Software Maintenance, 1999, pp. 146–151. [22] C.M. Chewar, D.S. McCrickard, Links for a human-centered science of design: integrated design knowledge environments for a software development process, in: Proceedings of the Hawaii International Conference on System Sciences, Big Island, HI, United States, 2005, p. 256. [23] C.W. Choo, The Knowing Organization: How Organizations Use Information to Construct Meaning, Create Knowledge, and Make Decisions, Oxford University Press, New York, 1998. [24] P.H. Christensen, The wonderful world of knowledge management, in: A.F. Buono, F. Poulfelt (Eds.), Challenges and Issues in Knowledge Management, Information Age Publishing, Greenwich, CT, USA, 2005, pp. 337–364. [25] R. Conradi, T. Dybå, An empirical study on the utility of formal routines to transfer knowledge and experience, in: Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering, Association for Computing Machinery, Vienna, Austria, 2001, pp. 268–276. [26] T.H. Davenport, L. Prusak, Working Knowledge: How Organizations Manage What They Know, Harvard Business School Press, Boston, MA, USA, 1998. [27] R. De Almeida Falbo, L.S.M. Borges, F.F.R. Valente, Using knowledge management to improve software process performance in a CMM level 3 organization, in: Proceedings of the Fourth International Conference on Quality Software, IEEE Computer Society, Braunschweig, Germany, 2004, pp. 162–169. [28] K.C. Desouza, Facilitating tacit knowledge exchange, Communications of the ACM 46 (6) (2003) 85–88. [29] K.C. Desouza, Y. Awazu, Y. Wan, Factors governing the consumption of explicit knowledge, Journal of the American Society for Information Science and Technology 57 (1) (2006) 36–43. [30] K.C. Desouza, T. Dingsøyr, Y. Awazu, Experiences with conducting project postmortems: reports versus stories, Software Process Improvement and Practice 10 (2) (2005) 203–215. [31] M. Dierkes, A. Berthoin Antal, J. Child, I. Nonaka, Handbook of Organizational Learning and Knowledge, Oxford University Press, Oxford, 2001. [32] T. Dingsøyr, H.K. Djarraya, E. Royrvik, Practical knowledge management tool use in a software consulting company, Communications of the ACM 48 (12) (2005) 96–100. [33] T. Dingsøyr, E. Royrvik, An empirical study of an informal knowledge repository in a medium-sized software consulting company, in: Proceedings of the International Conference on Software Engineering, Portland, OR, United States, 2003, pp. 84–92. [34] T. Dingsøyr, Knowledge Management in Medium-Sized Software Consulting Companies, PhD thesis, Norwegian University of Science and Technology, Department of Computer and Information Science, 2002. [35] T. Dingsøyr, R. Conradi, A survey of case studies of the use of knowledge management in software engineering, International Journal of Software Engineering and Knowledge Engineering 12 (4) (2002) 391–414. [36] T. Dingsøyr, G.K. Hanssen, Extending agile methods: postmortem reviews as extended feedback, in: Proceedings of the Fourth International Workshop on Learning Software Organizations, Springer Verlag, Chicago, IL, USA, 2002, pp. 4–12. [37] T. Dingsøyr, N.B. Moe, The impact of employee participation on the use of an electronic process guide: a longitudinal case study, IEEE Transactions on Software Engineering 34 (2) (2008) 212–225. [38] H.D. Doran, Agile knowledge management in practice, in: Proceedings of the Sixth International Workshop on Learning Software Organizations, Springer Verlag, Banff, Canada, 2004, pp. 137–143. [39] T. Dybå, Enabling Software Process Improvement: An Investigation on the Importance of Organizational Issues, PhD thesis, Norwegian University of Science and Technology, Department of Computer and Information Science, 2001. [40] T. Dybå, T. Dingsøyr, G.K. Hanssen, Applying systematic reviews to diverse study types: an experience report, in: Proceedings of the ESEM, Madrid, Spain, 2007.', '2001. [40] T. Dybå, T. Dingsøyr, G.K. Hanssen, Applying systematic reviews to diverse study types: an experience report, in: Proceedings of the ESEM, Madrid, Spain, 2007. [41] T. Dybå, B.A. Kitchenham, M. Jørgensen, Evidence-based software engineering for practitioners, IEEE Software 22 (1) (2005) 58–65. [42] M. Earl, Knowledge management strategies: towards a taxonomy, Journal of Management Information Systems 18 (1) (2001) 215–233. [43] M. Easterby-Smith, M.A. Lyles, The Blackwell Handbook of Organizational Learning and Knowledge Management, Blackwell Publishing, Malden, MA, USA, 2003. [44] C. Ebert, J. De Man, F. Schelenz, e-R&D: effectively managing and using R&D knowledge, in: A. Aurum et al. (Eds.), Managing Software Engineering Knowledge, Springer Verlag, Berlin, 2003, pp. 339–359. [45] J.S. Edwards, Managing software engineers and their knowledge, in: A. Aurum et al. (Eds.), Managing Software Engineering Knowledge, Springer Verlag, Berlin, 2003, pp. 5–27. [46] F.F. Fajtak, Kick-off workshops and project retrospectives: a good learning software organization practice, in: Proceedings of the Seventh International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2005, pp. 76–81. [47] P. Feher, A. Gabor, The role of knowledge management supporters in software development companies, Software Process Improvement and Practice 11 (3) (2006) 251–260. [48] R.L. Feldmann, K.-D. Althoff, On the status of learning software organisations in the year 2001, in: Proceedings of the Learning Software Organizations Workshop, Springer Verlag, Kaiserslautern, Germany, 2001, pp. 2–6. [49] H. Folkestad, E. Pilskog, B. Tessem, Effects of software process in organization development – a case study, in: Proceedings of the Sixth International Workshop on Learning Software Organizations, Springer Verlag, Banff, Canada, 2004, pp. 153–164. [50] R.L. Glass, V. Ramesh, V. Iris, An analysis of research in computing disciplines, Communications of the ACM 47 (6) (2004) 89–94. [51] G. Grabher, O. Ibert, Bad company? The ambiguity of personal knowledge networks, Journal of Economic Geography 6 (3) (2006) 251–271. [52] B.H. Hansen, K. Kautz, Knowledge mapping: a technique for identifying knowledge ﬂows in software organisations, in: Lecture Notes in Computer Science 3281, 2004, pp. 126–137. [53] M.T. Hansen, N. Nohria, T. Tierney, What is your strategy for managing knowledge?, Harvard Business Review 77 (2) (1999) 106–116. [54] D. Hislop, Mission impossible? Communicating and sharing knowledge via information technology, Journal of Information Technology 17 (2002) 165–177. [55] F. Houdek, C. Bunse, Transferring experience – a practical approach and its application on software inspections, in: Proceedings of the 1st Workshop on Learning Software Organizations, Kaiserslautern, Germany, 1999, pp. 59–68. [56] F. Houdek, K. Schneider, E. Wieser, Establishing experience factories at Daimler-Benz. An experience report, in: Proceedings of the 20th International Conference on Software Engineering, Kyoto, Japan, 1998, pp. 443–447. [57] P. Jalote, Knowledge infrastructure for project management, in: A. Aurum et al. (Eds.), Managing Software Engineering Knowledge, Springer Verlag, Berlin, 2003, pp. 361–375. [58] C. Johannson, P. Hall, M. Coquard, Talk to Paula and Peter – they are experienced, in: Proceedings of the 1st Workshop on Learning Software Organizations, Kaiserslautern, Germany, 1999, pp. 69–76. [59] T. Kahkonen, Agile methods for large organizations – building communities of practice, in: Proceedings of the Agile Development Conference, IEEE Computer Society, Salt Lake City, UT, United States, 2004, pp. 2–10. [60] A. Kankanhalli, B.C.Y. Tan, K.-K. Wei, Contributing knowledge to electronic knowledge repositories: an empirical investigation, MIS Quarterly 29 (2005) 113–143. [61] K. Kautz, K. Thaysen, Knowledge, learning and IT support in a small software company, Journal of Knowledge Management 5 (4) (2001) 349–357.', '113–143. [61] K. Kautz, K. Thaysen, Knowledge, learning and IT support in a small software company, Journal of Knowledge Management 5 (4) (2001) 349–357. [62] P. Kess, H. Haapasalo, Knowledge creation through a project review process in software production, International Journal of Production Economics 80 (1) (2002) 49–55. [63] P. Kettunen, Managing embedded software project team knowledge, IEE Software 150 (6) (2003) 359–366. [64] B.A. Kitchenham, Procedures for Performing Systematic Reviews, in Technical Report TR/SE-0401, 2004, Keele University. [65] B.A. Kitchenham, T. Dybå, M. Jørgensen, Evidence-based software engineering, in: Proceedings of the International Conference on Software Engineering, 2004, pp. 273–281. [66] S. Koenig, Integrated process and knowledge management for product deﬁnition, development and delivery, in: Proceedings of the IEEE International Conference on Software-Science, Technology & Engineering, 2003, pp. 133–141. [67] A. Koennecker, J. Ross, L. Graham, Lessons learned from the failure of an experience base initiative using a bottom-up development paradigm, in: Proceedings of the 24th Annual NASA Software Engineering Workshop, Washington, USA, 1999. [68] D. Kolb, Experiential Learning: Experience as the Source of Learning and Development, Prentice Hall, Englewood Cliffs, NJ, USA, 1984. [69] D. Kolb, Management and the learning process, in: K. Starkey (Ed.), How Organizations Learn, Thomson Business Press, London, 1996, pp. 270–287. [70] F. Kurniawati, R. Jeffery, The long-term effects of an EPG/ER in a small software organisation, in: Proceedings of the Australian Software Engineering Conference, Melbourne, Vic., Australia, 2004, pp. 128–136. [71] D. Landes, K. Schneider, F. Houdek, Organizational learning and experience documentation in industrial software projects, International Journal of Human Computer Studies 51 (3) (1999) 643–661. [72] S.-h. Liao, Knowledge management technologies and applications – literature review from 1995 to 2002, Expert Systems with Applications 25 (2003) 155– 164. [73] J. Liebowitz, A look at NASA Goddard space ﬂight center’s knowledge management initiatives, IEEE Software 19 (3) (2002) 40–42. [74] M. Lindvall, P. Costa, R. Tesoriero, Lessons learned about structuring and describing experience for three experience bases, in: Proceedings of the Third International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2001, pp. 106–119. [75] M. Lindvall, I. Rus, Knowledge management in software engineering, IEEE Software 19 (3) (2002) 26–38. [76] M. Lindvall, I. Rus, Knowledge Management for Software Organizations, in: A. Aybüke et al. (Eds.), Managing Software Engineering Knowledge, Springer Verlag, Berlin, 2003, pp. 73–94. [77] M. Lindvall, I. Rus, Lessons learned from implementing experience factories in software organizations, in: Proceedings of the Fifth International Workshop on Learning Software Organizations, Bonner Köllen Verlag, Luzern, Switzerland, 2003, pp. 59–64. [78] M. Lindvall, I. Rus, R. Jammalamadaka, R. Thakker, Software Tools for Knowledge Management, in Technical Reports, DoD Data Analysis Center for Software, Rome, NY, 2001. [79] K. Lyytinen, D. Robey, Learning failure in information systems development, Information Systems Journal 9 (2) (1999) 85–101. F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068 1067', '[80] M. Markkula, Knowledge management in software engineering projects, in: Proceedings of the International Conference on Software Engineering and Knowledge Engineering, Kaiserslautern, Germany, 1999, pp. 20–27. [81] N. Martin-Vivaldi, P. Collier, S. Kipling, Peer performance coaching: accelerating organizational improvement through individual improvement, in: Proceedings of the 2nd Workshop on Learning Software Organizations, Oulu, Finland, 2000, pp. 103–112. [82] L. Mathiassen, L. Vogelsang, The role of networks and networking in bringing software methods to practice, in: Proceedings of the Hawaii International Conference on System Sciences, Big Island, HI, United States, 2005, p. 256. [83] R. McDermott, Why information technology inspired but cannot deliver knowledge management, California Management Review 41 (1999) 103–117. [84] G. Melnik, F. Maurer, Direct verbal communication as a catalyst of agile knowledge sharing, in: Proceedings of the Agile Development Conference, Salt Lake City, UT, United States, 2004, pp. 21–31. [85] K. Mohan, B. Ramesh, Managing variability with traceability in product and service families, in: Proceedings of the 35th Hawaii International Conference on System Sciences, 2002, pp. 1309–1317. [86] S. Nerur, V. Balijepally, Theoretical reﬂections on agile development methodologies, Communications of the ACM 50 (2007) 79–83. [87] E. Niemela, J. Kalaoja, P. Lago, Toward an architectural knowledge base for wireless service engineering, IEEE Transactions on Software Engineering 31 (5) (2005) 361–379. [88] I. Nonaka, H. Takeuchi, The Knowledge-Creating Company, Oxford University Press, New York, 1995. [89] J. Nørbjerg, T. Elisberg, J. Pries-Heje, Experiences from using knowledge networks for sustaining Software Process Improvement, in: Proceedings of the Eighth International Workshop on Learning Software Organizations, Rio de Janeiro, Brazil, 2006, pp. 9–17. [90] T.J. Ostrand, E.J. Weyuker, A learning environment for software testers at AT&T, in: Proceedings of the 2nd Workshop on Learning Software Organizations, Oulu, Finland, 2000, pp. 47–54. [91] M.C. Paulk, C.V. Weber, B. Curtis, The Capability Maturity Model: Guidelines for Improving the Software Process, Addison Wesley, Reading, MA, USA, 1995. [92] M. Polanyi, The Tacit Dimension, Doubleday, Garden City, NY, USA, 1967. [93] S. Ramasubramanian, G. Jagadeesan, Knowledge management at Infosys, IEEE Software 19 (3) (2002) 53. [94] E. Ras, G. Avram, P. Waterson, S. Weibelzahl, Using weblogs for knowledge sharing and learning in information spaces, Journal of Universal Computer Science 11 (3) (2005) 394–409. [95] T. Ravichandran, A. Rai, Structural analysis of the impact of knowledge creation and knowledge embedding on software process capability, IEEE Transactions on Engineering Management 50 (3) (2003) 270–284. [96] O.M. Rodriguez, A.I. Martinez, A. Vizcaino, J. Favela, M. Piattini, Identifying knowledge management needs in software maintenance groups: a qualitative approach, in: Proceedings of the Fifth Mexican International Conference in Computer Science, Colima, Mexico, 2004, pp. 72–79. [97] T.R. Roth-Berghofer, Learning from HOMER, a case-based help desk support system, in: Proceedings of the Sixth International Workshop on Learning Software Organizations, Springer Verlag, Banff, Canada, 2004, pp. 88–97. [98] I. Rus, M. Lindvall, S.S. Sinha, Knowledge Management in Software Engineering, in Technical Reports, DoD Data Analysis Center for Software, Rome, 2001. [99] O. Salo, Systematical validation of learning in agile software development environment, in: Proceedings of the Seventh International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2005, pp. 106–110. [100] K. Schneider, What to expect from software experience exploitation, Journal of Universal Computer Science 8 (6) (2002) 570–580. [101] K. Schneider, J.-P. Von Hunnius, V.R. Basili, Experience in implementing a', 'of Universal Computer Science 8 (6) (2002) 570–580. [101] K. Schneider, J.-P. Von Hunnius, V.R. Basili, Experience in implementing a learning software organization, IEEE Software 19 (3) (2002) 46–49. [102] J.-P.v.H. Schneider Kurt, Experience reports: process and tools, Effective Experience Repositories for Software Engineering (2003). [103] U. Schultze, D.E. Leidner, Studying knowledge management in information systems research: discourses and theoretical assumptions, MIS Quarterly 26 (2002) 213–242. [104] D.G. Schwartz, Encyclopedia of Knowledge Management, Idea Group Reference, 2006. [105] L. Scott, T. Stålhane, Experience repositories and the postmortem, in: Proceedings of the Fifth International Workshop on Learning Software Organizations, Bonner Köllen Verlag, Luzern, Switzerland, 2003, pp. 79–82. [106] J. Segal, Organisational learning and software process improvement: a case study, in: Proceedings of the Third International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2001, pp. 68–82. [107] P.M. Senge, The Fifth Discipline: The Art & Practise of the Learning Organisation, Century Business, London, 1990. [108] H. Sharp, H. Robinson, An ethnographic study of XP practice, Empirical Software Engineering 9 (2004) 353–375. [109] D. Skuce, Knowledge management in software-design – a tool and a trial, Software Engineering Journal 10 (5) (1995) 183–193. [110] R. Stata, Organizational learning: the key to management innovation, in: K. Starkey (Ed.), How Organizations Learn, Thomson Business Press, London, 1996, pp. 316–334. [111] K.E. Sveiby, The New Organizational Wealth: Managing and Measuring Knowledge-Based Assets, Berret-Koehler Publishers, San Francisco, 1997. [112] J. Swan, H. Scarbrough, J. Preston, Knowledge management – the next fad to forget people? in: Proceedings of the Seventh European Conference on Information Systems, 1999, pp. 668–678. [113] A.H. Torres, N. Anquetil, K. Oliveira, Pro-active dissemination of knowledge with learning histories, in: Proceedings of the Eighth International Workshop on Learning Software Organizations, Rio de Janeiro, Brazil, 2006, pp. 19–27. [114] R. Trittmann, The organic and the mechanistic form of managing knowledge in software development, in: Proceedings of the Third International Workshop on Learning Software Organizations, Springer Verlag, Kaiserslautern, Germany, 2001, pp. 22–26. [115] J.-W. van Aalst, Knowledge Management in Courseware Development, PhD thesis, Technical University Delft, 2001. [116] C.G.v. Wangenheim, S. Weber, J.C.R. Hauck, G. Trentin, Experiences on establishing software processes in small companies, Information and Software Technology 48 (9) (2006) 890–900. [117] J. Ward, A. Aurum, Knowledge management in software engineering – describing the process, in: Proceedings of the Australian Software Engineering Conference, Melbourne, Vic., Australia, 2004, pp. 137–146. [118] E. Wenger, Communities of Practice: Learning, Meaning and Identity, Cambridge University Press, Cambridge, 1998. [119] K.P. Yglesias, IBM’s reuse programs: knowledge management and software reuse, in: Proceedings of the International Conference on Software Reuse, 1998, pp. 156–164. 1068 F.O. Bjørnson, T. Dingsøyr / Information and Software Technology 50 (2008) 1055–1068']","['CONCEPTS AND FINDINGS ON KNOWLEDGE   MANAGEMENT IN SOFTWARE ENGINEERING Findings are grouped under each identified  concept related to knowledge management  in software engineering.  Development of knowledge repositories  and initial use: • It  is  an  approach  to  support  risk   in  project  management  and  design  activities. • Users should be involved in develop - ment. Use of knowledge repositories over time: • Benefits can be realized quickly, tool  remains useful over time, and more  benefits accrue over time. • This  type  of  tool  can  be  used  for   different  kinds  of  knowledge  than  originally intended. Use of cartographic system: • This  kind  of  tool  was  used  for:  allo- cating  resources,  searching  for com- petence, identifying project opportu - nities and upgrading skills. • It was observed that it enabled lear - ning practice at both individual and  company level. Managing knowledge on the software de - velopment process:  • It  is  feasible  to  use  knowledge  ma- nagement  as  underlying  theory  to  develop key process areas to supple - ment the CMM. • No  matter  what  knowledge  mana - gement  approach  you  pursue  in  SPI,  you need to create both tacit and ex - plicit knowledge. Tacit is necessary to   change  practice,  explicit  is  necessary   to  create  an  organizational memory. • A techno centric approach to SPI may  impose unnatural work practices on   an  organization  and  fails  to  take  ac- count  of  how  process improvements  might occur spontaneously within a  community of practice. • The iterative approach of Unified Pro- cess ensures large effects in terms of   learning,  but  Unified  Process  also   improves  on  communication  and  work distribution in the company. • It  is  possible  to  define  and  imple - ment  software  process  in  a  benefi - cial and  cost efficient  manner  in  small   software  organizations.  Special consi- derations  must  be  given  to  their   specific  business  goals,  models, cha- racteristics, and resource limitations.  Managing knowledge through formal rou- tines: • Formal  routines  must  be  supplemen- ted  by  collaborative,  social processes  to promote effective dissemination  and organizational learning. Mapping of knowledge flows: • Knowledge  mapping  can  successfully   help  an  organization  to  select rele - vant focus areas for planning future  improvement initiatives. • Causal maps for risk modeling contri - butes to organizational learning. Process for conducting project reviews to  extract knowledge:  • Creating  a  suitable  environment  for   reflection,  dialogue,  criticism,  and in- teraction is salient to the conducting  of a postmortem. • The  organizational  level  can  only  be- nefit  from  the  learning  of  project  teams if the knowledge and reasoning  behind the process improvements is  converted into such an explicit format  that it can be utilized for learning in  organizational level also. Implications of social interaction on knowle- dge sharing: • Increasing  the  level  of  reflection  in   mentor  programmes  can  result  in  more double looped learning. How networks are used in software engi - neering: • Networks should be used in addition  to other activities when introducing  new software engineering method. • Studies show that networks built on  existing informal networks are more  likely to be successful. Factors that contribute to successful know- ledge management: • Suggested  model,  including  techno - logical,  organizational  and  human re- source factors  Learning processes used in practice: • Researches suggest that ongoing in - teraction between different learning  processes is important to improve  practice  Strategies that exist for managing software  engineering knowledge: • Evidence was found of using codifica - tion and personalization in software  companies The impact of knowledge management ini- tiatives: • Knowledge  pull  leads  to  more  effec-', '• Evidence was found of using codifica - tion and personalization in software  companies The impact of knowledge management ini- tiatives: • Knowledge  pull  leads  to  more  effec- tive  knowledge  management  than  knowledge push. • Knowledge needs to be internalized to  improve processes. Factors that enable effective knowledge  management: • Leadership is the most important ena- bler for knowledge management. Factors that contribute to use of knowled- ge artefacts:  • Perceived complexity, perceived ad - vantage and perceived risk contribute  to the use of knowledge management  artefacts. FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Finn Olav Bjørnson, Torgeir Dingsøyr, Knowledge management in software engineering: A systematic review of studied concepts, findings and research me- thods used, Information and Software Technology, Volume 50, Issue 11, October 2008, Pages 1055 1068, ISSN 0950 5849, http://dx.doi.org/10.1016/j.inf- sof.2008.03.006. Keywords: Software engineering Knowledge management Learning software organization Software process improvement Who is this briefing for? Software engineers practitioners  who want to make decisions  about agile software develop- ment based on scientific eviden- ce. Where the findings come  from? All findings of this briefing were  extracted from the systematic  review conducted by Bjørnson  and Dingsøyr. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg This briefing reports evidence on concepts  and major findings of knowledge manage - ment initiatives in software engineering ba - sed on scientific evidence from a systematic  review']","**Title:** Enhancing Knowledge Management in Software Engineering: Key Insights from a Systematic Review

**Introduction:**
This Evidence Briefing aims to summarize the findings of a systematic review conducted by Bjørnson and Dingsøyr (2008), which explores the empirical studies of knowledge management (KM) initiatives in software engineering. The review identifies key concepts, findings, and research methods utilized in this field, providing valuable insights for both researchers and practitioners interested in improving KM practices within software organizations.

**Main Findings:**
1. **Conceptual Framework**: The review categorizes KM studies within Earl's framework, which includes technocratic (systems, cartographic, engineering) and behavioral (organizational, spatial, strategic) schools. Most studies focus on technocratic aspects, particularly systems and engineering, with limited exploration of economic, spatial, and cartographic approaches.

2. **Empirical Evidence**: A total of 68 studies were included in the review, with 29 classified as empirical studies. The majority of these studies utilized case study methodologies, highlighting the need for further empirical research to validate findings across diverse contexts.

3. **Explicit vs. Tacit Knowledge**: A recurring theme across multiple studies is the importance of balancing the management of explicit knowledge (easily codified information) with tacit knowledge (experiential and context-dependent). Practitioners are encouraged to consider both types of knowledge in their KM initiatives to enhance effectiveness.

4. **Knowledge Repositories**: The development and use of knowledge repositories are prominent in the technocratic school. While many studies report positive outcomes from these tools, caution is advised against overly relying on codification strategies, as they may overlook the nuances of tacit knowledge sharing.

5. **Organizational Learning**: The behavioral school emphasizes the role of organizational structures and networks in facilitating knowledge sharing. Successful KM initiatives often leverage existing informal networks, suggesting that fostering trust and collaboration among employees can enhance knowledge exchange.

6. **Strategic Implications**: For organizations following traditional development methodologies, technocratic approaches may be more beneficial, while agile teams might find greater value in behavioral strategies. This distinction is crucial for practitioners when designing KM initiatives tailored to their development processes.

**Who is this briefing for?**
This briefing is intended for software engineering researchers seeking to identify research gaps and relevant methodologies, knowledge management practitioners looking to implement effective KM strategies, and organizational leaders aiming to enhance knowledge sharing within their teams.

**Where the findings come from?**
The findings presented in this briefing are derived from the systematic review conducted by Bjørnson and Dingsøyr, which analyzed empirical studies of knowledge management in software engineering, focusing on concepts, findings, and research methods.

**What is included in this briefing?**
The briefing includes an overview of the key concepts and findings related to knowledge management in software engineering, categorized by Earl's framework, along with implications for both research and practice.

**To access the original research article:**
Bjørnson, F. O., & Dingsøyr, T. (2008). Knowledge management in software engineering: A systematic review of studied concepts, findings, and research methods. *Information and Software Technology, 50*(11), 1055-1068. https://doi.org/10.1016/j.infsof.2008.03.006"
"['Critical Barriers for Offshore Software Development Outsourcing Vendors: A  Systematic Literature Review    Siffat Ullah Khan, Mahmood Niazi   School of Computing and Mathematics, Keele  University, ST5 5BG, UK  s.khan@epsam.keele.ac.uk, mkniazi@cs.keele.ac.uk  Rashid Ahmad  College of EME, National University of Science &  Technology, Rawalpindi, Pakistan  rashid@ceme.edu.pk      ABSTRACT—Software development outsourcing is a  contract-based relationship between client and vendor  organisations in which a client(s) contracts out all or part of  its software development activities to a vendor(s), who  provides agreed services for remuneration. The objective of  this paper is to identify various barriers that have a negative  impact on software outsourcing clients in the selection process  of offshore software development outsourcing vendors. We  have performed a Systematic Literature Review process for  the identification of barriers. We have identified barriers  ‘language and cultural barriers’, ‘country instability’, ‘lack of  project management’, ‘lack of protection for intellectual  property rights’ and ‘lack of technical capability’ that are  generally have a negative impact on outsourcing clients. The  results also reveal the similarities and differences in the  barriers identified in different continents. We suggest vendors  have to address different barriers in order to compete in the  offshore outsourcing business.  Keywords: Systematic Literature Review; Software  Development Outsourcing, Vendors  I. INTRODUCTION  Many companies are adopting the Global Software  Development (GSD) paradigm to reduce software  development cost [1]. Many vendor organisations are  struggling hard to better compete internationally for  attracting outsourced software development projects. Due  to the increasing trend of GSD we are interested to discover  which barriers have a negative impact on the software  development outsourcing clients in the selection of offshore  software development outsourcing vendors. This paper  presents an empirical study in which a Systematic  Literature Review (SLR) [2] is conducted in order to  identify  these barriers. Identifying these barriers will lead  software development outsourcing vendors in addressing  those barriers in order to be fully ready for software  development outsourcing initiatives. Our long term  research goal is to provide software development  outsourcing practitioners with a body of knowledge that can  help them to improve GSD processes.   In order to reduce development cost, offshore software  development outsourcing has become an important process  of GSD. Software development outsourcing is a contract- based relationship between client and vendor organisations  in which a client(s) contracts out all or part of its software  development activities to a vendor(s), who provides agreed  services for remuneration [3; 4]. Over the last decade, many  firms in the US and UK have outsourced software  development projects to offshore countries [5]. There are  many reasons for software development outsourcing [6].  Client organisations benefit from offshore outsourcing  because vendors in developing countries (offshore vendors)  usually cost one-third less than onshore vendors and even  less when compared with in-house operations [7].  Moreover offshore vendors improve their skills and service  quality with the experience of offshore outsourcing  projects, and they learn totally new ways to satisfy clients’  needs. It is professed that offshoring vendors can add  significant value to their clients’ supply chains [8].  However, in addition to the outsourcing benefits there are  many risks in an outsourcing process [9; 10].  Many problems have been reported in the offshore  software outsourcing process. One of the key challenges is  to handle complex communication and coordination  problems in conditions of time and cultural separation [5; 9;  11; 12]. Other challenges are to develop software', 'to handle complex communication and coordination  problems in conditions of time and cultural separation [5; 9;  11; 12]. Other challenges are to develop software  development outsourcing practices, creating confidence and  trust among the outsourcing companies and to manage the  expectations of what can and what cannot be done in a  distributed setting [5; 13-18]. However, despite the  importance of offshore software development outsourcing,  little empirical research has been carried out on offshore  software development outsourcing practices in general and  identification of barriers that have a significant impact on  client organisations in particular. To do this we intend to  address the following research questions:  RQ1. What barriers within a software outsourcing  vendor organisations have a negative impact on software  outsourcing clients?   RQ2. Do the identified barri ers vary from continent to  continent?  This paper is organised as follows. Section 2 describes  the background. Section 3 describes the research  methodology. In Section 4 findings from the systematic  literature review are presented and analysed with some  discussion. Section 5 describes the limitations; Section 6  provides the conclusion and future work.  2009 16th Asia-Pacific Software Engineering Conference 1530-1362/09 $26.00 © 2009 IEEE DOI 10.1109/APSEC.2009.16 79 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.', 'II. BACKGROUND AND MOTIVATION  In order to successfully design GSD initiatives, as  researchers, we need to be constantly aware of what really  undermine GSD processes. This will enable us to position  our research within an appropriate context. It is important  to discover which barrier will undermine GSD process, as  research shows that half of the companies that have tried  outsourcing have failed to realise the anticipated results  [19]. The knowledge of these barriers may help us to  develop new or improved GSD approaches, whose adoption  will better match organisations’ objectives.  GSD activities have been going on for more than a  decade. However, software development outsourcing  companies are still facing many problems. A number of  researchers have tried to address some of the issues of  software development outsourcing, e.g. (Nguyen et al.[13];  Oza [20], Oza and Hall [21], Oza et al. [16], Sabherwal  [22],). To highlight few of these: A study was conducted in  the UK to manage the offshore software outsourcing  relationships [20]. The focus of this study is around Indian  software vendor organizations and their client organizations  in US and European countries. A similar study was  conducted by Nguyen et al. [13] to examine the offshore  outsourcing relationships between the software vendors in  Vietnam and their corresponding European and American  clients. Sabherwal [14] has worked on the role of trust in  software outsourcing relationships. Rajkumar et al. [23]  have worked on the offshore software outsourcing risks,  benefits and conditions that are applicable in the Indian  software industry and corresponding clients in US.  Sakthivel [24] has also identified various risks related with  offshore outsourced software development projects.  Narayanaswarmy et al. [25] have worked on the  management of offshore outsourced software development  projects. They have proposed a research model in which  culture is considered as a prime factor affecting the choice  of control mechanisms in offshore outsourced software  development projects [25]. Aubert et al. [26] have  developed a framework for the completeness of outsourcing  contracts and associate costs in order to minimize risks.  They have conducted an empirical study in order to  measure different levels of outsourcing contract.   The work in this paper complements work previously  done in these studies. Most of the existing studies focus on  the topics of ‘outsourcing relationship’ and ‘outsourcing  trust’. Outsourcing relationships and outsourcing trust are  important areas to address. However, despite the increasing  importance and need for empirically tested body of  knowledge on different aspects of GSD, little empirical  research has been carried out in order to determine which  barriers have a significant impact on software outsourcing  clients in the selection process of offshore software  development outsourcing vendors. The knowledge about  these barriers will contribute in improving the readiness of  offshore software development vendors as vendors  organisations will try to address the barriers that have a  negative impact on client organisations. In addition,  understanding the GSD barriers will provide advice to GSD  practitioners on what barriers to address when developing  GSD strategies. In addition, no SLR has been performed on  this topic. Research in this  area is expected to provide  useful information for outsourcing vendor organisations.  In this paper we present an empirical study in which a  SLR is conducted in order to identify which barriers have a  negative impact on the software development outsourcing  clients in the selection of offshore software development  outsourcing vendors. A good understanding of the issues  involved in the selection of outsourcing vendors is expected  to help vendor organisations to address these issues in order  to compete internationally for attracting outsourced  software development projects.', 'to help vendor organisations to address these issues in order  to compete internationally for attracting outsourced  software development projects.   III. RESEARCH METHODOLOGY  We have used a Systematic Literature Review (SLR)  process [2] as the main approach for data collection  because a SLR is a defined and methodical way of  identifying, assessing, and analysing published primary  studies in order to investigate a specific research question.  Systematic reviews differ from ordinary literature surveys  in being formally planned and methodically executed. In  finding, evaluating, and summarising all available evidence  on a specific research question, a systematic review may  provide a greater level of validity in its findings than might  be possible in any one of the studies surveyed in the  systematic review.  A systematic review protocol was written to describe the  plan for the review, and this protocol is described in detail  in a technical report [27]. The major steps in our  methodology are:  • Determine the search strategy then perform the search  for relevant studies;  • Perform the study selection process;  • Apply study quality assessment;  • Extract data and analyse the extracted data.  Details on the course of these steps are described in the  following subsections.  A.  Search Strategy, and Search  The search strategy for the SLR is a plan to:  • Construct search terms by identifying population,  intervention  and outcome  • Find the alternative spellings and synonyms  • Verify the key words in any relevant paper  • Use Boolean Operators  An initial scoping study was conducted to determine the  resources to be searched, and the search terms to use for  each resource. In this scoping study a trial search was  conducted using the following search string on CiteSeer  digital library:   (""software outsourcing"" OR “IT outsourcing"" OR “IS/IT”)   AND (""vendor"" OR ""selection criteria"" OR ""readiness"" OR  ""client"" OR ""factors"" OR ""barriers"" OR ""models"").    The information retrieved through this search string was  used as a guide for the development and validation of the  80 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.', 'major search terms. The following final search string was  used:  (""software outsourcing"" OR ""information systems  outsourcing"" OR ""information technology outsourcing"" OR  ""IS outsourcing"" OR ""IT outsourcing"" OR ""CBIS  outsourcing"" OR ""computer-based information systems  outsourcing"" OR ""software facility management"" OR  ""software contracting-out"")  AND  ((barriers OR barrier OR obstacles OR hurdles OR risks  OR ""risk analysis"" OR ""critical factors"") OR (""selection  process"" OR ""selection criteria"" OR ""recruitment  procedure"" OR choosing OR methodology OR ""analyzing  vendor’s capability"" OR assessment OR ""evaluation  process"" OR agreement OR contracting OR alliance OR  co-ordination OR ""outsourcing relationship"") OR (vendor  OR vendors OR service-provider OR dealer OR trader OR  marketer OR seller OR developer) OR  (Clients OR client  OR outsourcer OR buyer OR customer OR purchaser OR  user OR consumer OR shopper) OR  (Undermine OR  damage OR challenge OR challenges OR risk) OR  (""negative impact"" OR ""relationship failure"" OR ""poor  results"" OR dissatisfaction OR disappointment OR  displeasure OR disagreement OR ""bad effect"" OR ""lack of  trust"" OR unconfident OR rejection OR ""uncertain  decision"" OR conflict OR uncertainties))  TABLE I  DATA SOURCES  Resource Total  Results  found  Primary  selection  Final  selection  IEEExplore 468 155 26 ACM 195 58 30 Science Direct 567 58 21 Google Scholar 54 32 13 CiteSeer 16 16 08 Total 1300 319 98   The scoping study identified an initial list of resources,  and an initial uniform search term. These were  incrementally modified during the scoping study. Different  resources required different concrete syntax for the search  terms. In the scoping study, some papers that were already  known to be relevant were used to check the inclusiveness  of the search terms. The resources searched in the scoping  study include databases, specific journals, and conference  proceedings. The final list of sources searched, their search  terms, and the number of publications found for each  resource are listed in Table I.   B. Publication Selection  1) Inclusion Criteria: Inclusion criteria are used to  determine which piece of literature (papers, technical  reports, etc.) found by the search term will be used for the  data extraction. The criteria are listed below:  • Studies that describe vendor’s capabilities for software  outsourcing  • Studies that describe the barriers that have a negative  impact on the software development outsourcing clients in  the selection of offshore software development outsourcing  vendors.  • Studies that describe the relationship between software  outsourcer and vendor  • Studies that  describe de-motivation in software  outsourcing   2) Exclusion Criteria: Exclusion criteria are used to  determine which piece of literature found by the search  term will be excluded. The criteria are listed below:  • Studies that are not relevant to the research questions.  • Studies that do not describe software outsourcing  vendor or client  • Studies that do not describe barriers in software  outsourcing vendor selection process  3) Selecting Primary Sources: The planned selection  process had two parts: an initial selection from the search  results of papers that could plausibly satisfy the selection  criteria, based on a reading of the title and abstract of the  papers; followed by a final selection from the initially  selected list of papers that satisfy the selection criteria,  based on a reading of the entire papers. The selection  process was performed by a primary reviewer. However, in  order to reduce the primary reviewer’s bias we have  performed the inter-rater reliability test in which a  secondary reviewer confirmed the primary reviewer results  by randomly selecting the set of primary sources.  C. Publication Quality Assessment  The measurement of quality is performed after final  selection of publications. The quality of publications is', 'by randomly selecting the set of primary sources.  C. Publication Quality Assessment  The measurement of quality is performed after final  selection of publications. The quality of publications is  assessed in parallel at the time of data extraction. The  quality checklist contains the following questions:  • Is it clear how the vendor screening was performed?  • Is it clear how the barriers in the selection of software  outsourcing vendor were identified?  Each of the above factors are marked as ‘YES’ or ‘NO’  or ‘NA’. The results of the study quality assessment were  not used to limit the selection of publications.  D. Data Extraction  The review was undertaken by a single researcher, who  was alone responsible for the data extraction. A secondary  reviewer was approached for guidance in case of an issue  regarding the data extraction.  The inter-rater reliability test was performed after the  data extraction process by the primary reviewer. The  secondary reviewer has selected 5 publications randomly  from the list of publication already chosen by the primary  reviewer. The secondary reviewer has independently  extracted the data from the randomly selected publication.  81 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.', 'The results were compared with the results produced by the  primary reviewer and no differences were found.  From each paper we extracted a list of Quotes, where  each Quote described a list of barriers that have a negative  impact on software outsourcing clients in the selection  process of offshore software development outsourcing  vendors.   The following data was extracted from each publication:  Date of review, Title, Authors, Reference, Database,  Critical Barriers, Methodology (interview, case study,  report, survey etc), Target Population, Sample Population,  Publication Quality Description, Organisation Type  (software house, university, research institute etc),  Company size (small, medium, large), Country/location of  the Analysis and Year.  E. Data Synthesis  A primary reviewer with the help of secondary reviewer  has performed data synthesis. At the end of the Data  Extraction phase described in section 3.4 we had identified  a list of barriers from the sample of 98 papers. The primary  researcher reviewed these in order to derive a list of  categories to classify these barriers and initially a list of 20  categories was identified. These were reviewed and some  of these categories were merged together and in this way  we got a final list of 16 barriers shown in Table II.   IV. RESULT  A. Barriers Identified through Systematic Literature  Review  In order to answer RQ1, Table II shows the list of  barriers identified through the SLR. ‘Language and cultural  barriers’ (56%) is the most common barrier identified in  our study. Over the last decade, many firms in the USA and  UK have outsourced software development projects to  other countries such as India, China, Russia and Malaysia  where English is not the first language [28]. In addition  these countries have different culture as compared to the  UK and USA. Different studies have described the impact  of language and Cultural differences on outsourcing  business:   • In a study conducted in the UK and India, Sahay et  al [5] have discussed different problems related to  transfer of UK culture to India. They have also  described the role of power and control during the  outsourcing business.   • In another study [29] some political and cultural  issues in the globalisation of software  development have been examined.   Our results indicate that ‘country instability’ (51%) has  a negative impact on software development outsourcing  clients. By ‘country instability’ we mean political  instability, corruption, peace  problems, terrorism threats  and uncertainty relating to trade and investment. Khan et al.  [30] have also identified this barrier as a critical barrier in  their study in India: “instability of the political situation  could act as a discouragement for the foreign investors to  offshore outsourcing in India. Speed to market is a very  important factor in certain firms. Therefore, if the  development process gets delayed due to impeding factors  like strikes or power cuts, it becomes difficult to continue  the process.”  Nearly half of the articles in our study described ‘lack  of project management’ as a barrier that can have a  negative impact on outsourcing clients. In the outsourcing  process an effective project management plays a vital role  as it has been a difficult task to manage the geographical  distributed teams: Ofer, and Arik (2007) have found that  improving the project planning is an effective tool in  dealing with high-risk projects; Sun-Jen and Wen-Ming  (2008) have reported the impact of project planning on  project duration; and Linda et al. (2004) have described the  lack of project planning as a risk to software projects.  Our results also indicate that ‘lack of technical  capability’ (47%) can undermine the selection of competent  vendor organisations. Research suggests that half of the  companies that have tried outsourcing have failed to realise  the anticipated results [19]. One of the reasons for software', 'vendor organisations. Research suggests that half of the  companies that have tried outsourcing have failed to realise  the anticipated results [19]. One of the reasons for software  development outsourcing failures is the difficulties in  creating confidence and trust among the outsourcing  companies [4; 15]. We argue that addressing ‘lack of  technical capability’ barrier can play a vital role in  establishing a good relationship between client and vendor  organisations as this will help vendor organisations to  provide adequate technical servi ces  to client organisations.  Different studies have also described the importance of this  barrier:  • A high-quality skilled manpower is the backbone of the  IT industry and vendors should employ high skilled  workers with professional degrees in Computer Science,  Engineering, Management and similar fields [31].  • Often a client organisation is eager to know the  technical capability of vendor organisation [13].  Forty seven percent of the articles in our study describe  ‘lack of protection for intellectual property rights’ as a  barrier to outsourcing due to the fact that there is no such  thing as an ""international intellectual property right"" that  automatically protects anybody’s work throughout the  world. Every country has its own national laws in order to  protect individuals’ work against unauthorised use.  However, it is always hard to implement these laws in order  to address issues relating to intellectual property rights [30].  In addition, our results indicate that issue of intellectual  property rights is critical and has great impact on  outsourcing clients in the selection of outsourcing vendors.  Forty four percent of the articles in our study describe  ‘communication gap’ as a barrier in outsourcing business.  ‘Poor relationship management’ is also one the common  barriers in our study, i.e. 44%. This suggests that poor  relationship management has a negative impact on the  outsourcing clients in the selection process of outsourcing  vendors. Understanding different factors in managing  software development outsourcing relationships can help to  ensure the long lasting relationships between clients and  vendors [4; 14; 32]. Different factors have been identified  82 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.', 'to effectively manage relationships between clients and  vendors such as credibility, capabilities and personal visits  .[4].   Forty three percent of articles in our study have cited a  ‘poor quality of service and system/process’ as a barrier.  We argue that in order to compete in an international  outsourcing business vendor companies need to improve  the quality of their processes and services. Indian software  companies have been reported to provide high quality  software [33] and this is the reason that in the software  export market, India is a dominant software outsourcing  provider [34].  We have also identified some other barriers that have a  negative impact on client organisations as shown in Table  II.  TABLE II. LIST OF BARRIERS  Barriers Freq  (n=98)  %  Communication gap 43 44  Country instability 50 51  Delays in delivery 22 22  Hidden costs 37 38  Incompatibility with client 10 10  Lack of Project Management 48 49  Lack of protection for intellectual  property rights  46 47  Lack of technical capability 46 47  Language and Cultural barriers 55 56  Lack of control over project 33 34  Poor quality of service and  system/process 42 43  Opportunistic behaviour 27 28  Poor contract management 42 43  Poor infrastructure 32 33  Poor relationship management 43 44  Strategic inflexibility 10 10  B. Comparison of the barriers across various continents  Our results show the number of articles reporting  studies related to different continents. Due to space  limitation, in this paper we have only compared the barriers  identified in three continents, i.e. Asia, America and  Europe. Our aim is to find whether these barriers differ  from continent to continent. We suggest that understanding  the similarities and differences in these barriers can  contribute to the body of knowledge of software  development outsourcing. This is because where articles  from different continents consider that a barrier has some  impact on client organisations then that barrier needs to be  taken very seriously by the vendor organisations in that  continent.    Because of the ordinal nature of the data we have used  the linear by linear association chi-square test in order to  find significant differences between barriers identified in  three continents. The linear by linear association test is  preferred when testing the significant difference between  ordinal variables because it is more powerful than Pearson  chi-square test [35].    Comparison of the barriers identified in three continents  indicates that there are more similarities than differences  between the barriers. We have found only three significant  differences between the three continents as shown in Table  III. Our findings show that ‘country instability’ (65%, 43%  and 71%), ‘lack of protection for intellectual property  rights’ (52%, 46% and 47%), and ‘poor contract  management’ (39%, 49% and 53%) are the most common  barriers in all three continents. ‘Lack of project  management’, ‘lack of technical capability’, ‘language and  cultural barriers’, ‘poor quality of service and  system/process’, and ‘poor relationship management’ are  common barriers in Asia and Europe. Table III shows that  clients in America and Europe want ‘control over project  and they want to avoid outsourcing vendors who have  ‘opportunistic behaviour’ in the outsourcing business. Our  results indicate that outsourcing clients in America (43%)  and Europe (59%) are having problems with ‘hidden cost’  during outsourcing business. ‘Communication gap’ is  common in Europe (59%). Due to different cultures and  languages in outsourcing business it is quite possible that a  message is misunderstood by one or more of the  outsourcing parties. In addition due to the geographical  distributed teams in outsourcing business, face-to-face  communication is not possible where one can clarify any  misunderstanding. In the outsourcing processes the  common methods for communications are email, phone and', 'communication is not possible where one can clarify any  misunderstanding. In the outsourcing processes the  common methods for communications are email, phone and  fax. However, in this modern age video conferencing is  also emerging as a common communication tool.  These findings indicate that outsourcing clients are  aware of the barriers that can undermine the whole  outsourcing process. The purpose of this study is to explore  different barriers that have a negative impact on  outsourcing clients in the selection of outsourcing vendors.  However, it is important to determine the reasons of why  these barriers are commonly cited by the client  organisations in Asia, Europe and America. We encourage  independent studies on this topic.    83 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.', 'TABLE III  SUMMARY OF BARRIERS ACROSS 3 CONTINENTS AS IDENTIFIED IN THE SLR  Barriers  Occurrence in SLR (n=98) Chi-square Test  (Linear-by-Linear  Association)  α = .05  Asia  (N=23)  America  (N=35)  Europe  (N=17)    X²    Df    p  Freq % Freq % Freq %  Communication gap 08 35 12 34 10 59 .600 1 .439  Country instability 15 65 15 43 12 71 4.126 1 .042  Delays in delivery 06 26 03 09 05 29 .121 1 .728  Hidden costs 08 35 15 43 07 41 .274 1 .601  Incompatibility with client 0 0 05 14 01 06 4.241 1 .039  Lack of Project Management 15 65 12 34 11 65 2.396 1 .122  Lack of protection for intellectual property rights 12 52 16 46 08 47 .147 1 .702  Lack of technical capability 10 44 13 37 10 59 .002 1 .965  Language and cultural barriers 13 57 13 37 12 71 .734 1 .392  Lack of control over project 05 22 13 37 06 35 1.054 1 .305  Poor quality of service and system/process 11 48 11 31 10 59 .280 1 .597  Opportunistic behaviour 04 17 13 37 06 35 .177 1 .674  Poor contract management 09 39 17 49 09 53 .034 1 .854  Poor infrastructure 09 39 10 29 06 35 .636 1 .425  Poor relationship management 09 39 11 31 08 47 3.906 1 .048  Strategic inflexibility 01 04 04 11 05 29 .803 1 .370    V. LIMITATIONS  How valid are our findings of barriers in the selection  process of offshore software development outsourcing  vendors? One possible threat to internal validity is that for  any specific article, their reported barriers may not have in  fact described underlying reason. We have not been able to  independently control this threat. The authors of these  studies were not supposed to report the original reasons  why these barriers have a negative impact of outsourcing  clients in the selection of outsourcing vendors. Many of the  contributing studies were self-reported experience reports,  case studies and empirical studies which may be subject to  publication bias.    How safe is it to generalise these findings?  Our  sample contains many articles from many countries. Our  findings are not based on any studies that used a random  sample of software-developing outsourcing organisations in  the world.  However, in the investigation of our research  questions, our study is the most comprehensive to date. The  issue of generalising these findings can also be considered  by comparing our findings with results from other related  studies, as discussed in sections 4.1 and 4.2. We found  many similarities in our findings and findings by other  people, and this provides some support for generalisation.  VI. CONCLUSION AND FUTURE WORK  Our findings indicate that ‘language and cultural  barriers’, ‘country instability’, ‘lack of project  management’, ‘lack of protection for intellectual property  rights’ and ‘lack of technical capability’ have a negative  impact on software development outsourcing clients in the  selection of software development outsourcing vendors. We  suggest that outsourcing vendors should focus on these  barriers in order to have a positive impact on outsourcing  clients and to win outsourcing contracts. We have also  compared these identified barriers across the reported  datasets for the continents of Asia, Europe and America and  found that there are more similarities than differences  between the barriers.  We encourage independent studies on this topic. This  will increase confidence in our findings and also track  changes in attitudes to offshore outsourcing over time. Our  ultimate aim is to develop a Software Outsourcing Vendors  Readiness Model (SOVRM) [36]. This paper contributes to  only one component of the SOVRM, i.e. the identification  of the barriers. The eventual outcome of the research is the  development of SOVRM to assist offshore outsourcing  vendors in assessing their readiness for software  development outsourcing activities. SOVRM will also  84 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.', ""assist in improving software development outsourcing  processes.  The SOVRM proposed will bring together and advance  the work that has been undertaken on frameworks and  models for outsourcing. Our contribution to improving  software development outsourcing processes will provide  other researchers with a firm basis on which to develop  different outsourcing processes that are based on an  understanding of how and where they fit into the software  development outsourcing activities. New outsourcing  practices could then be developed targeting software  development outsourcing projects.  Many research outputs end up with a model or  framework which never make it into industrial practice. We  expect our work will reduce this trend in outsourcing by  identifying a well understood and rationale outsourcing  vendors’ readiness model. Our aim is to help companies to  avoid randomly implementing promising new models and  frameworks just to see them be discarded.   ACKNOWLEDGEMENTS  We are thankful to the University of Malakand and  Higher Education Commission, Pakistan for sponsoring the  PhD research studies. We are also thankful to Professor  Pearl Brereton, Dr Mark Turner and Clive Jefferies for  providing assistance in the review process.  REFERENCES   [1] A. Kakabadse and N. Kakabadse: Outsourcing: Current and  future trends, Thunderbird International Business Review:  Wiley InterScience 47 (2). (2005) 183–204.   [2] EBSE-Technical-Report: Guidelines for performing  Systematic Literature Review s in Software Engineering,  EBSE Technical Report: EBSE-2007-01. (2007).  [3] T. Kern and L. Willcocks: Exploring information technology  outsourcing relationships: theory and practice, Journal of  Strategic Information Systems 9 (2000) 321-350.   [4] M. Ali-baber, J. Verner and P. Nguyen: Establishing and  maintaining trust in software outsourcing relationships: An  empirical investigation, The Journal of Systems and Software  80 (2007). (2007) 1438–1449.   [5] S. Sahay, B. Nicholson and S. Krishna: Global IT  outsourcing. Cambridge University Press.(2003).   [6] A. A. Bush, A. Tiwana and H. Tsuji: An Empirical  Investigation of the Drivers of Software Outsourcing  Decisions in Japanese Organizations, in press for publication,  Information and Software Technology Journal (2007)   [7] L. McLaughlin: An eye on India: Outsourcing debate  continues., IEEE Software 20 (3). (2003) 114-117.   [8] B. Shao, David, J.S.: The impact of offshore outsourcing on  IT workers in developed countries., Communications of the  ACM 50 (2). (2007) 89 - 94.   [9] H. Holmstrom, E. Ó. Conchúir, P. Ågerfalk and B.  Fitzgerald: Global Software Development Challenges: A  Case Study on Temporal, Geographical and Socio-cultural  Distance. International Conference on Global Software  Engineering. (2006) 3-11.   [10] D. Damian, I. Luis, S. Janice and K. Irwin: Awareness in the  Wild: Why Communication Breakdowns Occur. International  Conference on Global Software Engineering. (2007) 81-90.   [11] E. Beulen and P. Ribbers: Managing complex IT outsourcing- partnerships. 35th Hawaii International Conference on  System Sciences. (2002)   [12] D. Daniela, I. Luis, S. Janice and K. Irwin: Awareness in the  Wild: Why Communication Breakdowns Occur. International  Conference on Global Software Engineering. (2007) 81-90.   [13] P. Nguyen, M. Ali-baber and J. Verner: Trust in software  outsourcing relationships: an analysis of Vietnamese  practitioners' views. EASE. (2006) 10-19.   [14] R. Sabherwal: The role of trust in outsources IS development  projects, Communication of ACM 42 (1999) 80-86.   [15] R. Heeks, S. Krishna, B. Nicholson and S. Sahay: Synching  or Sinking: Global Software Outsourcing Relationships,  IEEE Software March/ April 2001 (2001) 54-60.   [16] N. V. Oza, T. Hall, A. Rainer and S. G. Grey: Trust in  software outsourcing relationships: An empirical  investigation of Indian software companies, Information &  Software Technology 48 (5). (2006) 345-354."", ""software outsourcing relationships: An empirical  investigation of Indian software companies, Information &  Software Technology 48 (5). (2006) 345-354.   [17] J. Stark, M. Arlt and D. Walker, H. T.: Outsourcing  Decisions and Models - Some Practical Considerations for  Large Organizations. International Conference on Global  Software Engineering. (2006) 12-17.   [18] M. Cataldo, M. Bass, Herbsleb, J. D. and L. Bass: On  Coordination Mechanisms in Global Software Development.  International Conference on Global Software Engineering .  (2007) 71-80.   [19] D. Bradstreet: Dun & Bradstreet's Barometer of Global  Outsourcing,  http://findarticles.com/p/articles/mi_m0EIN/is_2000_Feb_24/ ai_59591405, September. (2000).   [20] N. V. Oza: An empirical evaluation of client - vendor  relationships in Indian software outsourcing companies, PhD  thesis, University of Hertfordshire, UK (2006).   [21] N. V. Oza and T. Hall: Difficulties in managing offshore  outsourcing relationships: An empirical analysis of 18 high  maturity Indian software companies. 4th International  Outsourcing Conference, Washington DC. (2005)   [22] R. Sabherwal: The role of trust in outsources IS development  projects,, Communication of ACM 42 ( 1999) 80-86.   [23] T. M. Rajkumar and D. L. Dawley: Problems and issues in  offshore development of software, Strategic sourcing of  Information Systems: Perspectives and Practices, Wiley  Series in Information Systems (1997) 369-386.   [24] S. Sakthivel: Communication of the ACM 50 (4). (2007)   [25] R. Narayanaswarmy and R. M. Henry: Effects of culture on  control mechanisms in offshore outsourced IT projects.  Proceedings of the 2005 ACM SIGMIS CPR conference on  Computer personnel research . Atlanta, Georgia, USA (2005)  139 - 145.   [26] B. Aubert, J.-F. Houde, M. Patry and S. Rivard:  Characterestics of IT Outsourcing Contracts. Proceedings of  the 36th Hawaii International Conference on System  Sciences. (2003)   [27] K. Siffatullah and M. Niazi: Systematic Literature Review  Protocol for Software Outsourcing Vendors Readiness  Model. Technical Report: TR/08-01, Keele University, UK  (2008).  [28] W. Kobitzsch, D. Rombach and R. Feldmann, L.:  Outsourcing in India, IEEE Software March/ April 2001  (2001) 78-86.   [29] B. Nicholson and S. Sahay: Some political and cultural issues  in the globalisation of software development: case experience  from Britain and India, Information and Organization 11  (2001). (2001) 25–43.   85 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply."", ""[30] N. Khan, W. Currie, V. Weerakkody and B. Desai:  Evaluating Offshore IT Outsourcing in India: Supplier and  Customer Scenarios. Proceedings of the IEEE 36th Hawaii  International Conference on System Sciences (HICSS'03) .  (2003)   [31] A. B. Nauman, R. Aziz, A. F. M. Ishaq and M. Mohsin: An  analysis of capabilities of Pakistan as an offshore IT services  outsourcing destination. Proceedings of IEEE 8th  International INMIC, Multitopic Conference . (Dec. 2004)  403 - 408.   [32] J. Goo, R. Kishore, K. Nam, H. Rao, R.  and Y. Song: An  investigation of factors that influence the duration of IT  outsourcing relationships, Decision Support Systems 42 (4).  (2007) 2107-2125.   [33] S. C. Bhatnagar and S. Madon: The Indian software industry:  moving towards maturity, Journal of Information Technology  12 (4). (1997) 277-288.   [34] R. Terdiman and F. Karamouzis: Going Offshore to Globally  Source IT Services. Technical Report. Gartner Research,  (2002).  [35] B. Martin: An Introduction to Medical Statistics, 3rd edition.   Oxford medical publications.(2000).   [36] S. Khan, M. Niazi and R. Ahmad: A Readiness Model for  Software Outsourcing Vendors Readiness Model. IEEE  international conference on Global Software Engineering,  ICGSE-08, pp:273-277. Bangalore, India (2008) 273-277.     86 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:08:29 UTC from IEEE Xplore.  Restrictions apply.""]","['CRITICAL  BARRIERS  FOR  OFFSHORE  SOFTWARE  DEVELOPMENT  OUTSOURCING  VENDORS • Language and cultural barriers (56%) is  the most common barrier identified. • Studies have found that country ins - tability (51%) has a negative impact  on software development outsourcing  clients. By ‘country instability’ we mean  political instability, corruption, peace  problems, terrorism threats and uncer - tainty relating to trade and investment.   Half of the researches described  lack of  project management as a barrier that can  have a negative impact on outsourcing  clients. In the outsourcing process an ef - fective project management plays a vital  role as it has been a difficult task to ma - nage the geographical distributed teams. • Lack of technical capability  (47%) can  undermine the selection of competent  vendor organisations. Research suggests  that half of the companies that have tried  outsourcing have failed to realise the an - ticipated results. • 47% of the studies analyzed by the syste - matic review describe lack of  protection  for intellectual property rights  as a bar - rier to outsourcing due to the fact that  there is no such thing as an “international  intellectual property right” that automati- cally protects anybody’s work throughout  the world. Every country has its own na - tional laws in order to protect individuals’  work against unauthorised use. However,  it is always hard to implement these laws  in order to address issues relating to intel- lectual property rights. • 44% of the researches describe commu- nication gap as a barrier in outsourcing  business. Poor relationship management  is also one the common barriers with the  same percentage. • 43% of studies have cited poor quality of  service and system/process  as a barrier.  In order to compete in an international  outsourcing business vendor companies  need to improve the quality of their pro - cesses and services. Indian software com- panies have been reported to provide  high quality software and this is the rea - son that in the software export market,  India is a dominant software outsourcing  provider. • The complete list of barriers is presented  in Table 1. Frequency that each barrier  appear on the 98 researches analyzed by  the systematic review is on column Freq. Barries Freq (n=98) % Language and Cultural bar- riers 55 56 Country instability 50 51 Lack of Project Manage- ment  48 49 Lack of protection for intel- lectual property rights 46 47 Lack of technical capability 46 47 Communication gap 43 44 Poor relationship manage- ment 43 44 Poor quality of service and  system/process 42 43 Poor contract management 42 43 Hidden costs 37 38 Lack of control over project 33 34 Poor infrastructure 32 33 Opportunistic behaviour 27 28 Delays in delivery 22 22 Incompatibility with client 10 10 Strategic inflexibility 10 10 Table 1: Frequency of barries • Comparison od the barries identified in  three continents (Americca, Asiaand Eu - rope) indicates that there are more simi - larities the differences between the bar - riers. • Country instability appears in 65%, 43%  and 71% of the studies for Asia, America  and Europe, respectively. Lack of protec - tion for intellectual property rights (52%,  46% and 47%), and poor contract mana- gement (39%, 49% and 53%). Those are  the most common barriers in all three  continents. • Lack of project management, lack of te - chnical capability, language and cultural  barriers, poor quality of service and sys - tem/process, and poor relationship ma - nagement are common barriers in Asia  and Europe. • Clients in America and Europe want con- trol over project  and they want to avoid  outsourcing vendors who have opportu- nistic behaviour in the outsourcing busi - ness.  • The results indicate that outsourcing  clients in America (43%) and Europe (59%)  are having problems with hidden cost du- ring outsourcing business.  • Communication gap is common in Europe  (59%). Due to different cultures and lan -', 'clients in America (43%) and Europe (59%)  are having problems with hidden cost du- ring outsourcing business.  • Communication gap is common in Europe  (59%). Due to different cultures and lan - guages in outsourcing business it is quite  possible that a message is misunderstood  by one or more of the outsourcing parties.  In addition due to the geographical dis - tributed teams in outsourcing business,  face to face communication is not possible  where one can clarify any misunderstan - ding. FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Khan, S.U.; Niazi, M.; Ahmad, R., “Critical Barriers for Offshore Software Development Outsourcing Vendors: A Systematic Literature Review,” in  Software Engineering Conference, 2009. APSEC ‘09. Asia Pacific , vol., no., pp.79 86, 1 3 Dec. 2009. doi: 10.1109/APSEC.2009.16 Keywords: Software Outsourcing Vendors Who is this briefing for? Software engineers practitioners  who want to make decisions  about software development  outsourcing based on scientific  evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic  review conducted by Khan et al. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg This  briefing  reports  evidence  on  various   barriers  that  have  a  negative  impact  on  software outsourcing clients in the selection  process of offshore software development  outsourcing vendors based on scientific evi - dence from a systematic review.']","**Title:** Key Barriers in Offshore Software Development Outsourcing

**Introduction:**  
This briefing aims to summarize the critical barriers identified in the process of selecting offshore software development outsourcing vendors. The findings are based on a systematic literature review conducted by Khan, Niazi, and Ahmad, which highlights the challenges faced by clients in outsourcing relationships and suggests areas for vendors to improve in order to enhance their competitiveness in the global market.

**Main Findings:**  
The systematic literature review identified several significant barriers that negatively impact clients in the selection of offshore software development vendors:

1. **Language and Cultural Barriers (56%)**: Communication issues stemming from language differences and cultural misunderstandings are prevalent. Clients often face difficulties in conveying requirements and expectations clearly, leading to potential project failures.

2. **Country Instability (51%)**: Political instability, corruption, and threats such as terrorism create an uncertain environment for outsourcing. Clients are hesitant to engage with vendors from regions with such risks, as they can jeopardize project timelines and outcomes.

3. **Lack of Project Management (49%)**: Effective project management is crucial for coordinating distributed teams. Clients reported challenges in managing projects efficiently, which can lead to delays and increased costs.

4. **Lack of Protection for Intellectual Property Rights (47%)**: Concerns over the security of intellectual property are significant, as there is no universal legal framework that protects clients’ work globally. This uncertainty can deter clients from outsourcing to certain regions.

5. **Lack of Technical Capability (47%)**: Clients often worry about the technical skills of offshore vendors. A perceived lack of expertise can hinder trust and confidence in the vendor's ability to deliver quality outcomes.

6. **Communication Gap (44%)**: Misunderstandings and ineffective communication methods can lead to project misalignment and dissatisfaction.

7. **Poor Relationship Management (44%)**: Building and maintaining a strong relationship between clients and vendors is essential. Poor management of these relationships can lead to conflicts and dissatisfaction.

8. **Other Barriers**: Additional barriers identified include poor quality of service, hidden costs, and strategic inflexibility, which can further complicate the outsourcing process.

The review also compared these barriers across continents (Asia, America, and Europe) and found that while many barriers are consistent, there are specific regional concerns that vendors must address to better serve their clients.

**Who is this briefing for?**  
This briefing is intended for software development outsourcing practitioners, including vendors and client organizations, who are involved in or considering offshore software development initiatives.

**What is included in this briefing?**  
The briefing includes a summary of the barriers identified through a systematic literature review, highlighting their implications for both clients and vendors in the outsourcing process.

**Where the findings come from?**  
All findings are derived from the systematic literature review conducted by Khan, Niazi, and Ahmad, which analyzed various studies on offshore software development outsourcing.

**What is NOT included in this briefing?**  
This briefing does not include detailed statistical analyses or specific case studies from the original research. It focuses on summarizing the key barriers identified.

**Original Research Reference:**  
Khan, S. U., Niazi, M., & Ahmad, R. (2009). Critical Barriers for Offshore Software Development Outsourcing Vendors: A Systematic Literature Review. In Proceedings of the 16th Asia-Pacific Software Engineering Conference (APSEC 2009), 79-86. DOI: 10.1109/APSEC.2009.16."
"['Critical Success Factors for Offshore Software Development Outsourcing  Vendors: A Systematic Literature Review    Siffat Ullah Khan  a , Mahmood Niazi  a  and Rashid Ahmad  b  a  School of Computing and Mathematics, Keele University, ST5 5BG, UK  s.khan@epsam.keele.ac.uk, mkniazi@cs.keele.ac.uk    b  College of EME, National University of Science and Technology, Rawalpindi, Pakistan  rashid@ceme.edu.pk      Abstract  CONTEXT – Offshore software development  outsourcing is a modern business strategy for  producing high quality software at low cost   OBJECTIVE – To identify various Critical Success  Factors (CSFs) that have a positive impact on software  outsourcing clients in the selection process of offshore  software development outsourcing vendors.  METHOD – We have performed a Systematic  Literature Review process for the identification of  factors in the selection process of offshore software  development outsourcing vendors.  RESULTS – We have identified factors ‘cost-saving’,  ‘skilled human resource’, ‘appropriate infrastructure’  and ‘quality of product and services’ that are generally  considered important by the outsourcing clients.  The  results also reveal the similarities and differences in  the factors identified in different continents.  CONCLUSIONS – Cost-saving should not be  considered as the only prime factor in the selection  process of software development outsourcing vendors.  Vendors should have to address other factors in order  to compete in the offshore outsourcing business.  Key Words: Offshore software outsourcing vendors,  systematic literature review, critical success factors  1. Introduction  This research is premised on the need to gain an in- depth understanding of the range of criteria used by the  software development outsourcing clients for the  selection of software development outsourcing  vendors. Understanding the selection criteria will lead  software development outsourcing vendors in  addressing those criteria in order to be fully ready for  software development outsourcing initiatives. This  may also help to ensure the successful outcome of  outsourcing projects and long lasting relationships  between clients and vendors.  Offshore software development outsourcing is a  modern business strategy for developing high quality  software in low wages countries at low cost. Software  development outsourcing is a contract-based  relationship between client and vendor organisations in  which a client(s) contracts out all or part of its software  development activities to a vendor(s), who provides  agreed services for remuneration [1-3]. Over the last  decade, many firms in the U.S.A and UK have  outsourced software development projects to offshore  countries [4]. McKinsey consulting argues that for  every dollar of client organisation in U.S.A spending  on outsourcing to vendor organisation in India, U.S.A  benefits $1.14 and India gets 0.33 cents [5].   There are many reasons for software development  outsourcing [6]. Client organisations benefit from  offshore outsourcing because vendors in developing  countries (offshore vendors) usually cost one-third less  than onshore vendors and even less when compared  with in-house operations [7]. Moreover offshore  vendors improve their skills and service quality with  the experience of offshore outsourcing projects, and  they learn totally new ways to satisfy clients’ needs. It  is professed that offshoring vendors can add significant  value to their clients’ supply chains [8]. However, in  addition to the outsourcing benefits there are many  risks in an outsourcing process [9, 10].  Many problems have been reported in the offshore  software outsourcing process. One of the key  challenges is to handle complex communication and  coordination problems in conditions of time and  cultural separation [4, 9, 11, 12]. Other challenges are  to develop software development outsourcing  practices, creating confidence and trust among the', 'cultural separation [4, 9, 11, 12]. Other challenges are  to develop software development outsourcing  practices, creating confidence and trust among the  outsourcing companies and to manage the expectations  of what can and what cannot be done in a distributed  setting [4, 13-18]. However, despite the importance of  offshore software development outsourcing, little  empirical research has been carried out on offshore  2009 Fourth IEEE International Conference on Global Software Engineering 978-0-7695-3710-8/09 $25.00 © 2009 IEEE DOI 10.1109/ICGSE.2009.28 207 2009 Fourth IEEE International Conference on Global Software Engineering 978-0-7695-3710-8/09 $25.00 © 2009 IEEE DOI 10.1109/ICGSE.2009.28 207 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'software development outsourcing practices in general  and identification of factors that have a significant  impact on client organisations in particular. To do this  we intend to address the following research questions:  RQ1. What factors do offshore software  outsourcing vendors need to address in order to have a  positive impact on software outsourcing clients?  RQ2. Do the identified factors vary from continent  to continent?  This paper is organised as follows. Section 2  describes the background. Section 3 describes the  research methodology. In Section 4 findings from the  systematic literature review are presented and analysed  with some discussion. Section 5 describes the  limitations; Section 6 provides the conclusion and  future work.  2. Background and Motivation  Offshore software development outsourcing activities  have been going on for more than a decade. However,  software development outsourcing companies are still  facing many problems. In addition, different risks (e.g.  cost and time over-run, cultural differences) have also  been identified for software development outsourcing  [19].   A number of researchers have tried to address some  of the issues of software development outsourcing, e.g.  (Nguyen et al.[13]; Oza [20], Oza and Hall [21], Oza et  al. [16], Sabherwal [22],):  • A study to examine the outsourcing  relationship was conducted in Hertfordshire  University UK [20]. The focus of this study  was to manage the offshore software  outsourcing relationships and the author has  proposed a model for managing offshore  software outsourcing relationships. The focus  of the model is around Indian software vendor  organizations and their client organizations in  US and European countries.   • A similar study was conducted by Nguyen et  al. [13] where they have examined the  offshore outsourcing relationships between  the software vendors in Vietnam and their  corresponding European and American  clients. In this study the authors have  examined the perceptions of the Vietnamese  software outsourcing vendors for maintaining  and establishing trust in software outsourcing  relationships.    • Sabherwal [14] has also worked on the role of  trust in software outsourcing relationships  where different case studies were conducted  with vendor organizations in India and  Colombia and with client organizations in US,  UK, Netherland, Thailand and Oman.  • Rajkumar et al. [23] have worked on the  offshore software outsourcing risks, benefits  and conditions that are applicable in the  Indian software industry and corresponding  clients in US. A similar study was conducted  by Khan et al. [24] to examine the scale and  scope of offshore software outsourcing risks  and benefits in Indian software industry [24].  Their research is based on an empirical  investigation of the software outsourcing  vendors in India and client organizations in  the UK. Sakthivel [25] has also identified  various risks related with offshore outsourced  software development projects. In another  study, Iacovou et al. [26] have identified a  risk profile of offshore software development  projects that have outsourced from client  organizations in US to Indian software  outsourcing vendors.   • Narayanaswarmy et al. [27] have worked on  the management of offshore outsourced  software development projects. A research  model has been proposed in which culture is  considered as a prime factor affecting the  choice of control mechanisms in offshore  outsourced software development projects  [27].  • Aubert et al. [28] have developed a  framework for the completeness of  outsourcing contracts and associate costs in  order to minimize risks. They have conducted  an empirical study in order to measure  different levels of outsourcing contract.   The work in this paper complements work  previously done in these studies. Most of the existing  studies focus on the topics of ‘outsourcing  relationship’ and ‘outsourcing trust’. Outsourcing', 'The work in this paper complements work  previously done in these studies. Most of the existing  studies focus on the topics of ‘outsourcing  relationship’ and ‘outsourcing trust’. Outsourcing  relationships and outsourcing trust are important areas  to address. However, research suggests that half of the  companies that have tried outsourcing have failed to  realise the anticipated results [29]. There are many  reasons for outsourcing failures [12, 17, 18, 30]. One  of the major issues in the outsourcing business is to  find the capable outsourcing vendor to be ready to  undertake the outsourcing activities [29]. For example,  Dun & Bradstreet survey found 50% of outsourcing  relationships worldwide failed within five years due to  poor planning [29]. In a study conducted in the UK and  India, Sahay et al [4] have discussed different problems  related to transfer of UK culture to India. They have  also described the role of power and control during this  208 208 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'outsourcing business. Another study [31] has examined  some political and cultural issues in the globalisation  of software development. Despite the importance of  the these problems, little empirical research has been  carried out in order to determine which factors have a  significant impact on software outsourcing clients in  the selection process of offshore software development  outsourcing vendors. The knowledge about these  factors will contribute in improving the readiness of  offshore software development vendors as vendors  organisations will try to address the factors that have a  positive impact on client organisations. Previously, no  Systematic Literature Review (SLR) has been  performed on this topic. Research in this area is  expected to provide useful information for outsourcing  vendor organisations.  3. Research Methodology  We have used a Systematic Literature Review (SLR)  process [32] as the main approach for data collection  because a SLR is a defined and methodical way of  identifying, assessing, and analysing published primary  studies in order to investigate a specific research  question. Systematic reviews differ from ordinary  literature surveys in being formally planned and  methodically executed. They are intended to be  independently replicable, and so have a different type  of scientific value than ordinary literature surveys.  In  finding, evaluating, and summarising all available  evidence on a specific research question, a systematic  review may provide a greater level of validity in its  findings than might be possible in any one of the  studies surveyed in the systematic review.  A systematic review protocol was written to describe  the plan for the review, and this protocol is described  in detail in a technical report [33]. The major steps in  our methodology are:  • Determine the search strategy then perform the  search for relevant studies;  • Perform the study selection process;  • Apply study quality assessment;  • Extract data and analyse the extracted data.  Details on the course of these steps are described in  the following subsections.  3.1 Search Strategy, and Search  The search strategy for the SLR is a plan to:  • Construct search terms by identifying population,  intervention  and outcome  • Find the alternative spellings and synonyms  • Verify the key words in any relevant paper  • Use Boolean Operators  An initial scoping study was conducted to  determine the resources to be searched, and the search  terms to use for each resource. The major terms in the  search string come from our research questions. Their  complete description is provided in our Technical  Report [33]. The technical report was reviewed by 4  experts and necessary changes were made to the report  accordingly. In this scoping study a trial search was  conducted using the following search string on  CiteSeer digital library:  (""software outsourcing"" OR “IT outsourcing"" OR  “IS/IT”)  AND (""vendor"" OR ""selection criteria"" OR  ""readiness"" OR ""client"" OR ""factors"" OR ""barriers""  OR ""models"").    The information retrieved through this search string  was used as a guide for the development and validation  of the major search terms. The scoping study identified  an initial list of resources, and an initial uniform search  term. These were incrementally modified during the  scoping study. Different resources required different  concrete syntax for the search terms. In the scoping  study, some papers that were already known to be  relevant were used to check the inclusiveness of the  search terms. The resources searched in the scoping  study include databases, specific journals, and  conference proceedings. We have selected these  resources based on our previous SLR experience and  discussions with our colleagues at Keele University.  The final list of sources searched, their search terms,  and the number of publications found for each resource  are listed in Table 1.    3.2 Publication Selection', 'The final list of sources searched, their search terms,  and the number of publications found for each resource  are listed in Table 1.    3.2 Publication Selection  3.2.1 Inclusion Criteria  It is used to determine which piece of literature  (papers, technical reports, etc.) found by the search  term will be used for the data extraction.  Table 1 Data sources and search strategy  Resource Total  Results  found  Primary  selection  Final  selection  IEEExplore 468 155 36  ACM 195 58 34  Science Direct 567 58 30  Google Scholar 54 32 14  CiteSeer 16 16 08        209 209 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'The criteria are listed below:  • Studies that describe vendor’s capabilities for  software outsourcing  • Studies that describe the critical success  factors in the selection process of software  outsourcing vendor  • Studies that describe the relationship between  software outsourcer and vendor  • Studies that  describe motivation in software  outsourcing   3.2.2 Exclusion Criteria   It is used to determine which piece of literature found  by the search term will be excluded. The criteria are  listed below:  • Studies that are not relevant to the research  questions.  • Studies that don’t describe software  outsourcing vendor or client  • Studies that don’t describe factors in software  outsourcing vendor selection process  3.2.3 Selecting Primary Sources  The planned selection process had two parts: an initial  selection from the search results of papers that could  plausibly satisfy the selection criteria, based on a  reading of the title and abstract of the papers; followed  by a final selection from the initially selected list of  papers that satisfy the selection criteria, based on a  reading of the entire papers. In order to reduce the  researcher’s bias we have performed the inter-rater  reliability test.  3.3 Publication Quality Assessment  The measurement of quality is performed after final  selection of publications. The quality of publications is  assessed in parallel at the time of data extraction. The  quality checklist contains the following questions:  • Is it clear how the vendor screening was  performed?  • Is it clear how the CSFs in the selection of  software outsourcing vendor were identified?  Each of the above factors will be marked as ‘YES’  ‘NO’ or ‘NA’. The results of the study quality  assessment were not used to limit the selection of  publications.  3.4 Data Extraction  The review was undertaken by a single researcher, who  was alone responsible for the data extraction. A  secondary reviewer was approached for guidance in  case of an issue regarding the data extraction.  The inter-rater reliability test was performed after  the data extraction process by the primary reviewer.  The secondary reviewer has selected 5 publications  randomly from the list of publication already chosen  by the primary reviewer. The secondary reviewer has  independently extracted the data from the randomly  selected publication. The results were compared with  the results produced by the primary reviewer.  From each paper we extracted a list of Quotes,  where each Quote described a list of factors that have a  positive impact on software outsourcing clients in the  selection process of offshore software development  outsourcing vendors.   The following data was extracted from each  publication: Date of review, Title, Authors, Reference,  Database, Critical Success Factors: factors that have a  positive impact on software outsourcing clients in the  selection of software development outsourcing  vendors, Methodology (interview, case study, report,  survey etc), Target Population, Sample Population,  Publication Quality Description, Organisation Type  (software house, university, research institute etc),  Company size (small, medium, large),  Country/location of the Analysis and Year.  3.5 Data Synthesis  A primary reviewer with the help of secondary  reviewer has performed data synthesis.  At the end of the Data Extraction phase described in  section 3.4 we had identified a list of factors from the  sample of 122 papers. The primary researcher  reviewed these in order to derive a list of categories to  classify the CSFs and initially a list of 33 categories  was identified. These were reviewed and some of these  categories were merged together and in this way we  got a final list of 22 factors shown in Table 2.   4. Results  4.1. CSFs Identified through Systematic  Literature Review  In order to answer RQ1, Table 2 shows the list of CSFs  identified through the SLR. ‘Cost-saving’ is the most', '4. Results  4.1. CSFs Identified through Systematic  Literature Review  In order to answer RQ1, Table 2 shows the list of CSFs  identified through the SLR. ‘Cost-saving’ is the most  common factor in our study, i.e. 69%. This suggests  that low cost software production or to charge a fair  price has a positive impact on the outsourcing clients in  the selection process of outsourcing vendors. Due to  this factor the western countries are outsourcing  projects to developing countries to take advantage from  the reduced labour costs. In order to be competitive,  vendors organisations should provide better and  cheaper services to the clients [34].  210 210 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'Our results also indicate that ‘skilled human  resource’ (67%) is an important factor for the selection  of competent vendor organisations. Research suggests  that half of the companies that have tried outsourcing  have failed to realise the anticipated results [29]. One  of the reasons for software development outsourcing  failures is the difficulties in creating confidence and  trust among the outsourcing companies [3, 15]. We  argue that ‘skilled human resource’ can play a vital  role in establishing a good relationship between client  and vendor organisations as this will help vendor  organisations to provide adequate services to client  organisations. Different studies have also described the  importance of ‘skilled human resource’ factor:  • A high-quality skilled manpower is the  backbone of the IT industry and vendors  should employ high skilled workers with  professional degrees in Computer Science,  Engineering, Management and similar fields  [35].  • Often a client organisation is eager to know  the technical capability of vendor organisation  [13].   We have also found an ‘appropriate infrastructure  of vendor organisation’ as the important factor (i.e.  60%). By ‘appropriate infrastructure’ we mean:  • IT infrastructure/Network infrastructure/  Telecommunication infrastructure.  • Physical infrastructure (related both with the  country and the company) which includes  Telecom, power/electric supply, roads,  transportation, physical buildings, office  layouts, Internet access and sewer and water  system etc.  • Sufficient resources including hardware and  software to maintain large development  projects.  Our findings indicate that developing an  appropriate infrastructure by vendor organisations has  a positive impact on client organisations. Hence, in  order to be the victor in outsourcing projects vendor  organisations should check the IT resources, including  the number of servers, the intranet structure and the  performance of the systems resources prior to  undertake outsourcing activity [36].       Table 2 List of Success Factors     S.No Success Factors Frequency (n=122) Percentage  01 Cost-saving 84 69%  02 Skilled Human Resource 82                   67%  03 Appropriate Infrastructure 73 60%  04 Quality of Products and Services 69 57%  05 Efficient Outsourcing Relationships Management  59 48%  06 Organisation’s track record of successful projects 53 43%  07 Efficient Project Management 47 39%  08 Efficient Contract Management 45 37%  9 SPI Certification 41 34%  10 Knowledge of the Client’s Language and Culture  39 32%  11 Timely Delivery of the Product 30 25%  12 Knowledge Exchange 25 21%  13 Data Protection Laws 23 19%  14 Financial Stability 14 12%  15 Company Size (Large and Medium) 08 07%  16 Risk Sharing  08 07%  17 Pilot Project Performance 06 05%  18 Vendor’s Responsiveness 05 04%  19 Political Stability 03 03%  20 Overseas Offices 02 02%  21 Soft Deliverable 01 01%  22 Industry-University Linkage 01 01%      211 211 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'Fifty seven percent of articles in our study have  cited a ‘quality of products and services’ factor that has  a positive impact on client organisations. Due to the  growth in free markets of globalisation and  advancements in information and communication  technologies, organisations have to consider taking  advantage of outsourcing strategies, not only to utilize  the cost advantages but also to benefit from the  improved quality that offshore vendors provide [37].  Indian software companies have been reported to  provide high quality software [38]. This is the reason  that in the software export market, India is a dominant  software outsourcing provider [39]. These trends show  that ‘Quality of products and services’ is used as one of  the criteria in the selection of software development  outsourcing vendors.  Nearly half of the articles in our study cited  ‘efficient outsourcing relationships management’ as an  important factor for client organisations. This shows  that establishing reliable relationships can help to  ensure the successful outcome of outsourcing projects  and long lasting relationships between clients and  vendors [3, 14, 40]. We have also identifies  ‘organisation’s track record of successful projects’ and  ‘efficient project management’ factors that have a  positive impact on client organisations.  4.2. Comparison of the CSFs across various  continents  In order to answer RQ2, Table 3 shows the list of CSFs  identified in different continents. Figure 1 shows the  number of articles reporting studies related to different  continents. In this paper we have only compared the  factors identified in three continents, i.e. Asia, America  and Europe. Our aim is to find whether these factors  differ from continent to continent. We suggest that  understanding the similarities and differences in these  factors can contribute to the body of knowledge of  software development outsourcing. This is because  where articles from different continents consider that a  factor has a positive impact on client organisations  then that factor needs to be taken very seriously by the  vendor organisations in that continent.    Because of the ordinal nature of the data we have  used the linear by linear association chi-square test in  order to find significant differences between factors  identified in three continents. The linear by linear  association test is preferred when testing the significant  difference between ordinal variables because it is more  powerful than Pearson chi-square test [41].    Comparison of the factors identified in three  continents indicates that there are more similarities  than differences between the factors. Table 3 shows  that 20 factors are cited in Asia, 19 factors in America  and 19 factors in Europe. We have found only one  significant difference between the three continents (i.e.  efficient contract management). The percentage of  ‘efficient contract management’ is low in Asia (18%)  while it has high percentages in America (42%) and in  Europe (55%). We argue that as most of the clients are  from America and Europe so they consider contract  management important in order to avoid any risks  associated with the outsourcing process.    Our findings show that ‘cost-saving’, ‘skilled  human resource’ and ‘appropriate infrastructure’ are  the most important factors in all three continents.  ‘Efficient outsourcing relationships’, ‘efficient project  management’ and ‘SPI certification’ are important  factors in Asia (58%, 45% and 39%) and Europe (45%,  45% and 35%). ‘Quality of products and services’ and  ‘schedule’ (Timely Delivery of the Product) are  important factors in America (58% and 33%) and  Europe (65% and 35%). ‘Organisation’s track record  of successful-projects’ is important in Asia (55%).  Our findings indicate that the factors such as   ‘Company size’, ‘Political stability’, ‘Financial  stability’, ‘Industry-university linkage’, ‘Overseas-', 'Our findings indicate that the factors such as   ‘Company size’, ‘Political stability’, ‘Financial  stability’, ‘Industry-university linkage’, ‘Overseas-  offices’ and ‘Risk sharing’ are not important for client  organisations in Asia, Europe and America. However,  studies show that some of these factors are important in  outsourcing business. For example:  • Bhalla et al. argue that establishing offshore  delivery centre provides an exposure to  vendor organisations [34]. In addition, some  organisations like Mirantis, Reksoft and  StarSoft development labs have their overseas  offices in California St.Petersburg, Ukraine  and Cambridge in order to gain access to the  international outsourcing market [42].    • Government policies and political stability in  vendor countries can play important role in  attracting client organisations. Chinese  government is focussing on offshore  outsourcing industry from the last few years  [43], which may be a reason that China  proceeds rapidly to victor the offshore  software industry.          212 212 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'Figure 1 Final selection of papers from various continents  Asia,  Australia America,  Europe Asia,  America,  Europe Asia,  America Asia,  Australia,  America,  Europe Asia &  Europe AmericaEuropeAustraliaAsia Count 50 40 30 20 10 0   Table 3 Summary of factors across 3 continents as identified in the SLR  Success Factors Occurrence in SLR (N=122) Chi-square Test  (Linear-by-Linear  Association)  α = .05  Asia  (N=33) America  (N=43) Europe  (N=20)   X²    df    p  Freq % Freq % Freq %  Appropriate Infrastructure 21 64 23 53 11 55 .132 1 .716  Company Size (Large and Medium) 3 9 3 7 1 5 .905 1 .342  Cost-saving 22 67 32 74 17 85 1.482 1 .223  Data Protection Laws 6 18 8 19 5 25 .190 1 .663  Efficient Contract Management 61 81 84 2 1 1 5 5  .836 1 .028  Efficient Outsourcing Relationships  Management 19 58 14 33 9 45    .530    1    .467  Efficient Project Management 15 45 13 30 9 45 .346 1 .556  Financial Stability 6 18 3 7 4 20 1.752 1 .186  Industry-University Linkage  0 0 0 0 1 5    .074    1    .786  Knowledge Exchange 7 21 6 14 4 20 .003 1 .955  Knowledge of the Client’s Language and  Culture 11 33 10 23 7 35    1.149    1    .284  Organisation’s track record of successful  projects 18 55 18 42 9 45    2.045    1    .153  Overseas Offices 1 3 0 0 0 0 .149 1 .699  Pilot Project Performance 4 12 1 2 0 0 1.090 1 .296  Political Stability 1 3 1 2 1 5 .531 1 .466  Quality of Products and Services 16 48 25 58 13 65 1.421 1 .233  Risk Sharing 0 0 4 9 4 20 .024 1 .876  Skilled Human Resource 24 73 28 65 15 75 1.025 1 .311  Soft Deliverable 1 3 0 0 0 0 1.310 1 .252  SPI Certification 13 39 10 23 7 35 .045 1 .832  Timely Delivery of the Product 6 18 14 33 7 35 .024 1 .877  Vendor’s Responsiveness 2 6 2 5 1 5 1.030 1 .310  213 213 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', '• Economic stability and company size of  vendor organisations may influence clients  because financially instable and smaller in  size vendor organisations are usually riskier  and unreliable [44].  • “Financial soundness of outsourcing  organisation” may influence client’s  organisation in outsourcing vendor selection  process [45].  • Risk sharing is important but not essential for  vendor organisations in the context of  offshore outsourcing [46].  We argue that it is important to determine the  reasons of why these factors are not important for  client organisations in Asia, Europe and America. We  encourage independent studies on this topic.  5. Limitations  How valid are our findings of CSFs in the selection  process of offshore software development outsourcing  vendors? One possible threat to internal validity is that  for any specific article, their reported CSFs may not  have in fact described underlying reason. We have not  been able to independentl y control this threat. The  authors of these studies were not supposed to report the  original reasons why these CSFs were used during the  selection of vendors. It is also possible that in some  studies there may have been a tendency for particular  kinds of CSFs to not be reported. Many of the  contributing studies were self-reported experience  reports, case studies and empirical studies which may  be subject to publication bias.    How safe is it to generalise these findings?  Our  sample contains many articles from many countries  (Figure 1). Our findings are not based on any studies  that used a random sample of software-developing  outsourcing organisations in the world.  However, in  the investigation of our research questions, our study is  the most comprehensive to date. The issue of  generalising these findings can also be considered by  comparing our findings with results from other related  studies, as discussed in sections 4.1 and 4.2. We found  many similarities in our findings and findings by other  people, and this provides some support for  generalisation.  6. Conclusion and Future Work  We identified through the SLR, factors that are  generally considered critical by clients in the selection  of offshore software outsourcing vendor. We suggest  that focusing on these factors can help offshore  outsourcing vendors to improve their readiness for  software outsourcing activities.  Our findings indicate that cost-saving should not be  considered as the only prime factor in the selection  process of software development outsourcing vendors.  Vendors should have to address the factors such as  skilled human resource, appropriate infrastructure,  quality of products and services, efficient outsourcing  relationships management, organisation’s track record  of successful projects, and efficient project  management. We suggest that outsourcing vendors  should focus on these CSFs for improving their  readiness for the outsourcing activities. We have also  compared these identified factors across the reported  datasets for the continents of Asia, Europe and  America and found that there are more similarities than  differences between the factors.  We encourage independent studies on this topic.  This will increase confidence in our findings and also  track changes in attitudes to offshore outsourcing over  time. We believe that a good understanding of these  factors is vital in improving the vendor organisations  readiness for software development outsourcing  activities. From the findings of this study, we have  identified the following goals that we plan to follow in  future:   • Analyse the critical barriers in the selection  process of offshore outsourcing vendors.  • Conduct empirical studies to determine the  implementation of those factors which have  been frequently cited in our study.   • It is also important to determine the reasons of  why some factors are not important for client  organisations in Asia, Europe and America.', 'been frequently cited in our study.   • It is also important to determine the reasons of  why some factors are not important for client  organisations in Asia, Europe and America.  Our ultimate aim is to develop a Software  Outsourcing Vendors Readiness Model (SOVRM)  [47]. This paper contributes to only one component of  the SOVRM, i.e. the identification of the CSFs. We  will validate these findings via questionnaire surveys  in the software development outsourcing industry  which will be the second stage in the development of  SOVRM. Finally we will conduct 3 case studies for the  evaluation of SOVRM. The eventual outcome of the  research is the development of SOVRM to assist  offshore outsourcing vendors in assessing their  214 214 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', 'readiness for software development outsourcing  activities.  7. Acknowledgements  We are thankful to the University of Malakand,  Pakistan and Higher Education Commission, Pakistan  for sponsoring the Ph.D research studies under FDP  scholarship. We are also thankful to all our software  engineering group members at Keele University UK,  and especially to Professor Pearl Brereton, Dr Mark  Turner and Clive Jefferies for providing assistance in  the review process.  8. References   [1] T. Kern and L. Willcocks, ""Exploring information  technology outsourcing relationships: theory and practice,""  Journal of Strategic Information Systems , vol. 9, pp. 321- 350, 2000.  [2] P. C. Palvia, ""A dialectic view of information  systems outsourcing - pros and cons,"" Information and  Management, vol. 29, pp. 265-275, 1995.  [3] M. Ali-baber, J. Verner, and P. Nguyen,  ""Establishing and maintaining tr ust in software outsourcing  relationships: An empirical investigation,"" The Journal of  Systems and Software, vol. 80, pp. 1438–1449, 2007.  [4] S. Sahay, B. Nicholson, and S. Krishna, Global IT  outsourcing: Cambridge University Press., 2003.  [5] McKinsey-Consulting, ""O ffshoring is a Win-Win  Game?,"" Aug, 2003.  [6] A. A. Bush, A. Tiwana, and H. Tsuji, ""An  Empirical Investigation of the Drivers of Software  Outsourcing Decisions in Japanese Organizations,"" in press  for publication, Information and Software Technology  Journal, 2007.  [7] L. McLaughlin, ""An eye on India: Outsourcing  debate continues.,"" IEEE Software , vol. 20, pp. 114-117,  2003.  [8] B. Shao, David, J.S., ""The impact of offshore  outsourcing on IT workers in developed countries.,""  Communications of the ACM, vol. 50, pp. 89 - 94, 2007.  [9] H. Holmstrom, E. Ó. Conchúir, P. Ågerfalk, and B.  Fitzgerald, ""Global Software Development Challenges: A  Case Study on Temporal, Geographical and Socio-cultural  Distance,"" presented at International Conference on Global  Software Engineering, 2006.  [10] D. Damian, I. Luis, S. Janice, and K. Irwin,  ""Awareness in the Wild: Why Communication Breakdowns  Occur,"" presented at International Conference on Global  Software Engineering, 2007.  [11] E. Beulen and P. Ribbers, ""Managing complex IT  outsourcing-partnerships.,"" presented at 35th Hawaii  International Conference on System Sciences., 2002.  [12] D. Daniela, I. Luis, S. Janice, and K. Irwin,  ""Awareness in the Wild: Why Communication Breakdowns  Occur,"" presented at International Conference on Global  Software Engineering, 2007.  [13] P. Nguyen, M. Ali-baber, and J. Verner, ""Trust in  software outsourcing relationships: an analysis of  Vietnamese practitioners\' views,"" presented at EASE, 2006.  [14] R. Sabherwal, ""The role of trust in outsources IS  development projects,"" Communication of ACM , vol. 42, pp.  80-86, 1999.  [15] R. Heeks, S. Krishna, B. Nicholson, and S. Sahay,  ""Synching or Sinking: Global Software Outsourcing  Relationships,"" IEEE Software, vol. March/ April 2001, pp.  54-60, 2001.  [16] N. V. Oza, T. Hall, A. Rainer, and S. G. Grey,  ""Trust in software outsourcing relationships: An empirical  investigation of Indian software companies,"" Information &  Software Technology, vol. 48, pp. 345-354, 2006.  [ 1 7 ]  J .  S t a r k ,  M .  A r l t ,  a n d  D .  W a l k e r ,  H .  T . ,   ""Outsourcing Decisions and Models - Some Practical  Considerations for Large Organizations,"" presented at  International Conference on Global Software Engineering,  2006.  [18] M. Cataldo, M. Bass, Herbsleb, J. D., and L. Bass,  ""On Coordination Mechanisms in Global Software  Development,"" presented at International Conference on  Global Software Engineering, 2007.  [19] R. Gonzalez, J. Gasco, and J. Llopis, ""Information  systems outsourcing risks: a study of large firms,"" Industrial  management and data systems, vol. 105, pp. 45–62, 2005.  [20] N. V. Oza, ""An empirical evaluation of client -  vendor relationships in Indian software outsourcing', 'management and data systems, vol. 105, pp. 45–62, 2005.  [20] N. V. Oza, ""An empirical evaluation of client -  vendor relationships in Indian software outsourcing  companies,"" in School of Computer Science : University of  Hertfordshire, UK, 2006.  [21] N. V. Oza and T. Hall, ""Difficulties in managing  offshore outsourcing relationships: An empirical analysis of  18 high maturity Indian software companies,"" presented at  4th International Outsourcing Conference, Washington DC,  2005.  [22] R. Sabherwal, ""The role of trust in outsources IS  development projects,,"" Communication of ACM, vol. 42, pp.  80-86, 1999.  [23] T. M. Rajkumar and D. L. Dawley, ""Problems and  issues in offshore development of software,"" Strategic  sourcing of Information Systems: Perspectives and Practices,  Wiley Series in Information Systems, pp. 369-386, 1997.  [24] N. Khan, W. L. Currie, V. Weerakkody, and B.  Desai, "" Evaluating offshore IT outsourcing in India: supplier  and customer,"" presented at System Sciences, 2003.  Proceedings of the 36th A nnual Hawaii International  Conference, Hawaii, 2003.  [25] S. Sakthivel, Communication of the ACM , vol. 50,  2007.  [26] C. L. Iacovou and R. Nakatsu, ""A risk profile of  offshore-outsourced development projects,"" Communications  of the ACM, vol. 51, pp. 89-94, June,2008.  [27] R. Narayanaswarmy and R. M. Henry, ""Effects of  culture on control mechanisms in offshore outsourced IT  projects,"" presented at Proceedings of the 2005 ACM  SIGMIS CPR conference on Computer personnel research,  Atlanta, Georgia, USA, 2005.  215 215 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.', '[28] B. Aubert, J.-F. Houde, M. Patry, and S. Rivard,  ""Characterestics of IT Outsourcing Contracts,"" presented at  Proceedings of the 36th Hawaii International Conference on  System Sciences, 2003.  [29] D. Bradstreet, ""Dun & Bradstreet\'s Barometer of  Global Outsourcing,"" vol. 2007: Dun & Bradstreet, 2000.  [30] G. Kathleen and K. F., Whitlow, ""What Causes  Outsourcing Failures?,"" vol. 2007:  outsourcingbestpractices.com, 2004.  [31] B. Nicholson and S. Sahay, ""Some political and  cultural issues in the globalisation of software development:  case experience from Britain and India,"" Information and  Organization, vol. 11, pp. 25–43, 2001.  [32] B. Kitchenham, ""Procedures for Performing  Systematic Reviews,"" Keele University Technical  ReportTR/SE0401, 2004.  [33] K. Siffatullah and M. Niazi, ""Systematic Literature  Review Protocol for Software Outsourcing Vendors  Readiness Model,"" Technical Report: TR/08-01, Keele  University, UK, 2008.  [34] A. Bhalla, M. S. Sodhi, and B.-G. Son, ""Is more IT  offshoring better? An exploratory study of western  companies offshoring to South East Asia,"" Journal of  Operations Management, Elsevier , vol. 26, pp. 322-335,  2008.  [ 3 5 ]  A .  B .  N a u m a n ,  R .  A z i z ,  A .  F .  M .  I s h a q ,  a n d  M .   Mohsin, ""An analysis of capabilities of Pakistan as an  offshore IT services outsourcing destination,"" presented at  Proceedings of IEEE 8th International INMIC, Multitopic  Conference, Dec. 2004.  [36] J. Hongxun, D. Honglu, Y. Xiang, and S. Jun,  ""Research on IT outsourcing based on IT systems  management,"" presented at ACM International Conference  Proceeding Series; Vol. 156,, 2006.  [37] J. Hagel, Brown, J.S., ""The Only Sustainable  Edge.,"" Harvard Business School Press, Boston, 2005. 2005.  [38] S. C. Bhatnagar and S. Madon, ""The Indian  software industry: moving towards maturity,"" Journal of  Information Technology, vol. 12, pp. 277-288., 1997.  [39] R. Terdiman and F. Karamouzis, ""Going Offshore  to Globally Source IT Services. Technical Report,"" Gartner  Research 2002.  [40] J. Goo, R. Kishore, K. Nam, H. Rao, R. , and Y.  Song, ""An investigation of factors that influence the duration  of IT outsourcing relationships,"" Decision Support Systems ,  vol. 42, pp. 2107-2125, 2007.  [41] B. Martin, An Introduction to Medical Statistics,  3rd edition: Oxford medical publications, 2000.  [42] M. A. Cusumano, ""Where Does Russia fit into the  Global Software Industry?,"" in Communication of the Acm, ,  vol. 49, February, 2006.  [43] N. Zhang, ""An Analysis Framework of Factors  Influencing China Software and Information Service  Offshore Outsourcing,"" presented at IEEE Fifth International  Conference on   Information Technology: New Generations,  USA, 2008.  [44] N. Zhang, ""An Analysis Framework of Factors  Influencing China Software and Information Service  Offshore Outsourcing,"" presented at IEEE Fifth International  Conference on   Information Technology: New Generations,  2008. ITNG 2008., USA, 2008.  [45] R. E. Ahmad, ""Software maintenance outsourcing:  Issues and strategies,"" Journal of Computers and Electrical  Engineering, vol. 33, pp. 449-453, 2006.  [46] B. Ahmed, ""Is outsourcing of intangibles a real  source of competitive advantage?,"" International Journal of  Applied Quality Management, vol. 2, pp. 127-151, 1999.  [47] S. Khan, M. Niazi, and R. Ahmad, ""A Readiness  Model for Software Outsourcing Vendors Readiness Model,""  presented at IEEE international conference on Global  Software Engineering, ICGSE-08, pp:273-277, Bangalore,  India, 2008.    216 216 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:10:20 UTC from IEEE Xplore.  Restrictions apply.']","['CRITICAL SUCCESS FACTORS FOR OFFSHORE SOFTWARE DEVELOPMENT OUTSOURCING VENDORS • Among the researches analised in the sys- tematic review, Cost -saving is the most  common success factor (69%) which sug - gests that low cost software production or  to charge a fair price has a positive impact  on the outsourcing. • Studies also indicate that skilled human  resource (67%) is an important factor for  the selection of competent vendor orga - nisations. Research suggests that half of  the companies that have tried outsour - cing have failed to realise the anticipated  results. • Results show that appropriate infrastruc- ture of vendor organisation  as an im - portant factor (60%). By “appropriate in - frastructure” we mean: IT infrastructure,  network infrastructure,telecommunica - tion infrastructure, power/electric supply,  roads, transportation, physical buildings,  office layouts, internet access and sewer  and water system, and sufficient resour - ces including hardware and software to  maintain large development projects. • 57% of studies have cited that quality of  products and services has a positive im - pact on client organisations.   • Nearly half of researches mentioned effi- cient outsourcing relationships manage - ment as an important factor for client or- ganisations. This shows that establishing  reliable relationships can help to ensure  the successful outcome of outsourcing  projects and long lasting relationships be- tween clients and vendors  • The complete list of barriers is presented  in Table 1. Frequency that each barrier  appear on the 122 researches analyzed by  the systematic review is on column Freq. Success Factor Freq (n=122) % Cost saving 84 69 Skilled Human Resource 82 67 Appropriate Infrastructure 73 60 Quality of Products and  Services 69 57 Efficient Outsourcing Rela- tionships Management 69 48 Success Factor Freq (n=122) % Organisations track record  of successful projects 53 43 Efficient Project Manage- ment 47 39 Efficient Contract Manage- ment 45 37 SPI Certification 41 34 Knowledge of the Clients  Language and Culture 39 32 Timely Delivery of the Pro- duct 30 25 Knowledge Exchange 25 21 Data Protection Laws 23 19 Financial Stability 14 12 Company Size (Large and  Medium) 8 7 Risk Sharing 8 7 Pilot Project Performance 6 5 Vendors Responsiveness 6 5 Political Stability 3 3 Overseas Offices 2 2 Soft Deliverable 1 1 Industry University Linkage 1 1 • Comparison of the barriers identified in  three continents (Asia, America and Eu - rope) indicates that there are more simi - larities than differences between the bar- riers. • 20 factors are cited in Asia, 19 factors in  America and 19 factors in Europe. • Only one significant difference betwe - en the three continents has been found  (efficient contract management ). The  percentage of efficient contract mana - gement is low in Asia (18%) while it has  high percentages in America (42%) and in  Europe (55%). • Researches show that cost saving, skilled  human resource and appropriate infras- tructure are the most important factors in  all three continents. Efficient outsourcing  relationships, efficient project manage - ment and SPI certification are important  factors in Asia (58%, 45% and 39%) and  Europe (45%, 45% and 35%). Quality of  products and services and schedule ( Ti- mely Delivery of the Product ) are impor- tant factors in America (58% and 33%)  and Europe (65% and 35%). Organisa - tion’s track record of successful projects is  important in Asia (55%). • The results also indicate that factors such  as Company size, Political stability, Finan- cial stability, Industry university linkage,  Overseas offices and Risk sharing ar not  important for client organisations in Asia,  Europe and America. FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Khan, S.U.; Niazi, M.; Ahmad, R., “Critical Success Factors for Offshore Software Development Outsourcing Vendors: A Systematic Literature Review,” in Global Software Engi-', 'Khan, S.U.; Niazi, M.; Ahmad, R., “Critical Success Factors for Offshore Software Development Outsourcing Vendors: A Systematic Literature Review,” in Global Software Engi- neering, 2009. ICGSE 2009. Fourth IEEE  International Conference on , vol., no., pp.207 216, 13 16 July 2009. doi: 10.1109/ICGSE.2009.28 Keywords: Software Outsourcing Software Offshoring Vendors Who is this briefing for? Software engineers practitioners  who want to make decisions  about software development  outsourcing based on scientific  evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic  review conducted by Khan et al. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg This briefing reports evidence on critical suc- cess factors that have a positive impact on  software outsourcing clients in the selection  process of offshore software development  outsourcing vendors based on scientific evi - dence from a systematic review.']","**Title:** Key Factors for Selecting Offshore Software Development Vendors

**Introduction:**  
This Evidence Briefing summarizes critical success factors (CSFs) identified through a systematic literature review regarding the selection of offshore software development vendors. By understanding these factors, organizations can enhance their decision-making processes and improve outsourcing outcomes.

**Main Findings:**  
The systematic literature review identified several key factors that influence the selection of offshore software development vendors, highlighting the importance of a multifaceted approach beyond just cost considerations:

1. **Cost-Saving (69%):** While cost-efficiency remains a primary factor for clients, it should not be the sole criterion. Vendors must offer competitive pricing but also ensure quality and service reliability.

2. **Skilled Human Resource (67%):** The availability of skilled personnel is crucial. Vendors should prioritize hiring qualified professionals with relevant expertise to foster trust and effective collaboration with clients.

3. **Appropriate Infrastructure (60%):** Vendors must maintain robust IT and physical infrastructure, including reliable telecommunications and power supply, to support large-scale projects effectively.

4. **Quality of Products and Services (57%):** High-quality deliverables are essential for client satisfaction. Vendors should focus on maintaining standards to build long-term relationships with clients.

5. **Efficient Outsourcing Relationships Management (48%):** Establishing and managing strong relationships with clients is vital for successful project outcomes.

6. **Organization's Track Record (43%) and Efficient Project Management (39%):** A proven history of successful projects and effective management practices are significant indicators of a vendor's capability.

7. **Cultural and Language Knowledge (32%):** Understanding the client's language and culture can facilitate smoother communication and collaboration.

The review also revealed similarities in factors across continents, with cost-saving, skilled human resources, and appropriate infrastructure being consistently important in Asia, America, and Europe. However, some factors, such as efficient contract management, were more emphasized in America and Europe compared to Asia.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, project managers, and decision-makers involved in selecting offshore software development vendors.

**Where the findings come from:**  
The findings are derived from a systematic literature review conducted by Siffat Ullah Khan, Mahmood Niazi, and Rashid Ahmad, which analyzed 122 studies on the critical success factors in the selection process of offshore software development vendors.

**What is included in this briefing?**  
The briefing includes an overview of the critical success factors essential for vendor selection, insights into the comparative importance of these factors across different continents, and practical implications for organizations involved in outsourcing.

**What is not included in this briefing?**  
Detailed statistical analyses or specific case studies are not included. The focus is on summarizing the identified factors and their relevance.

**To access other evidence briefings on software engineering:**  
[Evidence Briefings Repository](http://ease2017.bth.se/)

**Original Research Reference:**  
Khan, S. U., Niazi, M., & Ahmad, R. (2009). Critical Success Factors for Offshore Software Development Outsourcing Vendors: A Systematic Literature Review. Proceedings of the 2009 Fourth IEEE International Conference on Global Software Engineering, 207-212. DOI: 10.1109/ICGSE.2009.28"
"['Deﬁnitions and approaches to model quality in model-based software development – A review of literature Parastoo Mohagheghi*, Vegard Dehlen, Tor Neple SINTEF, P.O. Box 124, Blindern, N-0314 OSLO, Norway article info Article history: Available online 21 April 2009 Keywords: Systematic review Modelling Model quality Model-driven development UML abstract More attention is paid to the quality of models along with the growing importance of modelling in soft- ware development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identiﬁed; i.e., correctness, completeness, consistency, com- prehensibility, conﬁnement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying deﬁnitions of model quality and identifying gaps for future research. /C2112009 Elsevier B.V. All rights reserved. 1. Introduction For years, modelling has been advocated as an important part of software development in order to tackle complexity by providing abstractions and hiding technical details. Due to the wide applica- tion of modelling, numerous informal and formal approaches to modelling have been developed, such as Entity Relationship Dia- grams (ERD) for modelling data, Speciﬁcation and Description Lan- guage (SDL) for modelling telecommunication systems, formal modelling languages such asZ and B, and the Uniﬁed Modeling Language (UML) which is the most widely modelling language used by industry today. Modelling was initially applied for communication between stakeholders and providing sketches (also called models or dia- grams) of what a software system must do or its design. Nowadays, industry tends to use models more and more for tasks other than describing the system, for example simulation, generating test cases and parts or all of the source code. The growing attention on using models in software development has subsequently brought the quality of models as a research area in forefront. In late 2000, the MDA (Model-Driven Architecture) 1 initiative was launched by OMG (Object Management Group) to promote using models as the essential artefacts of software development. Followed by MDD (Model-Driven Development) or MDE 2 (Model-Driven Engineering), we face a new paradigm in software development where models are the primary software artefacts and transforma- tions are the primary operations on models. In MDE, there is a lot to consider regarding the quality of models to ensure that right arte- facts are generated. Finally, since defects can be earlier detected and corrected in models, improving the quality of models will ultimately reduce maintenance costs[8]. The QiM (Quality in Model-driven engineering)3 project at SIN- TEF is concerned with the quality of artefacts and activities in model- based software development in general and MDE speciﬁcally. The term ‘‘model-based software development” in this article covers a spectrum of approaches where models are widely used in software development for more than visualizing the source code or providing informal sketches of design. Quality in model-based software devel- opment covers the quality of models, modelling languages, model- ling tools, modelling processes and even transformations performed on models. One of the outcomes of the QiM project has been identiﬁcation of constructs needed to develop quality models as described in[13]. With a ‘‘quality model” we mean a set of quality goals (also called quality attributes or quality characteristics in the literature) and their relations, accompanied by a set of practices or means to achieve the quality goals, evaluation methods for evaluat- ing quality goals and links to related literature. The focus of this arti-', 'means to achieve the quality goals, evaluation methods for evaluat- ing quality goals and links to related literature. The focus of this arti- cle is on identifying quality goals for models together with practices in model-based software development that can improve the quality of models, by performing a systematic review of literature on model quality. By identifying practices we take a preventive approach to software quality. Some tools and methods for assessing model qual- ity and empirical evidence that are reported in the covered literature are presented in this article as well. 0950-5849/$ - see front matter/C2112009 Elsevier B.V. All rights reserved. doi:10.1016/j.infsof.2009.04.004 * Corresponding author. Tel.: +47 22067497; fax: +47 22067350. E-mail addresses: parastoo.mohagheghi@sintef.no (P. Mohagheghi), vegard. dehlen@sintef.no (V. Dehlen),tor.neple@sintef.no (T. Neple). 1 http://www.omg.org/mda. 2 We use the term MDE in the remainder of this article to cover approaches where development is mainly carried out using models at different abstraction levels and possibly from different viewpoints, and where models provide a precise foundation for reﬁnement as well as transformation so that other artefacts are generated from models; thus also covering MDA and MDD.3 http://quality-mde.org/. Information and Software Technology 51 (2009) 1646–1669 Contents lists available atScienceDirect Information and Software Technology journal homepag e: www.elsevier.c om/locate/infsof', 'While model quality is covered in previous literature and various quality models are proposed; among them in [P15,P19,P22,P26,P38],4 these quality models have some short- comings as discussed in[12] and none of them provide their clas- siﬁcation of model quality goals based on an analysis of previous work. Therefore we have performed a systematic review of litera- ture discussing model quality to answer the following research questions: /C15RQ1. What quality goals are deﬁned in literature for models in model-based software development? /C15RQ2. What practices are proposed to achieve or improve the above quality goals? /C15RQ3. What types of models and modelling approaches are covered in literature? Since UML is currently the most widely used modelling lan- guage, UML models are the subject of most work related to the quality of models. However, UML may be used in multiple ways; from providing informal sketches to full system speciﬁcation end even as a programming language and extended for speciﬁc do- mains. Therefore literature on UML models covers different ap- proaches to modelling, while approaches where models play a central role in software development are in focus here. We emphasize that the focus of this article is on the quality of models representing or describing software systems and not the quality of system design or implementation; for example patterns and practices for object-oriented design and metrics on the design level. Unhelkar deﬁnes model quality as ‘‘the correctness and com- pleteness of software models and their meanings” and separates it from code quality and architecture quality [P38]. We share the same view in this article. It is also clear that the quality of model- ling languages, modelling tools and the expertise of people per- forming modelling will impact the quality of developed models. These issues are not covered in this article while some related work is discussed in[9]. The remainder of this article is organized as follows. The review process and the literature covered in this review are presented in Section 2 and validity threats are discussed. In Section3, we dis- cuss what models are and what roles they have in software devel- opment. Section4 answers RQ1 by identifying six model quality goals. These quality goals have been discussed previously in liter- ature but never put together and deﬁned in relation to one another. In Section5, we discussRQ2 by presenting the practices proposed in literature in order to improve the quality of models, together with types of models and modelling approaches related toRQ3 and the results of empirical studies whenever reported. Section6 provides a summary of the results while Section7 is discussion. Fi- nally, the article is concluded in Section8, answers to research questions are summarized and gaps for future research are discussed. 2. The review process and the included literature In [7], Kitchenham et al. provide guidelines for performing sys- tematic literature reviews (or in short systematic reviews) in soft- ware engineering. A systematic review is a means of evaluating and interpreting all available research relevant to a particular re- search question, topic area, or phenomenon of interest. A system- atic review is therefore a type of ‘‘secondary study” that reviews ‘‘primary studies” relating to a speciﬁc research question. A pri- mary study in[7] is deﬁned as an empirical study investigating a speciﬁc research question. The process of performing a systematic review should be rigorous and auditable and include a review pro- tocol. The above guidelines also discuss research question types in systematic reviews which are mostly related to evaluate the effect, cost or acceptance rate of a technology, thus reviewing empirical studies with quantitative data. However, systematic reviews can cover other research questions of interest to researchers as well. The goal of this review is to provide deﬁnitions and classiﬁcations', 'studies with quantitative data. However, systematic reviews can cover other research questions of interest to researchers as well. The goal of this review is to provide deﬁnitions and classiﬁcations while empirical evidence is also collected. As discussed by Jørgensen and Shepperd, the process of deﬁning an appropriate classiﬁcation for any purpose is usually a bottom- up process by which the researchers read some studies, specify some categories based on the papers they have read and their own experience of the domain, then read more and reﬁne the cat- egories as necessary[5]. We therefore identiﬁed a set of publica- tion channels where we had experienced that work on model quality would be published. We searched these publication chan- nels for papers published since 2000, starting the search in March 2007 and ending it in October 2007. The following publication channels were fully searched for pa- pers discussing quality of models and the model-driven approach: /C15The Software and Systems Modeling (SoSyM) journal since 2002 (the ﬁrst issue). /C15Proceedings of the UML conference from 2000 to 2004, suc- ceeded by the MoDELS conference. /C15Proceedings of The European Conference on MDA-Foundations and Applications (ECMDA-FA) started in 2005. /C15Proceedings of the International Conference on Software Engi- neering (ICSE). /C15Proceedings of OOPSLA, Conference on Object-oriented Pro- gramming Systems, Languages, and Applications. /C15Proceedings of the Quality in Modelling (QiM) workshops at MoDELS conference started in 2006. The following publication channels were searched with key- words; i.e., ‘‘quality + model”, ‘‘quality + model driven” and ‘‘model driven + experience”: /C15Journal of Systems and Software. /C15Information and Software Technology Journal. /C15Software Quality Journal. /C15Empirical Software Engineering Journal. /C15IEEE Xplore. /C15ACM digital library. During the search, we identiﬁed candidate papers by evaluating their title and abstract. All candidates were registered in a ﬁle. We then drew a map over the subjects covered in the papers (catego- rization) and selected relevant papers to this review. The main cri- terion for including a paper in this review is that the paper provides deﬁnitions of model quality or discusses approaches to improve model quality. Some additional papers were found in the references of the detected papers and we also included a num- ber of relevant books. This review covers 40 ‘‘Primary studies (P)” (or in short ‘‘stud- ies” in the remainder of the article 5) related to model quality; including 3 books, 1 Ph.D. thesis, 7 papers published in journals, 11 papers published in the proceedings of conferences, 17 papers published in the proceedings of various workshops, and 1 paper pub- lished online. One of the studies is published in 1994 (it is included since it is an important work and is referred in several other studies), 2 in 2000, 3 in 2001, 2 in 2002, 1 in 2003, 11 in 2004, 4 in 2005, 9 in 4 References beginning with ‘‘P” refer to primary studies covered in this review as given inAppendix I. 5 Not all the studies in our review are empirical studies and therefore a primary study in this article refers to studies covered in this review. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1647', '2006, and 7 in 2007. It is clear that the subject of model quality has gained more attention since 2004. A list of included studies is given inAppendix Inumbered from P1 to P40, whileAppendix II provides details on the number of studies detected in each publication channel and a short descrip- tion of studies is given inTable 1. Table 1 also shows modelling language or modelling approach in each study with bold font. Examples are ‘‘UML”, ‘‘MDE” and ‘‘quality model” (for studies dis- cussing quality models). Not surprisingly, most of the studies are concerned with the quality of UML models. However, studies in- clude a spectrum of approaches to modelling; from capturing sys- tem requirements to detailed design models, and to MDE including UML proﬁles and domain-speciﬁc modelling languages. We have followed the steps of deﬁning a review protocol and performing a systematic search as recommended in[7] while there are some deviations from the process of a systematic review; i.e., (a) we have not registered all the studies detected by using key- words but only the candidates for categorization; (b) we have searched a limited set of publication channels; and (c) we have only searched for recent publications. A more comprehensive re- view may be performed later by using this review’s classiﬁcations as search keys or focusing on single aspects of model quality. The main threat to the validity of the results of the review is publication bias; i.e., undetected studies when keywords are used and the uncovered publication channels. To improve the cover- age, other search engines and additional keywords may be used which may detect new studies that can improve the results. How- ever, we mean that the publication channels covered in this re- view are highly relevant for the subject of review. One may also search for studies published before 2000 or include more recent studies. There are conferences and workshops dedicated to model comprehension and model layout issues that are not covered in this review and could be covered if this aspect of model quality is in focus. A second threat is that we may have overlooked some relevant studies during identiﬁcation and categorization. Follow- ing the steps of a systematic review as recommended in [7] would increase the validity of identiﬁcation and the conﬁdence in the results. After providing an initial classiﬁcation of concepts, other results are easier to be added if they were undetected in the review process. Regarding the interest and knowledge of authors of the subject, all authors work in projects that include MDE but we are not aware of any biases when identifying and categorizing papers. 3. Models and model-based software development Models are representations of a (software) system at an abstract level [17]. In this review we use the term ‘‘model” as a description or representation of a software system or its environment for a cer- tain purpose, developed using a modelling language and thus con- forming to a metamodel. A model may consist of several ‘‘diagrams” where each diagram type gives a different view on the described system. For example UML 2.0 has 13 diagram types such as use case diagram, class diagram etc. In MDE, It is common to model a system at different abstraction levels as well; thus hav- ing several models of the same system. The role of models varies a lot in software development ap- proaches applied in companies. Fowler has identiﬁed three modes of UML use6: /C15UMLAsSketch: the emphasis of sketches is on selective commu- nication rather than complete speciﬁcation. These sketches should be acceptable by users. /C15UMLAsBlueprint: blueprints are developed by a designer whose job is to build a detailed design for a programmer to code up and thus UMLAsBlueprint requires correctness and completeness of models to some degree. /C15UMLAsProgrammingLanguage: semantics is added to UML mod- els to make them executable. Here models should have the qual-', 'models to some degree. /C15UMLAsProgrammingLanguage: semantics is added to UML mod- els to make them executable. Here models should have the qual- ity required for the purpose of execution. Brown has also discussed the spectrum of modelling as pre- sented in[18] and depicted inFig. 1. The left hand side of the spec- trum represents the traditional development without graphical modelling – the code is the main artefact. The right hand side of the spectrum represents the opposite of it, the code playing a sec- ondary role and the development is done solely using models (e.g., utilizing executable modelling techniques). The model-centric ap- proach is an ambitious goal of MDE as it still is based on code while the models are the main artefacts. Most (or all, if possible) of the code is generated from models; the developers, however, are given a possibility to add the code and synchronize it with models. The fact that the code can be altered after it is generated and it can be synchronized is close to the idea of roundtrip engineering, where the code and the model coexist and one is synchronized once the other is updated. Such a usage scenario can be seen as an advanced usage of models which is the extension of the idea of basic modelling. The basic modelling represents a situation when models are used as a documentation and as basic (usually architectural only) sketches of the software to be built. The models and the code coexist but the code is the main artefact which is used in the course of software development. In the code visualization scenario the code is the main artefact; models are generated auto- matically and are not used to develop software, but to provide means of understanding the code. Staron writes that there is no sharp borderline between which of the usage scenarios (except for ‘‘code only”) can be seen as a realization of MDE [18]. In this article we refer to model-based software development for approaches where models play a central role in software development, which covers the right hand side of Fig. 1. The covered literature covers approaches from basic model- ling to full MDE, while most refer to modelling for more than pro- viding sketches as in basic modelling. When moving from the left hand to the right hand side of the spectrum, quality requirements (or goals) for models change and quality of models become more important. For example if models and code coexist, models must be correct and complete (to some degree) and also be easy to modify to keep them in sync with the code. Thus quality goals vary depending on the purpose of models. MDE covers approaches where development is carried out using models; often at different abstraction levels and from multiple views. Although UML is the core language of the MDA initiative, MDE does not rely on UML. In fact it is often impossible to express the details of models required in a MDE approach in UML, which is the reason for extending UML and developing ‘‘UML proﬁles” or even Domain-Speciﬁc Modelling Languages (DSML). In this article we therefore cover research on the quality of UML models in addi- tion to research with MDE focus, including UML proﬁles and DSMLs. In MDE, models are subject of transformation to other models or text such as source code. The OMGs’ MDA approach differs be- tween CIM (Computational Independent Model), PIM (Platform independent Model) and PSM (Platform Speciﬁc Model) where more abstract models (CIM or PIM) can be transformed to PSM or directly to the implementation. For example, it might not be possible to completely specify an application as a PIM, but at least a large part of the application’s static structure and interface design should be captured and then translated into PSM or code[19].W e 6 See his bloghttp://martinfowler.com/bliki/. 1648 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'Table 1 A short summary of primary studies. Ref. Short description [P1] This book describes a collection of standards, conventions and guidelines for creating effectiveUML diagrams. It includes some general diagramming guidelines and some guidelines for common UML elements and diagrams. It also presents the Agile Modelling (AM) approach [P2] The authors have originally developed MCC+, a plug-in for Poseidon for model consistency checking using Description Logics (DL). In order to achieve portability, the tool is upgraded into a tool for software product lines, MCC-SPL, that may be instantiated for several differentUML modelling tools that admit the use of plug-ins [P3] The ﬁrst part of the paper describes heuristics and processes for creating semantically correctUML analysis and design models. It provides a set of conventions for different UML diagrams. The second part of the paper brieﬂy describes the internal research tool that was used to analyze Siemens models [P4] This paper describes a CMMI (Capability Maturity Model Integration) compliant approach to measurement and analysis during a model-driven requirements development process. It presents a set of metrics forUML requirement models that were used successfully on several Siemens projects, describing team dynamics, project size and stafﬁng, how the metrics were captured and used, and lessons learned [P5] The authors deﬁne consistency problems in the context of component-based development with theKobrA method, and suggest a checking mechanism using environment modelling. The approach is automated using the SPIN model checker [P6] The authors evaluated quality differences between UML analysis and design models created with and without modelling conventions. Modelling conventions were provided either as a list or a list supported by a tool for detection of their violation. The conclusion is that UML modelling conventions (regarding layout and syntax) are unlikely to affect representational quality. Conventions are needed that clarify which types of information are relevant to particular future model usages; e.g., in implementation and testing [P7] The author proposes a set of criteria to improve the aesthetics ofUML class diagramsbased on HCI principles. The paper also includes a layout algorithm which respects all these features and an implementation of a graph drawing framework which is able to produce drawings according to these criteria [P8] The author proposes a set of aesthetic criteria forUML class diagramsand discusses the relation between these criteria, HCI and design aspects of object-oriented software [P9] This paper proposes a development methodology for distributed applications based on the principles and concepts ofMDA. The paper identiﬁes phases and activities of an MDA-based development trajectory, and deﬁnes the roles and products of each activity in accordance with the Software Process Engineering Metamodel (SPEM) [P10] The authors present a way to relate informal requirements, in form ofUML use cases, to more formal speciﬁcations, written in OCL. The formal speciﬁcation can improve the informal understanding of the system by exposing gaps and ambiguities in the informal speciﬁcation [P11] This paper discusses ways to manage inconsistency (by analysis, monitoring and construction) and focuses on the second and third. The focus is on different views in conceptual models. The MERMAID modelling tool is used. The use of the Command pattern allows to implement the consistency by monitoring approach, and the use of complex commands and the Observer pattern allows for the realization of consistency by construction [P12] The paper considers the problem of consistency within and between artefacts. Based on UML,a new langaugeis formally deﬁned with less number of views. OCL is used to formulate both inter, and intra-consistency rules. These rules were partly implemented within OCL Evaluator tool', 'used to formulate both inter, and intra-consistency rules. These rules were partly implemented within OCL Evaluator tool [P13] This paper describes the ongoing MDD research efforts at Philips, introducing VAMPIRE/C0a light-weight model-driven approach to domain-speciﬁc software development [P14] The authors describe a process for constructingUML analysis model of an embedded system. The process uses goal models to capture requirements which also have constraints speciﬁed in formally-analyzable natural language properties. UML class diagrams and state diagrams are used to model structure and behaviour of the system and are transformed to formal speciﬁcations and formally analyzed for adherence to the behavioural properties captured in the goal model [P15] The authors discuss different modelling approaches applied forconceptual modelling.They further extend thequality modelof [P22] with new quality types such as social quality and empirical quality. The identiﬁed means are also extended. The framework is used to evaluate theObject Modeling Technique(OMT) which is the ancestor of UML for conceptual modelling, and shortcomings and strengths of the language are discussed [P16] The paper elaborates on the role of stereotypes from the perspective ofUML, and describes a controlled experiment aimed at evaluation of the role – in the context of model understanding. The results of the experiment support the claim that stereotypes with graphical icons for their representation play a signiﬁcant role in comprehension of models and show the size of the improvement [P17] The authors present aquality modelfor managingUML-based software development. The quality model includes purposes, quality characteristics and some metrics. They further discuss experiences in applying the quality model to several industrial case studies. Finally a tool is presented that visualizes the quality model [P18] This work reports on a controlled experiment to explore the effect of modelling conventions onUML diagrams on defect density and modelling effort. The results indicate that decreased defect density is attainable at the cost of increased effort when using modelling conventions, and moreover, that this trade-off is increased if tool-support is provided [P19] The Ph.D. thesis gives a thorough discussion ofUML usage, its diagrams and quality issues related to different purposes of modelling. Different quality models are also presented before presenting own contribution which is described also in [P17]. Different experiments performed by Lange and others as in [P21,P20] arediscussed with moredetails to cover the aspect of quality defects [P20] The authors report a multiple case study, in which 16 industrialUML models are explored for defects. The analysis results were discussed with the model developers to gain deeper insights into the quality problems. The level of occurrence for several defect types is discussed. Additionally, the paper explores the inﬂuence of factors such as model size, time pressure, and developers’ skill on model quality [P21] In this paper the authors identify tasks of model-centric software engineering and information that is required to fulﬁl these tasks. They propose views to visualize the information needed to support fulﬁlling the tasks, and metrics required to evaluate the status of tasks. The focus is onUML models [P22] The authors examine attempts to deﬁne quality as it relates toconceptual modelsand propose their ownquality modelwith three types of model quality: syntactic, semantic and pragmatic quality. The authors also propose means to improve the quality of models, for example performing inspections and simulating models [P23] The authors use a formal Object-Oriented speciﬁcation Language (OOL) to formalize and combineUML diagrams. It is thus possible to transform the consistency of UML models to the well-formedness of OOL speciﬁcations', 'UML models to the well-formedness of OOL speciﬁcations [P24] The authors introduce a general framework for formalizing a subset ofUML diagramsin terms of different formal languages based on a homomorphic mapping between metamodels describing UML and the formal language. The resulting speciﬁcations enable execution and analysis through model checking [P25] This white paper presents a set of guidelines for developing high-quality architecture models inUML [P26] This paper describes a theoretically-based set of best practices for ensuring that each step of a modelling process is performed correctly, followed by a proof of concept experiment demonstrating the utility of the method for producing a representation that closely reﬂects the real world. The paper introduces aquality model which differs between perceptual quality, descriptive quality, semantic, syntactic, pragmatic and inferential quality (continued on next page) P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1649', 'emphasize especially that future usage of models such as generat- ing test cases, code (partially or totally) or simulation drive identi- fying quality goals. In the next section we provide a deﬁnition of model quality goals that are important in model-based software development approaches depending on the purpose of models. 4. A classiﬁcation of model quality goals (RQ1) In [12] we have presented previous classiﬁcations of model quality goals7 such as: /C15The Lindland et al.’s quality framework has conceptual models in mind (models of speciﬁcation statements such as require- ment models) and classiﬁes model quality intosyntax (adhering to language rules),semantic (correct meaning and relations) and pragmatic quality (comprehensibility by the intended users) [P22]. /C15Additional model quality goals are added by Krogstie and Sølvberg to the Lindland et al. framework; such asorganizational quality (whether a model fulﬁls the goals of modelling and that all the goals of modelling are addressed through the model) and technical pragmatic quality deﬁned as being interpretable by tools [P15]. /C15Unhelkar classiﬁes model quality goals intosyntax (with focus on correctness),semantics or meaning (with focus on complete- ness, consistency and representing the domain), andaesthetics (with focus on symmetry and consistency in order to improve the look and to help understanding) [P38]. His work is on UML models. /C15Nelson and Monarchi provide an overview of quality models and discusses that modelling is a transformation from real world to an implementation in multiple steps [P26]. In each step one Code  only Code Code Code Code Code Model Code visualization Model Basic modelling Model Round-trip engineering Model Model centric Model Model only Fig. 1. Model-driven software development adoption spectrum, from[18]. Table 1(continued) Ref. Short description [P27] This paper compares a model developed in TheObject-Process Methodology(OPM), which has only a single diagram, to a model developed inObject Modeling Technique (OMT) with several diagrams. OPM shows to be more effective in terms of a better system speciﬁcation and some differences in comprehension are observed [P28] Five UML graphical notations are compared in this paper: for each, two semantically equivalent, yet syntactically different, variations were chosen from published texts. The purpose is to evaluate which notations are easier to understand for humans [P29] This paper reports on experiments assessing the effect of individual aesthetics in the application domain ofUML diagrams. Subjects’ preferences for one diagram over another were collected as quantitative data. Their stated reasons for their choice were collected as qualitative data. The work is similar to [P28] [P30] Object-Process Methodology’s (OPM)single-diagram approach is compared withUMLs multi-diagram approach regarding the level of comprehension and the quality of the constructed Web application models. The results suggest that OPM is better than UML in modelling the dynamics aspect of the Web applications. In specifying structure and distribution aspects, there were no signiﬁcant differences. The quality of the OPM models was superior to that of the corresponding UML models [P31] Business process models (diagrams) and object life cycles can provide two different views on behaviour of the same system, requiring that these diagrams are consistent with each other. The paper proposes an approach to establish their consistency. Object state diagrams are used to generate life cycles for each object type used in the process. The diagrams are developed inUML [P32] The authors propose the integrated technique related to metrics in aMDD context. The following topics are covered; (1) the application of a meta modelling technique to specify formally model-speciﬁc metrics, (2) the deﬁnition of metrics dealing with semantic aspects of models (semantic metrics) using domain', 'technique to specify formally model-speciﬁc metrics, (2) the deﬁnition of metrics dealing with semantic aspects of models (semantic metrics) using domain ontologies, and (3) the speciﬁcation technique for the metrics of model transformations based on graph rewriting systems. They use class diagram plus OCL to represent meta models with metrics deﬁnitions [P33] After presenting the context of modelling and the rationales behind the decision to useDSM, the paper describes the approach to the problems of promotion, process integration, usability and sustainable deployment of domain-speciﬁc solutions. The conclusion is that most challenges to deploy DSMs are not technical but human by nature [P34] In this paper, the key factors for the efﬁcient accomplishment of theMDA are discussed by means of an industrial case study. The factors identiﬁed are grouped into two categories – associated with usage and development of an MDA-based framework [P35] The paper describes a set of controlled experiments which were aimed at evaluating the role of stereotypes in improving comprehension ofUML models. The results of the experiments show that stereotypes play a signiﬁcant role in the comprehension of models and the improvement achieved both by students and industry professionals [P36] In model re-factoring behaviour should be preserved as speciﬁed by the model. This paper deﬁnes some behaviour preserving properties inherent to model re- factorings. AUML proﬁleis deﬁned with stereotypes extending the UML dependency relationship and its specializations to express the preservation properties between connected UML artefacts. Using a logic approach dependency relations are formalized and checked [P37] This is an experience report emphasizing the synergy resulting from combiningMDE and SPL (Software Product Line) technologies. The paper also discusses some challenges with using domain-speciﬁc solutions [P38] This book deﬁnes three aspects of aquality model: syntax, semantics and aesthetics. The book further deﬁnes three types of models: Models of Problem Spaces (MOPS), Models of Solution Space (MOSC) and Models of Background Space (MOBS). For each type of model, the author discusses necessary diagrams and quality check. Finally, For each type ofUML diagrams, checklists are provided in these three dimensions [P39] The authors report their experiences with aDSL for reinsurance business and ﬁnancial accounting. One experience is that that soft constraints (i.e., warnings instead of errors) are indispensable and should become an intrinsic part of DSLs [P40] The authors discuss that model correctness is fundamentally important in MDE. They discuss techniques to prove model correctness which cover model checking and automated theorem proving. Model analysis is applying semantic rules that look for situations that are semantically incorrect or suspicious. Different types of errors that might be detected or not detected by model analysis are discussed. Model analysis techniques are applied to industry design models developed inUML and DSL 7 The terms quality goals, characteristics, attributes, properties etc. are all used in literature and in different quality models. We use the term ‘‘quality goal” here to cover them all. 1650 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'should assure that the content is persevered. The author deﬁnes perceptual, descriptive, semantic, syntactic, pragmatic, andinferen- tial quality as quality types where some deﬁnitions match previ- ous work in [P22] and [P15]. Although the above deﬁnitions are useful, we do not see them often used in literature on model quality. Besides, the boundary be- tween syntax and semantics is sometimes blurred[4] and these terms are used inconsistently in literature. Therefore it is useful to have a classiﬁcation that is close to the concepts used in the lit- erature related to modelling. Based on the results of this review and our earlier work on developing a quality model for MDE as presented in[9,10,12],w e have identiﬁed six classes of model quality goals that are intro- duced in the remainder of this section. We also present related work that point to the origin of the deﬁnitions. C1-Correctness. Correctness is deﬁned as: (a) Including right elements and correct relations between them, and including correct statements about the domain; (b) Not violating rules and conventions; for example adhering to language syntax (well-formedness or syntactic correctness according to [P22]), style rules, naming guidelines or other rules or conventions. Several quality models deﬁne syntactic correctness relative to the modelling language andsemantic correctnessrelative to the do- main and our understanding of it. Nelson and Monarchi write that syntactic quality is determined by comparing the representation to the language while the meaning of the elements should be pre- served, called as semantic quality [P26]. Including right elements and relations is related to our under- standing of the domain and is calledsemantic validityin the frame- work of Lindland et al. [P22]. Berenbach calls a UML model semantically correctif it includes correct relations and is complaint with good practices and corporate standards [P3]. Unhelkar rather talks of errors and mentions that CASE tools keep language syntax errors to a minimum while the semantic aspect requires that the diagrams faithfully represent the underlying reality [P38]. C2-Completeness. Completeness is deﬁned as having all the nec- essary information that is relevant [P22] and being detailed en- ough according to the purpose of modelling. Berenbach deﬁnes requirement and analysis models as com- plete when they specify all the black-box behaviour of the mod- elled entity [P3,P4]. Complete models can then be used to deﬁne test cases and create project tasks. Others do not deﬁne complete- ness bur rather discuss incompleteness as missing elements in models [P27]. Mitchell writes that one should discover key prob- lem domain concepts and make sure that these are modelled in software; from system analysis models, through design and into code [P25]. Nelson and Monarchi write that the perception trans- formation should be complete and should not include anything that is not in the real world [P26]. C3-Consistency. Consistency is deﬁned as no contradictions in the model. It covers consistency between views or diagrams that belong to the same level of abstraction or development phase (hor- izontal consistency), and between models or diagrams that repre- sent the same aspect, but at different levels of abstraction or in different development phases (vertical consistency). It also covers semantic consistency between models; i.e., the same element does not have multiple meanings in different diagrams or models. Lange writes that the multi-diagram approach of UML entails the risk for contradictions between diagrams, so called inconsis- tencies. Besides, consistency defects can occur not only within a model between different diagrams, but also between models at dif- ferent abstraction levels [P19]. Consistency between diagrams or models of a system is important for correct interpretation of them. Ambler emphasizes consistency when modelling a system in the', 'ferent abstraction levels [P19]. Consistency between diagrams or models of a system is important for correct interpretation of them. Ambler emphasizes consistency when modelling a system in the sense that common elements have common names to avoid con- fusing readers [P1]. Berenbach also discusses consistency of deﬁni- tions across all diagrams [P3]. Several other studies discuss inconsistency problems and how to avoid them as discussed later in Section5. C4-Comprehensibility. Comprehensibility is deﬁned as being understandable by the intended users; either human users or tools. Lindland calls this pragmatic quality [P22], which is the term used by Krogstie and Sølvberg [P15] and Nelson and Monarchi [P26] as well. For humans, several aspects impact comprehensibility such as aesthetics of diagrams [P29,P38], organization of a model [P1,P3], model simplicity or complexity [P1], using concepts familiar for the users or selected from the domain ontology [P1,P25], and ﬁnal- ly using the correct type of diagram for the intended audience. For example, Berenbach writes that UML is ﬂexible regarding the choice of diagrams for deﬁning a process. Sequence, collaboration, activity and state diagrams can all be used. However, he recom- mends using sequence diagrams which is his experience are easiest to read for non-technical reviewers [P3]. Ambler has a set of guide- lines to improve readability of diagrams which are both related to aesthetics (such as avoiding crossing lines) and organization of ele- ments on a diagram [P1]. Unhelkar writes that once the syntax and the semantics are correct, we need to consider the aesthetics of the model [P38]. Aesthetics is simply style or ‘‘look and feel” which has a bearing on the models readability or comprehensibility. Focus of most literature is on comprehensibility by humans. For tools, having a precise or formal syntax and formal seman- tics helps analysis and generation. Krogstie and Sølveberg deﬁne ‘‘technical pragmatic quality” as to what extent tools can be con- structed to understand the models [P15]. C5-Conﬁnement. Conﬁnement is deﬁned as being in agreement with the purpose of modelling and the type of system; such as including relevant diagrams and being at the right abstraction le- vel. A model is a description from which detail has been removed intentionally. A conﬁned model does not have unnecessary infor- mation and is not more complex or detailed than necessary. Ambler writes that models should be kept simple and one must avoid details not necessary for the purpose of modelling [P1], where the motivation is to improve readability. Other motivations may be to avoid keeping several models in sync and reducing the effort spent on modelling. Mitchell emphasizes that adding details to a high-level diagram is different from adding design details [P25]. The ﬁrst makes an imprecise model more precise while the second adds unnecessary information. Developing the right model for the type of system or purpose also depend on selecting the right modelling language. However, our focus here is on model quality. C6-Changeability. Changeability is deﬁned as supporting changes or improvements so that models can be changed or evolved rapidly and continuously. Changeability is not mentioned in previous work as a separate quality goal although the need for updating models is obvious and mentioned in several studies. It is required since both the do- main and our understanding of it or requirements of the system evolve with time. Mitchell writes that a system must resemble the problem so that it may be changed when the problem domain changes [P25]. Jonkers et al. write that the modelling discipline should be close to the problem domain which enables simpler maintenance and evolution of models [P13]. Berenbach recom- mends avoiding early packaging since it implies partitioning and may result in frequent reorganizations [P3]. Changeability should be supported by modelling languages and', 'mends avoiding early packaging since it implies partitioning and may result in frequent reorganizations [P3]. Changeability should be supported by modelling languages and modelling tools as well. For example Krogstie and Sølvberg have P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1651', 'identiﬁed modiﬁability of language as a practice that helps achiev- ing validity and completeness [P15]. 6C goals. We call the above quality goals collectively for the6C (model quality) goals. Fig. 2shows the6C goals and their relations to other elements involved in modelling. Compared with the Lind- land et al.’s framework in [P22], we have added modelling rules and organization (deﬁning the goals of modelling) to the framework. Fig. 3 is another view of the6C goals that shows when in the development process they are important. The ﬁgure is inspired by the idea of viewing modelling as a set of transformations [P26]. A model is a representation of a system and should be com- plete relative to the system it wants to represent and according to the modelling goals deﬁned by the organization. It should also con- tain correct relations between elements and correct meanings. All these properties depend on the perception of the modeller of the domain and the purpose of modelling. The developed models are required to be correct relative to the language and modelling rules or conventions, and be comprehensible for interpretation by hu- mans or by tools for the purpose of generation, simulation or anal- ysis. Of course precise deﬁnition of quality goals depends on the context and the purpose of modelling, especially whether models are used for implementation, testing and maintenance of systems. Finally, we mean that other quality goals discussed in literature can be satisﬁed if the6C goals are in place. For example a model that is correct, complete and consistent does not allow multiple interpretations and all of the above goals are important in order to support reusability of models. The6C goals are identiﬁed based on the analysis of literature covered in this review and the list may therefore be modiﬁed or extended if new requirements are detected. In the next section we present the proposed approaches for improving the quality of models which are referred to as ‘‘prac- tices” in our quality model, together with the reported empirical evidence. 5. Practices proposed to improve the quality of models (RQ2and RQ3) In this section we discuss the practices proposed in the studies to be applied during modelling to improve the quality of models. Most practices are concerned witherror prevention, while some also facilitateerror detection. We have identiﬁed six classes of prac- tices which are presented throughout this section together with examples of empirical work. We also discuss their impact on the 6C goals introduced in Section4. The six practices are divided in two groups: (a) The ﬁrst group is related to ‘‘modelling process” and covers having a model-based development process, using model- ling conventions and the single-diagram approach 8; (b) The second group is related to ‘‘formal approaches and auto- mation” and covers formal models, domain-speciﬁc solu- tions and generating models from models. Table 2shows an overview of the studies covered in this review ordered after the proposed practice. The impact of practices on quality goals, the name of tool used or developed and the type of empirical evidence is also given. There are four studies that cover quality models in general and refer to most or all of the quality goals; i.e., [P15,P22,P26,P38]. InTable 2 there is a column called ‘‘Demo or Empirical approach” where the values are: /C15‘‘–” for studies that are pure discussion. This covers three studies. /C15‘‘Example” which shows that the proposed practice is applied on an example application to demonstrate its usefulness. An exam- ple is not empirical evidence. 16 studies include such examples. /C15‘‘Student experiment” indicates that a controlled experiment is performed with students as subjects; described in 9 studies. /C15‘‘Industrial case” refers to describing experience from applying a practice in industry. Industrial cases detected in this review do not have the level of formality required of a ‘‘case study” as', '/C15‘‘Industrial case” refers to describing experience from applying a practice in industry. Industrial cases detected in this review do not have the level of formality required of a ‘‘case study” as deﬁned in[20] such as a precise deﬁnition of research questions, context, data collection methods and results. We found descrip- tion or reference to industrial cases in 14 studies. The sum is 42 since two studies cover both student experiments and industrial cases; i.e., [P35,P19]. Although the focus of this sys- tematic review has not been on collecting evidence and appraising approaches, the data provide examples for evaluation and future Environment (Domain,  Organization) comprehensibility Model Language & Modelling  Rules Tools Human- users completeness correctness consistency comprehensibility confinement correctness changeability Fig. 2. The 6C model quality goals. Analysis & generation  tools Real World  (domain and  organization) Model Modelling language Modelling  tool Modeller <perceives> <elicits &  develops> completeness correctness confinement changeability Rules & guidelines <uses> <uses> Codecomprehens ibility com prehe nsibil ity correctnesscorrectness <uses> <generates> Human users (customers,  developers, etc.) <uses> <uses> <develops> consistency Fig. 3. Model-based software development with transformation of real world to running software. 8 Selecting a single-diagram approach depends on the modelling language. However, we chose to group it under modelling process since selecting suitable languages and diagrams is often a step of modelling processes as discussed later. 1652 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'Table 2 A classiﬁcation of primary studies regarding practices and their impact on quality goals. Ref. Type of model Practice and impact on quality goals Demo or empirical approach Tool Model-based software development [P22,P15] Conceptual models Activities such as ﬁltering and inspection should be performed to improve comprehensibility by humans Example – [P26] Conceptual models A good modelling process will provide mechanisms for error prevention, detection and correction. Descriptive quality (regardingcorrectness and completeness) can be prevented by interviewing key informants. Semantic errors (correctness) are best prevented through a cycle of data reduction, data display, and veriﬁcation. Human inspection (sometimes by multiple users) is also proposed to improve the models regardingcorrectness, completeness and comprehensibility Industrial case – [P1] UML diagrams The Agile Modelling (AM) approach is proposed which uses agile practices to increase agility (related tochangeability) Example – [P17,P19] UML diagrams/metrics Quality goals are identiﬁed based on the purpose of models, which is related to conﬁnement; i.e., focus on purpose. Metrics are deﬁned for different goals Industrial case QualityView for collecting metrics and visualization [P21] MDE approach/metrics Metrics are collected from models and visualized in order to support comprehension of diagrams, design quality evaluation andcompleteness Industrial case MetricView Evolution for collecting metrics and visualization [P32] MDE approach/metrics Metrics are proposed to be formally deﬁned and collected from various models/ diagrams Example – [P40] MDE approach/SDL and UML models Model analysis techniques should be applied to improvecorrectness of models. Rules can be deﬁned in the model analysis tools Industrial case – [P9] MDE approach A traceability strategy should be deﬁned to improveconsistency. An activity is deﬁned for selecting a modelling language that is expressive enough for the domain and the needs of modelling, which is related toconﬁnement Example – [P34] MDE approach MDE process is focused on transformations: based on the requirements, the team elicits requirements speciﬁc for the PIM and PSM transformations which in turn form the basis for UML proﬁles deﬁnition. See later under UML proﬁles for beneﬁts Industrial case – Modelling conventions [P28,P29] UML class and collaboration diagrams Conventions are regarding layout of UML diagrams. Some notations perform better than others oncomprehensibility by humans Student experiment – [P7,P8] UML class diagrams Layout of diagrams is discussed related to comprehensibility by humans. Some problems with comprehensibility may indicate poor design Example Graph drawing framework SugiBib [P35] UML architectural models Models should be checked for having enough details (completeness) and cross- checked forconsistency. Correct organization of models helpscomprehensibility by humans. Identifying domain concepts and keeping analysis models free of implementation helpsconﬁnement –– [P5] UML models in KobrA Conventions are proposed to improve inter-component and intra-component consistency in the KobrA approach Example SPIN model checker [P3] UML analysis and design models Examples of conventions are given in Table III. Except changeability,all other quality goalsare mentioned. The tool checks models for some of the conventions Example DesignAdvisor [P1] UML diagrams Conventions are proposed that coverall quality goals.Examples are given in Table III Example – [P38] UML diagrams Conventions are proposed that coverall quality goals.Examples are given in Table 3 Example – [P4] UML use case and other requirement diagrams Conventions are proposed to improvecompleteness of requirement models. These can be deﬁned as rules and be programmatically veriﬁed Industry case DesignAdvisor [P18] UML diagrams Conventions covercorrectness and comprehensibility of models. Reduced defect', 'can be deﬁned as rules and be programmatically veriﬁed Industry case DesignAdvisor [P18] UML diagrams Conventions covercorrectness and comprehensibility of models. Reduced defect density is attainable at the cost of increased effort and this trade-off is increased if tool-support is provided Student experiment – [P19,P20] UML diagrams An analysis of several industrial models for defects showed that lack of conventions creates defects regardingconsistency (for example in class names) andcorrectness (for example defaults regarding visibility and naming are kept where they should be changed) Industrial case SDMetrics for collecting metrics [P6] UML analysis and design models The impact of modelling conventions on representational quality is not veriﬁed. Conventions should be deﬁned from the purpose of models; i.e., related to conﬁnement Student experiment – Single-diagram approach [P27] OPM vs. UML models The single-diagram approach of OPM is compared to multiple diagrams of OMT. Completeness and in some aspectscomprehensibility by humansare improved. Completeness is easier to verify as well Student experiment – [P30] OPM vs. UML models OPM models (single diagram) were more correct than UML models for Web applications. Also OPM model was easier tocomprehend than UML diagrams since information is spread over several diagrams.Consistency of the model after an update is also better with a single-diagram approach Student experiment – (continued on next page) P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1653', 'work. We introduce the six practices in Sections5.1–5.6, and pro- vide a summary in Section5.7. 5.1. Model-based development process Several authors discuss the advantages of developing a model- based process or adapting the existing ones to MDE in order to im- prove the quality of models and the generated assets. A good mod- elling process will have mechanisms for preventing, detecting, and correcting errors at each step from observation to elicitation to analysis to ﬁnal representation [P26]. Berenbach writes that in his experience from industry (1) lack of process contributed to a large number of errors since modellers do not have process that guides where to start and conventions that provide uniformity; (2) lack of quality assurance (for example reviews) led to a stagger- ing number of errors; and (3) design models that originated from analysis had fewer errors thon those originated as designs [P3]. Nelson and Monarchi event write that instead of evaluating the Table 2(continued) Ref. Type of model Practice and impact on quality goals Demo or empirical approach Tool Formal models [P15,P22] Conceptual models Formal syntax helps syntacticcorrectness. Formal semantics helps semantic correctness (validity) andcompleteness. Exacutability of models helps comprehensibility by humans Example – [P24] UML class and behaviour diagrams Formalizing UML diagrams allows model checking and simulation, which helpscomprehensibility by humansand consistency Industrial case SPINs model checking and simulation, Hydra [P23] UML class and sequence diagrams, state machines Formalizing UML diagrams allows checking them forconsistency Example – [P11] Conceptual models Deﬁning a formal syntax and semantics for different views and implementing patterns such as the observer pattern improvesconsistency between diagrams. A new language is deﬁned that implements the proposal –– [P2] UML diagrams Consistency checks for UML diagrams are deﬁned in Description Logic (DL) Example MCC+ (Model Consistency Checker) and MCC-SPL [P14] UML diagrams and a goal model Behaviour consistency can be established between the goal model and UML behavioural diagrams by transforming UML models to formal ones and analyzing them for adherence to goals. SPIDER tool is used to translate natural language constraints into formal speciﬁcations Example Hydra for UML formalization, SPIDER [P10] UML use cases By adding OCL constraints to requirement models, ambiguity is removed from informal speciﬁcations which improvescorrectness. One may also check that the conditions are not contradictory which improvesconsistency. On the other hand, formal speciﬁcations are more difﬁcult to read for humans (negative impact on comprehensibility) Example – [P12] Context model, use case model and analysis model (based on UML) Adding OCL constraints allows us to check models for both intra and inter- consistency by using tools Example OCL Evaluator tool UML Proﬁles and DSMLs [P16,P35] UML proﬁles Stereotyped models are better understood (related tocomprehensibility by humans) Student experiments, industrial case – [P34] UML proﬁles Effective usage of UML in industrial applications strives for its customization for the speciﬁc purpose – thus the deﬁnition of a domain-speciﬁc modelling language. Being suitable for a domain is classiﬁed asconﬁnement. Constraints can be added to models to deﬁne restrictions on the usage of base modelling elements and thus improvecorrectness of the models Industrial case – [P36] UML proﬁles Adding OCL constraints to stereotypes in UML proﬁles restricts the wrong usage of elements and thus improvescorrectness. UML proﬁles allow deﬁning correct transformation Example RACOoN (Racer for Consistency) [P33,P37,P39] DSML DSMLs bridge communication gap between engineers and domain or business experts; related tocomprehensibility. Raising the abstraction level also improves comprehensibility. A DSML includes only elements and diagrams', 'experts; related tocomprehensibility. Raising the abstraction level also improves comprehensibility. A DSML includes only elements and diagrams necessary for the domain and thus improvesconﬁnement. However, developing a DSML and subsequently editors and code generators require high programming expertiseand updating it is costly Industrial case – [P13] DSML Modelling discipline should be close to the problem domain which enables simpler maintenance and evolution of models; related toconﬁnement, and improved comprehensibility by domain experts Industrial case – Generating models/diagrams from other models/diagrams [P31]UML diagrams Generating one diagram (here object life cycles) from another (object state diagrams) improvescompleteness and consistency between models/ diagrams Example – [P11] Conceptual models The observer pattern is implemented which generates necessary elements in diagrams when changes in one is observed. It improvesconsistency between diagrams –– 1654 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'quality of the ﬁnal representation, the representation process itself can be evaluated [P26]. 5.1.1. Examples and the impact on model quality There have been several attempts to adapt existing develop- ment processes to MDE by adding artefacts, activities and roles. Firstly, Gavras et al. have identiﬁed phases and activities of a mod- el-driven based development trajectory process that include phases related to MDE artefacts and tasks [P9]. These phases pre- cede the core development phase and include choosing technolo- gies and developing needed artefacts (such as metamodels, modelling language, transformations, validation rules and tools), as depicted inFig. 4. These artefacts are then deployed for use in the project execution phase, as depicted inFig. 5. The project exe- cution phase has an activity related to validation of models. The paper presents an outline of an example study that has been car- ried out where the roles and products of each activity are deﬁned in accordance with the Software Process Engineering Metamodel (SPEM), which is an OMG speciﬁcation to describe software devel- opment processes.9 In the same spirit as Gavras et al., Staron et al. have identiﬁed activities in a MDE based development process [P34]. The deﬁned process is iterative and involves normal tasks of software develop- ment such as requirements elicitation, development, test and deployment; but with additional MDE activities. For example based on the requirements, the team elicits requirements speciﬁc for the PIM and PSM transformations which in turn form the basis for UML proﬁles deﬁnition. There are seven phases in the MDE development process as shown inFig. 6. The process is developed within an industrial case study. The above two papers touch on an obviously important topic as having control of the quality of the MDE tooling is necessary to have control of the quality of systems being developed using the tools. Staron et al. emphasize deﬁning transformations prior to proﬁles: the model-driven software development process is fo- cused on transformations – thus it is transformations that are iden- tiﬁed during the requirements elicitation phase. As proﬁles are used as a means of making the transformations automated by pro- viding storage for additional information, they are not considered at this phase. Thus their process add identifying transformations and deﬁning proﬁles to the Gavras et al. process while it lacks deﬁning a traceability strategy. There are also approaches that include metrics and tool support for evaluating the progress and the quality of development arte- facts. Firstly, Lange et al. have identiﬁed tasks of model-centric software engineering (meaning that UML models are used for more than describing the system) and the information that is required to fulﬁl these tasks [P21]. Examples of tasks are program understand- ing or completeness evaluation. A task may consist of a number of questions about UML models. They propose to visualize the infor- mation required to support fulﬁlling the tasks and have developed a tool to support their approach on UML models; i.e., the Metric- ViewEvolution tool. The authors write that feedback from indus- trial case studies and a light-weight user experiment has been positive. The advantage of this tool over other tools that collect metrics from UML models is relating these metrics to performing tasks in a model-centric development process. Secondly, Berenbach proposes collecting metrics to evaluate completeness of activities, in his approach related to requirement modelling [P4]. The author proposes a model-centric requirement Fig. 4. Preliminary preparation phase in an MDE-based project as deﬁned in [P9]. Fig. 5. Project execution activities as deﬁned in [P9]. Elicit requirements for transformations Deploy solution Define  transformations Define profiles Develop profiles Develop  transformations test transformations and profiles start', 'Elicit requirements for transformations Deploy solution Define  transformations Define profiles Develop profiles Develop  transformations test transformations and profiles start Fig. 6. MDA-based framework development process deﬁned by Staron et al. [P34].9 http://www.omg.org/technology/documents/formal/spem.htm. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1655', 'process which covers steps for developing, reviewing and analyz- ing requirement models, including a set of project and quality met- rics. Berenbach writes that quality requirements are deﬁned in a way that they could be programmatically veriﬁed. For example completeness of requirement models are deﬁned as: /C15Every leaf use case (a use case with no included, extending or inheriting use cases) is concrete, with a special stereotype. The stereotypes of functional and non-functional requirements were used, and the stereotypes were given special icons for ease of visual inspection. /C15The leaf use cases become the high level requirements that are subject to a formal review. The model is also subject to a review, primarily to check for completeness and defects. Quality metrics are deﬁned towards requirement models and measured frequently, giving the possibility to give insight into the project progress. Examples are ‘‘the number of objects on dia- grams” or ‘‘concrete use cases not deﬁned”. It seems clear that hav- ing a high quality requirements process is important, and having ways of saying ‘‘how good” the requirements are is thus a clear advantage. In traditional development, it is quite common to perform code review, so in the model-based development process, model re- views should be natural as proposed in [P15,P22,P26]. Other qual- ity assurance techniques such as collecting metrics by tools and analyzing them should be part of a modelling process as well. Although we do not intend to cover these techniques, we have re- ferred to them whenever they are mentioned in the studies. Some papers have also provided guidelines or conventions regarding the modelling process. Examples are: /C15Berenbach recommends the following [P3]: the early modelling effort should cover the entire breadth of the domain. Identify ‘‘out of scope” use cases as early as possible. To support this, all the actors in the model should be shown on the ‘‘context” diagram. Discover business objects and their services through use case deﬁnition with sequence diagrams. Elicit requirements and processes by starting at interfaces and modelling inward. /C15Mitchell warns against assuming that all domain modelling must happen in the ﬁrst phase, and all design in later phases [P25]. I.e., an iterative approach is recommended; as also by Ambler [P1]. Ambler has developed the Agile Modelling (AM) 10 approach which is a collection of practices to be used in modelling and docu- mentation of software systems, as depicted inFig. 7. Models in AM are sketches developed for communication and not generation and the focus of Ambler is on UML models. The interesting aspect is, however, the focus of AM on change and agility in modelling which is often ignored by other modelling processes. Many of the practices apply for a model-based software development approach, such as: /C15Apply the right artefact(s). Each artefact has its own speciﬁc applications (related to conﬁnement). /C15Create simple content; the implication is that you should not add additional aspects to your models unless they are justiﬁable (related to conﬁnement). /C15Single source information; you should also model a concept once and once only, storing the information in the best place possible (to avoid inconsistency). The promoters of developing process thus assume that the def- inition of artefacts and procedures up-front will give a concise set of artefacts. Mandating their use will reduce the use of ‘‘non-nor- mative” techniques, and should make it easier to control quality. It is also a requirement that processes should include quality assur- ance activities, evaluating project progress and evaluating the quality of the developed artefacts. Finally, it is also an advantage if artefacts in one phase may be used to develop or generate arte- facts in the next phase in order to achieve completeness and con- sistency where one example is discussed in Section 5.6. For', 'if artefacts in one phase may be used to develop or generate arte- facts in the next phase in order to achieve completeness and con- sistency where one example is discussed in Section 5.6. For example use cases may be used to identify objects and their states. 5.1.2. Examples of empirical evidence Some of the proposed processes are either developed by indus- try or are evaluated in industrial cases: /C15Staron et al. performed a case in Volvo Information Technology and concluded that the project was successful [P34]. /C15Lange et al. evaluated the feedback from industry regarding their approach and tool as positive [P21]. Fig. 7. The best practices of Agile Modelling (AM) from [P1]. 10 http://www.agilemodeling.com/. 1656 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'Table 3 Examples of modelling conventions. In classifying the conventions, we have used the motivation given by the author. Berenbach has marked the conventions veriﬁable by tools by ‘‘auto” [P3]. Notice that selecting conventions depend onthe purpose of modelling. Correctness - The model should have a single entry point (auto) [P3] - Extending use case relationships can only exist between concrete use cases (auto) [P3] - Abstract use cases must be realized with included or inheriting concrete use cases (auto) [P3]- A concrete use case cannot include an abstract use case (auto) [P3]- An interface should only communicate with a concrete use case (auto) [P3]- Every actor in the model should communicate with use cases through interfaces (auto) [P3]- Every state with no outgoing transitions must model a terminal state in the world being modelled [P25] Completeness - Every diagram should have an associated description and status to detect incomplete work (auto) [P3]- Every artefact in a UML model should be visible on a diagram [P3]- Every artefact in a UML model should be visible on a diagram (in order to avoid having artefacts that are not used in a model) (auto) [P3]- Coherent low-level processes should be deﬁned with state or activity diagrams (auto) [P3]- Indicate unknowns with a question mark (in order to complete them later) [P1]- When you add detail to a model, it can be detail that adds precision to an imprecise model. Adding detail does not necessarily mean you’ve moved to a lower level of abstraction [P25] Consistency - The deﬁnition of a use case must be consistent across all diagrams deﬁning the use case (auto) [P3]- Every class in the design model should trace back to a use case in the analysis model (auto) [P3]- An interface class should derive from an analysis boundary class (auto) [P3]- Every expression in a speciﬁcation of behaviour can be cross-checked to concepts in a model of structure [P25]- The terms in every expression in the pre-condition and the post-condition must be deﬁned in the type model [P25] Comprehensibility (including aesthetics) - Class name should be a singular noun [P3]- The use case should be named from the point of view of customer [P3]- Use sequence rather than collaboration diagrams to deﬁne one thread/path for a process [P3]- Avoid crossing lines, diagonal or curved lines, close lines and sized symbols [P1]- Reorganize large diagrams into several smaller ones. Prefer single-page diagrams [P1]- Prefer well-known notation over esoteric notation [P1]- Apply common domain terminology in names (especially for requirements and analysis diagrams) and apply language naming conventions on design diagrams [P1]- Do not model every dependency or implied relationships (completeness here is in conﬂict with comprehensibility) [P1]. The focus is on high-level models - The depth of inheritance trees partially limits the physical dimensions of the drawing. Therefore these trees should be clearly visible and spatially separated from each other [P7,P8] - Class with a high number of outgoing relations indicate that classes depend too much on other classes [P7,P8]. (Note: from model quality to design quality) Conﬁnement - Avoid realization relationships and artefacts in the analysis model. Analysis model should be free of realization or implementation [P3]- A use case described as a sequence of interactions necessarily includes some design decisions. The same use case described using a pre-condition and a post-condition can be free of such design decisions [P25]- Apply the right artefact; for example avoid modelling data and user interface components in UML since UML does not yet address modelling these [P1] - Indicate visibility of operations only in the design model since this is a design issue [P1] - Check that the diagrams are free from unnecessary objects [P38]- Check that no attributes or operations are shown in object diagrams [P38] Changeability', '- Check that the diagrams are free from unnecessary objects [P38]- Check that no attributes or operations are shown in object diagrams [P38] Changeability - Avoid the early use of packages since they will require reorganizing [P3] P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1657', '/C15Berenbach reports that the proposed metrics for requirement modelling were applied in three different projects at Siemens and write that ‘‘as the team members gained experience with the measurement tools and increased ability with the UML, their productivity and conﬁdence rose dramatically” [P4]. The improvement is, however, not quantiﬁed. The feedbacks are positive but we so far lack detailed case stud- ies on the impact of development processes on the quality of models. 5.2. Modelling conventions Ambler deﬁnes conventions as ‘‘guidelines for creating effective (UML) diagrams; based on proven principles that will lead to dia- grams that are easier to understand and work with” [P1]. The term ‘‘convention” in the remainder of this article refers to modelling rules and styles as well. Lange et al. have identiﬁed four categories of conventions proposed for UML models [P18]: /C15Design conventions; e.g., high cohesion and low coupling. /C15Syntax conventions; Ambler presents a collection of 308 conven- tions for the style of UML. His conventions aim at understand- ability and consistency, and address naming issues to avoid inconsistency, layout issues and the simplicity of design [P1]. /C15Diagram conventions; deal with issues related to the visual rep- resentation of UML models in diagrams, such as those proposed in [P29]. /C15Application-domain speciﬁc conventions; such as using stereo- types in UML proﬁles. Design conventions are related to the quality of design and are not covered in this review. A good overview of such conventions and rules is provided by the VIDE project (VIsualize all moDel-dri- vEn programming) [16]. VIDE has performed an extensive review of literature to identify quality defects in MDE and have identiﬁed several classes of conventions such as design principles, anti- guidelines, aging symptoms and modelling styles. Most of the con- ventions are not speciﬁc to models but to software design in gen- eral. Application-speciﬁc conventions are covered in Section5.5 related to domain-speciﬁc approaches. Examples of other conven- tions are presented in the remainder of this section and inTable 3. Several studies mention that modelling conventions should be integrated in a modelling process and be supported by tools to be best effective. 5.2.1. Examples and the impact on model quality We found several examples of conventions proposed for UML models. Some would be relevant independent of the modelling lan- guage. Examples are: /C15In the book ‘‘The elements of UML 2.0 style”, Ambler describes a collection of conventions for creating effective UML diagrams [P1]. The book contains some general guidelines applicable to any type of UML diagrams, guidelines for common UML model- ling elements such as notes and stereotypes, and guidelines for speciﬁc UML diagrams. /C15In the book ‘‘Veriﬁcation and validation for quality of UML 2.0 models”, Unhelkar provides guidelines for modelling and check- lists to check UML diagrams for syntax, semantic and aesthetic issues [P38]. /C15Berenbach presents a set of heuristics for creating ‘‘complete” UML analysis and design models, which may further be analyzed by tools [P3]. The proposed conventions cover model organiza- tion in general, use case deﬁnitions, analysis models, business object models, and design models, and affect most of the quality goals. Several large models at Siemens were evaluated using the DesignAdvisor tool which checks models for some of the pro- posed conventions, while other conventions may be checked by inspections. /C15The KobrA method makes components the focus of the entire software development process by adopting a product-line strat- egy for their creation, maintenance, and deployment [1].I n KobrA, each component is described by a suite of UML diagrams as if it were an independent system in its own right. Choi and Bunse write that the use of UML diagrams and the recursive nat- ure of KobrA introduce two consistency issues in general; static', 'as if it were an independent system in its own right. Choi and Bunse write that the use of UML diagrams and the recursive nat- ure of KobrA introduce two consistency issues in general; static consistency and behavioural consistency. Static consistency mainly refers to structural and naming/type consistency among speciﬁcations describing different aspects of a component. A total of 59 rules are proposed to improve static consistency which can be ensured either by manual inspection or by mechanical syntactic name checking. However, KobrA does not include behaviour consistency rules. Choi and Bunse have there- fore proposed some rules and propose to check them using the model checker SPIN [P5]. /C15In a white paper by Mitchell, the author discusses some princi- ples for creating high-quality models based on their experience [P25]. The focus is mainly on cross-checking between architec- tural models and other models (use case descriptions, object models, and behaviour models such as state diagrams). The con- ventions cover several quality goals such as comprehensibility (regarding organization of a model, e.g., divide a system into technical domains and subject domains) and completeness (e.g., make sure these concepts are modelled in software, from systems analysis models, through design, and into code). Some of the conventions are related to improving the process of mod- elling and were discussed in Section5.1. /C15Eichelberger criticises UML for the lack of aesthetic principles, and UML tools for ignoring aesthetic principles [P7,P8]. The author proposes a set of aesthetic criteria for UML class dia- grams and discusses the relation between these criteria, HCI (Human Computer Interaction) and design aspects of object-ori- ented software. Some aesthetic problems indicate design prob- lems, for example ‘‘a class with a high number of outgoing relations indicate that the class depends on too many other clas- ses” or ‘‘many classes at the borders of a package and few classes in the centre imply coupling problems”. Many of the proposed conventions may be enforced by tools, but the problem today is that most modelling tools enforce syntac- tical and aesthetic constraints very weakly and semantic con- straints are not enforced at all [P17]. For error detection, one may use tools such as DesignAdvisor or perform inspections. 5.2.2. Examples of empirical evidence Lange et al. have performed a controlled experiment with stu- dents as subjects on the effect of (UML) modelling conventions on modelling effort and defect density [P18]. The conventions they have included in the experiment yield correctness (e.g., an abstract class should not be a leaf), comprehensibility (e.g., the number of sequence diagrams per use case should indicate how well the func- tionality of a use case is documented or described) and design is- sues (e.g., low coupling). The experiment results indicate that decreased defect density is attainable at the cost of increased effort when using modelling conventions, and moreover, that this trade- off is increased if tool-support is provided. Since we do not have the full list of conventions and the detailed results of experiments, it is difﬁcult to say which types of errors are prevented. In another student experiment described in [P6], DuBois et al. investigated whether using conventions has any effect onrepresen- tational qualityof a model; deﬁned as the clarity, completeness and 1658 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'validity of the information the model is meant to represent. They did not observe any difference in representational quality and con- cluded that conventions should rather focus on identifying and consistently representing those types of information required for the model’s future usage, e.g., in implementation and testing. Purchase et al. performed several student experiments on UML diagrams and the students’ preference of one diagram over an- other, checking layout issues such as the number of crosses, the number of bends, use of colours or fonts and the width of diagrams, specially for class and collaboration diagrams [P28]. Also ﬁve UML experts performed the experiment. The comprehension task was that of matching a given textual speciﬁcation against a set of dia- grams, indicating whether each diagram correctly matches the speciﬁcation or not. The set of diagrams included both correct and incorrect diagrams. Both the response time and the accuracy of responses were measured. Two examples of notations used in the experiment are shown inFig. 8. When matching diagrams to the speciﬁcations, the (a) notations performed better, while when identifying errors in the diagrams, the (b) notations which are less intuitive and more ambiguous had better performance. It appears that subjects are less at ease with these notations and are more likely to detect errors in the diagrams. There have also been experiments by Cox and Phalp on applying conventions and styles to textual use cases [3,15]. The results showed that their impact on quality is not always obvious and in an experiment comparing two guideline sets, the leaner one per- formed as well as the other. Thus there is little empirical evidence in the covered literature regarding the beneﬁts of conventions and the results of few stu- dent experiments are not conclusive. The quality impact of conven- tions seems to depend on the task, the complexity of conventions and tool support as well and empirical studies should describe these factors better in order to help evaluating the usefulness and cost. Aircraft Angle Monitor Flap Engine Navigation System Measurement Instruments Measured Values Calculated Values Opening Angle Monitoring Flaps Control Data Sending Velocity, Height and Location Calculation Path  Data <features> <consists of> of 2+ <yields> . <affects> (1000, 1000) <is enabled by> Fig. 9. An OPD showing a top-level view of an avionics navigation system [P27]. Fig. 8. Examples of the notational variations used in the experiments of Purchase et al. [P28]. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1659', '5.3. The single-diagram approach The Object Modeling Technique (OMT), the main ancestor of the Uniﬁed Modeling Language (UML), and UML are both approaches that advocate modelling a system in several diagrams. The main beneﬁt is being able to focus on one aspect at a time. On the other hand, it is easy to introduce inconsistencies between diagrams and if a reader must concurrently refer to multiple diagrams in order to understand an aspect, comprehensibility may decrease. Peleg and Dori call thisthe model multiplicity problem[P27] (or to be more consistent ‘‘the diagram multiplicity problem”). Lange also writes that in its recent version, UML 2.0 provides thirteen diagram types, such as the use case diagram, the class diagram and the activity diagram. Each diagram type provides a different view on the de- scribed system and may be of interest of a speciﬁc stakeholder. Eventually all diagrams of a model describe the same system. Thus there is overlap between diagrams which entails the risk for con- tradictions between diagrams, so called inconsistency defects [P19]. Reinhartz-Berger and Dori write that UML is unnecessarily complex in many ways, and this inherent deﬁciency hinders coher- ent modelling and comprehension of systems [P30]. Technical solutions that involve sophisticated CASE tools to im- pose consistency alleviate manual consistency maintenance, but they do not address the core problem of mental integration. Based on the above arguments, some advocate use of single-diagram (Pe- leg calls the approach for single model) approaches. 5.3.1. Examples and the impact on model quality We found one example of a single-diagram approach discussed and evaluated by student experiments in two papers; i.e., [P27] and [P30]. OPM speciﬁes both graphically and textually the sys- tem’s static-structural and behavioural–procedural aspects through a single unifying model. The elements of the OPM ontol- ogy are entities (things and states) and links. A thing is a general- ization of a (stateful) object and a process – the two basic building blocks of any OPM-based system model. Links can be structural or procedural. A structural link expresses a static relation between two objects. Procedural links describe the behaviour of a system. Fig. 9shows an example from [P27]. Objects are shown with rect- angles while processes are shown with ovals. Objects can partici- pates in processes as shown by circles and receive events shown by arrows. One may, however, argue that UML and other modelling lan- guages are multi-diagram because they are meant for industrial- sized application, and that a single-diagram language just doesn’t scale up to real-world use. Peleg and Dori write that in their expe- rience with real-life applications, OPM is easily and naturally scal- able. The scaling mechanism of OPM is based on detail-level decomposition (which includes zooming in/out, unfolding/folding, and state expressing/suppressing) rather than the viewpoint-based decomposition (into separate diagrams for the static aspect, the dynamic aspect, the functional aspect, etc.) which multi-diagram languages employ. UML 2.0 supports also detail-level decomposi- tion for example in sequence diagrams. 5.3.2. Examples of empirical evidence Peleg and Dori write that two major open questions related to diagram multiplicity vs. singularity have been (1) whether or not a single diagram, rather than a combination of several diagrams, enables the synthesis of a better system speciﬁcation and (2) which of the two alternative approaches yields a speciﬁcation that is easier to comprehend [P27]. They have addressed these two questions through a double-blind controlled experiment with stu- dents as subjects. The two approaches for modelling used in the experiment are OPM and OMT extended with Timed Statecharts. The students were divided in two groups: The participants of the ﬁrst group were asked to specify a system described textually in', 'experiment are OPM and OMT extended with Timed Statecharts. The students were divided in two groups: The participants of the ﬁrst group were asked to specify a system described textually in the problem statement using OMT, while the participants in the second group were asked to specify the same system using OPM. The quality of the resulting speciﬁcations was thoroughly ana- lyzed. The results showed that OPM was signiﬁcantly better than OMT in terms of overall speciﬁcation quality; evaluated by the number of errors in the speciﬁcations such as missing events or missing feature. For comprehension experiment, the ﬁrst group received spec- iﬁcations of a system in OPM and the second in OMT and they were requested to answer a questionnaire. The speciﬁcation comprehension results show that there were signiﬁcant differ- ences between the two methods in speciﬁc issues. The study is inconclusive regarding overall speciﬁcation comprehension. Retrieving information from a single OMT diagram was easier in some cases than retrieving the same information from the sin- gle OPM model, which is more complex. The relatively large number of issues for which no difference between the modelling languages exists underlines the advantage of focusing on a single diagram over spreading the speciﬁcation information over sev- eral diagrams. Reinhartz-Berger and Dori have also performed an experiment with students comparing UML and OPM for modelling of web applications [P30]. The goal of the experiment was to compare OPM to UML with respect to two aspects: (1) comprehension, namely the level of comprehending a given model expressed in each language, and (2) construction, i.e., the quality and ease of modelling a given system in each language. In some questions when evaluating comprehension, UML scored higher but the gen- eral conclusion while OPM was easier to understand and apply by untrained users. Both experiments show that there are cases in which the exis- tence of separate views can potentially help answering certain questions about a speciﬁc aspect of a system, which is expressed in only one type of diagram. But if answering a question needs combining information from multiple diagrams, the single-dia- gram approach scores higher. The single-diagram approach may also help identifying incompleteness and inconsistency defects. DSMLs tend to have fewer diagrams and thus solve the problems introduced by using multiple diagram modelling languages. Unfor- tunately, our search did not return any empirical evidence from industry on the beneﬁts of the single-diagram approach. 5.4. Formal models The syntax and semantics of UML and some other modelling languages are informal and imprecise, making analysis of models difﬁcult. Formal models have the advantage of being precise, sup- port formal analysis and proof, and allow execution and genera- tion. However, the usage of formal models has been limited: they are not expressive enough for many real world applications, formal models are often complex and hard to read, and constructing a for- mal model may be a difﬁcult, error prone and expensive process. Since UML is almost the de-facto modelling language in many domains and its informality is considered as a problem, many have tried at making it formal; either by relating it to a formal language, using OCL constraints and tools that may verify the constraints, or developing UML proﬁles. We discuss domain-speciﬁc approaches including UML proﬁles in Section 5.5. In most approaches, the authors have focused on a few UML diagrams since formalizing all diagrams would require a lot of effort and increase the complex- ity of the problem. 5.4.1. UML in combination with formal languages Under the multiple views of UML, the developers can decom- pose a software design into smaller parts of manageable scales. 1660 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'However, Liu et al. refer to literature that discusses several chal- lenging issues that inevitably arise from such a multi-view and multi-notational approach [P23]: /C15Consistency: the models of various views need to be syntacti- cally and semantically compatible with each other (i.e., horizon- tal consistency). /C15Transformation and evolution: a model must be semantically consistent with its reﬁnements (i.e., vertical consistency). /C15Traceability: a change in the model of a particular view should lead to corresponding consistent changes in the models of other views. /C15Integration: models of different views need to be seamlessly integrated before software production. Consistency checking and formal analysis of UML models have been widely studied in recent years. The majority of approaches fo- cus on the formalization of individual diagrams and only treat the consistency between one or two views. Liu et al. have used the Ob- ject-Oriented speciﬁcation Language (OOL) to formalize UML sys- tem models and check inter-diagram consistency. Class diagrams, sequence diagrams and state machines are formalized with adding OOL statements. Thus the consistency of UML models is trans- formed to the well-formedness of OOL speciﬁcations. In another paper, Haesen and Snoeck discuss the problems of consistency checking in general, and horizontal consistency check- ing in conceptual models speciﬁcally [P11]. They describe a meth- od called MERODE which has tackled the problem of inconsistency by deﬁning a formal syntax and semantics for different views. In order to keep this manageable, MERODE has drastically reduced the number of views and concepts that can be used. It should therefore be considered as a Domain-Speciﬁc Language (DSL) for the conceptual modelling of management information systems. Other than solving the inconsistency problem, advantages of formalism are model execution, simulation and analysis. For exam- ple, there have been various approaches to make formal speciﬁca- tion models, such as using executable Z or relating the formal speciﬁcation languageB to UML (see [P10] for references). McUmb- er and Cheng have developed a general framework for formalizing a subset of UML diagrams in terms of different formal languages based on a homomorphic mapping between metamodels describ- ing UML and the formal language [P24]. This framework enables the construction of a consistent set of rules for transforming UML models into speciﬁcations in the formal language. The resulting speciﬁcations derived from UML diagrams enable either execution through simulation or analysis through model checking, using existing tools. The paper includes an example of formalizing UML in terms of Promela, the system description language for SPIN. They have also constructed a prototype tool called ‘‘Hydra” and evaluated it on an industrial case of an embedded system. Using Hydra, they were able to move directly from UML class and behav- iour diagrams to model checking and simulation. According to the authors, SPINs model checking and simulation capabilities were extremely useful during the behaviour analysis. Konrad et al. have also translated UML models to formal speci- ﬁcations using Hydra [P14]. UML models were extended with con- straints to check their behaviour consistency for adherence to constraints deﬁned in a goal model. The work is part of a process called i 2MAP (incremental and iterative Modelling and Analysis Process). Finally, Choi and Bunse propose translating UML dia- grams into the input language of SPIN in order to check behaviour consistency [P5]. 5.4.2. Using OCL constraints UML focused primarily on the diagrammatic elements and gave meaning to those elements through English text. Later, a constraint language was added to the speciﬁcation, the Object Constraint Lan- guage or OCL.11 OCL allows the integration of both well-formed- ness rules and assertions (i.e., pre-conditions, post-conditions,', 'language was added to the speciﬁcation, the Object Constraint Lan- guage or OCL.11 OCL allows the integration of both well-formed- ness rules and assertions (i.e., pre-conditions, post-conditions, invariants) in UML models. The former are useful to validate espe- cially the syntax of a UML model, whereas the latter can be exploited to verify the conceptual constraints. Pre-conditions and post-conditions provide a mechanism to specify the properties re- quired before and after the execution of an operation, respectively, but do not specify how that operation internally works. The recent development of version 2 for both OCL and UML is a breakthrough in order to completely deﬁne the semantics of a method in an ob- ject-oriented system. In these latest versions, it is possible to deﬁne a behaviour speciﬁcation in OCL for any query operation (an oper- ation without side-effects). Adding OCL constraints allows analysis and veriﬁcation by tools. Giese and Heldal propose adding OCL constraints to the post-conditions of UML models to expose gaps and ambiguities in informal descriptions [P10]. Since formal models are harder to read, the authors propose producing informal models from formal ones; for example in a natural language. Hnatkowska and Walkowiak discuss the problem of consistency within and between artefacts [P12]. They have developed a devel- opment approach called the Robust Software Development Proﬁle (RSDP) which introduces three models: context model, use case model, and analysis model. They have used the OCL language to formulate both inter, and intra-consistency rules for these models. The rules are partly implemented within the OCL Evaluator tool. The above-mentioned models were deﬁned in Rational Rose and Poseidon for UML, next they were transformed to XML format, par- tially rewritten (if needed) and veriﬁed within the OCL Evaluator tool. However, Rational Rose and Poseidon were not consistent with the XMI MOF speciﬁcation, which limits their usage for con- sistency checking. 5.4.3. Examples of empirical evidence In our survey on MDE experiences form industry presented in [11], we described a case from Motorola that refers to the beneﬁts of formalism for catching defects by simulation. Otherwise, the studies related to formal models include only examples to demon- strate the practice. The only industrial case is mentioned in [P24] where details are left out. 5.5. Domain-speciﬁc modelling languages and UML extensions A domain consists of all possible statements that would be cor- rect and relevant for solving a problem. Hence, every unique prob- lem has a unique, usually, evolving domain [P22]. A Domain- Speciﬁc Language (DSL) is a language designed to be useful for a limited domain, in contrast to a General Programming Language (GPL) that is supposed to be useful for multiple application do- mains. This distinction is orthogonal to many other language clas- siﬁcations. For example, there are indifferently visual (graphical) or textual GPLs or DSLs. Similarly DSLs and GPLs may fall under var- ious categories of being object-oriented, event-oriented, rule-ori- ented, function-oriented, etc. A Domain-Speciﬁc Modelling Language (DSML) is thus a modelling language (usually visual) that is used for Domain-Speciﬁc Modelling (DSM). DSM has been sub- ject of a recent book by Kelly and Tolvanen[6] that we use here for deﬁnitions. In DSM, modelling is done by using domain con- cepts and a DSML also includes domain rules that prevent design- ers from making illegal designs. The implementation details are hidden and thus the level of abstraction is high. Domain-speciﬁc models are directly transformed to code by using domain-speciﬁc 11 http://www.omg.org/technology/documents/modeling_spec_catalog.htm. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1661', 'generators. Models can equally be used for executing, testing and debugging applications since they are formal. Also when using GPLs like UML, the base language may be ex- tended with domain-speciﬁc enhancements using proﬁles that let us add new attribute types for model elements, classify them with stereotypes and have domain-speciﬁc constraints by using OCL [6]. Thus DSMLs may be designed from scratch or by extending a base language. In UML there are two different ways of describing extensions; i.e., by MOF metamodel extensions and UML proﬁles (by deﬁning stereotypes with tags and constraints). Every ap- proach has advantages and disadvantages: /C15Designing a language from scratch or extending a metamodel needs expertise in language engineering and is a lot of work; also to develop an editor, code generator and modify tools. Safa writes that DSLs with a limited user base are costly to develop and maintain, and may have poor design or not be easy to use [P33]. Trask et al. also mention the difﬁculties in changing a metamodel and subsequently editors and code generators [P37]. The advantage is the possibility to design a language that meets the needs of a target domain. /C15UML proﬁles offer a limited extension mechanism since totally new types cannot be added to the language. Also UML proﬁles cannot allow taking anything away from UML and one cannot change the semantics of UML elements. The complexity of work- ing with UML is therefore still in place. Besides, many tools do not know how to deal with stereotyped elements. The advantage is being able to use third-party modelling tools. Although there is still weak support by tools for both ap- proaches, they have gained attention by industry because of the short-comings of GPLs and the promises of DSLs. 5.5.1. Examples and the impact on model quality Several aspects of model quality are improved by using a DSML or UML proﬁle as discussed below (since proﬁles are also a ﬁrst step towards DSML, we will use the DSML expression to cover both): /C15Comprehensibility by tools is improved by adding semantics to an informal language which facilitates analysis and generation. Stereotypes can also add additional properties – information – to the stereotyped element [P34]. /C15Using a language close to the domain improves comprehensibil- ity by humans, especially by non-technical domain experts. The solution domain gets closer to the problem domain which also improves maintainability of models. Proﬁles or DSM introduce simplicity to the design [P39] and narrow the communicative gap between engineering and business. /C15Constraints can be added to models to deﬁne restrictions on the usage of base modelling elements and thus improve correctness of the models [P34]. These constraints may be deﬁned in OCL when using UML. Note that a DSML is deﬁned formally supported by some tool [6]. Therefore the advantages of being formal are achieved, in addi- tion to better comprehensibility and conﬁnement (suitable for the domain). The impact on changeability may be positive or negative as discussed below. 5.5.2. Examples of empirical evidence Empirical evidence reported in the studies cover student exper- iments on the beneﬁts of using stereotypes and industry experi- ence reports on the beneﬁts and risks of using DSMLs. Kuzniarz et al. have performed a controlled experiment with students as subjects in order to evaluate the inﬂuence of UML stereotypes on the understanding of UML models [P16]. Two sets of models were used in the experiment; one stereotyped and one not. Understandability of the designs was measured by two depen- dent variables: (1) total score (NRESP) – the number of correct an- swers for each subject when asked questions about the design; (2) time (TSEC) – the time (in seconds) which was required to ﬁll in a questionnaire on the design of the systems. The results of the experiment support the claim that stereotypes with graphical', 'time (TSEC) – the time (in seconds) which was required to ﬁll in a questionnaire on the design of the systems. The results of the experiment support the claim that stereotypes with graphical icons for their representation play a signiﬁcant role in comprehen- sion of models. The subjects understood the stereotyped models better (the improvement achieved in this category was 52%) and they were more consistent in their answers when the stereotypes were involved. On average the relative amount of time for a correct answer was shorter for the stereotyped model. Staron et al. describe two additional experiments in order to verify the credibility of the design of the above experiment before replicating it in industry [P35]. The improvements were not signif- icant but stereotypes helped in particular when it comes to having correct answers. The industrial experiment was conducted at Volvo Information Technology, at their site in Gothenburg, Sweden with only four professionals. The number of correct answers was higher for the stereotyped models but the time spent varied. Staron et al. concluded that in general stereotypes improve the correctness of understanding of UML models. Improvements were achieved in all experiments – and half of the results were statistically signiﬁcant. We have found several cases describing experience and beneﬁts of using UML proﬁles or DSMLs in industry, but often without quantitative data. Two cases are mentioned here while other examples are to be found in[11]. The beneﬁts are often related to improved generation of correct code by adding semantics in do- main concepts, and improved understandability by using the lan- guage of the domain: /C15In a paper describing development at Phillips, Jonkers at al. write that using small DSMLs, as opposed to a universal modelling lan- guage such as UML, brings the modelling discipline much closer to the domain experts and at the same time enables simpler maintenance and evolution of such models, which contributes to the desired productivity increase as well as to the agility of the model-driven development [P13]. /C15In a paper from Matsushita Electric Works in Japan, Safa writes that practitioners considered UMLs object-oriented notations too far apart from theC procedural world used for implementa- tion [P33]. A corporate language has been evolved naturally over the years to express requirements, designs and implementations matters. It has notations, conventions and semantics that map precisely the problem domains, and it evolves incrementally when the problem domain changes. The approach has risks as well. Due to the metamodeling delay necessary to deﬁne visual languages, editors and compilers, the domain-speciﬁc modelling tool lags behind practice, so it is at risk of being perceived as constraining, especially for practitioners used to drawing with free-format whiteboards, pen and paper and general purpose diagram tools like Microsoft/C211PowerPoint. 5.6. Generating models or diagrams from other models or diagrams Abstraction is one of the main techniques to handle complexity of software development. In MDE, abstraction is combined with stepwise reﬁnement of models by transformations. Transforma- tions are either Model-to-Model (M2M) or Model-to-Text (M2T). A transformation takes one or several models as input and pro- duces a model (or models) or text, as output. During transforma- tion, output models are supplied with information not present in 1662 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'the input model. An example of such information is the platform concept during transformation of a PIM model into a PSM model. Thus generation supports separation of concerns and adding de- tails later; not by manual work but by applying transformations, where links between these artefacts can be preserved and used for analyzing the impact of changes, model debugging or synchro- nizing artefacts. Obviously, vertical consistency between models is improved when they are generated from other models. Transformations may also be applied between diagrams on the same abstraction le- vel to improve horizontal consistency in multi-diagram modelling approaches. Haesen and Snoeck discuss that consistency checking can be doneby analysis (an algorithm detects inconsistencies be- tween deliverables),by monitoring(meaning that a tool has a mon- itoring facility that checks every new speciﬁcation), and by construction or generation(meaning that a tool generates one deliv- erable from another and guarantees semantic consistency) [P11]. While monitoring is useful for error prevention and analysis for er- ror detection, the focus of this section is on generation for error prevention. 5.6.1. Examples and the impact on model quality Ryndina et al. write that there are two approaches for solving the horizontal consistency problem between two diagrams [P31]. One is to determine the overlap between two given diagrams where consistency conditions can be deﬁned and checked. The problem with this approach is that the two diagrams are deﬁned in two modelling languages and consistency conditions should be deﬁned across language boundaries. The second approach is to generate one diagram from the other. With this approach, consis- tency conditions are deﬁned between diagrams expressed in the same language. Fig. 10 shows the two approaches applied on business process and object life cycle diagrams. The latter ap- proach is beneﬁcial according to the writers, as deﬁning consis- tency conditions and interpreting inconsistencies between two diagrams in the same language is easier than across language boundaries. A prototype tool can verify that consistency condi- tions are held. Haesen and Snoeck have proposed to use the observer pattern to achieve consistency between different views in conceptual mod- els: as the user changes a speciﬁcation in one view, all other views are informed about the modiﬁcation [P11]. For example the com- mand of adding an object type creates the object type and the de- fault ﬁnite state machine for that object type. As a result, the developer will initially receive a ready-made default ﬁnite state machine in which (s)he can add, modify or delete states and tran- sitions to model the behaviour. This is an example of achieving consistency by construction using their terminology. 5.6.2. Examples of empirical evidence Studies covered in this review included only examples on applying the practice but no empirical evidence. 5.7. Summary of the section This section provided an overview of practices proposed to im- prove the quality of models. We identiﬁed six classes of practices and discussed them with examples, their impact on model quality and examples of empirical studies. While some practices are eval- uated by experiments and industrial cases, others are only demon- strated on examples.Table 4 summarizes this section where ‘‘+” indicates beneﬁts and ‘‘!” indicates drawbacks of practices. Practices can of course be combined, for example DSMLs often include constraints and formal semantics to prevent errors and facilitate generation, and the number of diagrams is often reduced. Having a model-based development process that includes guide- lines for modelling will help developers to set the practice in life. Using a practice will sometimes enhance one quality goal while affecting another one negatively. For example formal models are easier to analyze and verify by tools, but are not easier to compre-', 'Using a practice will sometimes enhance one quality goal while affecting another one negatively. For example formal models are easier to analyze and verify by tools, but are not easier to compre- hend by humans. Implementing each practice has some cost which should be weighted against the beneﬁts to ﬁnd the balance. In the next section we discuss the impact of practices on the6C goals introduced in Section4. 6. Integrating the results We introduced the6C model quality goals in Section4 and the six practices proposed to improve the quality of models in Section 5, while the impact of practices on the6C goals are summarized in this section. 6.1. The impact of the proposed practices on the 6C goals For each quality goal, the two classes of practices are discussed. We remind that the ﬁrst group related to ‘‘modelling process” in- UML AD Given BPM BPM with object states BPM = Business Process Model OLC = Object Life Cycle 1. Make object states explicit Given OLCs Generated OLCs 2. Generate  object life cycles 3. Check consistency  conditions 4. Determine  consistency UML SD Fig. 10. Solution overview in [P31]. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1663', 'cludes model-based development process, modelling conventions and the single-diagram approach, while the second group related to ‘‘formal approaches and automation” covers formal models, do- main-speciﬁc solutions and generation. C1-Correctness. Regarding modelling process, several studies propose using conventions in order to prevent syntactic and semantic errors [P1,P3,P38], while Unhelkar [P38] also provides checklists to check the correctness of UML diagrams. Berenbach proposes enforcing some of the conventions by tool [P3]. Perform- ing inspections [P15,P22,P26] and model analysis techniques [P40] are also proposed for quality assurance of models. Regarding formal approaches and automation, Giese and Heldal [P10], Staron et al. [P34] and van Der Straeten [P36] propose using OCL constraints in order to remove semantic ambiguities from models and prevent wrong usage of elements. Using stereotypes [P34] and deﬁning formal semantic of elements (for example in a DSML) are also techniques that remove ambiguity of informal semantics and thus improve semantic correctness. McUmber and Cheng introduce a framework for formalizing a subset of UML diagrams by mapping between metamodels describing UML and the formal language to be able to use model checking and simula- tion tools, for example SPIN to check for correctness and other quality goals such as consistency [P24]. C2-Completeness. Regarding modelling process, some research hints that single-diagram approaches have fewer defects regarding missing elements compared to the multi-diagram approaches [P27]. Conventions provided as guidelines, checklists or best prac- tices in modelling processes are also proposed to improve com- pleteness of models, as in [P1,P3,P25,P38]. Examples are to be found inTable 3. Tool-support is discussed by Berenbach who pro- poses deﬁning completeness rules that can be programmatically veriﬁed [P4] while metrics for completeness evaluation are pro- posed by Berenbach [P4] and Lange et al. [P21] who propose deﬁn- ing a special task for completeness evaluation that is performed based on metrics collected from models. Regarding formal approaches and automation, generating models from other models is a technique for achieving com- pleteness with an example given in [P31] where object models are generated from process models with all the necessary states. Table 4 The beneﬁts (marked with ‘‘+”) and drawbacks (marked with ‘‘!”) of the proposed practices and the state of evidence from empirical studies in the covered literature. Approach Main beneﬁts or drawbacks Empirical evidence Related to modelling process Model-based development process+ Having a development process adapted to MDE is impor- tant for controlling the quality of activities and artefacts and avoiding non-uniformity + Metrics may be added to evaluate progress and the qual- ity of artefacts + Best practices may be included in the process ! Developing or updating a process and training people on the process require effort ! Having a process does not guarantee that it is followed Although there is agreement in experience reports from industry regarding the importance and positive effect of process, the impact of processes on the productivity or quality is not properly documented Modelling conventions + Easy to provide based on experience and literature ! Increases effort; should not be overwhelming Student experiments show that tool support for enforcing and checking of conventions is important The impact on error prevention or detection is not always straightforward to predict. For example less intuitive conventions may be better for error detection Single-diagram approach + It is easier to detect incompleteness if the information is presented in one diagram ! All information gathered in one diagram may improve comprehensibility compared to cases where information from multiple diagrams must be integrated for comprehension The method is evaluated in student experiments that conﬁrm', 'comprehensibility compared to cases where information from multiple diagrams must be integrated for comprehension The method is evaluated in student experiments that conﬁrm single-diagram models have fewer errors while comprehensibility depends on the task Related to formal approaches and automation Formal models + Support for formal analysis and proof, execution and generation + Model checking and simulation tools are available ! Formal models are more difﬁcult to comprehend. How- ever, they may be combined with informal ones ! Formality comes at a cost Only described by examples DSMLs and UML extensions + Models developed using domain concepts are easier to comprehend for domain experts + Enough details for transformations may be added and models may be made formal + Constraints may be added to prevent wrong usage ! Developing a language and supporting editors and tools need expertise and is costly A number of student experiments are performed which verify the improvement in comprehensibility by suing UML stereotypes, although not always signiﬁcant DSMLs are receiving growing attention by industry, with positive feedback. However, keeping the language updated with the domain is difﬁcult Generating models/diagrams from models/diagrams + Supports separation of concerns since details may be added during transformation (like in DSLs) + May be applied to improve both horizontal and vertical consistency between artefacts ! Transformations are costly and should preserve model characteristics ! Transformation is a core practice in MDE supported by tools while the challenges of traceability and synchroniz- ing artefacts are not properly addressed Discussed by examples 1664 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'C3-Consistency. Regarding modelling process, conventions regarding consistency between diagrams are proposed in [P1,P5,P3, P25,P38] with examples given inTable 3. Gavras et al. propose deﬁning a traceability strategy in the modelling process, which refers to the ability to establish relationships between model concepts that represent the same concept in different models [P9]. Regarding formal approaches and automation, consistency has received more attention in literature than correctness and com- pleteness since it may be improved to a large extent by using tools and formal languages, for example inter- and intra-consis- tency rules may be checked by OCL evaluator tools [P12]. Consis- tency constraints may also de deﬁned in a DSML [P39] and consistency conditions can be checked during transformations. By using a formal language, consistency of UML diagrams can be transformed to well-formedness of the language speciﬁcations [P11,P23]. Finally, Haesen and Snoeck propose using tools that implement the observer patterns and generate the necessary ele- ments when diagrams are updated to keep diagrams consistent with one another [P11]. C4-Comprehensibility. Regarding modelling process and when it comes to comprehensibility by humans, we found conventions for naming and structuring of models [P1,P3,P25] (since information is easier to locate in a proper-named and well-organized model), aes- thetics of diagrams [P1,P28,P29,P7], closeness to users’ view [P3] and to the problem domain [P25], and documentation of models [P1,P3]. Examples can be found inTable 3. Finally, experiments on the single-diagram approach indicate that these models are sometimes easier to understand compared to the multi-diagram approach where information is spread over several diagrams, while comprehensibility is decreased if answering a question requires information on only one aspect that may be expressed in one UML diagram [P27,P30]. Regarding formal approaches and automation, while using for- mal languages in combination with UML is proposed to improve consistency, it is also discussed that formal models are more difﬁ- cult to read for humans and it is better to generate them from the informal ones and constraints should be ﬁrst written in a human- readable form [P34]. On the other hand, formality allows simula- tion and analysis which may improve comprehensibility by hu- mans [P22,P24,P34]. Using concepts close to the domain as in a DSML is also supposed to improve the comprehensibility of models especially by non-technical experts [P13,P39,P33] and some exper- iments suggest that stereotyped models are understood better [P16,P35]. Comprehensibility by tools is achieved by having a formal or precise semantics by adding such semantics in models, stereotypes and DSMLs, and of course during transformations. Some research discuss that problems with comprehensibility by users may indicate poor design [P7,P8] while good design improves also comprehensibility. We have not covered design quality in this review but this observation is interesting to have in mind. C5-Conﬁnement. Regarding modelling process, Berenbach [P3], Mitchell [P25] and Ambler [P1] have proposed conventions that im- pact conﬁnement such as using the right modelling artefacts, being free of implementation and design decisions for an analysis model, having focus on correct separation of concerns (also in [P5]), identi- fying scope as early as possible, having focus on domain concepts and starting modelling process from them, clear separation between models and what they should cover, and modelling from different views. Many of these conventions may be integrated in a model- based development process, for example Gavras et al. emphasize deﬁning an activity for selecting modelling languages and tools appropriate for the domain and the needs of modelling [P9]. Regarding formal approaches and automation, sometimes developing a DSML is the right solution for an organization', 'appropriate for the domain and the needs of modelling [P9]. Regarding formal approaches and automation, sometimes developing a DSML is the right solution for an organization [P13,P33,P39] since it includes only elements and diagrams neces- sary for the domain. C6-Changeability. Maintainability of models and updating them is a major challenge as it is with code, especially when models gets large and complex as it is in many industry applications. However, maintenance and evolution of models has not received much attention by now. Regarding modelling process, maintenance of models is dis- cussed in Agile Modelling (AM) which has recommendations such as a single source of information, creating simple content and depicting them simply. Regarding formal approaches and automation, Jonkers et al. dis- cuss that modelling in a DSML brings the modelling discipline much closer to domain experts and at the same time enables sim- pler maintenance and evolution of such models, which contributes to the agility of the model-driven development [P13]. On the other hand, Safa writes that updating the metamodel of a DSL and the associated tools with a limited user base is costly and as a result the language may lag behind the changes in the domain [P33]. We also think that generating models from other models is a prac- tice that allows keeping models in synch with one another when changes in one happen. Many practices regarding maintenance of code also apply to models. For example models that are well-organized and well-doc- umented are easier to maintain and update. A model which is eas- ier to communicate may reduce the ‘‘mythical man month”, the time that is normally taken to learn a new system, which will im- prove the cost effectiveness of maintaining systems[8]. Summary. Fig. 11 summarizes the literature on the impact of the practices on the6C goals where dashed lines indicate that both po- sitive and negative impact is observed. As depicted inFig. 11, the proposed practices often impact several quality goals: /C15Improving the modelling process may impact all quality goals if proper activities are included. /C15Coding conventions or styles have earlier been promoted to improve the quality of code. In order to improve the uniformity of models and prevent defects, some authors advocate the use of modelling conventions. Styles, rules and conventions are kind of best practices proposed to improve all aspects of model quality and should be included in a model-based development process. Changeability Consistency Comprehensibility by humans Confinement Completeness Correctness Formal approaches and automation DSMLsor  UML extensions Generating models  from models Formal models Modelling process Single-diagram approach Modelling conventions Model driven  development process s l a o g y t i l a u Qs e c i t c a r P Fig. 11. The impact of practices on model quality goals. Continuous lines indicate positive impact while dashed lines indicate that the impact may be positive or negative. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1665', '/C15Using less number of views is proposed to reduce the complex- ity of modelling and improve completeness. It may impact com- prehensibility by humans positive or negative. /C15Using a formal modelling language improves correctness and consistency of models and may also improve comprehensibility by humans if models may be simulated. Formal models are on the other hand more difﬁcult to read for humans. /C15DSMLs or UML proﬁles allow developing models with the vocab- ulary of the domain that is more comprehensible for humans. Other advantages are developing models that are formal, more concise, correct and suitable for code generation. However, updating the language and editors is difﬁcult and models should be updated with changes in the metamodel. /C15Model-based generation by transformations improves consis- tency between artefacts and a transformation tool can check mod- els for their correctness and completeness during transformation. 6.2. Assessing quality In this section we present some observations regarding tool sup- port and quality assurance techniques from the covered literature. The main methods for detecting errors or assessing quality are: /C15Inspections: several quality goals such as consistency, complete- ness and conﬁnement can be assessed by means of manual human inspections; as proposed in [P3,P22,P26], also by using checklists [P38]. Both modelling experts and non-technical experts should be involved in inspections; especially for evalu- ating comprehensibility and conﬁnement aspects. The OORT techniques (Object-Oriented Reading Techniques) is an example of systematic inspection techniques to inspect (‘‘compare”) UML diagrams with each other for completeness and consistency (vertical and horizontal)[2]. /C15Tools for error detection: some have developed tools that check models for inconsistency, incompleteness and incorrectness problems such as naming conﬂicts, missing associations and incorrectly deﬁned interfaces. Examples are the DesignAdvisor tool [P3] and SDMetrics [P20]. Tools for model checking based on formal approaches (adhering to rules and constraints for example related to consistency and requirement goals) are cov- ered as well such as the SPINs model checking and simulation environment [P5,P24], Hydra framework [P14], MCC+ [P2] and OCL Evaluator tool [P12]. /C15Collecting metrics from models: Berenbach proposes collecting metrics from models to evaluate their completeness [P4] and Lange et al. have developed the MetricViewEvolution tool for collecting metrics and visualization them [P21]. Saeki and Kaiya propose deﬁning metrics at the metamodel level [P32]. For evaluating the usefulness of practices, we found examples of: /C15Experiments: controlled experiments are often performed in aca- demia, for example related to using UML proﬁles [P16] or the single-diagram approach [P27]. /C15Pilot studies: some industry cases described in studies are actu- ally pilot studies performed in order to evaluate usefulness of an approach in a speciﬁc context, for example in [P26] and related to DSMLs [P33,P39]. /C15Feedback from practitioners: some studies have systematically collected feedback in industrial cases and analyzed them, such as in [P34]. 7. Discussion This article covered mainly two issues: identifying model quality goals ( 6C goals) important in model-based software development and an overview of practices proposed in literature to improve model quality, both based on the results of a systematic literature review. The question facing us is whether the6C classiﬁ- cation is more useful than other classiﬁcations deﬁned in [P15,P22,P26,P38]; such as dividing quality goals into syntax, semantics and pragmatic quality. When trying to deﬁne what ex- actly syntactic, semantic or other quality goals mean in the above studies, the authors often tend to use the same terminology cov- ered by the6C goals; i.e., correctness, completeness, consistency', 'actly syntactic, semantic or other quality goals mean in the above studies, the authors often tend to use the same terminology cov- ered by the6C goals; i.e., correctness, completeness, consistency etc. There are other quality goals for models – such as being simple – that are deﬁned as being comprehensible and easy to change, which are thus covered by our classiﬁcation. In addition to using a simple terminology, the6C classiﬁcation is based on the results of a systematic review of literature on model quality. In our opinion using the review results provides relevance. For example one may talk of consistency as a semantic quality type while the term ‘‘consistency between models” is better understood by practitioners and also used in several studies. Another question is whether we can join any two goals or re- move any without signiﬁcant impact on the discussion, which we mean is not possible. Other quality goals may be added if necessary and future research on the subject is necessary. Quality of models is especially important in MDE: /C15Since models are transformed into other models, they should be correct. Otherwise, the principle of ‘‘garbage in, garbage out” applies [P40]. /C15Model completeness is a prerequisite for transformation, consis- tency checking and implementation. Coverage is one of the requirements of completeness; for example all the use cases are covered in the implementation. /C15Consistency between diagrams becomes important if informa- tion from separate diagrams should be combined for the pur- pose of understanding or generation. Consistency between models of the same system is important for keeping them in sync for future evolution. /C15Comprehensibility either by tools or humans is the main reason for doing any modelling. /C15MDE often involves developing several models of the system with different purposes. Thus models should include informa- tion depending on the purpose. /C15Finally, models should be easy to change in order to support evolution and maintenance. Persuading industry to use models depends on whether modellers can change models easily and continuously. In short, it is hard or impossible to create something complete and correct from something incomplete, erroneous or inconsistent. Persuading industry to use MDE requires taking away some burden of development by providing tools and methodologies that support developing and maintaining high-quality models. For example a recent article on model-based testing states that the ultimate suc- cess of the approach relies on the quality of models that support them [14], such as having enough details (being complete). As the results of this review show, several practices are proposed for improving the quality of models and some studies also include empirical evidence. Studies on the impact of modelling processes on the quality of models report positive feedback from industry without providing details. While one may ﬁnd a lot of literature on the beneﬁts of software processes in general, the relevant tasks and activities for a MDE approach should be evaluated in industry cases. Studies discussing the beneﬁts of formal models and generat- ing models or diagrams from other models or diagrams contained only small examples, while modelling conventions, domain-spe- ciﬁc approaches and the single-diagram modelling approach are supported by industrial cases and student experiments. However, 1666 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'the evidence from the domain-speciﬁc cases supports the beneﬁts of formal methods to some degree since DSMLs are usually formal. Some practices may be enforced by CASE tools, for example OCL constraints prevent making wrong choices and tools can prevent syntactic errors and may keep models consistent with one another if consistency rules are deﬁned and support for checking is pro- vided. Experiments on applying modelling conventions have con- ﬁrmed that tool-support is important for reducing the effort spent on modelling when using conventions [P18]. Using the pro- posed practices especially supported by tools is thepreventive ap- proach or quality by construction. The second approach to quality is anassessing approachas discussed in Section6.2 based on static analysis of models: model checking for formal models, collecting metrics from models or getting feedback via inspections and inter- views. Finally, there is also apredictive approach, for example the quality of models may be evaluated from the quality of predictions made from them if they are used for simulation and prediction. However, the studies covered in this review did not include any examples on using this approach. 8. Conclusions and future work This article reviewed literature on the quality of models to an- swer three research questions. The results are summarized below. RQ1. What quality goals are deﬁned in literature for models in model-based software development?We identiﬁed six model quality goals relevant for model-based software development; i.e., correct- ness, completeness, consistency, comprehensibility by humans and tools, conﬁnement (as having precise modelling goals and being re- stricted to them) and changeability (as being easily extensible and modiﬁable). While some of these quality goals such as consistency are studied in depth and solutions are proposed and implemented in tools, others – such as changeability – are less discussed in the covered literature. RQ2. What practices are proposed to achieve or improve the above quality goals?We identiﬁed six practices and divided them in two groups. The ﬁrst group is related to modelling process and covers having a model-based development process, using modelling con- ventions and the single-diagram approach. The second group is re- lated to formal approaches and automation and covers formal models, UML proﬁles and domain-speciﬁc modelling languages, and generating models or diagrams from other models or dia- grams. We discussed the impact of the proposed practices on the 6C goals with examples and empirical evidence reported in the covered literature. RQ3. What types of models and modelling approaches are covered in literature? Most research covered UML models, however, in ap- proaches where models play a central role in software develop- ment or on the right hand side of the spectrum shown inFig. 1. However, even when models are merely sketches, their quality has gained attention since high-quality models ease communica- tion between development teams. We also found literature cover- ing UML proﬁles and Domain-Speciﬁc Languages in the spirit of model-driven engineering. Empirical evidence in the covered literature is also included in the article. Modelling conventions and the single-diagram model- ling approach have been subject of student experiments that con- ﬁrm some beneﬁts but question others. For example the impact of conventions depends on the task and tool-support. The beneﬁts of model-based development process and domain-speciﬁc modelling approaches (including UML proﬁles) are observed in industrial cases while formal models and generating models/diagrams from models/diagrams are mostly discussed by examples and no empir- ical evidence was detected in the covered literature. Additional evi- dence may, however, be detected by performing a review with focus on empirical studies. The main purpose of this article has been to provide deﬁnitions', 'dence may, however, be detected by performing a review with focus on empirical studies. The main purpose of this article has been to provide deﬁnitions and classiﬁcations that can be part of a quality model with focus on model quality. We have developed a tool for visual speciﬁcation of quality models as presented in[12] where we intend to insert the results of this review. The next challenge of improving model qual- ity is to select quality goals for a given context and to identify prac- tices that may be applied in that context. Quality goals vary in the lifecycle of a project and for different types of models. For example the degree of required formality and detail vary. Models may also be the intermediate or the ﬁnal products of software development. In short, a model should ‘‘ﬁt for the purpose”. Thus a goal-driven process for selecting quality goals and practices is proposed which is subject of our future work. Other research gaps are identiﬁed as well. While traditional quality assurance techniques such as inspections and measurement are applicable to models, they should be adapted to modelling pur- poses, tasks and artefacts involved. Managing changeability and complexity of large and complex models, keeping them consistent and verifying quality on the model level are challenges in model- driven engineering that are not yet properly covered. Performing literature reviews is time consuming and integrat- ing the results is not easy. The main beneﬁts are, however, to pro- vide new insight and identify research gaps. One challenge of this review was selecting a terminology for classifying model quality goals that is based on the existing work and is considered useful, without being difﬁcult to understand for practitioners. Since our classiﬁcation is based on the terminology used in the reviewed lit- erature, we mean that it provides relevance and understandability. We must further improve the classiﬁcation by increasing the breath of search for studies especially with focus on quality prom- ises of the model-driven engineering approach. We are involved in the MODELPLEX project12 which has the vision to evolve modelling technologies and tools for complex system development. In MODEL- PLEX, an empirical research plan is deﬁned in order to evaluate the impact of modelling technologies and tools on several attributes such as the productivity of software developers and the quality of models or generated artefacts. The results of empirical work will be used to evaluate the quality impact of model-drive engineering and the usefulness of our classiﬁcation. Acknowledgements This work has been funded by the Quality in Model-Driven Engineering project (cf. http://quality-mde.org/) at SINTEF and the European Commission within the 6th Framework Programme project MODELPLEX Contract Number 034081 (cf. http:// www.modelplex.org). We thank Dr. Marcela Fabiana Genero Bocco and Dr. Michel Chaudron for their valuable comments and suggestions. Appendix I. List of primary studies included in the review [P1] S.W. Ambler, The Elements of UML 2.0 Style, Cambridge University Press, 2005. [P2] M.C. Bastarrica, S. Rivas, P.O. Rossel, Designing and imple- menting a product family of model consistency checkers, in: Pro- ceedings of the Workshop on Quality in Modelling (QiM’07) held at MODELS 2007, 2007, pp. 36–49. [P3] B. Berenbach, The evaluation of large, complex UML analy- sis and design models, in: Proceedings of the 26th International Conference on Software Engineering (ICSE’04), 2004, pp. 232–241. 12 http://www.modelplex-ist.org/. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1667', '[P4] B. Berenbach, Metrics for model-driven requirements development, in: Proceedings of the 28th International Conference on Software Engineering (ICSE’06), 2006, pp. 445–451. [P5] Y. Choi, C. Bunse, Behavioral consistency checking for com- ponent-based software development using the KobrA approach, in: Proceedings of the 3rd International Workshop, Consistency Prob- lems in UML-based Software Development III – Understanding and Usage of Dependency Relationships, held at UML 2004, 2004, pp. 83–98. [P6] B. DuBois, C. Lange, S. Demeyer, M. Chaudron, A qualitative investigation of UML modeling conventions, in: Proceedings of the 1st Workshop on Quality in Modeling (QiM’06) held at MoDELS 2006, 2006, pp. 79–84. [P7] H. Eichelberger, Aesthetics of class diagrams, in: Proceed- ings of the 1st IEEE International Workshop on Visualizing Soft- ware for Understanding and Analysis (VISSOFT 2002), IEEE CS Press, 2002, pp. 23–31. [P8] H. Eichelberger, Nice class diagrams admit good design? in: Proceedings of the ACM Symposium on Software Visualization, 2003, pp. 159-ff. [P9] A. Gavras, M. Belaunde, L.F. Pires, J.P.A. Almeida, Towards an MDA-based development methodology for distributed applica- tions, in: Proceedings of the EWSA’04, 1st European Workshop on Software Architecture, LNCS, vol. 3047, Springer Berlin/Heidelberg, 2004, pp. 230–240. [P10] M. Giese, R. Heldal, From informal to formal speciﬁcations in UML, in: Proceedings of the UML 2004, LNCS, vol. 3273, Springer Berlin/Heidelberg, 2004, pp. 197–211. [P11] R. Haesen, M. Snoeck, Implementing consistency manage- ment techniques for conceptual modelling, in: Proceedings of the 3rd International Workshop, Consistency Problems in UML-based Software Development III – Understanding and Usage of Depen- dency Relationships, held at UML 2004, 2004, pp. 99–113. [P12] B. Hnatkowska, A. Walkowiak, Consistency checking of USDP models, in: Proceedings of the 3rd International Workshop Consistency Problems in UML-based Software Development III – Understanding and Usage of Dependency Relationships, 2004, pp. 59–70. [P13] H. Jonkers, M. Stroucken, R. Vdovjak, Bootstrapping do- main-speciﬁc model-driven software development within Philips, in: Proceedings of the 6th OOPSLA Workshop on Domain Speciﬁc Modeling (DSM’06), 2006, 10 p. [P14] S. Konrad, H.J. Goldsby, B.H.C. Cheng, i2MAP: an incremen- tal and iterative modelling and analysis process, in: Proceedings of the MoDELS 2007, LNCS, vol. 4735, 2007, pp. 451–466. [P15] J. Krogstie, A. Sølvberg, Information Systems Engineering: Conceptual Modeling in a Quality perspective, Kompendiumforla- get, Norway, 2000. [P16] L. Kuzniarz, M. Staron, C. Wohlin, An empirical study on using stereotypes to improve understanding of UML models, in: Proceedings of the 12th IEEE International Workshop on Program Comprehension, 2004, pp. 14–23. [P17] C.F.J. Lange, Managing model quality in UML-based soft- ware development, in: Proceedings of the 13th IEEE International Workshop on Software Technology and Engineering Practice (STEP’05), 2005, pp. 7–16. [P18] C.F.J. Lange, B. DuBois, M.R.V. Chaudron, S. Demeyer, An experimental investigation of UML modeling conventions, in: Pro- ceedings of the MODELS’06, 2006, pp. 27–41. [P19] C.F.J. Lange, Assessing and Improving the Quality of Mod- eling – A Series of Empirical Studies about the UML, Ph.D. thesis, Technische Universiteit Eindhoven, 2007. [P20] C.F.J. Lange, M.R.V. Chaudron, Defects in industrial UML models – a multiple case study, in: Proceedings of the Workshop on Quality in Modelling (QiM’07) at MODELS 2007, 2007, pp. 50– 65. [P21] C.F.J. Lange, M.A.M. Wijns, M.R.V. Chaudron, A visualiza- tion framework for task-oriented modeling using UML, in: Pro- ceedings of the 40th Annual Hawaii International Conference on System Sciences (HICSS 2007), 2007, pp. 289a–289a. [P22] O.I. Lindland, G. Sindre, A. Sølvberg, Understanding qual- ity in conceptual modelling, IEEE Software 11 (2) (1994) 42–49.', 'System Sciences (HICSS 2007), 2007, pp. 289a–289a. [P22] O.I. Lindland, G. Sindre, A. Sølvberg, Understanding qual- ity in conceptual modelling, IEEE Software 11 (2) (1994) 42–49. [P23] Z. Liu, H. Jifeng, X. Li, Y. Chen, Consistency and reﬁnement of UML models, in: Proceedings of the 3rd International Workshop Consistency Problems in UML-based Software Development III – Understanding and Usage of Dependency Relationships, 2004, pp. 23–40. [P24] W.E. McUmber, B.H.C. Cheng, A general framework for formalizing UML with formal languages, in: Proceedings of the IEEE International conference on Software Engineering (ICSE’01), 2001, pp. 433–442. [P25] R. Mitchell, High-quality modeling in UML, 2001,<http:// www.inferdata.com/resources/whitepapers/HQmodeling.4.pdf>. [P26] H.J. Nelson, D.E. Monarchi, Ensuring the quality of concep- tual representations, Software Quality Journal 15 (2) (2007) 213– 233. [P27] M. Peleg, D. Dori, The model multiplicity problem: exper- imenting with real-time speciﬁcation methods, IEEE Transactions on SE 26 (8) (2000) 742–759. [P28] H.C. Purchase, L. Colpoys, M. McGill, D. Carrington, C. Brit- ton, UML class diagram syntax: an empirical study of comprehen- sion, in: Proceedings of the Australian Symposium on Information, 2001, pp. 113–120. [P29] H.C. Purchase, J.A. Allder, D. Carrington, Graph layout aes- thetics in UML diagrams: user preferences. Journal of Graph Algo- rithms and Applications 6 (3) (2002) 255–279. [P30] I. Reinhartz-Berger, D. Dori, OPM vs. UML – experimenting with comprehension and construction of web application models, Empirical Software Engineering 10 (1) (2005) 57–80. [P31] K. Ryndina, J.M. Küster, M. Call, Consistency of business process models and object life cycles, in: Proceedings of the 1st Workshop on Quality in Modelling (QiM’06) at MoDELS 2006, LNCS, vol. 4364, 2006, pp. 80–90. [P32] M. Saeki, H. Kaiya, Model metrics and metrics of model transformation, in: Proceedings of the 1st Workshop on Quality in Modeling (QiM’06) held at MoDELS 2006, 2006, pp. 31– 46. [P33] L. Safa, The practice of deploying DSM, report from a Jap- anese appliance maker trenches, in: Proceedings of the 6th OOPSLA Workshop on Domain Speciﬁc Modeling (DSM’06), 2006, 12 p. [P34] M. Staron, L. Kuzniarz, L. Wallin, Case study on a process of industrial MDA realization: determinants of effectiveness, Nor- dic Journal of Computing 11 (3) (2004) 254–278. [P35] M. Staron, L. Kuzniarz, C. Wohlin, Empirical assessment of using stereotypes to improve comprehension of UML models: a set of experiments, Journal of Systems and Software 79 (5) (2006) 727–742. [P36] R. van Der Straeten, Formalizing behaviour preserving dependencies in UML, in: Proceedings of the 3rd International Workshop Consistency Problems in UML-based Software Develop- ment III – Understanding and Usage of Dependency Relationships, 2004, pp. 71–82. [P37] B. Trask, D. Paniscotti, A. Roman, V. Bhanot, Using model- driven engineering to complement software product line engineer- ing in developing software deﬁned radio components and applica- tions, in: ACM SIGPLAN International Conference on Object- Oriented Programming, Systems, Languages and Applications (OOPSLA’06), 2006, pp. 846–853. [P38] B. Unhelkar, Veriﬁcation and Validation for Quality of UML 2.0 Models, Wiley, 2005. [P39] H. Wegener, Balancing simplicity and expressiveness: designing domain-speciﬁc models for the reinsurance industry, 1668 P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669', 'in: Proceedings of the 4th OOPSLA Workshop on Domain-Speciﬁc Modelling (DSM’04), 2004. [P40] F. Weil, B. Mastenbrook, D. Nelson, P. Dietz, A. van der Berg, Automated semantic analysis of design models, in: Proceed- ings of the MoDELS 2007, LNCS, vol. 4735, 2007, pp. 166–180. Appendix II. Publication channels and summary of the primary studies The numbers of primary studies identiﬁed in each publication channel are in (). The titles of publication channels with more than three primary studies appear in italic. Books (3) Ph.D. thesis (1) Software and Systems Modeling (SoSyM) Journal (0) Software Quality Journal (1) Empirical Software Engineering Journal (1) Journal of Systems and Software (1) Information and Software Technology (0) UML/MoDELS conferences (4) QiM workshops at MoDELS(5) Other workshops at UML/MoDELs conferences(5) ECMDA-FA conferences (0) ICSE, International Conference on Software Engineering(3) OOPSLA, Conference on Object-oriented programming systems, languages, and applications (1) DSM workshops at OPPSLA(3) Digital libraries(6) Searched Internet for publications of authors as given in references of other papers (Lange and Staron)(4) Other Internet search (1) Known from before (1) (this is [P22]) References [1] C. Atkinson, J. Bayer, C. Bunse, E. Kamsties, O. Laitenberger, R. Laqua, D. Muthig, B. Paech, J. Wust, J. Zettel, Component-based Product Line Engineering with UML, Addison-Wesley Publishing Company, 2002. [2] R. Conradi, P. Mohagheghi, T. Arif, L.C. Hedge, G.A. Bunde, A. Pedersen, Object- oriented reading techniques for inspection of UML models – an industrial experiment, in: Proceedings of the European Conference on Object-Oriented Programming (ECOOP’03), LNCS, vol. 2749, 2003, pp. 483–501. [3] K. Cox, K. Phalp, Replicating the CREWS use case authoring guidelines experiment, Empirical Software Engineering 5 (2000) 245–267. [4] D. Harel, B. Rumpe, Modeling Languages: Syntax, Semantics and All that Stuff. Technical Paper Number MCS00-16, The Weizmann Institute of Science, Rehovot, Israel, 2000. [5] M. Jørgensen, M. Shepperd, A systematic review of software development cost estimation studies, IEEE Transactions on SE 33 (1) (2007) 33–53. [6] S. Kelly, J.P. Tolvanen, Domain-Speciﬁc Modelling, Enabling Full Code Generation, IEEE Computer Society Publications, 2008. [7] B. Kitchenham, Guidelines for performing systematic literature reviews in software engineering, v2.3, EBSE Technical Report 2007-01, developed by Software Engineering Group at Keele University and Department of Computer Science at University of Durham, 2007, 65 p. [8] A. MacDonald, D. Russell, B. Atchison, Model-driven development within a legacy system: an industry experience report, in: Proceedings of the Australian Software Engineering Conference, 2005, pp. 14–22. [9] P. Mohagheghi, J.Ø. Aagedal, Evaluating quality in model-driven engineering, in: International Workshop on Modeling in Software Engineering (MiSE’07) ICSE Workshop, 2007, 6 p. [10] P. Mohagheghi, V. Dehlen, Developing a quality framework for model-driven engineering, in: Proceedings of the 2nd Workshop on Quality in Modeling at MoDELS 2007, LNCS, vol. 5002, 2007, pp. 275–286. [11] P. Mohagheghi, V. Dehlen, Where is the proof? – a review of experiences from applying MDE in industry, in: Proceedings of the 4th European Conference on Model Driven Architecture Foundations and Applications (ECMDA’08), LNCS, vol. 5095, 2008, pp. 432–443. [12] P. Mohagheghi, V. Dehlen, T. Neple, Towards a tool-supported quality model for model-driven engineering, in: Proceedings of the 3rd Workshop on Quality in Modelling (QiM’08) at MODELS 2008, 2008, 15 p. [13] P. Mohagheghi, V. Dehlen, A metamodel for specifying quality models in model-driven engineering, in; Proceedings of the Nordic Workshop on Model Driven Engineering, 2008, pp. 51–65. [14] A.D. Neto, R. Subramanyan, M. Vieira, G.H. Travassos, F. Shull, Improving evidence about software technologies, a look at model-based testing, IEEE', 'Driven Engineering, 2008, pp. 51–65. [14] A.D. Neto, R. Subramanyan, M. Vieira, G.H. Travassos, F. Shull, Improving evidence about software technologies, a look at model-based testing, IEEE Software 23 (3) (2008) 10–13. [15] K.T. Phalp, J. Vincent, K. Cox, Improving the quality of use case descriptions: empirical assessment of writing guidelines, Software Quality Journal 15 (4) (2007) 383–399. [16] J. Rech, A. Spriestersbach, D.4.1: quality defects in model-driven software development, deliverable of Vide project 2007, <http://www.vide-ist.eu/ extern/VIDE_D4.1.pdf>. [17] J. Rumbaugh, I. Jacobson, G. Booch, The Uniﬁed Modeling Language Reference Manual, Addison-Wesley, 1999. [18] M. Staron, Adopting model driven software development in industry – a case study at two companies, in: Proceedings of the MoDELS 2006, LNCS, vol. 4199, 2006, pp. 57–72. [19] A. Watson, A brief history of MDA, upgrade, European Journal for the Informatics Professional IX (2) (2008). URL <http://www.upgrade- cepis.org>. [20] R.K. Yin, Case Study Research: Design and Methods, Saga Publications, 2003. P. Mohagheghi et al. / Information and Software Technology 51 (2009) 1646–1669 1669']","['DEFINITIONS AND APPROACHES TO MODEL QUALITY IN MODEL-BASED SOFTWARE DEVELOPMENT • Most practices are concerned with error  prevention, while some also facilitate er - ror detection. • Lack of process contributed to a large  number of errors since modelers do not  have process that guides where to start  and conventions that provide uniformity. • Lack of quality assurance (for example re- views) led to a staggering number of er - rors. • Design models that originated from analy- sis had fewer errors than those originated  as designs. • Having control of the quality of the MDE  tooling is necessary to have control of the  quality of systems being developed using  the tools. • For evaluating the progress and the qua - lity of development artifacts, a study  identified tasks of model-centric softwa - re engineering and the information (e.g.,  questions about the UML model) that is  required to fulfill these tasks. • It seems clear that having high quality  requirements process is important, and  having ways of saying ‘‘how good” the re- quirements are is thus a clear advantage. • Quality assurance techniques such as  collecting metrics by tools and analyzing  them should be part of a modeling pro - cess. • Guidelines or conventions regarding the  modeling process: - the early modeling effort should cover  the entire breadth of the domain. - Identify ‘‘out of scope” use cases as  early as possible. - Discover business objects and their  services through use case definition  with sequence diagrams.  - Elicit requirements and processes  by starting at interfaces and modeling  inward. - Create simple content; the implication  is that you should not add additional  aspects to your models unless they are  justifiable. - Single source information; you should  also model a concept once and once  only, storing the information in the best  place possible. • It is also an advantage if artifacts in one  phase may be used to develop or genera- te artifacts in the next phase in order to  achieve completeness and consistency. • Many of the proposed conventions may  be enforced by tools, but the problem to- day is that most modeling tools enforce  syntactical and aesthetic constraints very  weakly and semantic constraints are not  enforced at all. • For error detection, one may use tools  such as Design Advisor or perform inspec- tions. • There is little empirical evidence in the  covered literature regarding the benefits  of conventions and the results of few stu- dent experiments are not conclusive.  • The quality impact of conventions seems  to depend on the task, the complexity of  conventions and tool support as well and  empirical studies should describe these  factors better in order to help evaluating  the usefulness and cost. • Some studies suggest that UML is unne - cessarily complex in many ways, and this  inherent deficiency hinders coherent mo- deling and comprehension of systems. • Technical solutions that involve sophisti - cated CASE tools to impose consistency  alleviate manual consistency maintenan - ce, but they do not address the core pro - blem of mental integration. Some indicate  the use of single-diagrams. • Some authors have proposed to use the  observer pattern to achieve consistency  between different views in conceptual  models: as the user changes a specifica - tion in one view, all other views are infor- med about the modification. • Practices can, of course, be combined, for  example DSMLs often include constraints  and formal semantics to prevent errors  and facilitate generation, and the num - ber of diagrams is often reduced. Having  a model-based development process that  includes guide. FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Keywords: Model quality Model-driven development Who is this briefing for? Software engineers practitioners  who want to make decisions  about quality in model-based sof- tware development on scientific  evidence. Where the findings come  from? All findings of this briefing were', 'who want to make decisions  about quality in model-based sof- tware development on scientific  evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic  review conducted by Mohagheghi  et al. . What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg This briefing reports the practices found to  improve the quality of models.']","**Title:** Enhancing Model Quality in Software Development

**Introduction:**  
This Evidence Briefing summarizes key findings from a systematic review of literature focused on model quality in model-based software development. The goal is to clarify what constitutes model quality and how it can be improved, providing actionable insights for practitioners involved in software modeling.

**Main Findings:**  
The systematic review analyzed 40 studies published since 2000 and identified six key quality goals for models, known as the 6C goals: 

1. **Correctness**: Models must accurately represent the intended domain without violating syntactic rules or conventions.
2. **Completeness**: Models should include all relevant information necessary for their intended purpose, ensuring that no critical elements are omitted.
3. **Consistency**: Models must not contain contradictions, maintaining coherence both within individual models and across different models at various abstraction levels.
4. **Comprehensibility**: Models should be easily understandable by their intended users, which includes both human stakeholders and automated tools.
5. **Confinement**: Models should remain focused on their specific purpose without unnecessary complexity or detail.
6. **Changeability**: Models should be adaptable to changes in requirements or understanding of the domain, allowing for easy updates and modifications.

To achieve these quality goals, the review highlights six practices:

1. **Model-based Development Process**: Implementing structured processes that guide modeling activities can significantly enhance model quality.
2. **Modeling Conventions**: Establishing guidelines for model creation can help ensure consistency and correctness.
3. **Single-Diagram Approach**: Using a single diagram to represent a model can reduce complexity and improve comprehensibility.
4. **Formal Models**: Utilizing formal languages can enhance correctness and consistency, although they may reduce human comprehensibility.
5. **Domain-Specific Modelling Languages (DSMLs)**: Tailoring languages to specific domains can improve both comprehensibility and correctness.
6. **Generating Models from Other Models**: Transforming models can help maintain consistency and completeness across different representations.

Empirical evidence from the reviewed studies suggests that while many practices show promise in improving model quality, their effectiveness often depends on the context and specific application. For instance, using modeling conventions can reduce defects but may increase initial effort. Similarly, formal models facilitate analysis but can be challenging for users to interpret.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, project managers, and educators involved in model-based software development who seek to enhance the quality of their modeling practices.

**Where the findings come from?**  
All findings are based on a systematic review conducted by Mohagheghi et al. (2009), which analyzed literature on model quality in model-driven development.

**What is included in this briefing?**  
The briefing includes a summary of the 6C goals for model quality, the practices proposed to enhance these goals, and insights from empirical studies regarding the effectiveness of these practices.

For additional evidence briefings on software engineering, please visit: [http://www.example.com](http://www.example.com)

**Original Research Reference:**  
Mohagheghi, P., Dehlen, V., & Neple, T. (2009). Definitions and approaches to model quality in model-based software development – A review of literature. Information and Software Technology, 51(9), 1646-1669. doi:10.1016/j.infsof.2009.04.004"
"['Empirical studies of agile software development: A systematic review Tore Dyba˚ *, Torgeir Dings øyr SINTEF ICT, S.P. Andersensv. 15B, NO-7465 Trondheim, Norway Received 22 October 2007; received in revised form 22 January 2008; accepted 24 January 2008 Available online 2 February 2008 Abstract Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A system- atic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identiﬁed 1996 studies, of which 36 were identiﬁed as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the beneﬁts and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of ﬁndings, according to topic, that can be compared for relevance to their own settings and situations. /C2112008 Elsevier B.V. All rights reserved. Keywords: Empirical software engineering; Evidence-based software engineering; Systematic review; Research synthesis; Agile software development; XP; Extreme programming; Scrum Contents 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 834 2. Background – agile software development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 834 2.1. The field of agile software development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 834 2.2. Summary of previous reviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836 2.3. Objectives of this review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837 3. Review method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837 3.1. Protocol development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837 3.2. Inclusion and exclusion criteria. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837 3.3. Data sources and search strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 838 3.4. Citation management, retrieval, and inclusion decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 838 3.5. Quality assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 839 3.6. Data extraction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840 3.7. Synthesis of findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840 4. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840 4.1. Overview of studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840', '4.1. Overview of studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840 4.2. Research methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 841 4.3. Methodological quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 841 4.4. Introduction and adoption of agile development methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842 0950-5849/$ - see front matter/C2112008 Elsevier B.V. All rights reserved. doi:10.1016/j.infsof.2008.01.006 * Corresponding author. Tel.: +47 73 59 29 47; fax: +47 73 59 29 77. E-mail addresses: tore.dyba@sintef.no (T. Dyba˚), torgeir.dingsoyr@sintef.no (T. Dings øyr). www.elsevier.com/locate/infsof Available online at www.sciencedirect.com Information and Software Technology 50 (2008) 833–859', '4.4.1. Introduction and adoption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 843 4.4.2. Development process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 843 4.4.3. Knowledge and project management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844 4.5. Human and social factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844 4.5.1. Organizational culture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844 4.5.2. Collaborative work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 845 4.5.3. Team characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 846 4.6. Perceptions on agile methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 846 4.6.1. Customer perceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 846 4.6.2. Developer perceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 847 4.6.3. Student perceptions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 847 4.7. Comparative studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 847 4.7.1. Project management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 847 4.7.2. Productivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 848 4.7.3. Product quality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849 4.7.4. Work practices and job satisfaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849 5. Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849 5.1. Benefits and limitations of agile development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850 5.2. Strength of evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 851 5.3. Implications for research and practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 851 5.4. Limitations of this review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 853 6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 853 Appendix A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 853 Appendix B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855', 'Appendix B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855 Appendix C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857 Appendix D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 858 1. Introduction The issue of how software development should be orga- nized in order to deliver faster, better, and cheaper solu- tions has been discussed in software engineering circles for decades. Many remedies for improvement have been suggested, from the standardization and measurement of the software process to a multitude of concrete tools, tech- niques, and practices. Recently, many of the suggestions for improvement have come from experienced practitioners, who have labelled their methodsagile software development . This movement has had a huge impact on how software is devel- oped worldwide. However, though there are many agile methods, little is known about how these methods are car- ried out in practice and what their eﬀects are. This systematic review seeks to evaluate, synthesize, and present the empirical ﬁndings on agile software develop- ment to date, and provide an overview of topics researched, their ﬁndings, strength of the ﬁndings, and implications for research and practice. We believe this overview will be important for practitioners who want to stay up to date with the state of research, as well as for researchers who want to identify topic areas that have been researched or where research is lacking. This review will also help the sci- entiﬁc community that works with agile development to build a common understanding of the challenges that must be faced when investigating the eﬀectiveness of agile meth- ods. The results of such investigation will be relevant to the software industry. The article is organized as follows: In Section 2, we give an overview of agile software development, identify the theoretical roots, and existing reviews. Section3 describes the methods used for this review. Section 4 reports the ﬁnd- ings of the review after ﬁrst presenting an overview of the studies, the research methods used, the quality of the meth- odology, and a description of the studies in four main the- matic groups. Section5 discusses beneﬁts and limitations, strength of evidence, and implications for research and practice. Section6 concludes and provides recommenda- tions for further research on agile software development. 2. Background – agile software development We ﬁrst describe the ﬁeld of agile development, core ideas, how this ﬁeld relates to other disciplines, and sum- marize the critique of agile development. We then summa- rize previous reviews of the agile literature, justify the need for this review, and state the research questions that moti- vated the review. 2.1. The ﬁeld of agile software development Methods for agile software development constitute a set of practices for software development that have been cre- ated by experienced practitioners[68]. These methods can be seen as a reaction to plan-based or traditional methods, which emphasize ‘‘a rationalized, engineering-based approach” [21,47] in which it is claimed that problems are fully speciﬁable and that optimal and predictable solu- 834 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'tions exist for every problem. The ‘‘traditionalists ” are said to advocate extensive planning, codiﬁed processes, and rig- orous reuse to make development an eﬃcient and predict- able activity[11]. By contrast, agile processes address the challenge of an unpredictable world by relying on ‘‘people and their crea- tivity rather than on processes” [21,47]. Ericksson et al. [27] deﬁne agility as follows: agility means to strip away as much of the heaviness, com- monly associated with the traditional software-develop- ment methodologies, as possible to promote quick response to changing environments, changes in user require- ments, accelerated project deadlines and the like. (p. 89) Williams and Cockburn [66] state that agile develop- ment is ‘‘about feedback and change ”, that agile methodol- ogies are developed to ‘‘embrace, rather than reject, higher rates of change ”. In 2001, the ‘‘agile manifesto ” was written by the prac- titioners who proposed many of the agile development methods. The manifesto states that agile development should focus on four core values1: /C15Individuals and interactions over processes and tools. /C15Working software over comprehensive documentation. /C15Customer collaboration over contract negotiation. /C15Responding to change over following a plan. In an article that describes the history of iterative and incremental development, Larman and Basili [40] identify Dynamic Systems Development Method (DSDM) [60] as the ﬁrst agile method, followed by extreme programming (XP) [9], which originated from the Chrysler C3 project in 1996 [5]. In 1998, the word ‘‘agile ” was used in combination with ‘‘software process ” for the ﬁrst time [6]. Several further methods followed, including the Crystal family of methods [16], EVO [28], Feature-Driven Development [50], Lean Development [52] and Scrum [56]. In 2004, a new version of XP appeared [10]. See Table 1 for an overview of the most referenced agile development methods, and Table 2 for a comparison of traditional and agile development. Many have tried to explain the core ideas in agile soft- ware development, some by examining similar trends in other disciplines. Conboy and Fitzgerald[19], for example, describe agility as what is known in other ﬁelds as ‘‘ﬂexibil- ity” and ‘‘leanness ”. They refer to several sources of inspi- ration, primarily: /C15Agile manufacturing, which was introduced by researchers from Lehigh University in anattempt for the USA to regain its competitive position in manufacturing. Key concepts in Table 1 Description of main agile development methods, with key references Agile method Description Reference Crystal methodologies A family of methods for co-located teams of diﬀerent sizes and criticality: Clear, Yellow, Orange, Red, Blue. The most agile method, Crystal Clear, focuses on communication in small teams developing software that is not life-critical. Clear development has seven characteristics: frequent delivery, reﬂective improvement, osmotic communication, personal safety, focus, easy access to expert users, and requirements for the technical environment [16] Dynamic software development method (DSDM) Divides projects in three phases: pre-project, project life-cycle, and post project. Nine principles underlie DSDM: user involvement, empowering the project team, frequent delivery, addressing current business needs, iterative and incremental development, allow for reversing changes, high-level scope being ﬁxed before project starts, testing throughout the lifecycle, and eﬃcient and eﬀective communication [60] Feature-driven development Combines model-driven and agile development with emphasis on initial object model, division of work in features, and iterative design for each feature. Claims to be suitable for the development of critical systems. An iteration of a feature consists of two phases: design and development [50]', 'work in features, and iterative design for each feature. Claims to be suitable for the development of critical systems. An iteration of a feature consists of two phases: design and development [50] Lean software development An adaptation of principles from lean production and, in particular, the Toyota production system to software development. Consists of seven principles: eliminate waste, amplify learning, decide as late as possible, deliver as fast as possible, empower the team, build integrity, and see the whole [52] Scrum Focuses on project management in situations where it is diﬃcult to plan ahead, with mechanisms for ‘‘empirical process control ”; where feedback loops constitute the core element. Software is developed by a self-organizing team in increments (called ‘‘sprints ”), starting with planning and ending with a review. Features to be implemented in the system are registered in a backlog. Then, the product owner decides which backlog items should be developed in the following sprint. Team members coordinate their work in a daily stand-up meeting. One team member, the scrum master, is in charge of solving problems that stop the team from working eﬀectively [56] Extreme programming (XP; XP2) Focuses on best practice for development. Consists of twelve practices: the planning game, small releases, metaphor, simple design, testing, refactoring, pair programming, collective ownership, continuous integration, 40-h week, on-site customers, and coding standards. The revised ‘‘XP2” consists of the following ‘‘primary practices ”: sit together, whole team, informative workspace, energized work, pair programming, stories, weekly cycle, quarterly cycle, slack, 10-minute build, continuous integration, test-ﬁrst programming, and incremental design. There are also 11 ‘‘corollary practices ” [9,10] 1 http://agilemanifesto.org T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 835', 'agile manufacturing are integrating customer-supplier rela- tionships, managing change, uncertainty, complexity, uti- lizing human resources, and information[30,55]. /C15Lean development [67], which is rooted in the Toyota Production System [49] from the 1950s. Some of the core ideas in this system were to eliminate waste, achieve quality ﬁrst time, and focus on problem solving. Meso and Jain [44] have compared ideas in agile devel- opment to those in Complex Adaptive Systems by provid- ing a theoretical lens for understanding how agile development can be used in volatile business environments. Turk et al.[64] have clariﬁed the assumptions that underlie processes of agile development and also identiﬁes the limi- tations that may arise from these assumptions. In the liter- ature, we also ﬁnd articles that trace the roots of agile development to the Soft Systems Methodology of Peter Checkland[14], New product development [63] and Ack- oﬀ’s interactive planning [4]. Nerur and Balijepally [46] compare agile development to maturing design ideas in architectural design and strategic management: ‘‘the new design metaphor incorporates learn- ing and acknowledges the connectedness of knowing and doing (thought and action), the interwoven nature of means and ends, and the need to reconcile multiple world-views ” (p. 81). However, agile development methods have also been criticized by some practitioners and academics, mainly focusing on ﬁve aspects: 1. Agile development is nothing new; such practices have been in place in software development since the 1960s [43]. 2. The lack of focus on architecture is bound to engender suboptimal design-decisions [42,61]. 3. There is little scientiﬁc support for many of the claims made by the agile community [42]. 4. The practices in XP are rarely applicable, and are rarely applied by the book [34]. 5. Agile development methods are suitable for small teams, but for larger projects, other processes are more appro- priate [17]. It has also been suggested that the social values embraced by extreme programming makes agile teams make ineﬀective decisions, which are contrary to those that the group members desire[41]. 2.2. Summary of previous reviews Introductions to and overviews of agile development are given by Abrahamsson et al. [2], Cohen et al. [17],a n d Erickson et al. [27]. These three reports describe the state of the art and state of the practice in terms of characteris- tics of the various agile methods and lessons learned from applying such methods in industry. We summarize each of these previous overviews brieﬂy. The ﬁrst review of the existing literature on agile soft- ware development was done in a technical report published by Abrahamsson et al. at VTT in 2002[2]. The report dis- cusses the concept of agile development, presents processes, roles, practices, and experience with 10 agile development methods, and compares the methods with respect to the phases that they support and the level of competence that they require. Only DSDM and the Rational Uniﬁed Pro- cess [38] were found to give full coverage to all phases of development, while Scrum mainly covers aspects related to project management. Abrahamsson et al. found anec- dotal evidence that agile methods are ‘‘eﬀective and suit- able for many situations and environments”, but state that very few empirically validated studies support these claims. The report was followed by a comparative analysis of nine agile methods in 2003[3], where it is stated that empirical support for the suggested methods remains scarce. Cohen et al.’s review published in 2004 [17] emphasizes the history of agile development, shows some of the roots to other disciplines, and, in particular, discusses relations between agile development and the Capability Maturity Model (CMM)[51]. They further describe the state of the art with respect to the main agile methods and their char- acteristics. They also describe the state of the practice,', 'Model (CMM)[51]. They further describe the state of the art with respect to the main agile methods and their char- acteristics. They also describe the state of the practice, which resulted from an online discussion between 18 prac- Table 2 Main diﬀerences between traditional development and agile development [47] Traditional development Agile development Fundamental assumption Systems are fully speciﬁable, predictable, and are built through meticulous and extensive planning High-quality adaptive software is developed by small teams using the principles of continuous design improvement and testing based on rapid feedback and change Management style Command and control Leadership and collaboration Knowledge management Explicit Tacit Communication Formal Informal Development model Life-cycle model (waterfall, spiral or some variation) The evolutionary-delivery model Desired organizational form/structure Mechanistic (bureaucratic with high formalization), aimed at large organizations Organic (ﬂexible and participative encouraging cooperative social action), aimed at small and medium- sized organizations Quality control Heavy planning and strict control. Late, heavy testing Continuous control of requirements, design and solutions. Continuous testing 836 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'titioners, many of whom were involved in deﬁning the var- ious agile development methods. They discuss issues such as the introduction of, and project management in, agile development. They also present experiments and surveys, and seven case studies of agile development. The authors believe that agile methods will be consolidated in the future, just as object-oriented methods were consolidated. Further, they do not believe that agile methods will rule out traditional methods. Rather, they believe that agile and traditional methods will have a symbiotic relationship, in which factors such as the number of people working on a project, application domain, criticality, and innovativeness will determine which process to select. In 2005, Erickson et al. [27] described the state of research on XP, agile software development, and agile modelling. With respect to XP, they found a small number of case stud- ies and experience reports that promote the success of XP. The XP practice of pair programming is supported by a more well-established stream of research, and there are some stud- ies on iterative development. Erickson et al. recommend that the other core practices in XP be studied separately in order to identify what practices are working (for recent studies of practices, see [22,26]). Further, they see challenges with matching agile software development methods with stan- dards such as ISO, and they argue that this is an area that needs further research. There was much less research on agile modelling than on XP. 2.3. Objectives of this review In a short time, agile development has attracted huge interest from the software industry. A survey in the USA and Europe reveals that 14% of companies are using agile methods, and that 49% of the companies that are aware of agile methods are interested in adopting them[1]. In just six years, the Agile 2 conference has grown to attract a larger attendance than most conferences in software engineering. Rajlich [53] describes agile development as a paradigm shift in software engineering, which has emerged from inde- pendent sources: studies of software life cycles and iterative development. ‘‘The new paradigm brings a host of new topics into the forefront of software engineering research. These topics have been neglected in the past by researchers inspired by the old paradigm, and therefore there is a backlog of research problems to be solved.” (p. 70). No systematic review of agile software development research has previously been published. The existing reviews that were presented in the previous section only partially cover the empirical studies that exist today. Fur- ther, the previous reviews do not include any assessment of the quality of the published studies, as in this systematic review. This means that practitioners and researchers have to rely on practitioner books in order to get an overview. We hope that this article will be useful for both groups, and that it will make clear which claims on agile software development are supported by scientiﬁc studies. The objective of the review is to answer the following research questions: 1. What is currently known about the beneﬁts and limita- tions of agile software development? 2. What is the strength of the evidence in support of these ﬁndings? 3. What are the implications of these studies for the soft- ware industry and the research community? In addition to producing substantive ﬁndings regarding agile software development, the review also aims to advance methodology for integrating diverse study types, including qualitative research, within systematic reviews of software engineering interventions. The result of this work has been reported separately[23], and is not further described here. 3. Review method Informed by the established method of systematic review [31,35,36], we undertook the review in distinct stages: the development of review protocol, the identiﬁca- tion of inclusion and exclusion criteria, a search for rele-', 'review [31,35,36], we undertook the review in distinct stages: the development of review protocol, the identiﬁca- tion of inclusion and exclusion criteria, a search for rele- vant studies, critical appraisal, data extraction, and synthesis. In the rest of this section, we describe the details of these stages and the methods used. 3.1. Protocol development We developed a protocol for the systematic review by fol- lowing the guidelines, procedures, and policies of the Camp- bell Collaboration,3 the Cochrane Handbook for Systematic Reviews of Interventions [31], the University of York’s Cen- tre for Reviews and Dissemination’s guidance for those car- rying out or commissioning reviews[35], and consultation with software engineering specialists on the topic and meth- ods. This protocol speciﬁed the research questions, search strategy, inclusion, exclusion and quality criteria, data extraction, and methods of synthesis. 3.2. Inclusion and exclusion criteria Studies were eligible for inclusion in the review if they presented empirical data on agile software development and passed the minimum quality threshold (see Section 3.5). Studies of both students and professional software developers were included. Inclusion of studies was not restricted to any speciﬁc type of intervention or outcome measure. The systematic review included qualitative and quantitative research studies, published up to and includ- ing 2005. Only studies written in English were included. 2 www.agile200X.org 3 www.campbellcollaboration.org T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 837', 'Studies were excluded if their focus, or main focus, was not agile software development or if they did not present empirical data. Furthermore, as our research questions are concerned with agile development as a whole, and its underlying assumptions, studies that focused on single techniques or practices, such as pair programming, unit testing, or refactoring, were excluded. In addition to agile methods in general, we included the following speciﬁc methods: XP, Scrum, Crystal, DSDM, FDD, and Lean. Finally, given that our focus was on empirical research, ‘‘lessons learned ” papers (papers without a research ques- tion and research design) and papers merely based on expert opinion were also excluded. 3.3. Data sources and search strategy The search strategy included electronic databases and hand searches of conference proceedings. The following electronic databases were searched: /C15ACM Digital Library /C15Compendex /C15IEEE Xplore /C15ISI Web of Science /C15Kluwer Online /C15ScienceDirect – Elsevier /C15SpringerLink /C15Wiley Inter Science Journal Finder In addition, we hand-searched all volumes of the follow- ing conference proceedings for research papers: /C15XP /C15XP/Agile Universe /C15Agile Development Conference Fig. 1 shows the systematic review process and the num- ber of papers identiﬁed at each stage. In stage 1, the titles, abstracts, and keywords of the articles in the included elec- tronic databases and conference proceedings were searched using the following search terms: (1) agile AND software (2) extreme programming (3) xp AND software (4) scrum AND software (5) crystal AND software AND (clear OR orange OR red OR blue) (6) dsdm AND software (7) fdd AND software (8) feature AND driven AND development AND software (9) lean AND software AND development All these search terms for agile articles were combined by using the Boolean ‘‘OR ” operator, which entails that an article only had to include any one of the terms to be retrieved. That is, we searched: 1 OR 2 OR 3 OR 4 OR 5 OR 6 OR 7 OR 8 OR 9 Excluded from the search were editorials, prefaces, arti- cle summaries, interviews, news, reviews, correspondence, discussions, comments, reader’s letters and summaries of tutorials, workshops, panels, and poster sessions. This search strategy resulted in a total of 2946 ‘‘hits ” that included 1996 unduplicated citations. 3.4. Citation management, retrieval, and inclusion decisions Relevant citations from stage 1 ( n = 1996) were entered into and sorted with the aid of EndNote. They were then imported to Excel, where we recorded the source of each citation, our retrieval decision, retrieval status, and eligibility decision. For each subsequent stage, separate EndNote databases and Excel sheets were established. At stage 2, both authors sat together and went through the titles of all studies that resulted from stage 1, to deter- mine their relevance to the systematic review. At this stage, we excluded studies that were clearly not about agile soft- ware development, independently of whether they were empirical or not. As an example, because our search strat- egy included the term ‘‘xp and software”, we got several ‘‘hits” on articles about Microsoft’s Windows XP operat- ing system. In addition, because we used the term ‘‘agile and software”, we got several hits on articles related to agile manufacturing. Articles with titles that indicated clearly that the articles were outside the scope of this sys- tematic review were excluded. However, titles are not always clear indicators of what an article is about. Some authors’ use of ‘‘clever” or witty titles can sometimes obscure the actual content of an article. In such cases, the articles were included for review in the next stage. At this stage, 1175 articles were excluded. At stage 3, studies were excluded if their focus, or main focus, was not agile software development or if they did not present empirical data. However, we found that', 'At stage 3, studies were excluded if their focus, or main focus, was not agile software development or if they did not present empirical data. However, we found that abstracts were of variable quality; some abstracts were Stage 1 n = 1,996 Identify relevant studies – search databases and conference proceedings Stage 2 n = 821Exclude studies o the basis of titles Stage 3 n = 270Exclude studies on the basis of abstracts Stage 4 n = 36Obtain primary papers and critically appraise studies – on Fig. 1. Stages of the study selection process. 838 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'missing, poor, and/or misleading, and several gave little indication of what was in the full article. In particular, it was not always obvious whether a study was, indeed, an empirical one. Therefore, at this stage, we included all stud- ies that indicated some form of experience with agile devel- opment. If it was unclear from the title, abstract, and keywords whether a study conformed to the screening cri- teria, it was included for a detailed quality assessment (see below). At this stage, we divided the abstracts among ourselves and a third researcher in such a way that each abstract was reviewed by two researchers independently of each other. For the 821 abstracts assessed, the number of observed agreements was 738 (89.9%). We also computed the Kappa coeﬃcient of agreement, which corrects for chance agree- ment [18]. The Kappa coeﬃcient for stage 3 assessments was 0.78, which is characterized as ‘‘substantial agreement ” by Landis and Koch [39]. All disagreements were resolved by discussion that included all three researchers, before proceeding to the next stage. As a result of this discussion, another 551 articles were excluded at this stage, which left 270 articles for the detailed quality assessment. 3.5. Quality assessment Each of the 270 studies that remained after stage 3 was assessed independently by both authors, according to 11 criteria. These criteria were informed by those proposed for the Critical Appraisal Skills Programme (CASP)4 (in particular, those for assessing the quality of qualitative research [29]) and by principles of good practice for con- ducting empirical research in software engineering [37]. The 11 criteria covered three main issues pertaining to quality that need to be considered when appraising the studies identiﬁed in the review (seeAppendix B ): /C15Rigour. Has a thorough and appropriate approach been applied to key research methods in the study? /C15Credibility. Are the ﬁndings well-presented and meaningful? /C15Relevance. How useful are the ﬁndings to the software industry and the research community? We included three screening criteria that were related to the quality of the reporting of a study’s rationale, aims, and context. Thus, each study was assessed according to whether: 1. The study reported empirical research or whether it was merely a ‘‘lessons learned ” report based on expert opinion. 2. The aims and objectives were clearly reported (including a rationale for why the study was undertaken). 3. There was an adequate description of the context in which the research was carried out. The ﬁrst of these three criteria represents the minimum quality threshold of the review and was used to exclude non-empirical research papers (seeAppendix B ). As part of this screening process, any single-technique or single- practice papers were also identiﬁed and excluded. Five criteria were related to the rigour of the research methods employed to establish the validity of data collec- tion tools and the analysis methods, and hence the trust- worthiness of the ﬁndings. Consequently, each study was assessed according to whether: 4. The research design was appropriate to address the aims of the research. 5. There was an adequate description of the sample used and the methods for identifying and recruiting the sample. 6. Any control groups were used to compare treatments. 7. Appropriate data collection methods were used and described. 8. There was an adequate description of the methods used to analyze data and whether appropriate methods for ensuring the data analysis were grounded in the data. In addition, two criteria were related to the assessment of the credibility of the study methods for ensuring that the ﬁndings are valid and meaningful. In relation to this, we judged the studies according to whether: 9. The relationship between the researcher and partici- pants was considered to an adequate degree. 10. The study provided clearly stated ﬁndings with credible results and justiﬁed conclusions.', '9. The relationship between the researcher and partici- pants was considered to an adequate degree. 10. The study provided clearly stated ﬁndings with credible results and justiﬁed conclusions. The ﬁnal criterion was related to the assessment of the relevance of the study for the software industry at large and the research community. Thus, we judged the studies according to whether: 11. They provided value for research or practice. Taken together, these 11 criteria provided a measure of the extent to which we could be conﬁdent that a particular study’s ﬁndings could make a valuable contribution to the review. Each of the 11 criteria was graded on a dichoto- mous (‘‘yes ” or ‘‘no ”) scale. Again, only criterion 1 was used as the basis for including or excluding a study. Of the 270 articles assessed for quality, the number of observed agreements regarding inclusion/exclusion based on the screening criterion was 255 (94.4%). The corresponding Kappa coeﬃcient was 0.79. Again, all disagreements were resolved by discussion that included all three researchers. At this stage, another 234 lessons-learned or single-practice articles were excluded, leaving 33 primary and 3 secondary studies4 www.phru.nhs.uk/Pages/PHD/CASP.htm T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 839', 'for data extraction and synthesis. A summary of the quality assessment criteria for these studies is presented in Table 3 . 3.6. Data extraction During this stage, data was extracted from each of the 33 primary studies included in this systematic review according to a predeﬁned extraction form (seeAppendix C). This form enabled us to record full details of the articles under review and to be speciﬁc about how each of them addressed our research questions. When we piloted the extraction process we found that extracting data was hindered by the way some of the pri- mary studies were reported. Due to this, we also found that we diﬀered too much in what we actually extracted for independent extraction to be meaningful. As a conse- quence, all data from all primary studies were extracted by both authors in consensus meetings. The aims, settings, research methods descriptions, ﬁnd- ings, and conclusions, as reported by the authors of the pri- mary studies, were copied verbatim into NVivo, from QSR Software,5 a specialist software package for undertaking the qualitative analysis of textual data. 3.7. Synthesis of ﬁndings Meta-ethnographic methods were used to synthesize the data extracted from the primary studies [48]. The ﬁrst stage of the synthesis was to identify the main concepts from each primary study, using the original author’s terms. The key concepts were then organized in tabular form to enable comparison across studies and the reciprocal trans- lation of ﬁndings into higher-order interpretations. This process is analogous to the method of constant comparison used in qualitative data analysis[45,62]. When we identiﬁed diﬀerences in ﬁndings, we investigated whether these could be explained by the diﬀerences in methods or characteris- tics of the study setting. In a meta-ethnographic synthesis, studies can relate to one another in one of three ways: they may be directly comparable as reciprocal translations; they may stand in opposition to one another as refutational translations; or taken together they may represent a line of argument [13]. Table 4 shows Noblit and Hare’s seven-step process for conducting a meta-ethnography. This process of reciprocal and refutational translation and synthesis of studies achieved three things with respect to answering our overarching question about the beneﬁts and limitations of agile software development. First, it iden- tiﬁed a set of higher-order interpretations, or themes, which recurred across studies. Second, it documented that agile software development contains both positive and negative dimensions. Finally, it highlighted gaps in the evidence about the applicability of agile methods to software development. 4. Results We identiﬁed 36 empirical studies on agile software devel- opment. Thirty-three are primary studies (S1-S33) and three are secondary studies (S34-S36); seeAppendix A . In what follows, we discuss the primary studies. These cover a range of research topics, were done with a multitude of research methods, and were performed in settings that ranged from professional projects to university courses. Key data, along with a description of the domain in which each primary study was conducted, is presented inAppendix D. We categorized the studies into four main groups: (1) introduction and adoption, (2) human and social factors, (3) customer and developer perceptions, and (4) compara- tive studies. Three studies did not ﬁt into any of these cat- egories. They provide baseline data on various aspects of agile development (S1, S8, S11). We now describe characteristics of the studies, describe the research methods applied, and assess the quality of the studies. Then, we present the studies included in the four categories mentioned above. 4.1. Overview of studies With respect to the kinds of agile method that have been studied, we see from Table 5 that 25 (76%) of the studies in this review were done on XP. Studies on agility in general', 'With respect to the kinds of agile method that have been studied, we see from Table 5 that 25 (76%) of the studies in this review were done on XP. Studies on agility in general come next, with ﬁve (15%) of the studies. Scrum and Lean 5 See http://www.qsrinternational.com/ Table 4 Seven phases of meta-ethnography (Noblit and Hare [48]) 1. Getting started 2. Deciding what is relevant to the initial interest 3. Reading the studies 4. Determining how the studies are related 5. Translating the studies into one another 6. Synthesizing translations 7. Expressing the synthesis Table 3 Quality criteria 1. Is the paper based on research (or is it merely a ‘‘lessons learned ” report based on expert opinion)? 2. Is there a clear statement of the aims of the research? 3. Is there an adequate description of the context in which the research was carried out? 4. Was the research design appropriate to address the aims of the research? 5. Was the recruitment strategy appropriate to the aims of the research? 6. Was there a control group with which to compare treatments? 7. Was the data collected in a way that addressed the research issue? 8. Was the data analysis suﬃciently rigorous? 9. Has the relationship between researcher and participants been considered to an adequate degree? 10. Is there a clear statement of ﬁndings? 11. Is the study of value for research or practice? 840 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'Software Development were studied in only one empirical research article each. If we look at the level of experience of the employees who perform agile development in the reviewed studies (Appendix D ), we see that 24 (73%) of the studies that investigated agile projects dealt with employees who are beginners (less than a year of experience in agile develop- ment). Four (12%) studies dealt with mature agile develop- ment teams (at least one year of experience in agile development). Two studies did not indicate whether it was a beginner or mature team that was studied and for three studies (surveys) this classiﬁcation was not applicable. Most studies (24 of 33, 73%) dealt with professional software developers. The remaining nine (27%) were con- ducted in a university setting. Most projects were of short duration and were completed in small teams. Table 6 gives an overview of the studies according to publication channel. We see that the conferences XP and Agile Development have the largest number of studies. Most of the studies were published in conferences (26 of 33, 79%), while seven (21%) appeared in scientiﬁc journals. Regarding the year of publication, we found no empiri- cal studies of agile software development prior to 2001. However, from 2001 we found a steady increase of studies with one empirical research study published in 2001, one in 2002, three in 2003, 12 in 2004, and 16 published in 2005. 4.2. Research methods The number and percentage of publications using each research method is listed in Table 7 . Information on which studies belong to which category is given in Appendix D . Of the 13 single-case studies, nine were done in projects in industry. The material for the other four studies was taken from projects where students did the development. Interestingly, three of these studies took their data from the same project. Only one of the single-case studies in industry was done on a mature development team. For the 11 multiple-case studies, all were done in indus- try, but only three of the studies were on mature teams. The number of cases varied from two to three. Three of the four surveys were done on employees in software companies, while one was done on students. The three experiments were all done on students, with team sizes ranging from three to 16. For the two mixed-method studies, Melnik and Maurer (S22) reported on a survey amongst students in addition to interviews and notes from discussions. The study by Baskerville et al. (S3), reported on 10 case studies in companies, in combination with ﬁnd- ings from group discussions in a ‘‘discovery colloquium ” that was inspired by principles in action research [8]. 4.3. Methodological quality As mentioned in Section 3, we chose to assess each of the primary studies according to 11 quality criteria based on the Critical Appraisal Skills Programme (CASP) 6 and by principles of good practice for conducting empirical research in software engineering (e.g.,[37]). A summary of the questions used to assess the quality of these studies is presented inTable 3 . The detailed subcriteria are pre- sented in Appendix B . Both authors rated each criterion of each study independently. When discrepancies arose, Table 6 Distribution of studies after publication channel and occurrence Publication channel Type Number Percent XP 200X Conference 5 15 Agile Development Conference Conference 5 15 IEEE Software Journal 3 9 Profes Conference 2 6 ASEE FEC Conference 1 3 Computer Supported Cooperative Work Journal 1 3 CSMR Conference 1 3 Empirical Software Engineering Journal 1 3 ENCBS Conference 1 3 ENC Conference 1 3 EuroMicro Conference 1 3 EuroSPI Conference 1 3 HICCS Conference 1 3 HSSE Conference 1 3 IASTED ICSEA Conference 1 3 ICSE Conference 1 3 ISESE Conference 1 3 ITIICT Conference 1 3 Journal of database management Journal 1 3 Metrics Conference 1 3 Software Quality Journal Journal 1 3 XP/Agile Universe Conference 1 3 Total 33 100 Table 7 Studies by research method', 'Journal of database management Journal 1 3 Metrics Conference 1 3 Software Quality Journal Journal 1 3 XP/Agile Universe Conference 1 3 Total 33 100 Table 7 Studies by research method Research method Number Percent Single-case 13 39 Multiple-case 11 33 Survey 4 12 Experiment 3 9 Mixed 2 6 Total 33 100 6 www.phru.nhs.uk/casp/casp.htm. Table 5 Studies after type of agile method used in the study Agile method Number Percent XP 25 76 Generala 51 5 Scrum 1 3 Lean software development 1 3 Otherb 13 Total 33 100 a ‘‘General” refers to studies on agility in general. b ‘‘Other” refers to a company-internal agile method. T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 841', 'these were discussed and the study was reread to determine the ﬁnal scores of each criterion. Taken together, these 11 criteria provide a measure of the extent to which we can be conﬁdent that a particular study’s ﬁndings can make a valuable contribution to the review. The grading of each of the 11 criteria was done on a dichotomous (‘‘yes” or ‘‘no ”) scale. The results of the quality assessment are shown in Table 8 , in which a ‘‘1” indicates ‘‘yes” (or OK) to the question, while ‘‘0 ” indi- cates ‘‘no ” (or not OK). Because we only included research papers in this review, all included studies were rated as OK on the ﬁrst screening criterion. However, two of the included studies still did not have a clear statement of the aims of the research. All stud- ies had some form of description of the context in which the research was carried out. For three of the studies, the chosen research design did not seem appropriate to the aims of the research. As many as 25 out of the 33 primary studies did not have a recruitment strategy that seemed appropriate for the aims stated for the research. Ten of the studies included one or more groups with which to compare agile methods. As many as seven and eight stud- ies, respectively, did not adequately describe their data col- lection and data analysis procedures. In only one study was the recognition of any possibility of researcher bias mentioned. We frequently found the following: methods were not well described; issues of bias, validity, and reliability were not always addressed; and methods of data collection and analysis were often not explained well. None of the studies got a full score on the quality assessment and only two studies got one negative answer. Twenty-one studies were rated at two or three negative answers, while 10 stud- ies were rated as having four or more negative answers. The highest number of negative answers was seven. 4.4. Introduction and adoption of agile development methods Several studies addressed how agile development meth- ods are introduced and adopted in companies; see Table 9. We characterized these studies as falling into three broad groups: those that discuss introduction and adoption, those that discuss how the development process is changed, and Table 8 Quality assessment Study 1 2 3 4 5 6 7 8 9 10 11 Total Research Aim Context R. design Sampling Ctrl. Grp Data coll. Data anal Reﬂexivity Findings Value S1 1 1 1 0 0 0 1 1 0 1 1 7 S2 1 1 1 1 1 1 1 1 0 1 1 10 S3 1 1 1 1 0 0 0 0 0 1 1 6 S4 1 1 1 1 1 0 1 1 0 1 1 9 S5 1 1 1 1 0 1 1 1 0 1 1 9 S6 1 1 1 1 0 1 1 1 0 1 1 9 S7 1 1 1 1 0 1 1 1 0 1 1 9 S8 1 1 1 1 0 0 1 0 0 1 1 7 S9 1 1 1 1 1 0 1 1 0 1 1 9 S10 1 0 1 1 0 1 1 1 0 1 1 8 S11 1 1 1 1 0 0 0 0 0 1 1 6 S12 1 1 1 1 1 0 1 1 0 1 1 9 S13 1 1 1 1 0 0 0 1 0 1 1 7 S14 1 1 1 1 0 1 1 1 0 1 1 9 S15 1 1 1 1 0 1 1 1 0 1 1 9 S16 1 1 1 1 0 0 1 1 0 1 1 8 S17 1 1 1 1 0 0 1 1 0 1 1 8 S18 1 1 1 1 0 0 1 0 0 1 1 7 S19 1 1 1 1 0 0 1 1 0 1 1 8 S20 1 1 1 1 0 0 1 1 0 1 1 8 S21 1 1 1 1 0 0 1 1 0 1 1 8 S22 1 1 1 1 1 0 1 1 0 1 1 9 S23 1 0 1 0 0 0 0 0 0 1 1 4 S24 1 1 1 1 0 0 1 1 0 1 1 8 S25 1 1 1 1 0 0 0 0 0 1 1 6 S26 1 1 1 1 0 0 0 0 0 1 1 6 S27 1 1 1 1 0 0 1 1 0 1 1 8 S28 1 1 1 1 1 1 1 1 0 1 1 10 S29 1 1 1 1 1 0 1 1 0 1 1 9 S30 1 1 1 1 1 0 1 1 0 1 1 9 S31 1 1 1 0 0 0 0 0 1 1 1 6 S32 1 1 1 1 0 1 1 1 0 1 1 9 S33 1 1 1 1 0 1 1 1 0 1 1 9 Total 33 31 33 30 8 10 26 25 1 33 33 842 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'those that discuss how knowledge and projects are managed. However, some researchers argue that there is nothing new about agile methods. Hilkka et al. (S9) studied two development organizations in Finland, and concluded that XP is ‘‘old wine in new bottles ”. XP ‘‘formalizes several habits that appear naturally ( ... ) close customer involve- ment, short release cycles, cyclical development, and fast response to change requests”. In a company-internal devel- opment department, the researchers found that: the tools and techniques of XP had been employed for more than 10 years and had been applied in a quite sys- tematic fashion, though the company had never made a deliberate decision to use XP. (p. 52) Another ‘‘new economy ” company was more aware of developments in the ﬁeld: the XP process had more or less emerged as a novel way of solving time and budget constraints. The developers were aware of XP practices, but did not choose to engage in it by the book. (p. 52) However, most studies treated agile development as something ‘‘new ” and that consequently requires introduc- tion and adoption. 4.4.1. Introduction and adoption Svensson and Ho¨st (S30) present the results of introduc- ing a process based on XP to a large software development company. The process was introduced to a pilot team that worked for eight months. Svensson and Ho¨st concluded that the introduction of the process proved diﬃcult, due to the complexity of the organization. They advise compa- nies that want to introduce agile development methods to assess existing processes, with the following goals in mind: determining what to introduce; clarifying terminology to simplify communication with the rest of the company; avoiding underestimating the eﬀort needed to introduce and adapt XP; and introducing the practice of continuous testing early, because it takes time and eﬀort to introduce this properly. In contrast, Bahli and Zeid (S2) studied how a Canadian organization shifted from a waterfall process to XP, and found that ‘‘even though team members had no prior expe- rience with XP (except one week of training), they found the model easy to use and useful to develop information systems”. A development manager described the shift as follows: The ﬁrst week was tough, no one of my guys have a strong experience with XP. But they quickly caught up and we got quite good results. A lot of work is needed to master XP but we are getting there. (p. 8) The study reports that the development team found using the waterfall model to be an ‘‘unpleasant experi- ence”, while XP was found to be ‘‘beneﬁcial and a good move from management ”. The XP project was delivered a bit less late (50% time-overrun, versus 60% for the tradi- tional), and at a signiﬁcantly reduced cost overrun (25%, compared to 50% cost overrun for the traditional project). Bahli and Zeid claim that the adoption of XP was facil- itated by ‘‘a high degree of knowledge creation enabled by mutual development of information systems developers and users ”. Karlstro¨m and Runeson (S29) found that XP teams experienced improved communication, but were perceived by other teams as more isolated. Their study is described in detail below. 4.4.2. Development process Tessem (S31) set up a project with researchers and stu- dents to learn more about how practices in XP work. The project lasted for three weeks and had two deliveries (three planned, but reduced because of ‘‘severe underesti- mation in the beginning”). Six people worked on the pro- ject. The experience of the people varied from programming experience only from university courses to people experienced with professional development. The aim of the project was to develop a web application for group work in university courses. Tessem reports on expe- rience with key practices of XP. Pair programming was found to be a ‘‘positive experience, enhancing learning and also leading to higher quality”. However, three of', 'rience with key practices of XP. Pair programming was found to be a ‘‘positive experience, enhancing learning and also leading to higher quality”. However, three of the programmers also reported it to be ‘‘extremely ineﬃ- cient”, ‘‘very exhausting ”, and ‘‘a waste of time ”. Towards the end of the project, single programming was used to a greater extent than pair programming. Tessem suggests that there is a connection between this shift in program- ming methods and a higher occurrence of problems in Table 9 Study aims for studies on the introduction and adoption of agile development methods Study Study aim S2 Understand what diﬀerentiates waterfall and XP development, and examine the impact of knowledge creation on the adoption of XP S9 Study why and how XP is adopted and used in everyday software production S12 Study the integration of agile teams into stage-gate software product development S23 Test the applicability of lean techniques to developing software S29 Study how an agile process aﬀects collaboration in a software development organization S30 Study the introducing of a process based on XP in an evolutionary and maintenance-based software development environment S31 Understand how newcomers practice extreme programming, and how practice is improved over time T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 843', 'the end. Frequent partner changes are suggested as a way to achieve optimal learning and to increase collective code ownership. Further, the on-site customer role was per- ceived as ‘‘very valuable by all programmers”. Tessem also found that test-ﬁrst programming contributed to ‘‘higher quality in code”, while the project struggled to get func- tional tests running. A study by Svensson and Ho ¨st (S29) also provides insight into the development process, but focused primarily on how agile development aﬀects customer collaboration. This study was done in a software development company in Sweden with 250 software developers, who were respon- sible for over 30 software systems. A modiﬁed process was introduced that mainly followed XP. The researchers found that having the customer on-site enabled better collabora- tion with the customer, because it provided an arena for detailed discussions. Concepts from lean development were introduced in a large company’s information systems department in a study organized by Middleton (S23). The techniques were tried on two two-person teams that were maintaining a ﬁnancial and management information system. The teams were instructed to change their work practice, so that it involved shorter feedback cycles and completing work before taking on more. In the beginning, many errors were discovered in the work, which led to a time of ‘‘frustration and low productivity”. One of the teams made fewer errors over time, but the other team continued to make a high number of errors, which also led to an early termination of the study. According to Mid- dleton, this was because one person in the fault-prone team felt overqualiﬁed for the work and was not willing to discuss his work with others, and was ‘‘unable to produce work with- out errors”. There was no infrastructure in the company to handle this problem. Although the experiment was short and only successful for one team, Middleton claimed that ‘‘by moving responsibility for measuring quality from the manger to the workers, a much quicker and more thorough response to defects was obtained”. Hilkka et al. (S9) found that in the cases they studied, XP worked best with experienced developers with domain and tool knowledge. The tools facilitated fast delivery and easy modiﬁcation of prototypes. In addition, continu- ous feedback was found to be a key factor for success. 4.4.3. Knowledge and project management The study by Bahli and Zeid (S2) examined knowledge sharing in an XP project and a traditional project. They found that when the XP model was used, the creation of tacit knowledge improved as a result of frequent contacts: Because the XP model’s main characteristics are short iterations with small releases and rapid feedback, close user participation, constant communication and coordi- nation and collective ownership, knowledge and the capability to create and utilize knowledge among the development team members are eminent. (p. 4) Hilkka et al. also underline the importance of skilled team members with solid domain knowledge: ‘‘without these kinds of persons, the chosen approach would proba- bly have little possibility to succeed” (S9). Karlstro¨m and Runeson (S12) studied the feasibility of applying agile methods in large software development pro- jects, using stage-gate project management models. They report ﬁndings from introductory trials with XP in three companies: ABB, Ericsson Microwave Systems, and Voda- phone Group. They found that the engineers were moti- vated by the principles of agile development, but that the managers were initially afraid and needed to be trained. They also found that as a result of using agile development, the engineers focused on past and current releases, while the managers focused increasingly on current and future releases. A potential problem was that technical issues were raised too early for management. In the study by Tessem (S31), the planning game prac-', 'releases. A potential problem was that technical issues were raised too early for management. In the study by Tessem (S31), the planning game prac- tice of XP was used to estimate the size of work. Estimates made by the project team at the beginning of the project were about one third of what turned out to be correct, which is explained by both the team’s lack of estimation experience and coarse user stories. Estimates improved towards the end of the project. Several of the study partic- ipants mentioned that during the project, there were not enough discussions on design and architecture. In a study by Svensson and Ho ¨st (S30), the planning game activity was found to have a positive eﬀect on collab- oration within the company, because it provided the orga- nization with better insight into the software development process. 4.5. Human and social factors Several studies examined various human and social fac- tors related to agile development; see Table 10. Three broad topics were investigated: the impact of organizational cul- ture, how collaborative work takes place in agile develop- ment, and what characterizes agile development teams. 4.5.1. Organizational culture Robinson and Sharp (S26) found that XP has the ability to thrive in radically diﬀerent organizational settings in an ethnographically informed study of three companies in the UK. The companies were studied with respect to three fac- tors: organizational type, organizational structure, and physical and temporal settings. These factors are described in Table 11 . Case A was a large multinational bank with an XP team that was ‘‘a small part of the bank’s software development activities”. Case B was a medium-sized company that pro- duces content security software, using only XP. Case C was a small start-up company that had used XP since the begin- ning to develop web-based intelligent advertisements. Robinson and Sharp (S26) found that, despite the vari- ations in organization type, structure, and physical setting, XP was working well. They list a number of consequences 844 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'for development that were generated by the organizational culture. Case C is further described in another publication (S27). Here, the development team is described as: a self-managing, self-organizing community with a cul- ture that emphasized shared responsibility. There was a rhythm to life that enabled people to organize their work tasks in a way that gave them common ownership of the work product and control over how it was achieved. The rhythm was comfortable and relaxed, yet purposeful and productive. (p. 373) In case C, the authors claim that the organization seemed to behave in an agile fashion. They found no signs of such normal software development artefacts as model- ling techniques, requirements documents, or minutes of meetings. The working mode in this company resembles descriptions of communities of practice in the literature on knowledge management[65]. The organizational culture aﬀected how XP was carried out, with respect to behaviour, beliefs, attitudes and values, organizational structure, as well as physical and temporal settings. 4.5.2. Collaborative work Collaborative work in XP development has been studied from three angles: the role of conversation in collaborative work, how progress is tracked, and how work is standardized. With respect to conversation, Robinson and Sharp (S25) describe pairing as a process of: purposeful talk where they [two developers] discuss, explore, negotiate and progress the task at hand. This talk has a complex structure with identiﬁable episodes of exploration, creation, ﬁxing & reﬁning, overlaid with explaining, justifying & scrutinising. (p. 102) Paring is described as intense and stressful, and one pair’s conversation would frequently spread to other pairs. Mackenzie and Monk (S16) also emphasize the importance of conversations, claiming that it constitutes ‘‘talking code into existence”. With respect to tracking progress, Robinson and Sharp described it as happening on two levels: the daily rhythm and the rhythm oriented around the iteration. Progress was communicated in daily stand-up meetings, and teams would often have ceremonies around releasing code. One team studied by Robinson and Sharp used a toy cow that was tilted to make a ‘moo’ sound when new code was released. Chong (S5) reports similar ﬁndings, stating that ‘‘XP makes developing software visually and aurally available”. Studies of collaborative work also ﬁnd that the work patterns are standardized. Chong (S5) observed that: shared understandings manifested themselves in the con- sistent, uniform patterns of work in which the team members engaged... the XP framework required that they work at the same time, in the same place, and in lar- gely the same ways. (p. 8) One practice that standardizes work in XP is the plan- ning game, which is described in use by Mackenzie and Monk (S16): the card game knit together in a rule-governed process a very disparate set of work processes and relations involving management, the customer or client and all the members of the software development team. (p. 114) Table 10 Study aims for studies on human and social factors Study Study aim S5 Study how the social behaviour of individuals reﬂects beliefs and understanding with regard to software development S16 Evaluate key aspects of XP in a real-world setting as well as contemporary practices of software engineering S24 Study the characteristics of XP teams S25 Explore the social side of practices in mature XP teams S26 Explore the impact of organizational culture on XP practice in mature teams S27 Gain insight into the culture and community of XP S33 Explore whether it is possible to identify roles within an extreme programming team that has associated personality characteristics Table 11 Diﬀerences in organization type, structure, and physical setting for the three cases studied in S26 Organization type Organizational structure Physical setting Case A Hierarchical XP team collaborated with two customer representatives', 'Organization type Organizational structure Physical setting Case A Hierarchical XP team collaborated with two customer representatives Large, open-plan ﬂoor with workstations Case B Collaborative Internal ‘‘on-site ” customers, outside testers Open-plan space with groups of workstations for pair programming, meeting rooms and cubicles Case C Little or no central control Minimal organizational structure Open-plan oﬃce with pair programming area T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 845', 'Mackenzie and Monk claimed that the process spans the usual boundaries between project managers and software developers. 4.5.3. Team characteristics Robinson and Sharp (S24) claim that agile development teams have faith in their own abilities, show respect and responsibility, establish trust, and preserve the quality of working life. Young et al. (S33) used a technique called ‘‘repertory grid analysis” to identify good personality char- acteristics for members of XP development teams. Faith in one’s own abilities was observed to have two aspects in the study by Robinson and Sharp (S24): believ- ing that the team was capable of achieving the tasks at hand, and understanding what the limitations were. The team received feedback on their beliefs from successfully executing code, from a satisﬁed customer, and from sup- port and encouragement from each other. Preserving the quality of working life was observed through constructive discussions in the planning game, tak- ing into account the needs of individuals in pair program- ming, and adhering to 40-h work-weeks. In addition, one team took regular breaks and identiﬁed several ways to relieve developers in hectic periods. Respect for one’s team members and a sense of respon- sibility were manifested via the way in which work was assigned; active agreement was required. ‘‘Individuals clearly felt that they had the respect of their fellow team members and were therefore empowered to take on respon- sibility in this way”. In the study by Robinson and Sharp (S24), trust was found to be pervasive: The nature of the trust relationship here transcends the immediate business of two individuals pairing and is persistent. It also applies across pairs (and sub-teams), with each pair trusting the others to do their part, and it extends beyond the 12 practices. (p. 146) Young et al. (S33) investigated what personality traits it is beneﬁcial for team members to possess in agile develop- ment. They discussed the traits of roles such as team leader, technical lead, architect, good (XP) team member, and bad team member. Good XP team members are described as ‘‘analytical, with good interpersonal skills and a passion for extending his knowledge base (and passing this on to others).” 4.6. Perceptions on agile methods Several studies have investigated how agile methods are perceived by diﬀerent groups. We describe ﬁndings from studies that examined the perceptions amongst customers, developers, and university students.Table 12 gives an over- view of the aims of these studies. 4.6.1. Customer perceptions Several aspects of customer perceptions are discussed in the literature on agile development. Some have addressed how satisﬁed customers are with agile methods, others describe the customer role, and some focus on the collabo- ration between a customer and the development team. With respect to the customer’s satisfaction with agile development methods, Ilieva et al. (S10) studied the intro- duction of an agile method based on XP and the Personal Software Process[32]. They state that the customer had constant control over the development process, which was ‘‘highly praised by the customer at the project sign- oﬀ”. In addition, Mann and Maurer (S17) found, in a study on the impact of Scrum on overtime and customer satisfac- tion, that customers believed that the daily meetings kept them up to date and that planning meetings were helpful to ‘‘reduce the confusion about what should be developed”. The attitude of the customers was found to change from ‘‘one of ambivalence to becoming involved”. The custom- ers stated that their satisfaction with the project that was based on XP was greater than with previous projects at the company. However, Mann and Maurer stress that the customer should be trained in the Scrum process so that they will understand the new expectations that the developers will have of them. The role of the customer is also the focus in the study by', 'should be trained in the Scrum process so that they will understand the new expectations that the developers will have of them. The role of the customer is also the focus in the study by Martin et al. (S19), on three XP projects with on-site cus- tomers. In all cases, they found that the customer was under stress and committed working long hours, although Table 12 Study aims for the perceptions of customers, developers, and students Study Study aim S2 Understand what diﬀerentiates waterfall and XP development, and examine the impact of knowledge creation on the adoption of XP S10 Compare empirical ﬁndings of an agile method based on XP with a baseline, with respect to productivity, defect rates, cost, and schedule deviations S13 Provide empirical data on customer eﬀort usage, customer perceptions, and project team perceptions of on-site customer in an XP project S17 Study the impact of Scrum on overtime, as well as on customer and developer experience S18 Compare the job satisfaction of developers that use XP practices with developers that do not use XP practices S19 Provide empirical data on the role of on-site customer role in XP S20 Study the role of the customer in projects using XP and outsourcing S21 Study student perceptions of agile practices S22 Study how students perceive agile development methods 846 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'all the customers were supported by an acceptance team, various technical advisors, or senior personnel: The existing XP Customer practices appears to be achieving excellent results, but they also appear to be unsustainable, and so constitute a great risk to XP pro- jects, especially in long or high pressure projects (p. 12) Martin et al. (S20) also studied the role of the customer in outsourced projects, and found that this was challenging because the customer was required to become acclimatized to the diﬀerent cultures or organizations of the developers. Koskela and Abrahamsson (S13) analyzed the role of the customer in an XP project and found that most of the time was spent on participating in planning game ses- sions and acceptance testing, followed by retrospective ses- sions at the end of release cycles. 4.6.2. Developer perceptions Mannaro et al. (S18) surveyed the job satisfaction amongst employees in software companies that used XP and companies that did not use agile development meth- ods. One hundred and twenty-two people completed a web-based questionnaire. The bulk of these were from Eur- ope and the United States. Ninety-ﬁve percent of the employees who used XP answered that they would like their company to continue using their current development process, while the number for the employees in companies that did not use agile devel- opment methods was 40%. In addition, the employees in the companies that used XP were signiﬁcantly more willing to use the development process in the future than the employees in companies that did not use XP. Further, Mannaro et al. (S18) claimed that employees who use XP have greater job satisfaction, feel that the job environment is more comfortable, and believe that their productivity is higher. In particular, 73% of the employees who used pair programming claim that this practice speeds up the soft- ware development process. In the study by Ilieva et al. (S10), developers found pair programming to be ‘‘a very useful style of working as everyone was strictly conforming to the coding standards”. However, the authors also note that working 40 h a week in pairs requires a lot of concentration, and that as a result, the developers became exhausted. Mann and Maurer (S17) found that the introduction of Scrum led to a reduction of overtime, and all developers recommended the use of Scrum in future projects. The developers were more satisﬁed with the product, and saw that the Scrum process fostered more customer involve- ment and communication. One developer said that ‘‘the Scrum process is giving me conﬁdence that we are develop- ing the software that the customer wants”. A study by Bahli and Zeid (S2) used the Technology Acceptance Model [54] to study the adoption of XP in a company that develops medical information systems. They found that employees saw XP as easy to use and useful, and that employees intended to use this development pro- cess in the future. 4.6.3. Student perceptions Melnik and Maurer (S21, S22) report on student percep- tions of agile development methods in two studies, one from 2002 and one from 2005. They found that 240 stu- dents who responded to a survey at the Southern Alberta Institute of Technology and at the University of Calgary in Canada were ‘‘very enthusiastic about core agile prac- tices”. The ﬁndings were consistent across educational programmes. The students found that working in agile teams helped them to develop professional skills such as communication, commitment, cooperation, and adaptability. Seventy-eight percent of the respondents stated that they believe that XP improves the productivity of small teams. This ﬁgure is comparable to the ﬁndings of Mannaro et al. (S18) on pair programming and productivity for employees in soft- ware companies. Further, 76% of the respondents believed that using XP improved the quality of code, and 65% would recommend using XP to any company for which they may work in the', 'ware companies. Further, 76% of the respondents believed that using XP improved the quality of code, and 65% would recommend using XP to any company for which they may work in the future. Of those that recommended using XP to their future employers, a large number preferred to work in pairs. In the 2002 study, Melnik and Maurer (S21) present qualitative ﬁndings on perceptions of XP in general, pair programming, test-ﬁrst design, and the planning game. Most students found pair programming to be helpful, but some expressed concern when the the members of the pair had diﬀerent levels of competence. One student stated that ‘‘There was a huge diﬀerence in skill level in my pair, so we weren’t very productive when I wasn’t driving ”. In addi- tion, test-ﬁrst design was diﬃcult for many students. The authors believe that this is because design in itself is very diﬃcult, and writing the tests ﬁrst forces students to make design decisions early. 4.7. Comparative studies One third (11) of the reviewed primary studies provided some form of comparison of agile development against an alternative; seeTable 13. Using our interpretations as a basis, these comparisons can be grouped into four higher-order comparative topics: project management, productivity, product quality, and team characteristics. Non-comparative studies that mention one or more of these issues are also included in this section. 4.7.1. Project management The management of software projects has long been a matter of interest. Agile methods have reinforced this inter- est, because many conventional ideas about management are challenged by such methods. Ceschi et al. (S4) found, in their survey of plan-based and agile companies, that agile methods improved the management of the develop- ment process as well as relationships with the customer. T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 847', 'In particular, they found that companies that use agile methods prefer to organize their processes in more releases and that the managers of such companies are more satisﬁed with the way they plan their projects than are plan-based companies. Moreover, Baskerville et al. (S3) found that ‘‘Internet-speed development” project management diﬀers from that of traditional development in that ‘‘Projects do not begin or end, but are an ongoing operation more akin to operations management.” (Baskerville et al. (S3), p. 77). Karlstro¨m and Runeson (S12) studied how traditional stage-gate project management could be combined with agile methods. In a case study of three large companies, they found that agile methods give the stage-gate model powerful tools for microplanning, day-to-day work con- trol, and reporting on progress. They also found that they were able to communicate much more eﬀectively when using the working software and face-to-face meetings of agile methods than when using written documents. In turn, the stage-gate model provided the agile methods with a means to coordinate with other development teams and to communicate with marketing and senior management. Their conclusion was that it is feasible to integrate agile methods with stage-gate project management to improve cost control, product functionality, and on-time delivery. A central concern for agile methods is to attend to the real needs of the customer, which are often not stated explicitly in a more or less complete requirements speciﬁcation. Thus, Dagnino et al. (S6) compared and contrasted the use of an evolutionary agile approach with a more traditional incre- mental approach in two diﬀerent technology development projects. They showed that by planning in detail only the fea- tures and requirements to be implemented in a speciﬁc cycle, the agile team was more able to incorporate changes in requirements at a later stage with less impact on the project. In addition, by delivering in-progress software to the cus- tomer more frequently, the agile team was able to demon- strate business value more quickly and more often than the traditional, iterative team. Combined with continuous feed- back by the customer, this lead to a sharp increase in cus- tomer satisfaction on the agile project. Similarly, Ceschi et al. (S4) found that the tighter links between the customer and the development team resulted in agile companies being more satisﬁed with their customer relationships than plan-based companies. Furthermore, Sillitti et al.’s (S28) survey of project managers found that companies that use agile methods are more customer-cen- tric and ﬂexible than document-driven ones, and that com- panies that use agile methods seem to have a more satisfactory relationship with the customer. However, with respect to human resource management, Baskerville et al. (S3) concluded that compared to tradi- tional development, team members of agile teams are less interchangeable, and more diﬃcult to describe and identify. 4.7.2. Productivity Four studies compared the productivity of agile teams with the productivity of teams using traditional develop- ment methods (S7, S10, S14, S32); seeTable 14 . Ilieva et al. (S10) compared the productivity of two similar pro- jects, one of which used traditional methods and the other of which used XP. They measured the productivity for three iterations of each project. Overall, the results showed a 42% increase in productivity for the agile team. The increase in productivity was largest for the ﬁrst iteration, while there was virtually no diﬀerence in productivity for the last iteration. The case study by Layman et al. (S14) compared an old release developed with traditional methods with a new Table 13 Study aims for comparative studies Study Study aim S3 Understand how and why Internet-speed software development diﬀers from traditional software development S4 Compare agile methods with plan-based methods with respect to project management practices', 'S4 Compare agile methods with plan-based methods with respect to project management practices S6 Compare the eﬀectiveness of incremental and evolutionary process models for technology development projects S7 Compare diﬀerences in resource utilization and eﬃciency in products developed using sequential, incremental, evolutionary, and extreme programming S10 Compare empirical ﬁndings of an agile method based on XP with a baseline, with respect to productivity, defect rates, cost, and schedule deviations S12 Study the integration of agile teams into stage-gate software product development S14 Compare the eﬀectiveness of XP with traditional development, with respect to pre- and post-release quality, programmer productivity, customer satisfaction, and team moral S15 Compare XP with a traditional approach with respect to quality S18 Compare the job satisfaction of developers that use XP practices with developers that do not use XP practices S28 Compare agile and document-driven approaches in managing uncertainty in software development S32 Compare plan-driven and agile development with respect to team cohesion and product quality Table 14 Comparisons of productivity Study Productivity TRAD ProductivityAGILE Productivity gain (%) S7 3 LOC/h a 13.1 LOC/h 337 S10 3.8 LOC/h 5.4 LOC/h 42 S14 300 LOC/month 440 LOC/month 46 S32 157 LOC/ engineerb 88 LOC/engineer /C044 a V-model. b Comparisons were made between two one-semester courses; however, the actual hours worked by the members of the teams were not measured. 848 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'release developed with agile methods. The results showed a 46% increase in productivity for the new agile release com- pared with the old, traditional release. However, the agile team had notably greater domain and programming lan- guage expertise and project manager experience, because three of the team members on the new release had previ- ously worked on the old release. Dalcher et al. (S7) performed an experiment in which ﬁf- teen software teams developed comparable software prod- ucts using four diﬀerent development approaches (V- model, incremental, evolutionary, and XP). The greatest diﬀerence in productivity was between the V-model teams and the XP teams, with the XP teams being, on average, 337% more productive than the V-model teams. However, this productivity gain was due to the XP team delivering 3.5 times more lines of code without delivering more functionality. Contrary to the studies by Dalcher et al. (S7), Ilieva et al. (S10), and Layman et al. (S14), Wellington et al. (S32) found a 44% decrease in productivity for an XP team compared with a traditional team. Furthermore, Svensson and Ho¨st (S30) found no change in overall productivity when comparing results from before and after the introduc- tion of an agile process. However, they did ﬁnd evidence that when the agile process was introduced, the team improved their productivity during the ﬁrst iterations. In addition, Mannaro et al. (S18) asked their subjects whether the team’s productivity had increased signiﬁcantly as a result of the development process that was used. On a scale from 1 (Strongly Disagree) to 6 (Strongly Agree), the mean for the non-XP developers was 3.78, while the mean for the XP developers was one scale point higher (4.75). Similarly, 78% of Melnik and Maurer’s (S22) respondents either believed or believed strongly that using XP improves the productivity of small teams. 4.7.3. Product quality Several aspects of product quality were examined by the studies in this review. For example, comparing the results for a new release of a project to those for an old release, Layman et al. (S14) found a 65% improvement in pre- release quality and a 35% improvement in post-release quality. Ilieva et al. (S10) found 13% fewer defects reported by the customer or by the quality assurance team in an XP project than in a non-XP project. In Wellington et al.’s (S32) study, the XP team’s code scored consistently better on the quality metrics used than the traditional team. In addition, the quality of the code delivered by the XP team was signiﬁcantly greater than that delivered by the traditional team. However, both teams agreed that the traditional team had developed a better and much more consistent user interface. Macias et al. (S15) measured the internal and external quality of the products developed by 10 XP teams and 10 traditional teams. However, in contrast to Layman et al. and Wellington et al., they found no diﬀerence in either internal or external quality between the XP teams and the traditional teams. With respect to product size, the XP model teams in Dalcher et al.’s (S7) study delivered 3.5 times more lines of code than the V-model teams. This is in sharp contrast to Wellington et al.’s (S32) results, which showed that the traditional team delivered 78% more lines of code than the XP team. However, in contrast to both Dalcher et al. and Wellington et al., Macias et al. (S15) found no diﬀer- ence in product size between the XP teams and the tradi- tional teams. 4.7.4. Work practices and job satisfaction A few studies made qualitative comparisons of social behaviour. Chong (S5), for example, performed an ethno- graphic study to compare the work routines and work practices of the software developers on an XP team and a non-XP team. Chong’s observations suggest that certain features of XP promote greater uniformity in work routine and work practice across individual team members and that, consequently, XP provides a framework for standard-', 'features of XP promote greater uniformity in work routine and work practice across individual team members and that, consequently, XP provides a framework for standard- izing the work of software development and making it more visible and accessible to the members of a software development team. An important part of the XP philosophy is to increase overall team cohesion by making everyone in the team responsible for the source code. However, Wellington et al.’s (S32) study of team cohesion and individuals’ attachment to the project in XP and non-XP teams yielded equal or higher scores for every aspect of cohesion for the non-XP teams. However, at the same time, the study indi- cated a lack of cohesion across subteams for the non-XP team (the XP team was not divided into subteams). The point of departure for Mannaro et al. (S18) was the importance of job satisfaction for the eﬀectiveness of the software development process. Consequently, they per- formed a survey to compare the job satisfaction of develop- ers that used XP practices with that of developers that did not use them. The results of their study showed that the developers viewed XP practices favourably and indicated that developers who use XP practices are more comfortable with their job environment and more satisﬁed with their jobs than developers that do not use XP practices. 5. Discussion The present review identiﬁed a greater number of studies than did previous reviews. Abrahamsson et al. (S34) wrote in their 2002 review that the existing evidence consists mainly of practitioners’ success stories. Cohen et al. (S35) found seven case studies on agile development in their 2004 report, we included none of these in our ﬁnal set of studies, because they were either lessons learned studies or single-practice studies. Further, Erickson et al.’s (S36) 2004 review found four ‘‘case studies and lessons learned reports”, none of which we included in our review. This systematic review shows that there are many more empiri- T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 849', 'cal studies on agile development methods in general than have previously been acknowledged. In contrast to the pre- vious reviews, this review used an explicit search strategy combined with explicit inclusion and exclusion criteria. We now address our research questions, starting by dis- cussing what we found regarding the beneﬁts and limitations of agile software development. The second subsection dis- cusses the strength of evidence of these ﬁndings, while the third subsection discusses the implications of the ﬁndings for research and practice. Finally, we discuss the limitations of this systematic review. 5.1. Beneﬁts and limitations of agile development The studies that address the introduction and adoption of agile methods do not provide a uniﬁed view of current practice, but oﬀer a broad picture of experience and some contradictory ﬁndings. XP was found to be diﬃcult to introduce in a complex organization, yet seemingly easy in other types of organizations. This is consistent with ear- lier ﬁndings that suggest that agile development methods are more suitable for small teams than for larger projects [17]. It is likely that the ease with which XP can be intro- duced will depend on how interwoven software develop- ment is with the other functions of the organization. Most studies reported that agile development practices are easy to adopt and work well. Beneﬁts were reported in the following areas: customer collaboration, work pro- cesses for handling defects, learning in pair programming, thinking ahead for management, focusing on current work for engineers, and estimation. With respect to limitations, the lean development technique did not work well for one of the teams trying it out, pair programming was seen as ineﬃcient, and some claimed that XP works best with experienced development teams. A further limitation that was reported by one of the studies, which has also been repeatedly mentioned in the literature[42,61], was the lack of attention to design and architectural issues. A recurring theme in studies on agile development is what we have called human and social factors and how these factors aﬀect, and are aﬀected by, agile development methods. A beneﬁt of XP was that it thrived in radically diﬀerent environments; in organizations that varied from having a hierarchical structure to little or no central con- trol. In addition, customer involvement and physical set- tings varied greatly for the successful XP teams studied. It seems to be possible to adopt XP in various organiza- tional settings. Further, conversation, standardization, and the tracking of progress have been studied and are described as mechanisms for creating awareness within teams and organizations. In addition, studies of XP indi- cate that successful teams manage to balance a high level of individual autonomy with a high level of team autonomy and corporate responsibility. They have faith in their own abilities and preserve the quality of their working lives. Good interpersonal skills and trust were found to be important characteristic for a successful XP team. Many studies have sought to identify how agile methods are perceived by diﬀerent groups. Studies on customer per- ceptions report thatcustomers are satisﬁed with the oppor- tunities for feedback and responding to changes. However, we also found that the role of on-site customer can be stressful and cannot be sustained for a long period.Devel- opers are mostly satisﬁed with agile methods. Companies that use XP have reported that their employees are more satisﬁed with their job and that they are more satisﬁed with the product. There were mixed ﬁndings regarding the eﬀec- tiveness of pair programming and several developers regard it as an exhausting practice, because it requires heavy concentration.University students perceive agile methods as providing them with relevant training for their future work and believe that these methods improve the', 'heavy concentration.University students perceive agile methods as providing them with relevant training for their future work and believe that these methods improve the productivity in teams. However, they reported that pair programming was diﬃcult when there was a large skill dif- ferential between the members of the pairs. In addition, test-ﬁrst development was reported to be diﬃcult for many students. The group of comparative studies, in which variations of traditional development are compared to variations of agile development, is very interesting. It has been found that traditional and agile development methods are accom- panied by diﬀering practices of project management. Some studies suggest beneﬁts in projects that use agile methods because changes are incorporated more easily and business value is demonstrated more eﬃciently. In addition, we found that it is also possible to combine agile project man- agement with overall traditional principles, such as the stage-gate project management model. A limitation that was mentioned is that team members are less interchange- able in agile teams, which has consequences for how pro- jects are managed. With respect to the productivity of agile and traditional teams, three of the four comparative studies that address this issue found that using XP results in increased productivity in terms of LOC/h. However, none of these studies had an appropriate recruitment strat- egy to ensure an unbiased comparison. There are also ﬁnd- ings from several of the non-comparative studies that indicate that the subjects themselves believe that the pro- ductivity increases with the use of agile methods. With respect to product quality, most studies report increased code quality when agile methods are used, but, again, none of these studies had an appropriate recruitment strategy to ensure an unbiased comparison. The size of the end product seems not to be correlated with the method of development used. Diﬀerent studies have reported larger, smaller, and equal sizes of end product for traditional versus agile methods. The eﬀect on work practices and job satisfaction of using agile and traditional methods has not been established conclu- sively. Some studies have found that work practice is more standardized when agile methods are used and that job satisfaction is greater. However, a study of team cohesion did not ﬁnd any improvement of cohesion in an XP team. 850 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', '5.2. Strength of evidence Several systems exist for making judgments about the strength of evidence in systematic reviews (see [7] for an overview). Most of these systems suggest that the strength of evidence can be based on a hierarchy with evidence from systematic reviews and randomized experiments at the top of the hierarchy and evidence from observational studies and expert opinion at the bottom of the hierarchy [36]. The inherent weakness with evidence hierarchies is that randomized experiments are not always feasible and that, in some instances, observational studies may provide better evidence. To cope with the weaknesses of evidence hierarchies, we used the GRADE (Gradingof Recommendations Assess- ment, Development and Evaluation) workinggroup deﬁni- tions to grade the overall strength of the evidences high, moderate, low, or very low[7] (see Table 15 ). According to GRADE, the strength of evidence can be determined on the basis of the combination of four key elements, i.e., in addition to study design, study quality, consistency, and directness are also evaluated. The GRADE system ini- tially categorizes evidence concerning study design by assigning randomized experiments a high grade and obser- vational studies a low grade. However, by considering the quality, consistency, and directness of the studies in the evi- dence base, the initial overall grade could be increased or decreased, i.e., evidence from inconsistent, low-quality experiments may be assigned a low grade, while strong or very strong evidence of association from two or more high-quality observational studies may be assigned a high grade [7]. Regarding study design, there were only three experi- ments in the review (two randomized trials), while the remaining primary studies were observational. That there are few experiments is natural because we included only studies that addressed agile methods as a whole and excluded ones that investigated speciﬁc practices in isola- tion. This is consistent with Shadish et al.’s comments that experiments are best used to investigate speciﬁc cause-eﬀect phenomena [57]. Consequently, our initial categorization of the total evidence in this review based on study design islow. We now consider the quality, consistency, and directness of the studies in the evidence base. With respect to the quality of the studies, methods were not, in general, described well; issues of bias, validity, and reliability were not always addressed; and methods of data collection and analysis were often not explained well (see Section 4.3). As many as 25 out of the 33 primary studies did not have a recruitment strategy that seemed appropri- ate for the aims stated for the research and 23 of the studies didnot use other groups or baselines with which to com- pare their ﬁndings. Furthermore, in only one study was the possibility of researcher bias mentioned (seeTable 9 ). Using these ﬁndings as a basis, we conclude that there are serious limitations to the quality of the studies that inevitably increasesthe risk of bias or confounding. Hence, we must be circumspect about the studies’ reliability. With respect to consistency, i.e., the similarity of esti- mates of eﬀect across studies, we found diﬀerences in both the direction of eﬀects and the size of the diﬀerences in eﬀects, i.e., we found no consistent evidence of association from two or more studies with no plausible confounders nor did we ﬁnd direct evidence from studies with no major threats to validity. These inconsistencies might by due to imprecise or sparse data, and reporting bias. With respect to directness, i.e., the extent to which the people, interventions, and outcome measures are similar to those of interest, we found that most studies were con- cerned with XP. This leaves an uncertainty about the directness of evidence for other agile methods. However, given that most of the studies regarding XP were per- formed with student subjects or professionals who had lit-', 'directness of evidence for other agile methods. However, given that most of the studies regarding XP were per- formed with student subjects or professionals who had lit- tle or no experience in agile development, this also raises an issue regarding the directness of evidence for XP. In addi- tion, very few studies provided direct comparisons of inter- ventions; hence, we had to make comparisons across studies. However, such indirect comparisons leave greater uncertainty than direct comparisons because of all the other diﬀerences between studies that can aﬀect the results. Our judgment is thus that there are major uncertainties about the directness of the included studies. Combining the four components of study design, study quality, consistency, and directness, we ﬁnd that the strength of the evidence in the current review regarding the beneﬁts and limitations of agile methods, and for deci- sions related to their adoption, isvery low . Hence, any esti- mate of eﬀect that is based on evidence of agile software development from current research is very uncertain. This is consistent with criticisms that have been raised regarding the sparse scientiﬁc support for many of the claims made by the agile community[42]. 5.3. Implications for research and practice This systematic review has a number of implications for research and practice. For research, the review shows a clear need for more empirical studies of agile development Table 15 Deﬁnitions used for grading the strength of evidence [7] High Further research is very unlikely to change our conﬁdence in the estimate of eﬀect Moderate Further research is likely to have an important impact on our conﬁdence in the estimate of eﬀect and may change the estimate Low Further research is very likely to have an important impact on our conﬁdence in the estimate of eﬀect and is likely to change the estimate Very low Any estimate of eﬀect is very uncertain T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 851', 'methods. Agile development has had a deep impact on the software industry in recent years. In our opinion, this should lead to a greater interest amongst researchers as to what has driven the trend and what the eﬀects are of the changes that emerge in response to the adoption of agile development. This review also shows that, with rare exceptions, only XP has been studied. Hence, research on other agile approaches that are popular in industry should be a prior- ity when designing future studies. In our opinion, manage- ment-oriented approaches, such as Scrum, are clearly the most under-researched compared to their popularity in industry. Another striking ﬁnding is that only one research group in the world has studied mature agile development teams. If we want to investigate the potential of agile methods, we clearly need to direct more resources towards investigat- ing the practices of mature teams. The review shows that a range of research methods have been applied. We need to employ both ﬂexible and ﬁxed research designs if we are to gain a deeper understanding of agile development. Edmondson and McManus[25] argue that the research design needs to ﬁt the current state of theory and research. They divide this state into three cat- egories: nascent, intermediate, and mature; seeTable 16 . For agile software development, we believe the current state of theory and research on methods is clearly nascent, which suggests a need for exploratory qualitative studies. Rajlich[53] phrased it as a ‘‘backlog of research problems to be solved ”. Other areas of research on agile software development, such as studies of particular practices, like pair program- ming, or areas that connect well to existing streams of soft- ware engineering research, might be described as being at an intermediate, or even a mature, state. A major challenge is to increase the quality of studies on agile software development. In [58],S j øberg et al. discuss measures to increase the quality of empirical studies in soft- ware engineering in general. Recently, Ho ¨st and Runeson [33] have suggested a checklist to use in case studies in soft- ware engineering. The recent special issue of Information and Software Technology on qualitative software engineer- ing research [20] provides many useful examples of approaches for study designs, data collection, and analysis that should be relevant for future studies of agile software development. The state of research with respect to con- trolled experiments has been described thoroughly in a sur- vey by Sj øberg et al. [59]. In order to increase the usefulness of the research for industry and to provide a suﬃcient number of studies of high quality on subtopics related to agile development, we think that researchers in the ﬁeld should collaborate to determine a common research agenda. It lies beyond the scope of this article to suggest such an agenda, but we hope that the synthesis of research presented herein may provide the inspiration to create one. For practitioners, this review shows that many promis- ing studies of the use of agile methods have been reported. Although serious limitations have been identiﬁed, e.g., that the role of on-site customer seems to be unsustainable for long periods and that it is diﬃcult to introduce agile meth- ods into large and complex projects, the results of the review suggest that it is possible to achieve improved job satisfaction, productivity, and increased customer satisfaction. The strongest, and probably most relevant, evidence for practice is from the studies of mature agile teams, which suggests that it is necessary to focus on human and social factors in order to succeed. Speciﬁcally, it seems that a high level of individual autonomy must be balanced with a high level of team autonomy and corporate responsibility. It also seems important to staﬀ agile teams with people that have faith in their own abilities combined with good inter- personal skills and trust.', 'also seems important to staﬀ agile teams with people that have faith in their own abilities combined with good inter- personal skills and trust. Evidence also suggests that instead of abandoning tradi- tional project management principles, one should rather take advantage of these principles, such as state-gate pro- ject management models, and combine them with agile pro- ject management. The evidence also suggests that agile methods not necessarily are the best choice for large pro- jects. Thus, consistent with recommendations provided by others[11,12,15], we suggest that practitioners carefully study their projects’ characteristics and compare them with the relevant agile methods’ required characteristics. Due to the limited number and relatively poor quality of the primary studies in this review, it is impossible to oﬀer more deﬁnitive and detailed advice. Rather, this review provides an overview of research carried out to date, which must be critically appraised by companies in order to iden- tify similarities and diﬀerences between the studies reported and their own situation. A particular important aid in this appraisal is the description of the context of the studies in Table 16 Categories of methodological ﬁt in ﬁeld research [25] State of prior theory and research Nascent Intermediate Mature Research questions Open-ended inquiry about a phenomenon of interest Proposed relationships between new and established constructs Focused questions and/or hypotheses relating existing constructs Type of data collected Qualitative, initially open-ended data that need to be interpreted for meaning Hybrid (both qualitative and quantitative) Quantitative data: focused measures where extent or amount is meaningful Theoretical contribution A suggestive theory, often an invitation for further work on the issue or set of issues opened up by the study A provisional theory, often one that integrates previously separate bodies of work A supported theory that may add speciﬁcity, new mechanisms, or new boundaries to existing theories 852 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'this review ( Appendix D ). A further aid would be to apply the principles of evidence-based software engineering in order to support and improve the decisions about what methods and technologies to employ[24]. The review clearly shows the need for more research in order to determine the situations in which advice on agile development that has been oﬀered by practitioners may suitably be applied. We would like to urge companies to participate in research projects in the future, in order to target research goals that are relevant for the software industry. Action research is one such way of organizing collaboration between industry and researchers that would be highly relevant for a nascent ﬁeld such as agile software development. 5.4. Limitations of this review The main limitations of the review are bias in the selection of publications and inaccuracy in data extraction. To help to ensure that the process of selection was unbiased, we devel- oped a research protocol in advance that deﬁned the research questions. Using these questions as a basis, we identiﬁed key- words and search terms that would enable us to identify the relevant literature. However, it is important to recognize that software engineering keywords are not standardized and that they can be both discipline- and language-speciﬁc. Therefore, due to our choice of keywords and search strings, there is a risk that relevant studies were omitted. To avoid selection bias, we piloted every part of the review process, and in particular, the search strategy and citation manage- ment procedure, in order to clarify weaknesses and reﬁne the selection process. Furthermore, since our focus was on empirical research, we excluded ‘‘lessons learned ” papers and papers that were based merely on expert opinion. If the review had included this literature, the current study could, in principle, have provided more data. In that event, it might have been possible to draw more general conclu- sions. To further ensure the unbiased selection of articles, a multistage process was utilized that involved three research- ers who documented the reasons for inclusion/exclusion at every step, as described in Section 3 and also as suggested by Kitchenham [36]. When we piloted the data extraction process, we found that several articles lacked suﬃcient details about the design and ﬁndings of a study and that, due to this, we dif- fered too much in what we actually extracted. As a conse- quence, all data from all the 33 primary studies were extracted by the two authors in consensus meetings accord- ing to a predeﬁned extraction form ( Appendix C ). How- ever, we often found that the extraction process was hindered by the way some of the primary studies were reported. Many articles lacked suﬃcient information for us to be able to document them satisfactorily in the extrac- tion form. More speciﬁcally, we frequently found that methods were not described adequately, that issues of bias and validity were not always addressed, that methods of data collection and analysis were often not explained well, and that samples and study settings were often not described well. There is therefore a possibility that the extraction process may have resulted in some inaccuracy in the data. 6. Conclusion We identiﬁed 1996 studies from searches of the litera- ture, of which 36 were found to be research studies of acceptable rigour, credibility, and relevance. Thirty-three of the 36 studies identiﬁed were primary studies, while three were secondary studies. The studies fell into four thematic groups: introduction and adoption, human and social factors, perceptions of agile methods, and comparative studies. We identiﬁed a number of reported beneﬁts and limitations of agile devel- opment within each of these themes. However, the strength of evidence is very low, which makes it diﬃcult to oﬀer spe- ciﬁc advice to industry. Consequently, we advise readers from industry to use this article as a map of ﬁndings', 'of evidence is very low, which makes it diﬃcult to oﬀer spe- ciﬁc advice to industry. Consequently, we advise readers from industry to use this article as a map of ﬁndings according to topic, which they can use to investigate rele- vant studies further and compare the settings in the studies to their own situation. The studies investigated XP almost exclusively, and only a few of the studies on XP were done on mature develop- ment teams. A clear ﬁnding of the review is that we need to increase both the number and the quality of studies on agile software development. In particular, agile project management methods, such as Scrum, which are popular in industry, warrant further attention. We see that there is a backlog of research issues to be addressed. In this con- text, there is a clear need to establish a common research agenda for agile software development and for future ﬁeld studies to pay more attention to the ﬁt between their research methods and the state of prior work. Acknowledgements The work in this paper was supported by the Research Council of Norway through the project Evidence-Based Software Engineering (181685/I30). We are grateful to Geir K. Hanssen at SINTEF ICT, who participated in selecting and assessing the studies included in this review. We are also grateful to Chris Wright for proofreading the paper. Appendix A. Studies included in the review [S1] P. Abrahamsson, J. Koskela, Extreme program- ming: a survey of empirical data from a controlled case study, in: Proceedings – 2004 International Symposium on Empirical Software Engineering, ISESE 2004, Aug 19–20 2004, Redondo Beach, CA, United States, 2004. [S2] B. Bahli, E.S.A. Zeid, The role of knowledge crea- tion in adopting extreme programming model: an empirical study, in: ITI 3rd International Confer- T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 853', 'ence on Information and Communications Technol- ogy: Enabling Technologies for the New Knowledge Society, 2005. [S3] R. Baskerville, B. Ramesh, L. Levine, J. Pries-Heje, S. Slaughter, Is internet-speed software development diﬀerent? IEEE Software 20(6) (2003) 70–77. [S4] M. Ceschi, A. Sillitti, G. Succi, S. De Panﬁlis, Pro- ject management in plan-based and agile companies, IEEE Software 22(3) (2005) 21–27. [S5] J. Chong, Social behaviours on XP and non-XP teams: a comparative study, in: Proceedings of the Agile Development Conference (ADC’05), 2005. [S6] A. Dagnino, K. Smiley, H. Srikanth, A.I. Anton, L. Williams, Experiences in applying agile software devel- opment practices in new product development, in: Pro- ceedings of the 8th IASTED International Conference on Software Engineering and Applications, November 9–11, Cambridge, MA, United States, 2004. [S7] D. Dalcher, O. Benediktsson, H. Thorbergsson, Development life cycle management: a multiproject experiment, in: Proceedings of the 12th Interna- tional Conference and Workshops on the Engineer- ing of Computer-Based Systems (ECBS’05), 2005. [S8] A. Fruhling, K. Tyser, G.-J. De Vreede, Experiences with extreme programming in telehealth: developing and implementing a biosecurity health care applica- tion, in: Proceedings of the 38th Hawaii International Conference on System Sciences (HICCS), Hawaii, USA, 2005. [S9] M.-R. Hilkka, T. Tuure, R. Matti, Is extreme pro- gramming just old wine in new bottles: a comparison of two cases, Journal of Database Management 16(4) (2005) 41–61. [S10] S. Ilieva, P. Ivanov, E. Stefanova, Analyses of an agile methodology implementation, in: Proceedings 30th Euromicro Conference, IEEE Computer Soci- ety Press, 2004, pp. 326–333. [S11] T. Jokela, P. Abrahamsson, Usability assessment of an extreme programming project: close co-operation with the customer does not equal to good usability, in: Product Focused Software Process Improvement, Lecture Notes in Computer Science, vol. 3009, Springer Verlag, Berlin, 2004, pp. 393–407. [S12] D. Karlstro ¨m, P. Runeson, Combining agile meth- ods with stage-gate project management, IEEE Soft- ware 22(3) (2005) 43–49. [S13] J. Koskela, P. Abrahamsson, On-site customer in an XP project: empirical results from a case study, in: T. Dingsøyr (Ed.), Software Process Improvement, Proceedings, Lecture Notes in Computer Science, vol. 3281, Springer-Verlag, Berlin, 2004, pp. 1–11. [S14] L. Layman, L. Williams, L. Cunningham, Exploring extreme programming in context: an industrial case study, Agile Development Conference, 2004. [S15] F. Macias, M. Holcombe, M. Gheorghe, A formal experiment comparing extreme programming with traditional software construction, in: Proceedings of the Fourth Mexican International Conference on Computer Science (ENC 2003), 2003. [S16] A. Mackenzie, S. Monk, From Cards to Code: How Extreme Programming Re-Embodies Programming as a Collective Practice, Computer Supported Coop- erative Work, vol. 13, 2004, pp. 91–117. [S17] C. Mann, F. Maurer, A case study on the impact of scrum on overtime and customer satisfaction, Agile Development Conference, 2005. [S18] K. Mannaro, M. Melis, and M. Marchesi, Empirical analysis on the satisfaction of IT employees compar- ing XP practices with other software development methodologies, in: Extreme Programming and Agile Processes in Software Engineering, Proceedings, Lecture Notes in Computer Science, vol. 3092, Springer Verlag, 2004, pp. 166–174. [S19] A. Martin, R. Biddle, J. Noble, The XP customer role in practice: three studies, Agile Development Conference, 2004. [S20] A. Martin, R. Biddle, J. Noble, When XP met out- sourcing, in Extreme Programming and Agile Pro- cesses in Software Engineering, Proceedings, Lecture Notes in Computer Science, vol. 3092, Springer Verlag, Berlin, 2004, pp. 51–59. [S21] G. Melnik, F. Maurer, Perceptions of agile practices: a student survey, in: Proceedings, eXtreme Program- ming/Agile Universe 2002, Lecture Notes in Com-', '[S21] G. Melnik, F. Maurer, Perceptions of agile practices: a student survey, in: Proceedings, eXtreme Program- ming/Agile Universe 2002, Lecture Notes in Com- puter Science, vol. 2418, Springer Verlag, 2002, pp. 241–250. [S22] G. Melnik, F. Maurer, A cross-program investiga- tion of student’s perceptions of agile methods, in: International Conference on Software Engineering (ICSE), St. Louis, MI, USA, 2005. [S23] P. Middleton, Lean software development: two case studies, Software Quality Journal 9(4) (2001) 241– 252. [S24] H. Robinson, H. Sharp, The characteristics of XP teams, in: Extreme Programming and Agile Pro- cesses in Software Engineering, Lecture Notes in Computer Science, vol. 3092, Springer Verlag, Ber- lin, 2004, pp. 139–147. [S25] H. Robinson, H. Sharp, The social side of technical practices, in: Extreme Progamming and Agile Pro- cesses in Software Engineering, Lecture Notes in Computer Science, vol. 3556, Springer Verlag, Ber- lin, 2005, pp. 100–108. [S26] H.S. Robinson, Organisational culture and XP: three case studies, in: Proceedings of the Agile Con- ference (ADC’05), 2005. [S27] H. Sharp H. Robinson, An ethnographic study of XP practice, Empirical Software Engineering, 9(4) (2004) 353–375. [S28] A. Sillitti, M. Ceschi, B. Russo, G. Succi, Managing uncertainty in requirements: a survey in documenta- tion-driven and agile companies, in: Proceedings of the 11th International Software Metrics Symposium (METRICS), 2005. 854 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', '[S29] H. Svensson, M. Ho ¨st, Introducing agile process in a software maintenance and evolution organization, in: Ninth European Conference on Software Main- tenance and Reengineering (CSMR’05), 2005. [S30] H. Svensson, M. Ho ¨st, Views from an organization on how agile development aﬀects its collaboration with a software development team, Lecture Notes in Compuer Science, vol. 3547, Springer Verlag, Ber- lin, 2005, pp. 487–501. [S31] Tessem, Experiences in learning xp practices: a qual- itative study, in: XP 2003, vol. 2675, Springer Ver- lag, Berlin, 2003, pp. 131–137. [S32] C.A. Wellington, T. Briggs, C.D. Girard, Compari- son of student experiences with plan-driven and agile methodologies, in: Proceeedings of the 35th ASEE/ IEEE Frontiers in Education Conference, 2005. [S33] S.M. Young, H.M. Edwards, S. Mcdonald, J.B. Thompson, Personality characteristics in an XP team: A repertory grid study, in: Proceedings of Human and Social Factors of Software Engineering (HSSE), St. Louis, MI, USA, 2005. [S34] P. Abrahamsson, O. Salo, J. Ronkainen, J. Warsta, Agile software development methods: Review and analysis, VTT Technical report, 2002. [S35] D. Cohen, M. Lindvall, P. Costa, An introduction to agile methods, in: M. V. Zelkowitz (Ed.), Advances in Computers, Advances in Software Engineering, vol. 62, Elsevier, Amsterdam, 2004. [S36] J. Erickson, K. Lyytinen, K. Siau, Agile modeling, Agile software development, and extreme program- ming: the state of research, Journal of Database Management 16(4) (2005) 88–100. Appendix B. Quality assessment form Screening questions: 1. Is this a research paper? Consider: –Is the paper based on research (or is it merely a “lessons l earned” report based on expert  opinion?  Yes  No 2. Is there a clear statement of the aims of the research? Consider: –Is there a rationale for why the study was undertaken? –Is the study’s focus or main focus on Agile Software Development? –Does the study present empirical data? –Is there a clear statement of the study’s primary outcome (i.e. time-to-market, cost, or product  or process quality)?  Yes  No 3. Is there an adequate description of the context in which the research was carried out? Consider whether the researcher has identified: –The industry in which products are used (e.g. banking, telecommunications, consumer goods,  travel, etc) –The nature of the software development organization (e.g. in-house department or  independent software supplier) –The skills and experience of software staff (e.g. with a language, a method, a tool, an  application domain) –The type of software products used (e.g. a design tool, a compiler) –The software processes being used (e.g. a company standard process, the quality assurance  procedures, the configuration management process)  Yes  No If question 1, or both of questions 2 and 3, receive a “No” response do not continue with the  quality assessment. Detailed questions: Research design 4. Was the research design appropriate to address the aims of the research?  Consider: – Has the researcher justified the research design (e.g. have they discussed how they decided  which methods to use)?  Yes  No T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 855', 'Data collection 7. Was the data collected in a way that addressed the research issue?  Consider: –Were all measures clearly defined  (e.g. unit and counting rules)? –Is it clear how data was collected (e.g. semi-structured interviews, focus group etc.)? –Has the researcher justified  the methods that were chosen? –Has the researcher made the methods explicit (e.g. is there an indication of how interviews  were conducted, did they use an interview guide)? –If the methods were modified during the study, has the researcher explained how and why? –Whether the form of the data is clear (e.g. tape recordin g, video material, notes etc.) –Whether quality control methods were used to ensure completeness and accuracy of data  collection  Yes  No Data analysis 8. Was the data analysis sufficiently rigorous?  Consider: –Was there an i n-depth description of the analysis process? –If thematic analysis was used, is it clear how the categories/ themes were derived from the  data? –Has sufficient data been presented to support the findings? –To what extent has contradictory data been taken into account? –Whether quality control methods were used to verify the results  Yes  No Reflexivity (research partnership relations/recognition of researcher bias) 9. Has the relationship between researcher and participants been considered adequately?  Consider: –Did the researcher critically examine their own role, potential bias and influence during the  formulation of research questions, sample recruitment, data collection, and analysis and  selection of data for presentation? –How the researcher respond ed to events during the study and whether they considered the  implications of any changes in the research design.  Yes  No Findings 10. Is there a clear statement of findings?  Consider: –Are the findings explicit (e.g. magnitude of effect)? –Has an adequate discussion of the evidence, both for and against the researcher’s arg uments,  been demonstrated? –Has the researcher discussed the credibility of their findings (e.g. triangulation, respondent  validation, more than one analyst)? –Are limitations of the study discussed explicitly? –Are the findings discussed in relation to the original research questions? –Are the conclusions justified by the results?  Yes  No Sampling 5. Was the recruitment strategy appropriate to the aims of the research? Consider: –Has the researcher explained  how the participants or cases were identified and selected? –Are the cases defined and described precisely? –Were the cases representative of a d efined population? –Have the researchers explained why the participants or cases they selected were the most  appropriate to provide access to the type of knowledge sought by the study? –Was the sample size sufficiently large?  Yes  No Control group 6. Was there a control group with which to compare treatments?  Consider: –How were the controls selected? –Were they repres entative of a defined population? –Was there anyth ing special about the controls? –Was the non-response high? Could non-respondents be different in any way?  Yes  No 856 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', 'Value of the research 11. Is the study of value for research or practice?  Consider: –Does the researcher discuss the contribution the study makes to existing knowledge or  understanding (e.g. do they consider the findings in relation to current practice or relevant  research-based literature)? –Does the research identify new areas in which research is necessary? –Does the researcher discuss whether or how the findings can be transferred to other  populations, or consider other ways in which the research can be used?  Yes  No Appendix C. Data extraction form Study description 1. Study identiﬁer Unique id for the study 2. Date of data extraction 3. Bibliographic reference Author, year, title, source 4. Type of article Journal article, conference paper, workshop paper, book section 5. Study aims What were the aims of the study? 6. Objectives What were the objectives? 7. Design of study Qualitative, quantitative (experiment, survey, case study, action research) 8. Research hypothesis Statement of hypotheses, if any 9. Deﬁnition of agile software development given in study Verbatim from the study 10. Sample description Size, students, professionals (age, education, experience) 11. Setting of study Industry, in-house/supplier, products and processes used 12. Control group Yes, no (number of groups, sample size) 13. Data collection How was the data obtained? (questionnaires, interviews, forms) 14. Data analysis How was the data analyzed? (qualitative, quantitative) Study ﬁndings 1. Findings and conclusions What were the ﬁndings and conclusions? (verbatim from the study) 2. Validity Limitations, threats to validity 3. Relevance Research, practice Appendix D. Overview of primary studies ID Research method Agile method Agile experience Professional/ Student Project duration Team size Domain, comment S1 Singlecase XP Beginner Student 8,4 weeks 4 Research prototype developed S2 Multicase XP Beginner Professional 1 year 9 Medical information systems S3 Mixed General – Professional – NA Web development S4 Survey General NA Professional – NA NA S5 Multicase XP Beginner Professional – 7–12 Mid-size software start-up S6 Multicase Other Beginner Professional 2700 h 5 Industrial automation S7 Experiment XP Beginner Student 1 year 3–4 NA S8 Singlecase XP Beginner Professional 21 months 4 Medical information systems S9 Multicase XP Beginner Professional NA/18 months 6/4 Factory system + communication system S10 Singlecase XP Beginner Professional 900 h 4 Financial software S11 Singlecase XP Beginner Student 8,4 weeks 4 Research prototype developed S12 Multicase General Beginner Professional – – Industrial automation/Defence/Telecom S13 Singlecase XP Beginner Student 8,4 weeks 4 Research prototype developed S14 Singlecase XP Beginner Professional 3,5 months 10 Airline company software S15 Experiment XP Beginner Student 1 semester 4–5 NA S16 Singlecase XP Beginner Professional – 6–12 Knowledge management software S17 Singlecase Scrum Beginner Professional 22 months 4–6 Oil and gas software S18 Survey XP NA Professional NA NA NA (continued on next page ) T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 857', 'Appendix C. (continued) ID Research method Agile method Agile experience Professional/ Student Project duration Team size Domain, comment S19 Multicase XP Beginner Professional 15/45/18 11/8/16 – S20 Multicase XP Beginner Professional 15 months/18+ months 11/60 – S21 Survey XP Beginner Student NA NA NA S22 Mixed General Beginner Student – NA NA S23 Multicase LSD Beginner Professional 3 days 2 Financial system S24 Multicase XP Mature Professional – 8/23 Web applications/document software S25 Multicase XP Mature Professional – 7/23/8 Mid-size software start-up S26 Multicase XP Mature Professional – 12/20/8 Banking/Content security software/web-applications S27 Singlecase XP Mature Professional – 10 – S28 Survey General NA Professional – NA NA S29 Singlecase XP Beginner Professional – – Software house S30 Singlecase XP Beginner Professional – – Software maintenance and evolution S31 Singlecase XP Beginner Student 3 weeks 6 Educational software S32 Experiment XP Beginner Student 1 semester 16 NA S33 Singlecase XP – Professional – 6 Software house Several numbers for a study in the columns ‘Project duration’ and ‘Team size’ indicate that the study included several teams. References [1] North American and European Enterprise Software and Services Survey, Business Technographics Ed., 2005. [2] P. Abrahamsson, O. Salo, J. Ronkainen, J. Warsta, Agile software development methods: review and analysis, VTT Technical report, 2002. [3] P. Abrahamsson, J. Warsta, M.T. Siponen, J. Ronkainen, New directions on agile methods: a comparative analysis, in: Proceedings of the 25th International Conference on Software Engineering (ICSE’03), IEEE Press, 2003. [4] R.L. Ackoﬀ, Alternative types of planning, in: Ackoﬀ’s Best: His Classic Writings on Management, Wiley, New York, 1999, pp. 104– 114. [5] A. Anderson, R. Beattie, K. Beck, D. Bryant, M. Dearment, M. Fowler, M. Fronczak, R. Garzaniti, D. Gore, B. Hacker, C. Hendrickson, R. Jeﬀries, D. Joppie, D. Kim, P. Kowalsky, D. Mueller, T. Murasky, R. Nutter, A. Pantea, D. Thomas, Chrysler goes to extremes, Distributed Computing Magazine (October) (1998) 24–28. [6] M. Aoyama, Web-based agile software development, IEEE Software 15 (6) (1998) 56–65. [7] D. Atkins, D. Best, P.A. Briss, M. Eccles, Y. Falck-Ytter, S. Flottorp, G.H. Guyatt, R.T. Harbour, M.C. Haugh, D. Henry, S. Hill, R. Jaeschke, G. Leng, A. Liberati, N. Magrini, J. Mason, P. Middleton, J. Mrukowicz, D. O’connell, A. D Oxman, B. Phillips, H.J. Schu¨nemann, T.T.-T. Edejer, H. Varonen, G.E. Vist, J.W. Williams Jr., Z. Stephanie, Grading quality of evidence and strength of recommendations, BMJ 328 (1490) (2004). [8] D. Avison, F. Lau, M. Myers, P.A. Nielsen, Action research, Communications of the ACM 42 (1) (1999) 94–97. [9] K. Beck, Extreme Programming Explained: Embrace Change, Addi- son-Wesley, 2000, ISBN 0-201-61641-6. [10] K. Beck, Extreme Programming Explained: Embrace Chage, second ed., Addison-Wesley, 2004, ISBN 978-0321278654. [11] B. Boehm, Get ready for agile methods, with care, IEEE Computer 35 (1) (2002) 64–69. [12] B. Boehm, R. Turner, Balancing Agility and Discipline: A Guide for the Perplexed, Addison-Wesley, Boston, 2003, ISBN 978-0321186126. [13] N. Britten, R. Campbell, C. Pope, J. Donovan, M. Morgan, R. Pill, Using meta ethnography to synthesise qualitative research: a worked example, Journal of Health Services Research and Policy 7 (4) (2002) 209–215. [14] P. Checkland, J. Scholes, Soft Systems Methodology in Action, Wiley, Chichester, 1990, ISBN 0-471-98605-4. [15] A. Cockburn, Selecting a project’s methodology, IEEE Software 17 (4) (2000) 64–71. [16] A. Cockburn, Crystal Clear: A Human-Powered Methodology for Small Teams, Addison-Wesley, 2004, ISBN 0-201-69947-8. [17] D. Cohen, M. Lindvall, P. Costa, An introduction to agile methods, in: M.V. Zelkowitz (Ed.), Advances in Computers, Advances in Software Engineering, vol. 62, Elsevier, Amsterdam, 2004. [18] J. Cohen, A coeﬃcient of agreement for nominal scales, Educational', 'in: M.V. Zelkowitz (Ed.), Advances in Computers, Advances in Software Engineering, vol. 62, Elsevier, Amsterdam, 2004. [18] J. Cohen, A coeﬃcient of agreement for nominal scales, Educational and Psychological Measurement 20 (1960) 37–46. [19] K. Conboy, B. Fitzgerald, Toward a conceptual framework of agile methods: a study of agility in diﬀerent disciplines, in: Proceedings of XP/Agile Universe, Springer Verlag, 2004. [20] Y. Dittrich, M. John, J. Singer, B. Tessem, For the special issue on qualitative software engineering research, Information and Software Technology 49 (6) (2007) 531–539. [21] T. Dyba ˚, Improvisation in small software organizations, IEEE Software 17 (5) (2000) 82–87. [22] T. Dyba ˚, E. Arisholm, D. Sj øberg, J. Hannay, F. Shull, Are two heads better than one? On the eﬀectiveness of pair-programming, IEEE Software 24 (6) (2007) 10–13. [23] T. Dyba˚, T. Dings øyr, G.K. Hanssen, Applying systematic reviews to diverse study types: an experience report, in: Proceedings of the 1st International Symposium on Empirical Software Engineering and Measurement (ESEM’07), IEEE Computer Society, Madrid, Spain, 2007, pp. 225–234. [24] T. Dyba ˚, B. Kitchenham, M. J ørgensen, Evidence-based soft- ware engineering for practitioners, IEEE Software 22 (1) (2005) 58–65. [25] A.C. Edmondson, S.E. Mcmanus, Methodological ﬁt in management ﬁeld research, Academy of Management Review 32 (4) (2007) 1155– 1179. [26] H. Erdogmus, M. Morisio, M. Torchiano, On the eﬀectiveness of the test-ﬁrst approach to programming, IEEE Transactions on Software Engineering 31 (3) (2005) 226–237. [27] J. Erickson, K. Lyytinen, K. Siau, Agile Modeling, Agile software development, and extreme programming: the state of research, Journal of Database Management 16 (4) (2005) 88–100. [28] T. Gilb, Competitive Engineering: A Handbook for Systems Engi- neering, Requirements Engineering, and Software, Elsevier Butter- worth-Heinemann, Oxford, 2005, ISBN 0-7506-6507-6. [29] T. Greenhalgh, How to Read a Paper, second ed., BMJ Publishing Group, London, 2001. [30] A. Gunasekaran, Agile manufacturing: A framework for research and development, International Journal of Production Economics 62 (1– 2) (1999) 87–105. 858 T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859', '[31] J.P.T. Higgins, S. Green (Eds.), Cochrane Handbook for System- atic Reviews of Interventions, Version 5.0.0 (updated February 2008), The Cochrane Collaboration, 2008. Available from: <www.cochrane-handbook.org>. [32] W.S. Humphrey, PSP: A Self-Improvement Process for Software Engineers, Addison-Wesley, 2005, ISBN 978-0321305497. [33] M. Ho¨st, P. Runeson, Checklists for software engineering case study research, in: Proceedings of the First International Symposium on Empirical Software Engineering and Measurement, IEEE, Madrid, Spain, 2007, pp. 479–481. [34] G. Keefer, Extreme Programming Considered Harmful for Reliable Software Development 2.0, AVOCA GmbH, Online Report, 2003. [35] K.S. Khan, G. Ter Riet, J. Glanville, A.J. Sowden, J. Kleijnen, Undertaking Systematic Review of Research on Eﬀectiveness, CRD’s Guidance for those Carrying Out or Commissioning Reviews, CRD Report Number 4, second ed., NHS Centre for Reviews and Dissemination, University of York, 2001. [36] B.A. Kitchenham, Guidelines for performing Systematic Literature Reviews in Software Engineering Version 2.3, Keele University and University of Durham, EBSE Technical Report, 2007. [37] B.A. Kitchenham, S.L. Pﬂeeger, L.M. Pickard, P.W. Jones, D.C. Hoaglin, K. El Emam, J. Rosenberg, Preliminary guidelines for empirical research in software engineering, IEEE Transactions on Software Engineering 28 (8) (2002) 721–734. [38] P. Krutchen, The Rational Uniﬁed Process: An Introduction, third ed., Addison-Wesley, Boston, 2003. [39] J.R. Landis, G.G. Koch, The measurement of observer agreement for categorical data, Biometrics 33 (1) (1977) 159–174. [40] C. Larman, V.R. Basili, Iterative and incremental development: a brief history, IEEE Computer 36 (6) (2003) 47–56. [41] J. Mcavoy, T. Butler, The impact of the Abilene Paradox on double- loop learning in an agile team, Information and Software Technology 49 (6) (2007) 552–563. [42] P. Mcbreen, Questioning Extreme Programming, Pearson Education, Boston, MA, USA, 2003, ISBN 0-201-84457-5. [43] H. Merisalo-Rantanen, T. Tuure, R. Matti, Is extreme programming just old wine in new bottles: a comparison of two cases, Journal of Database Management 16 (4) (2005) 41–61. [44] P. Meso, R. Jain, Agile software development: adaptive systems principles and best practices, Information Systems Management 23 (3) (2006) 19–30. [45] M.B. Miles, M. Huberman, Qualitative Data Analysis: An Expanded Sourcebook, second ed., Sage Publications, 1994, ISBN 0803955405. [46] S. Nerur, V. Balijepally, Theoretical reﬂections on agile development methodologies, Communications of the ACM 50 (3) (2007) 79–83. [47] S. Nerur, R. Mahapatra, G. Mangalaraj, Challenges of migrating to agile methodologies, Communications of the ACM (May) (2005) 72– 78. [48] G.W. Noblit, R.D. Hare, Meta-Ethnography: Synthesizing Qualita- tive Studies, Sage Publications, London, 1988. [49] T. Ohno, Toyota Production System: Beyond Large-scale Production, Productivity Press, New York, USA, 1988, ISBN 0-915299-14-3. [50] S.R. Palmer, J.M. Felsing, A Practical Guide to Feature-driven Development, Prentice Hall, Upper Saddle River, NJ, 2002, ISBN 0- 13-067615-2. [51] M.C. Paulk, C.V. Weber, B. Curtis, M.B. Chrissis, The Capability Maturity Model: Guidelines for Improving the Software Process, Addison-Wesley, Boston, 1995, ISBN 0-201-54664-7. [52] M. Poppendieck, T. Poppendieck, Lean Software Development – An Agile Toolkit for Software Development Managers, Addison-Wesley, Boston, 2003, ISBN 0-321-15078-3. [53] V. Rajlich, Changing the paradigm of software engineering, Com- munications of the ACM 49 (8) (2006) 67–70. [54] C.K. Riemenschneider, B.C. Hardgrave, F.D. Davis, Explaining software developer acceptance of methodologies: a comparison of ﬁve theoretical models, IEEE Transactions on Software Engineering 28 (12) (2002) 1135–1145. [55] L.M. Sanchez, R. Nagi, A review of agile manufacturing systems, International Journal of Production Research 39 (16) (2001) 3561– 3600.', '(12) (2002) 1135–1145. [55] L.M. Sanchez, R. Nagi, A review of agile manufacturing systems, International Journal of Production Research 39 (16) (2001) 3561– 3600. [56] K. Schwaber, M. Beedle, Agile Software Development with Scrum, Prentice Hall, Upper Saddle River, 2001. [57] W.R. Shadish, T.D. Cook, D.T. Campbell, Experimental and Quasi- Experimental Designs for Generalized Causal Inference, Houghton Miﬄin Company, Boston, 2002. [58] D. Sj øberg, T. Dyba ˚,M .J ørgensen, The Future of Empirical Methods in Software Engineering Research, in: Future of Software Engineering (FOSE’07), IEEE, 2007, pp. 358–378. [59] D. Sj øberg, J.E. Hannay, O. Hansen, V.B. Kampenes, A. Karahasa- novic, N.-K. Liborg, A.C. Rekdal, A survey of controlled experi- ments in software engineering, IEEE Transactions on Software Engineering 31 (9) (2005) 733–753. [60] J. Stapleton, DSDM: Business Focused Development, second ed., Pearson Education, 2003, ISBN 978-0321112248. [61] M. Stephens, D. Rosenberg, Extreme Programming Refactored: The Case Against XP, Apress, Berkeley, CA, 2003, ISBN 1-59059-096-1. [62] A. Strauss, J. Corbin, Basics of Qualitative Research, second ed., Sage Publications, 1998, ISBN 0-8039-5939-7. [63] H. Takeuchi, I. Nonaka, The new product development game, Harvard Business Review (January) (1986) 137–146. [64] D. Turk, R. France, B. Rumpe, Assumptions underlying agile software- development processes, Journal of Database Management 16 (4) (2005) 62–87. [65] E. Wenger, Communities of Practice: Learning, Meaning and Identity, Cambridge University Press, Cambridge, UK, 1998, ISBN 0-521- 43017-8. [66] L. Williams, A. Cockburn, Agile software development: it’s about feedback and change, IEEE Computer 36 (6) (2003) 39–43. [67] J.P. Womack, D.T. Jones, D. Roos, The Machine that Changed the World: The Story of Lean Production – Toyota’s Secret Weapon in the Global Car Wars that is Now Revolutionizing World Industry, Free Press, 2007, ISBN 978-0743299794. [68] P. A˚ gerfalk, B. Fitzgerald, Flexible and distributed software pro- cesses: old petunias in new bowls? Communications of the ACM 49 (10) (2006) 27–34. T. Dyba˚, T. Dings øyr / Information and Software Technology 50 (2008) 833–859 859']","['EVIDENCES OF BENEFITS AND LIMITATIONS OF  AGILE SOFTWARE DEVELOPMENT INTRODUCTION AND ADOPTION OF AGILE  METHODS • XP  was  found  to  be  difficult  to  intro- duce  in  a  complex  organization,  yet  seemingly easy in other types of organi- zations. • It is likely that the ease with which XP  can be introduced will depend on how  interwoven software  development  is   with  the  other  functions  of  the orga - nization.   • Benefits  were  reported  in  the   following  areas:  customer  collabora - tion,  work processes  for  handling  de - fects,  learning  in  pair  programming,   thinking  ahead for management, focu - sing on current work for engineers, and  estimation.  • The lean development technique did not  work well for one of the teams trying it  out,  pair  programming  was  seen  as   inefficient,  and  some  claimed  that  XP  works best with experienced develop - ment teams.   • A further limitation of agile methods  that was reported by one of the  studies,  as the lack of attention to design and ar- chitectural issues.  HUMAN AND SOCIAL FACTORS OF AGILE  METHODS • A  benefit  of  XP  was  that  it  thrived   in  radically  different  environments;  in  organizations  that  varied  from  having   a  hierarchical  structure  to  little  or  no  central control.  • Customer  involvement  and  physical   settings  varied  greatly  for  the  suc - cessful XP  teams  studied.  It  seems  to   be  possible  to  adopt  XP  in  various  organizational settings.  • Studies of XP indicate that successful te- ams manage to balance a high level of  individual  autonomy  with  a  high  level   of  team  autonomy  and  corporate res- ponsibility.  • They have faith in their own abilities and  preserve the quality of their working li - ves.  • Good interpersonal skills and trust were  found to be important characteristic for  a successful XP team. PERCEPTIONS ON AGILE METHODS • Studies report that customers are satis- fied with the opportunities for feedback  and responding to changes that agile  methods promote.  • However, the role of on site customer  was reported to be stressful and cannot  be sustained for a long period.  • Developers are mostly satisfied with  agile methods.  • Companies that use XP have reported  that their employees are more satisfied  with their job and that they are more sa- tisfied with the product they work with.  • There  were  mixed  findings  regarding   the  effectiveness  of  pair  programming  and several developers regard it as an  exhausting practice, because it requires  heavy concentration.  • University  students  perceive  agile  me- thods  as  providing  them  with  relevant  training  for  their  future  work  and  be- lieve  that  these  methods  improve  the  productivity  in  teams.  However,  they   reported  that  pair  programming  was  difficult when there was a large  skill di- fferential  between  the members of the  pairs.  In  addition,  test  irst  develop - ment  was reported  to  be  difficult  for   many students. COMPARISONS BETWEEN AGILE METHODS  AND ALTERNATIVES • Some  studies  suggest  benefits  in  pro- jects  that  use  agile  methods  because  changes  are  incorporated  more  easily   and  business  value  is  demonstrated  more efficiently.  • Results show that it is also possible to  combine agile project management  with overall  traditional  principles,  such   as  the  stage gate  project  management  model. • A limitation that was mentioned is that  team members are less interchangeable  in agile teams, which has consequences  for how projects are managed. • With respect to the productivity of agile  and traditional teams, three of the four  comparative  studies  that  address  this   issue  found  that  using  XP  results  in  increased productivity in terms of LO - C/h.  • However,  none  of  these  studies  had   an  appropriate  recruitment  strategy   to ensure an unbiased comparison.  • There  are  also  findings  from  several', 'C/h.  • However,  none  of  these  studies  had   an  appropriate  recruitment  strategy   to ensure an unbiased comparison.  • There  are  also  findings  from  several   of  the  non comparative  studies  that  indicate  that  the  subjects  themselves   believe  that  the  productivity  increases  with the use of agile methods. • Most studies report increased code  quality when agile methods are used,  but, again, none of these studies had an  appropriate recruitment strategy to en- sure an unbiased comparison.  • The  size  of  the  end  product  seems   not  to  be  correlated  with  the  method   of development used. Different  studies  have reported larger,  smaller, and equal  sizes of end product for traditional ver - sus agile methods.  • The effect on work practices and job sa- tisfaction of using agile and traditional  methods has not been established con - clusively. MAIN FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Tore Dybå and Torgeir Dingsøyr. 2008. Empirical studies of agile software development: A systematic review. Inf. Softw. Technol. 50, 9 10 (August  2008), 833 859. DOI=http://dx.doi.org/10.1016/j.infsof.2008.01.006 Keywords: Agile software development XP   Extreme programming   Scrum Who is this briefing for? Software engineers practitioners  who want to make decisions  about agile software develop- ment based on scientific eviden- ce. Where the findings come  from? All findings of this briefing were  extracted from the systematic  review conducted by Dyba and  Dingsøyr. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg This  briefing  reports  evidence  on  what  is   currently  known  about  the  benefits  and  limitations  of  agile  software  development   based  on  scientific  evidence  from  a syste- matic review.']","**Title: Understanding Agile Software Development: Insights from Empirical Research**

**Introduction:**  
This evidence briefing summarizes the findings from a systematic review of empirical studies on agile software development conducted by Dyba and Dingsøyr. The aim is to provide practitioners with a clear overview of the benefits, limitations, and implications of agile methodologies, as well as to highlight the current state of research in this area.

**Core Findings:**  
The systematic review identified 36 empirical studies on agile software development, categorized into four themes: introduction and adoption, human and social factors, perceptions of agile methods, and comparative studies. The findings reveal both benefits and limitations of agile practices:

1. **Benefits:**
   - Enhanced customer collaboration and satisfaction due to frequent feedback and iterative processes.
   - Improved job satisfaction among developers using agile methods, particularly in environments that support autonomy and teamwork.
   - Increased productivity reported in several studies, with agile teams often delivering higher volumes of code in shorter time frames compared to traditional methods.

2. **Limitations:**
   - Agile methods, particularly Extreme Programming (XP), can be challenging to implement in large, complex organizations. There is a risk of burnout among teams, especially when the on-site customer role is unsustainable over long periods.
   - Concerns about insufficient attention to architectural design and long-term planning in agile practices, which may lead to suboptimal solutions.
   - The majority of studies focused on XP, indicating a need for more research on other agile methodologies like Scrum and Lean, which are gaining popularity in the industry.

3. **Strength of Evidence:**
   - The overall strength of evidence regarding agile software development is rated as very low due to methodological weaknesses in the studies, including lack of control groups and insufficient recruitment strategies. This suggests that while there are promising findings, they should be interpreted with caution.

4. **Implications for Research and Practice:**
   - There is a clear need for more rigorous empirical studies on agile development, especially focusing on diverse methodologies and mature teams.
   - Practitioners are encouraged to consider their specific project characteristics when adopting agile methods, as the effectiveness can vary based on context.

**Who is this briefing for?**  
This briefing is intended for software engineering practitioners, project managers, and researchers interested in understanding the implications of agile methodologies in software development.

**Where the findings come from?**  
The findings are derived from a systematic review of empirical studies conducted by Dyba and Dingsøyr, published in the journal Information and Software Technology in 2008.

**What is included in this briefing?**  
This briefing includes a summary of the benefits and limitations of agile software development, insights into the strength of evidence, and implications for both research and practice.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research on agile software development:**  
[https://doi.org/10.1016/j.infsof.2008.01.006](https://doi.org/10.1016/j.infsof.2008.01.006)"
"['Forecasting of software development work effort: Evidence on expert judgement and formal models Magne Jørgensen⁎ Simula Research Laboratory, P .O. Box 134, NO-1325 Lysaker, Norway Abstract The review presented in this paper examines the evidence on the use of expert judgement, formal models, and a combination of these two approaches when estimating (forecasting) software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgement-based effort estimates was higher than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgement-based effort estimates were estimation models not calibrated to the organization using the model, and important contextual information possessed by the experts not included in the formal estimation models. Four of the reviewed studies evaluated effort estimates based on a combination of expert judgement and models. The mean estimation accuracy of the combination-based methods was similar to the best of that of the other estimation methods. © 2007 International Institute of Forecasters. Published by Elsevier B.V . All rights reserved. Keywords: Judgemental forecasting; Combining forecasts; Comparative studies; Evaluating forecasts; Forecasting practice 1. Introduction Clients require effort and cost estimates of software projects as inputs to investment analyses. Similarly, project managers require effort estimates to enable planning and to control the software development work. Unfortunately, many software development effort esti- mates are quite inaccurate. A recent review of estimation accuracy studies indicated that software projects expend on average 30–40% more effort than is estimated (Moløkken-Østvold & Jørgensen, 2003). There seems to have been no substantial improvement in estimation accuracy over the years. Software projects experience severe delivery and management problems due to plans based on overoptimistic effort estimates. The negative effects of overoptimism are accentuated by (i) software bidding rounds where those companies that provide overoptimistic effort estimates are more likely to be selected, and (ii) overconfidence in the accuracy of the estimates; for example, 90% confidence effort prediction intervals only include the actual effort 60–70% of the time (Jørgensen, Teigen, & Moløkken, 2004). Software researchers have been addressing the problems of effort estimation for software development projects since at least the 1960s; see, e.g.,Nelson International Journal of Forecasting 23 (2007) 449–462 www.elsevier.com/locate/ijforecast ⁎ Fax: +47 67 82 82 01. E-mail address:magnej@simula.no. 0169-2070/$ - see front matter © 2007 International Institute of Forecasters. Published by Elsevier B.V . All rights reserved. doi:10.1016/j.ijforecast.2007.05.008', '(1966). Most of the research has focused on the con- struction of formal software effort estimation models. The early models were typically regression-based. Soon, however, more sophisticated effort estimation models appeared, for example models founded on case- based reasoning, classification and regression trees, simulation, neural networks, Bayesian statistics, lexical analyses of requirement specifications, genetic pro- gramming, linear programming, economic production models, soft computing, fuzzy logic modeling, statis- tical bootstrapping, and combinations of one or more of these models. A recent review (Jørgensen & Shepperd, 2007) identified 184 journal papers that introduced and evaluated formal models for software development effort estimation. Many of these studies describe the re- examination and improvement of previously proposed estimation methods. Several estimation models have been included in commercially promoted tools. A sur- vey byMoores and Edwards (1992)found that 61% of the IT managers in the UK had heard about at least one of these software development effort estimation tools. The use of formal estimation models has also been promoted by software process improvement frame- works and in software engineering education readings. In spite of the extensive research into estimation models, the high degree of availability of commercial estimation tools that implement the models, the aware- ness of these estimation tools, and the promotion of model-based estimation in software engineering text- books, software engineers typically use their expert judgement to estimate effort (Heemstra & Kusters, 1991; Hihn & Habib-Agahi, 1991). The limited use of models may be a sign of the irrational behaviour of software professionals. It may, on the other hand, be the case that expert judgement is just as accurate or has other advantages that render the current low use of effort estimation models rational. This leads to the research questions of this paper: i) Should we expect more accurate effort estimates when applying expert judgement or models? ii) When should software development effort estimates be based on expert judgement, on models, or on a combination of expert judgement and models? Extending Jørgensen (2004a), I review studies that compare the accuracy of software development effort estimates based on estimation models with those based on expert judgement and on a combination of these two approaches. The review process, limitations and results are included as Section 4. The factors examined in the review are derived from the discussion of the task of software development effort estimation in Section 2, and previous findings on the relative performance of model and judgement-based predictions are presented in Section 3. Section 5 provides concluding remarks about the implications of the findings of the review. 2. Software development effort estimation For the purpose of this review, I separate expert judgement and model-based effort estimates based on the type of mental process applied in the“quantification step”, i.e., the step where an understanding of the software development estimation problem is translated into a quantitative measure of the required effort. I define judgement-based effort estimates to be based on a tacit (intuition-based) quantification step, and model- based effort estimates to be based on a deliberate (mechanical) quantification step; see, for example, Hogarth (2001)for an elaboration of the meaning of these terms. The quantification step is the final step of the process, leading to an effort estimate for the total project or a project activity. If the final step is judge- mental, the process is categorized as judgement-based. If the final step is mechanical, the process is categorized as model-based. There will be a range of quite different estimation processes belonging to each of the catego- ries, i.e., neither expert judgement nor model-based', 'as model-based. There will be a range of quite different estimation processes belonging to each of the catego- ries, i.e., neither expert judgement nor model-based effort estimation should be considered simply as“one method”. When the outputs of two or more completed estimation processes are combined, we categorize the process as combination-based, and describe whether the combination step is judgemental or mechanical. The term“expert” in this paper is used to denote all individuals with competence in estimating software de- velopment effort. In most studies, the expert is a soft- ware development professional, but we also use the term “expert” to denote, for example, a student with previous experience in effort estimation and the development of software for the type of task under consideration. 2.1. Expert judgement-based effort estimation pro- cesses Most of the steps in the expert judgement-based effort estimation processes, e.g., the breaking down of the project into activities, may be explicit and can be 450 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', ""reviewed readily. The quantification steps, however, are based on intuition to a significant degree, and are seldom based on explicit, analytical argumentation. This assessment of the quantification steps as being based on intuition is indicated both by a lack of analytical argumentation and by the frequent use of phrases such as“I think that…” and “I feel that…”; see for example the transcribed estimation team discus- sions inJørgensen (2004b). Similar results are reported in the software cost estimation study inMukhopad- hyay, Vicinanza, and Prietula (1992): “… the verbal protocol contained little explicit information about the cognitive processes involved in selecting a source project [i.e., the selection of analogous projects as a basis for the estimate of the new project].” The poor understanding of the quantification step is also an in- dication that it is intuition-based. According toBrown and Siegler (1993), psychological research on real- world quantitative expert estimation“has not culmi- nated in any theory of estimation, not even in a coherent framework for thinking about the process”. 2.2. Model-based effort estimation processes There are many different types of software develop- ment effort estimation models available.Briand and Wieczorek (2002)categorize and describe many of these models. An example of a very simple“rule-of-thumb” estimation model is a model that contains, among other rules, the rule that a“small” program module with “high” complexity requires about 30 work-hours. How- ever, a program module's size and degree of com- plexity are typically not known with high precision at the time of the estimation, and are typically based on expert judgement. The example illustrates that model- based effort estimation processes may rely very much on expert judgement-based input. As a consequence, model outputs may also be biased towards overoptimism or be impacted by the presence of irrelevant information. More complex effort estimation models may be based on sophisticated analyses and dependencies between effort and other variables in sets of previously completed projects, and result in formulae of the following type: Effort ¼ a Sizeb⁎ Adjustment factor: The size variable can, for example, be a measure of the ‘size of functionality,’ derived from the require- ments specified by the client or the estimated number of ‘lines of code’ to be programmed. The adjustment factor is typically derived from a weighted sum of the answers to questions relating to the complexity of the development, project member skills, and the tools used to support the development process. The adjustment factor may also include the input of a productivity factor, i.e., a measure of the historical productivity of similar projects. Many estimation models assume that there are organization-independent and stable relationships be- tween the variables, e.g., that parametersa and b in the above formula are approximately the same for all soft- ware development projects. Other estimation models recommend that core relationships be calibrated to the situation in which they are used. The difference in model calibration to the organization in which the model is used may be an important factor for estimation accuracy, and important for our review. The assumptions that many models make regarding situation-independent core relationships between size and effort may be a major cause of the inaccuracy of the estimation models. There is evidence to support the view that models calibrated to a particular organization, e.g., through deriving the model from the organization's own historical data only, may lead to an improvement in the estimation accuracy. This evidence is provided byMurali and Sankar (1997) and Jeffery, Ruhe, and Wieczorek (2000), among other studies. To analyze how differences in the level of calibration to a particular organization affect the relative performance of models and expert judgement in the"", ""studies. To analyze how differences in the level of calibration to a particular organization affect the relative performance of models and expert judgement in the review, we use three categories of calibration level: • Low calibration (adjustment relative to a“nominal” project): The model assumes an organization- independent dependency between effort and other variables. The adjustment to the estimation situation at hand is done through standardized adjustment factors related to differences between the“nominal” (typical) project and the project to be estimated, e.g., add 20% to the total effort if the project applies a development method for the first time. No statistical analyses based on the organization's own historical project data are performed. Most commercial esti- mation models and several of the noncommercial estimation models are of this type. • Medium calibration (adjustment through the use of organization-specific productivity values): Models 451M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462"", ""in this category make assumptions similar to those in the low calibration category. The main difference is that some of the standardized adjustments relative to a“nominal” project are replaced with the use of organization-specific productivity values. • High calibration (estimation models derived from organization specific data only):M o d e l si nt h i s category are generated from a dataset of projects that have previously been completed in the organization in which the model is supposed to be applied or in organizations with similar types of projects. There are many possible approaches to generating the models, e.g., regression analysis, case-based reason- ing, or neural network development. 3. Prior research There are many studies on expert- and model-based judgement. In addition, there are numerous studies on related topics, such as intuition vs analysis, and tacit vs deliberate processes. In this section I have tried to present a set of representative results. 3.1. Clinical vs Statistical Prediction In 1954, Meehl published his so-called“disturb- ing little book” Clinical versus Statistical Predic- tion: A Theoretical Analysis and a Review of the Evidence(Meehl, 1954). In it, Meehl summarizes twenty empirical studies and finds that clinicians (who provide expert judgements) are usually out- performed by actuarial methods (statistical predic- tion models). Meehl (1986) states, based on an updated set of reviewed studies, that“When you are pushing 90 investigations, predicting everything from the outcomes of football games to the diagnosis of liver disease and when you can hardly come up with a half dozen studies showing even a weak tendency in favour of the clinician, it is time to draw a practical conclusion.” A more recent meta- analysis, extending the studies summarized by Meehl, is provided inGrove, Zald, Lebow, Snitz, and Nelson (2000). That study found that “… mechanical pre- dictions of human behaviors are equal or superior to clinical prediction methods for a wide range of circumstances.” Dawes, Faust, and Meehl (1989) emphasize the following two factors that underlie the superiority of statistical models: i) Models are consistent; the same input always leads to the same conclusion, while experts are inconsistent. ii) Models ensure that variables contribute to a conclusion based on their actual predictive power and relationship to the criterion of interest. Experts have problems in distinguishing between val id and invalid variables, due, among other things, to poor and misleading feedback about the accuracy of judgement. The importance of the two factors is also supported by a substantial amount of independent empirical evidence, e.g., by studies on the“the dilution ef- fect” in expert judgement (Wallner & Zimbelman, 2003). Transferring Meehl's recommendation“it is time to draw a practical conclusion” naïvely to the context of software effort estimation, we are exhorted to use models and to stop using expert judgement when estimating software development effort. There are, however, at least two issues that may make this type of conclusion premature in the context of software development: • The performance of an estimation model depends on the properties of the relationships it attempts to model. In the domain of software develop- ment effort estimation, the validity of basic model assumptions, e.g., the stability of an effort–size relationship (Dolado, 2001; Kitchenham, 1992), are contentious, and may have lower validity than the essential assumptions made when using models in other domains. In medicine, for example, the assumption of stable underlying (biology-based) relationships may be more plausible than in soft- ware development contexts where the technology, the types of software produced, and the production methods, change frequently. • Dawes et al. (1989)specify that a condition for a fair comparison is that both the model and the expert base their predictions on the same input data."", 'methods, change frequently. • Dawes et al. (1989)specify that a condition for a fair comparison is that both the model and the expert base their predictions on the same input data. This condition is typically not met in the field settings for software development effort estima- tion. In fact, to meet this condition we may have to remove some of the information typically used by the experts in field settings, i.e., create a situation that in many ways would be perceived asunfair, and deviate from the natural setting of effort estimation. 452 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', ""3.2. Contextual information It may be possible to include most contextual information in a model. When software effort estima- tion models typically choose to include only a few variables, and potentially not all variables are impor- tant, the reasons are of a practical nature: • a large number of variables can easily lead to overfitting and lower accuracy when there are small data sets to learn from; • models need to be simple if their users are to understand them; • the development of a proper model may be too complex or take too much effort; and • variables with the potential to become important are many, and in most cases are not actually important. For example, the most important input in software development effort estimation situations is a textual description of the requirements to be met by the software system, together with oral information col- lected at meetings with the clients. This textual and oral information contains a great deal of knowledge that it is scarcely practical to provide as an input to a model, e.g., highly specific information that enables the developers to understand the steps needed to perform the programming tasks or the importance of a particular requirement for a particular client. The aggregated and translated model version of this textual and oral information which is provided as an input to estimation models can hardly be said to be“the same data,” and our context may consequently be different from that assumed byDawes et al. (1989). Another example of the important contextual information typically pos- sessed by the experts but not necessarily easily trans- ferred to a model is very specific information (so-called “broken leg” cues) about the software developers allocated to the task. The experts may, for example, possess a lot of information about the differences in productivity among the developers, which may be huge, or may know that one of the developers has successfully solved a very similar task earlier. Howev- er, this additional information possessed by the experts does not always lead to a more accurate judgement. For example, the presence of information of lesser rele- vance may easily have strong, unwanted impacts on judgement-based software development effort esti- mates (Jørgensen & Sjøberg, 2004), and the total effect of more contextual information on the experts' judgements is not obvious. It may be of particular relevance for the review in this paper to examine previous studies on the per- formance of expert and model predictions in situations where the experts possess additional (contextual) information, i.e., comparisons which are closer to real-life situations in software development effort estimation. The search for studies of this type mainly produced forecasting studies. Several researchers in forecasting seem to question the generality of the finding that models are more accurate than experts. Lawrence and O'Connor (1996), for example, observe that many of the studies that report the superiority of model-based judgement seem to be based on an environment where the important variables are well- established, prespecified and not autocorrelated, and where there is little contextual information that only the expert possesses; i.e., that the results are based on environments that favour model-based judgement more than many real-life forecasting environments do. Findings suggesting th at there are forecasting situations that may benefit from expert judgement include: • judgement-based forecasts were more accurate than statistical models in situations that contained a substantial amount of contextual information (Goodwin, 2000; Webby & O'Connor, 1996). • judgement-based forecasts were better in unstable, changing situations, while the models performed better during periods of stability ( Sanders & Ritzman, 1991). • A combination of model- and expert-based judge- ment was frequently better than either alone (Blattberg & Hoch, 1990; Goodwin, 2000)."", 'better during periods of stability ( Sanders & Ritzman, 1991). • A combination of model- and expert-based judge- ment was frequently better than either alone (Blattberg & Hoch, 1990; Goodwin, 2000). However, there are also findings that indicate the opposite, e.g., that the inclusion of irrelevant informa- tion leads to the superiority of model-based judgement (Whitecotton, Sanders, & Norris, 1998). The existence of situations where the benefits of contextual informa- tion are large enough to compensate for judgemental inconsistency and improper weighting emphasize that a comparison of expert- and model-based effort estima- tion accuracy needs to take into account the amount and nature of the contextual information. 453M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', ""3.3. Expertise A limitation of many studies comparing expert judgement and models is that they are based on the average performance of a set of experts who are chosen more or less arbitrarily, and not, for example, on the performance of thebest experts. The value of studying the performance of university students in conducting a complex task in a domain where they have little experience is not always obvious. Not surprisingly, there are several authors that question many of the results on the basis of a lack of ecological validity; see, for example,Bolger and Wright (1994). Shanteau (1992)emphasizes that the characteristics of a task play an important role in the performance and learning of experts. Software development effort estimation has characteristics of both poor and good expert performance. While the characteristics“some errors expected” and “problem decomposable” may lead to good expert performance, “decisions about behaviour” and “unique task” may lead to poorer expert performance. It is consequently difficult to decide, based on Shanteau's work, how much experts will be able to learn and improve with increased experience in real-life software development effort estimation con- texts, i.e., how the level of estimation expertise is connected with the amount of experience. In most situations in which software development effort is estimated, there are several competing estimation models and several expert estimators to select from or to combine. The selection of the model and expert is typically expert judgement-based. Select- ing improper models or experts may lead to very inaccurate predictions, and hence, the process by which an estimation method is selected may be essential for this review.Hogarth (2005)makes a similar point when he examines the trade-off between biased, intuition- based judgements and the risk involved in selecting or executing analytical rules. Analytical errors are more likely when the analytical complexity, as perceived by the person selecting the rule, is high. So far, there has been no study in the context of software development on experts' ability to select proper models and experts, and only a few studies on formal strategies for selecting estimation models and experts; see, e.g.,Shepperd and Kadoda (2001). An important issue for the review is, consequently, whether the risk of selecting very inac- curate estimation methods is higher when selecting a model or when selecting an expert. It may, for example, be the case that complex effort estimation models are sometimes the most accurate, but are also connected with the most inaccurate estimates, due to overfitting to one type of situation. Finally, expertise in using estimation models and expertise in applying expert judgement may have different side-effects regarding the actual work per- formed. There may, for example, be a stronger effect of “self-fulfilling prophecies” when applying expertise in making judgements compared to expertise in using models; i.e., people's ownership and commitment related to expert judgement may be stronger than that to model output. We were unable to find any studies on this side-effect of using different types of models for effort estimation. However, there are related findings, e.g., findings on the positive effect of effort estimate accountability on estimation accuracy in software development contexts (Lederer & Prasad, 2000). 4. The review 4.1. The review process The review process aims to identify and analyse empirical studies that compare expert judgement-based and model-based software development effort estima- tion. The identification of relevant studies is based on an examination of software development effort esti- mation journal papers identified in a recent review (Jørgensen & Shepperd, 2007). That review currently constitutes, as far as we know, the most complete list of journal papers on software development effort estima- tion, and can be accessed atwww.simula.no\\BESTweb."", 'constitutes, as far as we know, the most complete list of journal papers on software development effort estima- tion, and can be accessed atwww.simula.no\\BESTweb. Potentially relevant papers presented at conferences were identified through a manual inspection of the studies resulting from a search in the library database Inspec for papers including the terms (‘effort estima- tion’ or ‘cost estimation’) and‘software development’ (last search conducted February 2006). In spite of this fairly comprehensive search for relevant papers, there may still be missing papers which are relevant. As an illustration, when we contacted the authors of the reviewed papers, one of them made us aware of a relevant paper not found by our search. In total, seventeen relevant papers were identified. One of the papers was excluded, namely Pengelly (1995), due to incomplete information about how the 454 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', ""estimates were derived, which left sixteen papers for review. The sixteen studies are reviewed with respect to important contextual factors, i.e., the factors identified in the discussion in Sections 2 and 3. The main design factors and results reviewed for each study are as follows: Design factors • Study design • Estimation method selection process • Estimation models • Calibration level • Model use expertise and degree of mechanical use of model • Expert judgement process • Expert judgement estimation expertise • Possible motivational biases in estimation situation • Estimation input • Contextual information • Estimation complexity • Fairness limitations • Other design issues Results: • Accuracy • Variance • Other results The factors are explained and applied in Appendix A. Sixteen is a small number of studies when at- tempting to analyze how the numerous design factors potentially affect the estimation accuracy of models and expert judgements differently. In addition, since none of the reviewed studies were explicitly designed to identifywhen we could expect expert judgement or models to perform better, much information about several of the factors is missing. When our interpreta- tion of factor values is based, to a large extent, on a qualified guess, we have described this interpretation as “probable”. We sent the results of our review to the authors of each of the sixteen studies and urged them to inform us of any incorrect classifications and inter- pretations regarding their own study. Authors repre- senting thirteen of the sixteen papers responded. The authors' responses led only to minor corrections. The main evaluation measure in this review is estimation accuracy, i.e., the deviation between the estimated and actual effort. This should not be taken to imply that we think that other measures, e.g., flexibility in the use of the method, the cost of the estimation process, or the ease of understanding the basis of the estimates, are unimportant. The reasons for not emphasizing these factors are that they deserve reviews on their own and (the practical reason) that none of the studies reported criteria for any comparison other than accuracy. 4.2. Review limitations The review work revealed several factors limiting the validity of the results of the studies, including the following: • Lack of information about the expert judgement- based process. Most studies do not describe the expert judgement-based estimation process. This means that while there are many different models evaluated, expert judgement is lumped into one category. This is particularly unfortunate, given the potentially large differences between unstructured, unaided expert judgement and expert judgement supported by a well-structured estimation process, detailed checklists, proper feedback, and historical data. • Different estimation methods were used on different estimation tasks in field studies.The study reported in Grimstad and Jørgensen (2006)exemplifies how a comparison of model-based and expert judge- ment-based estimation in field settings can be biased by the use of expert judgement in situations where it is not possible to use estimation models. A straightforward comparison of the accuracy of effort estimates for projects that applied both estimation models and expert judgement yielded the result that using models led to significantly more accurate estimates. However, it was also observed that the estimation model was seldom used at an early stage of the project, and was never used when the estimator had no experience with similar projects. Both these situations are, however, connected with a higher than average estimation complexity. When only comparing estimation tasks with similar estimation complexities, model-based and expert judgement-based estimates were found to be accurate to the same degree. Unfortunately, 455M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462"", ""none of the other field studies in our review perform this kind of analysis. The results reported in Grimstad and Jørgensen (2006)suggest that it is likely that the expert judgement-based performance is better, in actual fact, than is reported in the reviewed field studies, but more evidence is needed to confirm our conjecture. • Imprecise use of terminology.Few of the reviewed studies reported that steps had been taken to ensure that the term ‘estimate’ was used with the same meaning when using models and expert judgement. If models are more likely to provide the most likely effort and experts are more likely to provide the planned or budgeted effort, this may mean that expert judgement-based estimates are, in actual fact, less accurate than is reported in situations where a tendency towards overoptimism is present. However, the overall effect on the results of the review of using estimation terminology imprecisely is not well understood. • Different “loss functions” of models and experts. None of the reviewed studies analyzed the“loss functions” of the estimation methods, and it is dif- ficult to draw conclusions about the impact of this issue from the results of our review. If expert judgements are, consciously or unconsciously, based on more appropriate and flexible loss functions than the loss function of the estimation models, the reported accuracy results may provide an overly negative view of the experts' performance. For example, while most software effort estimation models are based on the assumption that over- and underestimation are equally bad, judgement-based effort estimates may be based on an assumption that effort estimates that are too high would lead to inefficient development work, and should be avoided more than estimates that are too low. • Estimation accuracy affected by effort manage- ment. A strong belief in an effort estimate may lead to a stronger belief in the plan that is made and a greater commitment to following the plan. If this belief depends on the estimate's correspondence with an expert's gut feeling regarding the correct- ness of the estimate, the results may be biased in favour of the expert. Consequently, it may be the ability to better work to the estimate or plan that leads to a better expert judgement performance, and not a stronger skill in estimating accurately. • Experts estimating in groups. Software companies frequently assign the task of estimating effort to groups. This may be the rule rather than the excep- tion when the projects are large. However, only one of the reviewed studies enabled a comparison of the output of models with the output from a group of experts. • Unpublished results. The effect of unpublished results is unknown. It may, for example, be the case that several of the studies where self-developed estimation models are evaluated and are found to yield less accurate estimates than the experts are not published. These limitations mean that the results of the review should be interpreted carefully, and that better-designed studies are needed to deliver robust results about when to apply model-based and when to apply expert judgement-based effort estimates. Such studies should include proper descriptions of all the design factors outlined in Section 4.1, and aim at a better understand- ing of when and why one method is more accurate in one particular context. In spite of the strong limitations of the reviewed studies, I believe that it is worthwhile to summarize the available evidence. To know the current state of our knowledge is of value, even if the review should show that our knowledge is modest due to study design limitations. 4.3. Results In this section I try to answer the research questions stated in the Introduction: • Did models or expert judgement lead to the most accurate estimates? (Section 4.3.1) • When did the estimation models, the expert judgements, and the combination of these two approaches each lead to the most accurate esti-"", 'accurate estimates? (Section 4.3.1) • When did the estimation models, the expert judgements, and the combination of these two approaches each lead to the most accurate esti- mates? (Section 4.3.2) Details of the review data are provided as Appendix A. 4.3.1. Which estimation method yields the most accurate effort estimates? The reviewed studies report the accuracy results differently. Hence, it is not possible to summarize 456 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', 'the results as simply“method X leads, on average, to an A% improvement in estimation accuracy”. Instead, we have described the accuracy results as reported by the study itself in Appendix A, and have produced in Table 1a simple categorization of whether a study reported that the models or the experts had the best estimation accuracy. The comparison is made relative to themost accurate, average, and least accurate performance of the models and the experts. Not all studies report data that allow all variants of compar- isons; e.g., most studies report only the average ac- curacy of the experts. When a study evaluates only one estimation model or expert, the accuracy of that model or expert is categorized as the average model or expert accuracy inTable 1. The studies are sorted chronologically, i.e., Study 1 was conducted before Study 2, etc. Table 1 shows, for example, that there were only two studies (Studies 2 and 12) that enabled a comparison of the most accurate model and the most accurate expert, and that both of these studies found that the most accurate expert was more accurate than the most accurate model. The principal finding that may be derived from Table 1is that the review does not support the view that we should replace expert judgement with models in software development effort estimation situations. On the other hand, neither does it support the view that software development effort estimation models are useless. A comparison of the average accuracy of the models with the average accuracy of the experts shows that ten studies found increased accuracy with the use of expert judgement and six with the use of estimation models. The unit inTable 1 is the study. The studies vary considerably, however, in the number of observations included. This means that, although Study 1 has only 14 observations and Study 9 has 140, they both have the same weight inTable 1. To test whether a change of study unit would make a difference, we weighted the estimation accuracy of the twelve studies reporting the MAPE (Studies 1, 2, 6, 7, 8, 9, 10, 11, 12, 13, 15, and 16) in accordance with the number of observations included in the study. This resulted in a weighted MAPE of the experts which was slightly better than that of the models (99% vs 107%). The four studies which were not part of this analysis (Studies 3, 4, 5, and 14) included two studies in favor of models and two in favor of expert judgement. The high values of the weighted MAPEs of both the experts and the models are largely due to a few laboratory studies with a large number of observations and lacking most of the infor- mation available in many real-life estimation contexts.. Removing the laboratory study with the most inaccu- rate estimates (Study 2), for example, reduced the weighted MAPE to 78% for both the expert and model- based effort estimates. A typical value of the MAPE for effort estimation in field settings is, as reported in the Introduction, 30–40%. The field studies (Studies 3, 4, 5, 8, 10, and 16) have the most observations, and may have the greatest external validity. Of the field studies, three are in favour of using models and three in favour of using expert judgement; none of them reported large differences in accuracy related to the use of models and expert judgement in estimating software development effort, i.e., the general result here is that there were no large difference between models and experts. Only the three smallest field studies reported the MAPE, and for this reason, we have not included the weighted MAPE for the field studies alone. Table 1 Experts vs models Expert more accurate Model more accurate Most accurate model vs most accurate expert Studies 2 and 12 No studies Most accurate model vs average accuracy of experts Study 6 Studies 1, 2, 7, 9, 11, 12, and 14 Most accurate model vs least accurate expert No studies Studies 2 and 12 Average accuracy of models vs most accurate expert Studies 2 and 12 No studies', 'Most accurate model vs least accurate expert No studies Studies 2 and 12 Average accuracy of models vs most accurate expert Studies 2 and 12 No studies Average accuracy of models vs average accuracy of experts Studies 1, 2, 3, 5, 6, 7, 9, 10, 11, and 13 Studies 4, 8, 12, 14, 15, and 16 Average accuracy of models vs least accurate expert No studies Studies 2 and 12 Least accurate model vs most accurate expert Studies 2 and 12 No studies Least accurate model vs average accuracy of experts Studies 1, 2, 6, 7, 9, and 11 Studies 12, and 14 Least accurate model vs least accurate expert No studies Studies 2 and 12 457M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', 'A possible objection to the results inTable 1is that the models are not mechanically used, i.e., the use is better described as“expert judgement in disguise”. If this is the case, the review merely compares one type of expert judgement with another. This possibility is difficult to exclude for some of the reviewed studies. Eight of the studies (Studies 2, 6, 9, 10, 11, 12, 14, and 15), however, describe a rather mechanical use of the models, i.e., the model users had limited or no oppor- tunity to adjust the input to yield a model output in accordance with their“gut feeling”. A comparison of the average accuracy of the experts and models for that subset of studies shows that the expert judgement led to more accurate effort estimates in five of these eight studies, i.e., the degree of mechanical use of the models seems not to explain the lack of model superiority in our review. The model users had previous experience in the use of models in all of these eight studies. In eight of the studies, the model builder and evaluators are the same (Studies 6, 7, 9, 10, 11, 12, 13, and 14). In these studies, the vested interest of showing benefit from the model may be higher than in the other studies. An analysis of the results shows that in spite of this vested interest, the average accuracy of the experts was better than that of the self-developed models in five out of the eight studies. Interestingly, the recent studies are more frequently in favour of using models than the early studies. However, it is too early to see whether this is a trend due to estimation models having improved over the years or is only due to a random variation in the study design and the types of models evaluated. Assume that we were able to select the best model. On this assumption,Table 1suggests that the use of this model is likely to lead to more accurate estimates than the judgements of either the average or the least accurate experts, but not more accurate estimates than the judgements of the best expert. Now assume that we are unskilled in model selection and select the least accurate model. In this case,Table 1suggests that only the judgements of the least accurate experts will be less accurate than the output of this model. The ability to select the best models has been little studied in the context of software development and may deserve more attention. The results reported inMacDonell and Shepperd (2003)suggest that using formal rules (e.g., the rule-based induction algorithms) to select the best model does not yield the desired result. It is of equal importance to select good experts, since the least accurate expert performed worse than the models in each study. Research results suggest that it is possible, to some extent, to select among the best estimation experts by emphasizing relevant expe- rience from very similar projects (Jørgensen, 2004b; Jørgensen & Sjøberg, 2002), e.g., based on whether the estimators recall close analogies or not. Another way to identify the most accurate experts is to use their previous estimation accuracies to predict their future accuracy. InJørgensen, Faugli, and Gruschke (2007)it is reported that, among twenty experienced software professionals with similar skill levels and backgrounds, the correlation between the estimation accuracy of previous and future programming tasks was 0.40, and that using the previous estimation errors to predict the most overoptimistic estimator (out of two) for future tasks would yield a 68% success rate. An evaluation of effort estimates combining the inputs from experts and models is included in only four of the studies (Studies 1, 12, 13 and 14). All studies except Study 1 combined expert judgement-based esti- mates with estimates from models with a high levels of calibration. Study 1 evaluated the judgemental combi- nation of expert judgement and two models with a low level of calibration. In that study, the combined estimate was as accurate as the best model and slightly better than', ""nation of expert judgement and two models with a low level of calibration. In that study, the combined estimate was as accurate as the best model and slightly better than the expert judgement-based estimate. In Study 12, the experts judgementally combined the models' and their own judgement-based effort estimates. This combina- tion led to an improvement in accuracy compared to the use of either models or expert judgement alone. Study 13 found that expert judgement-based effort estimates were slightly better than those based on a mechanical combi- nation of estimation methods. Study 14 found that expert judgement, regression analysis-based models, and case- based reasoning-based models complemented each other well, i.e., when one method was not very accu- rate, it was likely that at least one of the other models was significantly more accurate. A simple average of the three methods improved the accuracy compared to the best individual method, i.e., the regression-based method. The details of the results for the combination- based estimates are included in Appendix A. 4.3.2. When to use expert judgement and models Table 2 compares the average accuracy of the model-based estimates with the average accuracy of 458 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462"", ""the expert judgement-based estimates for each study relative to the model calibration levels: low, medium and high, as described in Section 2.2. Some studies provide“mixed evidence”, e.g., Study 2 found that one model with a low level of calibration was more accurate, and another with the same level of cali- bration was less accurate, than the average accuracy of the experts. Note that some of the studies do not report enough information for us to decide on the calibration level of the models, and so are not in- cluded inTable 2. When the level of calibration is not reported, we only reported our assessment (quali- fied guess) inTable 2 when this assessment was confirmed by one of the authors of the paper re- porting the study. One study may provide more than one result. Table 2 suggests a weak connection between how well models perform relative to experts and the level of model calibration, i.e., models should be calibrated to the situation in which they are used to compete with expert judgement. The studies which provide counter- evidence of the connection between the calibration level and performance are Studies 2 and 14. A discus- sion with the author of Study 14 suggests that a pos- sible reason for the model's performing well in spite of the low calibration may have been that the set of projects that led to the construction of the estimation model was similar to the set of projects on which the model was applied, i.e., that the model was reasonably well-calibrated to the organizational context“by acci- dent”.T h e“mixed evidence” of the models with a low level of calibration in Study 2 is caused mainly by one expert who provided extremely inaccurate estimates, which does not provide strong counterevidence for the proposed connection. Interestingly,Table 2 sug- gests that the proportion of studies evaluating models with high calibration is higher for the most recent studies, i.e., there seems to have been a shift from general estimation models towards more situation- tailored models. This may explain the trend of im- proved model accuracy over the years that is suggested by Table 1. The level of contextual information, i.e., the amount of information possessed only by the experts, was derived from the study design description. The authors of the papers describing the study were given the opportunity to correct our assessment of the contextual information.Table 3summarizes this information and compares the average accuracy of the models with the average accuracy of the experts for each study. As can be seen, the majority of the studies were based on providing different inputs to the experts than to the models, which is what actually happens in real- life software development contexts. Only four studies provided the same information to the models and the experts. Hence, it is difficult to draw conclusions about the importance of contextual information for the relative estimation performance of experts and models based onTable 3alone. It is interesting to note that in three of the four studies the experts were more accurate than the models, even when they possessed the same information. The importance of contextual information for the accuracy of the expert judgement-based effort estimates may be better illustrated by a comparison of the average accuracy (MAPE) of expert estimation-based effort estimates in the studies where the experts did not have contextual information (Studies 2, 6, 11 and 12), and the subset of the other studies that reported the MAPE (Studies 7, 8, 9, 10, 13, 14, 15, and 16). When the experts were given the same input as the models, the average MAPE is 157%. When the experts are given additional contextual information, the average MAPE is 36%. The two groups of studies may not be completely comparable, i.e., there may be differences in the esti- mation complexity, but the big difference in accuracy nevertheless suggests that the performance of the experts"", 'comparable, i.e., there may be differences in the esti- mation complexity, but the big difference in accuracy nevertheless suggests that the performance of the experts improves substantially with contextual information. Few of the studies report results regarding the accuracy by type of estimation task. In fact, only two Table 2 Evidence on the relationship between accuracy and the level of model calibration Low calibration Medium calibration High calibration The model is less accurate than the average expert Studies 1, 5, 6, and 7 Study 9 Studies 6, and 10 The model is more accurate than the average expert Study 14 Studies 8, and 15 Studies 9, 12, 14, and 16 “Mixed evidence” Study 2 No studies Studies 7, 11, and 13 459M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', ""studies (Studies 3 and 8) report this type of infor- mation, and then only related to the size of the projects to be estimated, stating that larger projects are typically more difficult to estimate. The results of these two studies imply that the main benefit of estimation models is to avoid large overruns in situa- tions known to induce a strong degree of overopti- mism. This evidence is weak at present, but fits with common sense, which indicates that models are less affected by wishful thinking than software profes- sionals are. 5. Concluding remarks In the reviewed studies, the models failed to systematically perform better than the experts when estimating the effort required to complete software development tasks. Possible reasons for this include: • The experts have natural advantages in that they typically possess more information and are more flexible in how the information (or lack of informa- tion) is processed. • It may be difficult to build accurate software devel- opment effort estimation models. In particular, the lack of stable relationships and the use of small learning data sets may easily lead to models being overfitted to the available data. The models' ability to weight variables more cor- rectly, to reduce biases, and to produce consistent estimates may consequently have been insufficient to compensate for the low quality of the models and their inability to use all of the relevant contextual informa- tion. The software development community is, conse- quently, still in a position where the evidence supports neither a replacement of models with expert judgement, nor a replacement of expert judgement with models. If, as suggested inMacDonell and Shepperd (2003), there is a high degree of independence between es- timates based on common effort estimation models and expert judgement, and it is difficult to devise rules for selecting the most accurate estimation method, the solution seems to be to use a combination of models and experts. Based on the modest evidence to date, two con- ditions for producing more accurate expert judgement- based effort estimates seem to be that the models are not calibrated to the organization using them, and that the experts possess important contextual information not included in the formal models and apply it efficiently. The use of models, either alone or in combination with expert judgement, may be particu- larly useful when i) there are situational biases that are believed to lead to a strong bias towards overoptimism; ii) the amount of contextual information possessed by the experts is low; and iii) the models are calibrated to the organization using them. Two of the reviewed studies evaluated a mechanical combination, and two studies a judgemental combination, of expert judge- ment and models. The results from these four studies suggest that combined estimates lead to accuracy levels similar to the best of the other estimation methods, regardless of type of combination. So far, there have been two different types of responses to our findings. Most researchers outside the software engineering community seem to find it surprising that the models are not better than the experts, while most software engineering researchers and practitioners seem to find it surprising that the experts would not be even better in comparison with the models. Hopefully, our results will lead to more studies in domains similar to software development, leading to a better understanding of when to use a model and when to use expert judgement. There are still many important unanswered questions and claims with little or no evidence. Acknowledgements Thanks to Scott Armstrong, Fred Collopy, Jason Dana, Robin Dawes, Robin Hogarth, and Michael Roy for useful comments on drafts of this paper. Table 3 Evidence on the relationship between accuracy and the existence of contextual information Same information given to models and expert Experts provided with more information than the models The model is less"", 'contextual information Same information given to models and expert Experts provided with more information than the models The model is less accurate than the average expert Studies 2, 6, and 11 Studies 1, 3, 5, 7, 9, 10, and 13 The model is more accurate than the average expert Study 12 Studies 4, 8, 14, 15, and 16 460 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462', 'Appendix A. Review of the studies Appendix A is available online at http://www. forecasters.org/ijf/data.htm. References Anda, B., Benestad, H. C., & Hove, S. E. (2005). A multiple-case study of effort estimation based on use case points.ISESE 2005 (Fourth International Symposium on Empirical Software Engineering) (pp. 407−416). Noosa, Australia: IEEE Computer Society. Atkinson, K., & Shepperd, M. (1994). Using function points to find cost analogies.European software cost modelling meeting, Ivrea, Italy. Bergeron, F., & St-Arnaud, J. Y . (1992). Estimation of information systems development efforts: A pilot study. Information and Management, 22(4), 239−254. Blattberg, R. C., & Hoch, S. J. (1990). Database models and managerial intuition: 50% model+50% manager.Management Science, 36(8), 887−899. Bolger, F., & Wright, G. (1994). Assessing the quality of expert judge- ment: Issues and analysis.Decision Support Systems, 11(1), 1−24. Briand, L. C., & Wieczorek, I. (2002). Resource estimation in software engineering. In J. J. Marcinak (Ed.),Encyclopedia of software engineering(pp. 1160−1196). New York: John Wiley & Sons. Brown, N. R., & Siegler, R. S. (1993). Metrics and mappings: A framework for understanding real-world quantitative estimation. Psychological Review, 100(3), 511−534. Dawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus actuarial judgement.Science, 243, 1668−1674. Dolado, J. J. (2001). On the problem of the software cost function. Information and Software Technology, 43(1), 61−72. Goodwin, P. (2000). Improving the voluntary integration of statistical forecasts and judgement.International Journal of Forecasting, 16(1), 85−99. Grimstad, S., & Jørgensen, M. (2006). A Framework for the Analy- sis of Software Cost. International Symposium on Empirical Software Engineering(pp. 58−65). ACM Press. Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C. (2000). Clinical versus mechanical prediction: A meta-analysis. Psychological Assessment, 12(1), 19−30. Heemstra, F. J., & Kusters, R. J. (1991). Function point analysis: Evaluation of a software cost estimation model.European Journal of Information Systems, 1(4), 223−237. Hihn, J., & Habib-Agahi, H. (1991). Cost estimation of software intensive projects: A survey of current practices.International conference on software engineering, Austin, TX, USA(pp. 276−287). Los Alamitos, CA, USA: IEEE Comput. Soc. Press. Hogarth, R. M. (2001).Educating intuition.Chicago: University of Chicago Press. Hogarth, R. M. (2005). Deciding analytically or trusting your intuition? The advantages and disadvantages of analytic and intuitive thought. In T. Betsch & S. Haberstroh (Eds.),Mahwah, the routines of decision making(pp. 67−82). NJ: Erlbaum. Jeffery, D. R., Ruhe, M., & Wieczorek, I. (2000). A comparative study of two software development cost modeling techniques using multi-organizational and company-specific data.Informa- tion and Software Technology, 42(14), 1009−1016. Jørgensen, M. (1997). An empirical evaluation of the MkII FPA estimation model.Norwegian Informatics Conference, Voss, Nor- way (pp. 7−18). Oslo: Tapir. Jørgensen, M. (2004a). A review of studies on expert estimation of software development effort.Journal of Systems and Software, 70(1–2), 37−60. Jørgensen, M. (2004b). Top-down and bottom-up expert estimation of software development effort. Information and Software Technology, 46(1), 3−16. Jørgensen, M., Faugli, B., & Gruschke, T. (2007). Characteristics of software engineers with optimistic predictions. Journal of Systems and Software, to appear. Jørgensen, M., & Shepperd, M. (2007). A systematic review of software development cost estimation studies. IEEE Transac- tions on Software Engineering, 33(1), 33−53. Jørgensen, M., & Sjøberg, D. I. K. (2002). Impact of experience on maintenance skills. Journal of Software Maintenance and Evolution: Research and Practice, 14(2), 123−146. Jørgensen, M., & Sjøberg, D. I. K. (2004). The impact of customer', ""maintenance skills. Journal of Software Maintenance and Evolution: Research and Practice, 14(2), 123−146. Jørgensen, M., & Sjøberg, D. I. K. (2004). The impact of customer expectation on software development effort estimates.Interna- tional Journal of Project Management, 22, 317−325. Jørgensen, M., Teigen, K. H., & Moløkken, K. (2004). Better sure than safe? Over-confidence in judgement based software development effort prediction intervals.Journal of Systems and Software, 70(1–2), 79−93. Kitchenham, B. (1992). Empirical studies of assumptions that underlie software cost-estimation models. Information and Software Technology, 34(4), 211−218. Kitchenham, B., Pfleeger, S. L., McColl, B., & Eagan, B. (2002). An empirical study of maintenance and development estimation accuracy.Journal of Systems and Software, 64(1), 57−77. Kusters, R. J., van Genuchten, M. J. I., & Heemstra, F. J. (1990). Are software cost-estimation models accurate? Information and Software Technology, 32(3), 187−190. Lawrence, M., & O'Connor, M. (1996). judgement or models: The importance of task differences.Omega, International Journal of Management Science, 24(3), 245−254. Lederer, A. L., & Prasad, J. (2000). Software management and cost estimating error.Journal of Systems and Software, 50(1), 33−42. MacDonell, S. G., & Shepperd, M. J. (2003). Combining techniques to optimize effort predictions in software project management. Journal of Systems and Software, 66(2), 91−98. Meehl, P. E. (1954). Clinical versus statistical prediction: A theoretical analysis and a review of the evidence.Minneapolis, US: University of Minnesota Press. Meehl, P. E. (1986). Causes and effects of my disturbing little book. Journal of Personality Assessment, 50, 370−375. Moløkken-Østvold, K., & Jørgensen, M. (2003). A review of surveys on software effort estimation. International Symposium on Empirical Software Engineering (ISESE 2003)(pp. 223−230). Rome, Italy: IEEE Computer Society. Moores, T. T., & Edwards, J. S. (1992). Could large UK corporations and computing companies use software cost estimating tools?— A survey. European Journal of Information Systems , 1(5), 311−319. 461M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462"", ""Mukhopadhyay, T., Vicinanza, S. S., & Prietula, M. J. (1992). Examining the feasibility of a case-based reasoning model for software effort estimation.MIS Quarterly, 16(2), 155−171. Murali, C. S., & Sankar, C. S. (1997). Issues in estimating real-time data communications software projects.Information and Soft- ware Technology, 39(6), 399−402. Myrtveit, I., & Stensrud, E. (1999). A controlled experiment to assess the benefits of estimating with analogy and regression models. IEEE Transactions on Software Engineering, 25(4), 510−525. Nelson, E. A. (1966).Management handbook for the estimation of computer programming costs. AD-A648750, Systems Develop- ment Corp. Niessink, F., & van Vliet, H. (1997). Predicting maintenance effort with function points. International Conference on Software Maintenance, Bari, Italy(pp. 32−39). Los Alamitos, CA, USA: IEEE Computer Society. Ohlsson, N., Wohlin, C., & Regnell, B. (1998). A project effort estimation study.Information and Software Technology, 40(14), 831−839. Pengelly, A. (1995). Performance of effort estimating techniques in current development environments.Software Engineering Jour- nal, 10(5), 162−170. Ribu, K. (2001).Estimating object-oriented software projects with uses cases. Informatics, University of Oslo. MSc thesis. Sanders, D. E., & Ritzman, L. P. (1991). On knowing when to switch from quantitative to judgemental forecasts.International Journal of Forecasting, 11(6), 27−37. Shanteau, J. (1992). Competence in experts: The role of task characteristics. Organizational Behaviour and Human Decision Processes, 53(2), 252−266. Shepperd, M., & Kadoda, G. (2001). Comparing software prediction techniques using simulation. IEEE Transactions on Software Engineering, 27(11), 1014−1022. Vicinanza, S. S., Mukhopadhyay, T., & Prietula, M. (1991). Software effort estimation: An exploratory study of expert performance. Information Systems Research, 2(4), 243−262. Walkerden, F., & Jeffery, R. (1999). An empirical study of analogy- based software effort estimation.Empirical Software Engineer- ing, 4(2), 135−158. Wallner, W. S., & Zimbelman, M. F. (2003). A cognitive footprint in archival data: Generalizing the dilution effort from laboratory to field settings. Organizational Behavior and Human Decision Processes, 91, 254−268. Webby, R. G., & O'Connor, M. J. (1996). judgemental and statistical time series forecasting: A review of the literature.International Journal of Forecasting, 12(1), 91−118. Whitecotton, S. M., Sanders, D. E., & Norris, K. B. (1998). Improving predictive accuracy with a combination of human intuition and mechanical decision aids.Organizational Behaviour and Human Decision Processes, 76(3), 325−348. Magne Jørgensen received the Diplom Ingeneur degree in Wirtschaftswissenschaf- ten from the University of Karlsruhe, Germany, in 1988 and the Dr. Scient. degree in informatics from the University of Oslo, Norway in 1994. He has about 10 years industry experience as software developer, project leader and manager. He is now professor in software engineer- ing at University of Oslo and member of the software engineering research group of Simula Research Laboratory in Oslo, Norway with many international publications on software cost forecasting. Magne Jørgensen has supported software cost forecasting improvement work and been responsible for cost forecasting courses in several software companies. 462 M. Jørgensen / International Journal of Forecasting 23 (2007) 449–462""]","['FORECASTING  OF  SOFTWARE  DEVELOPMENT  WORK  EFFORT:  EVIDENCE  ON  EXPERT  JUDGEMENT  AND  FORMAL  MODELS • The review does not support the view  that we should replace expertjudgement  with models in software development ef- fort estimation situations.  • On the other hand, neither does it support  the view that software development ef - fort estimation models are useless. • The average accuracy of the experts was  better than that of the self-developed  models in five out of the eight studies. • None of the analyzed studies reported  large differences in accuracy related to  the use of models and expert judgement  in estimating software development ef - fort. That is, there is no large difference  between models and experts. • Using formal rules (e.g., the rule-based  induction algorithms) to select the best  model does not yield the desired result. • Among twenty experienced software pro- fessionals with similar skill levels and ba - ckgrounds, the correlation between the  estimation accuracy of previous and futu- re programming tasks was 0.40, and that  using the previous estimation errors to  predict the most overoptimistic estimator  (out of two) for future tasks would yield a  68% success rate. • When experts and models judgment are  combined, an improvement in accuracy is  observed compared to the use of either  models or expert judgement alone. • One study (Study 13) found that expert  judgement-based effort estimates were  slightly better than those based on a me - chanical combination of estimation me - thods. • One study (Study 14) found that expert  judgement, regression analysis-based  models, and case-based reasoning-based  models complemented each other well,  i.e., when one method was not very ac - curate, it was likely that at least one of  the other models was significantly more  accurate. • In three studies the experts were more  accurate than the models, even when  they possessed the same information. • The main benefit of estimation models  is to avoid large overruns in situations  known to induce a strong degree of ove - roptimism. FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Magne Jørgensen, Forecasting of software development work effort: Evidence on expert judgement and formal models, International Journal of Forecasting, Volume 23, Issue  3, July–September 2007, Pages 449 462, ISSN 0169 2070, http://dx.doi.org/10.1016/j.ijforecast.2007.05.008. Keywords: Software productivity Who is this briefing for? Software engineers practitioners  who want to make decisions  about software development  estimation based on scientific  evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic  review conducted by Jørgensen. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg']","**Title: Balancing Expert Judgement and Formal Models in Software Development Effort Estimation**

**Introduction:**
This evidence briefing summarizes findings from a review of studies examining the effectiveness of expert judgement versus formal models in estimating software development work effort. Given the persistent challenges in accurate effort estimation, understanding the strengths and weaknesses of these approaches is crucial for practitioners in software engineering.

**Core Findings:**
1. **Expert Judgement vs. Formal Models:** The review of sixteen studies indicated that expert judgement-based estimates were more accurate than model-based estimates in ten of those studies. This suggests that experts often leverage contextual knowledge that formal models may overlook, leading to better estimates in many cases.

2. **Calibration of Models:** The accuracy of model-based estimates was significantly influenced by the level of calibration to the specific organization. Models that are not tailored to the organization's context often performed poorly. High-calibration models, derived from an organization’s historical data, tended to yield better estimates.

3. **Combination Approaches:** The review found that methods combining expert judgement and models yielded accuracy rates comparable to the best individual methods. This suggests that leveraging both approaches can mitigate the weaknesses inherent in relying solely on one method.

4. **Contextual Information:** Experts often have access to critical contextual information that formal models do not account for. This includes insights about team dynamics, project complexity, and unique client requirements which can significantly enhance estimation accuracy.

5. **Situational Biases:** Models may be particularly beneficial in environments prone to overoptimism, where expert judgement might lead to inflated estimates. In such cases, models can provide a more grounded perspective.

6. **Implications for Practice:** Practitioners should consider the context of their projects when choosing estimation methods. For projects with rich contextual information, expert judgement may be preferable. Conversely, in situations where bias is a concern, or contextual information is lacking, formal models may provide a more reliable alternative.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and decision-makers involved in project estimation and planning.

**Where the findings come from?**
The findings are derived from a review of empirical studies conducted by Magne Jørgensen, published in the *International Journal of Forecasting* (2007).

**What is included in this briefing?**
The briefing includes an overview of the comparative accuracy of expert judgement and formal models, the impact of model calibration, the effectiveness of combined approaches, and the role of contextual information in estimation accuracy.

**What is NOT included in this briefing?**
This briefing does not delve into the detailed statistical methodologies of the reviewed studies or provide raw data from the studies themselves.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the original research:**
Jørgensen, M. (2007). Forecasting of software development work effort: Evidence on expert judgement and formal models. *International Journal of Forecasting*, 23(4), 449-462. DOI: [10.1016/j.ijforecast.2007.05.008](https://doi.org/10.1016/j.ijforecast.2007.05.008)"
"['On the generation of requirements speciﬁcations from software engineering models: A systematic literature review Joaquín Nicolás*, Ambrosio Toval Software Engineering Research Group, Departamento de Informática y Sistemas, Universidad de Murcia, Campus de Espinardo, 30071 Murcia, Spain article info Article history: Received 29 October 2008 Received in revised form 27 March 2009 Accepted 1 April 2009 Available online 9 April 2009 Keywords: Speciﬁcation generation from software engineering model Textual requirements generation from software engineering model Requirements document generation from software engineering model Systematic literature review abstract System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and deﬁne requirements in precise detail for system developers. The beneﬁts of both lists of textual requirements (usually written in natural language) and software engineering models (usually speciﬁed in graphical form) can be brought together by combining the two approaches in the speciﬁcation of system and software require- ments documents. If, moreover, textual requirements are generated from models in an automatic or clo- sely monitored form, the effort of specifying those requirements is reduced and the completeness of the speciﬁcation and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements speciﬁcations from software engineering models. /C2112009 Elsevier B.V. All rights reserved. Contents 1. Introduction . . . ..................................................................................................... 1292 2. Planning the systematic literature review . . . . . . . . . . . . . . . . ................................................................ 1292 2.1. Scope . . . . . . . . . . . . . . . ......................................................................................... 1293 2.2. Research questions . . . . ......................................................................................... 1293 2.3. Search process . . . . . . . . ......................................................................................... 1293 2.4. Inclusion and exclusion criteria . . . . . . . . . . ......................................................................... 1293 2.5. Quality assessment . . . . ......................................................................................... 1293 2.6. Data collection. . . . . . . . ......................................................................................... 1294 2.7. Data analysis . . . . . . . . . ......................................................................................... 1294 3. Results. . . . . . . ...................................................................................................... 1294 3.1. Search results and deviations from protocol ......................................................................... 1294 3.2. Synthesis of the proposals . . . . . . . . . . . . . . ......................................................................... 1295 4. Discussion. . . . . ..................................................................................................... 1298 4.1. RQ1. What value can be drawn from the literature with regard to the generation of requirements specifications from software engineering models?. . . . . . . . . . . . . . . . ......................................................................................... 1298 4.2. RQ2. What techniques have been addressed in this field? . . . . . . . . ...................................................... 1299 4.2.1. Literate modelling. . . . . . . . . .............................................................................. 1299', '4.2.1. Literate modelling. . . . . . . . . .............................................................................. 1299 4.2.2. RESCUE and REDEPEND: generation of candidate natural language requirements from the i* framework . . . . . . . . . . . ..... 1300 4.2.3. Goal-oriented requirements engineering with KAOS and Objectiver . . . . . . ........................................ 1300 4.2.4. Other requirements specification derivations from goal modelling . . . . . . . ........................................ 1301 4.2.5. Deriving requirements from business modelling . . . ........................................................... 1301 4.2.6. Deriving requirements from UML models. . . . . . . . . ........................................................... 1301 4.2.7. Use cases, scenarios and user stories . . . . . . . . . . . . ........................................................... 1302 4.2.8. Deriving requirements from user interface modelling.......................................................... 1303 4.2.9. Limitations of this SLR . . . . . .............................................................................. 1303 0950-5849/$ - see front matter/C2112009 Elsevier B.V. All rights reserved. doi:10.1016/j.infsof.2009.04.001 * Corresponding author. Tel.: +34 968 39 85 25; fax: +34 968 36 41 51. E-mail addresses:jnr@um.es (J. Nicolás),atoval@um.es (A. Toval). Information and Software Technology 51 (2009) 1291–1307 Contents lists available atScienceDirect Information and Software Technology journal homepage: www.else vier.com/locate/infsof', '5. Product requirements derivation in software product lines . . . . . . . . . . . . . . . ................................................... 1304 6. Conclusions and further work . ......................................................................................... 1305 Acknowledgements . . . . . . . . . ......................................................................................... 1306 References . ........................................................................................................ 1306 1. Introduction In a study on future research directions in requirements engi- neering, Cheng and Atlee[18] afﬁrm that there has been little work on how to interconnect various types of requirements models, and that further research is needed on how to integrate requirements engineering techniques so that they can be used synergistically. According to Goldsmith [29], despite widely held beliefs stating that models and/or code prototypes are the appropriate means to capture and communicate requirements, requirements need to be written in words if they are to be appropriately reviewable. Fur- thermore, Davis’ reﬂection[20] on the improvement of the require- ments management process concludes that the combined use of software engineering models and lists of requirements in natural language is a good practice to improve such a process, since it per- mits the beneﬁts of the two approaches to be combined. On the one hand, modelling techniques are usually expressive, precise, and facilitate the development team’s speciﬁcation and under- standing of the requirements. On the other hand, lists of require- ments in natural language can serve as a contract between clients and developers and simplify requirements management. These requirements lists both make the validation of requirements by clients easier and clarify the size of the project and the actual state of the requirements development. The study of the proposals to combine business (system) or software models and textual requirements, thereby, becomes relevant. The norm in software engineering has been the speciﬁcation of system or software models from requirements written in natural language. However, if the possibility of extracting information from models as textual requirements in an automatic or closely monitored manner is also included, then the combined use of mod- els and textual requirements is made easier, and some additional beneﬁts can be obtained: /C15The effort of writing the requirements is reduced. As is widely accepted (see for example the IEEE 830-1998 standard[31]), a requirement must be unambiguous, complete, consistent and veriﬁable. The writing of a requirements speciﬁcation satisfying these criteria is a meticulous task and may require considerable effort. The automatic or closely monitored derivation of textual requirements from models can lead to productivity gains in the requirements speciﬁcation process. /C15The completeness of the requirements speciﬁcation is improved. The generation of part of the requirements in an automatic or closely monitored manner contributes to the completeness of the requirements speciﬁcation, since it is easier for the stake- holders to accept or reﬁne requirements than to recall them. In the opinion of Maiden et al.[45], people are better at identi- fying errors of commission rather than those of omission. /C15The traceability between models and textual requirements is automated. CMMI (Capability Maturity Model Integration) [16] recommends bidirectional traceability between requirements and development products. The CMMI’s Level 2 (within the Requirements Management process area) speciﬁcally recom- mends the maintenance of ‘‘bidirectional traceability (among requirements and work products)” (SP 1.4). The overall objective of this work is to carry out a comprehen- sive review and synthesis of the current research and practices re- ported in the literature related to the combination of software', 'The overall objective of this work is to carry out a comprehen- sive review and synthesis of the current research and practices re- ported in the literature related to the combination of software engineering models and textual requirements in the requirements speciﬁcation process, and in particular those approaches that gen- erate text with which to document the models. Our intention is to study the literature to ﬁnd methods and techniques dealing with the generation, translation, combination, integration, or synchroni- zation of (system or software) models and textual requirements, in this order (from models to requirements), thus taking advantage of the beneﬁts listed above. The reverse problem (from textual requirements to models) is not within the scope of this paper. It is a problem that practitioners have traditionally addressed in an informal, ad hoc manner, and more research is needed to achieve software engineering models generation which starts from infor- mal requirements. The objective of this work has been achieved through a sys- tematic literature review (SLR), following the approach of Biolchi- ni et al.[12]. A SLR is a research technique to analyze the state- of-the-art in a particular ﬁeld of knowledge by formally deﬁning the problem statement, the sources of information, the search strings, the criteria for inclusion and exclusion of the papers found in the searches, the quantitative analysis to be undertaken (if necessary), and the templates for ordering the information col- lected from the papers. This technique comes from Medical Re- search and has recently been adapted to software engineering (see the work of Kitchenham et al., e.g.[36,37]; the presentation of the SLR in this paper is particularly inspired by the structure of [37]). The structure of this paper is as follows: Section2 summarizes the main aspects of the design of the SLR. Section3 presents the re- sults of the searches, reports on some deviations from the previ- ously established protocol, and tabulates a synthesis of the results of the SLR which uses a taxonomy to classify the studies re- viewed. Section4 discusses the results based on the research ques- tions formulated in Section 2: the interest in the integration between models and textual requirements is ﬁrst justiﬁed (Section 4.1), and each proposal selected in the SLR is then brieﬂy analyzed (Section 4.2). Product requirements derivation from software prod- uct lines models is an interesting issue not covered by the papers found in the SLR. We have therefore decided to perform a kind of ‘‘mini-SLR” on this topic which is fully reported in Section5. Final- ly, Section6 presents our conclusions and further work, including ﬁve issues that we consider to be key for a RMDB (Requirements Management Database) tool supporting a seamless integration be- tween models and textual requirements. 2. Planning the systematic literature review The review protocol followed in this work is summarized in this section. Section 2.1 states the scope of this research. Section2.2 presents the research questions that guide the SLR. Section2.3 pre- sents the planning of the search process. Section2.4 presents the inclusion and exclusion criteria. Section2.5 shows the data col- lected from the selected studies, and ﬁnally Section 2.6 shows the data analysis. The SLR protocol has been designed and executed by the ﬁrst author of this paper as part of his PhD work, and his advisor (the second author of this paper) has revised the protocol, the included and excluded papers, and has discussed the results of the review with him. 1292 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', '2.1. Scope The overall objective of the SLR has been stated in Section1, but its scope requires further reﬁnement if it is to be precisely deﬁned. We are interested in reviewing the correspondences m2rs: MOD ?REQ and m2rd: MOD?REQDOC that are present in the literature, where: /C15MOD is the set of all themodels used in software development that provide a graphical, diagrammatical representation of the speciﬁcation of an aspect of a system or item of software (e.g. a UML class diagram, an i* SD context model, a feature model, or even a GUI navigation model). A model is constructed by means of atechnique. In terms of the Meta Object Facility (MOF) layered metadata architecture [4], MOD consists of all the Level M1 models that are suitable for graphical representation. /C15REQ is a set that consists of all the sets oftextual requirements.In short arequirement is a condition or capability that must be met or possessed by a system or item of software. A textual require- ment is a requirement deﬁned either (1) as a textual statement; (2) as a textual statement plus attributes, including traceability relationships (e.g. IEEE 830[31]); or (3) as a template ﬁlled in with text (e.g. VOLERE [52]). Options (2) and (3) are clearly equivalent. Textual requirements can be speciﬁed either by means of natural language (e.g. traditionalshall-statements)o r by a formal language (e.g. SCR[21]). /C15REQDOC is the set of all system and softwarerequirements docu- ments, which usually combine narrative descriptions, textual requirements and graphical models. /C15m2rs is a correspondence that maps a system or software model m to a set of requirementsrs. /C15m2rd is a correspondence that maps a system or software model m to a requirements documentrd. As regards the REQ set, as was explained in Section1, we are especially interested in textual requirements written in natural language, but formal languages also play an important role in cer- tain speciﬁc domains and are therefore included within the scope of the SLR. Regarding m2rs and m2rd, we are especially interested in those approaches that are suitable for automation, and which allow these mappings to be performed automatically or in a closely monitored form. Those approaches that describe the creation of a require- ments document starting from models are also included within the scope of the SLR, despite the fact that they are not presented with the aim of being automated, but solely to ‘‘guide” the work of a requirements engineer when writing a requirements docu- ment starting from models. Note that m2rs and m2rd are deﬁned as correspondences and not as applications, so that an element in the MOD domain may have more than one image. 2.2. Research questions The research questions that we intend to answer in this SLR are the following: RQ1. What value can be drawn from the literature with regard to the generation of requirements speciﬁcations (textual requirements and requirements documents) from software engineering models? RQ2. What techniques have been addressed in this ﬁeld? (i.e. the techniques used to build the initial software engineering models, the techniques used to build the corresponding textual requirements and requirements documents, and the transfor- mation procedures). 2.3. Search process The following sources have been selected to perform the SLR: /C15IEEE Digital Library (www.computer.org/portal/site/csdl/ index.jsp) /C15ACM Digital Library (portal.acm.org) /C15Science@Direct (www.sciencedirect.com) /C15MetaPress (Kluwer + Springer) (www.metapress.com) /C15Wiley InterScience (www.interscience.wiley.com) /C15Google Scholar (scholar.google.com) The main journals and events of the software engineering com- munity were sought starting from these sources, in particular those concerning requirements engineering (including conferences and workshops such as RE, ICRE, REFSQ, SREIS, AWRE and WER). Google scholar was selected to complete the set of conferences', 'those concerning requirements engineering (including conferences and workshops such as RE, ICRE, REFSQ, SREIS, AWRE and WER). Google scholar was selected to complete the set of conferences and workshops searched, and to seek grey literature in the ﬁeld (e.g. white papers and technical reports). All of the previously-mentioned sources have search engines based on keywords. The search string deﬁned is the following: (‘‘from” OR ‘‘generation” OR ‘‘generating” OR ‘‘combination” OR ‘‘combining” OR ‘‘derivation” OR ‘‘deriving” OR ‘‘integration” OR ‘‘integrating”) AND (‘‘models” OR ‘‘speciﬁcations” OR ‘‘scenarios” OR ‘‘use cases” OR ‘‘features” OR ‘‘stories”) AND (‘‘documentation” OR ‘‘documents” OR ‘‘requirements”). 2.4. Inclusion and exclusion criteria A tentative application of the search string has shown that, in many cases, it is sufﬁcient to read the title of the contributions to consider them as candidates for selection in the SLR, since the terms of the query are commonly used in literature, and lead to many papers which are not related to the subject of this SLR. When the title is not sufﬁcient to determine the inclusion of the paper as a candidate, the abstract is then read and, if necessary, the intro- duction and even the whole paper. With regard to the exclusion criteria, candidate papers present- ing RMDB tools which do not present speciﬁc procedures for com- bining models and requirements are not within the scope of this research. Our intention is that this SLR should concentrate upon techniques and transformation procedures, leaving aside the tool market, since (1) it is difﬁcult to obtain access to all the tools (many are proprietary tools); (2) there is a vast number of RMDB tools; and (3) the market changes continuously. Duplicate reports of the same study are also excluded in the SLR: only the most com- plete version of the study is included. Papers are not excluded on the basis of their publication date (1) in order to be able to detect whether the subject of the SLR was actively addressed in literature during a certain period and then abandoned; and (2) to enable us to discover old papers that could provide ideas which could be adapted to current software engineering techniques. 2.5. Quality assessment In this SLR the quality of the selected studies is addressed by using the following criteria as a basis: /C15Publication place. In this respect all the selected sources seek scholarship journals and conferences only, with the exception of Google Scholar, which seeks a wider spectrum of papers. We do not expect a large number of papers in this SLR and thus we are initially open to analyze ideas coming from any journal or conference, including grey literature. We opt to remark in the analysis of the studies which are technical reports or white papers. J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1293', '/C15Tool support. If any, we study the tool supporting the approach and whether it consists of a prototype or a more mature tool. /C15Validation procedures. We encode each study with three levels of validation. From lower to higher: (ACS) The study is shown through academic case studies or even through examples. In some cases these are drawn from literature; (ICS) The study has been put into practice in an industrial case study; (IP) The study can be considered as part of the industrial practice in a company or domain, for example because a commercial tool supporting the approach is available in the marketplace. The research method – e.g. Action-Research – , if any, it is also reported. 2.6. Data collection A data extraction form adapted from Biolchini et al.[12] was ﬁlled in for each selected work (seeTable 1). The form consists of a section of objective results which correspond to those written by the authors of the work and another section of subjective results related to the reviewers’ impressions with regard to the topic of this SLR. The section of objective results brings together the re- search method of the study (if reported), the problems and limita- tions reported by the authors (if any), and a summary of the results of the study. When analyzing the contributions, special attention was paid in the results summary to the sentences that reinforce the interest of the subject under study (research question RQ1); the method proposed in the approach, the initial models, and the target statements (research question RQ2); and the RMDB tools in- volved, together with the validation procedures. 2.7. Data analysis The data collected were tabulated to show: /C15The identiﬁer assigned to the study in the SLR, its authors, bib- liographic reference, source and year of publication. /C15The classiﬁcation of the study following the taxonomy proposed (presented in Section3.2). /C15The initial model and the kind of textual statements generated, either in formal or natural language (concerning RQ2). /C15The method in which the proposal takes place, tool support, and the validation procedures (concerning RQ2). In our view, the tabulation of results related to research ques- tion RQ1 is not the best manner of presentation since the rationale is of a narrative nature. The justiﬁcations for interest in the subject of this SLR that have been collected from the selected studies and which we consider to be of most note are therefore presented in Section 4.1. 3. Results 3.1. Search results and deviations from protocol The search string was adapted to be used in the search engine of each source. The number of papers found per source is summarized in Table 2, together with those marked as candidates and those ﬁ- nally selected. The search string was formulated by using words in common usage and, after applying the inclusion criteria, most of the studies found were not labelled as candidate studies. After applying the exclusion criteria to the candidate studies, the se- lected studies were then deﬁned by source.Table 3shows the can- didate studies which were not selected, and why. A total of 23 studies were selected (seeTable 2). Finally, identical studies found in several sources had to be removed, resulting in 15 different se- lected studies. After studying the bibliographies of the selected papers we no- ticed an important trend related to the research questions,literate modelling (Sections 4.1 and 4.2.1), and believed that the SLR would not be complete if this trend were not reported. We therefore decided to introduce a deviation from protocol by completing the set of selected studies with the sections of bibliography and related work of these 15 selected papers. Three interesting papers not ini- tially found in the SLR were thus selected, which were traced from the bibliographies and related work of the studies that already ap- peared in the SLR. Six papers that we knew were related to this to-', 'the bibliographies and related work of the studies that already ap- peared in the SLR. Six papers that we knew were related to this to- pic were also selected, despite their not appearing in the searches. We are conscious that this is another deviation from protocol that concerns the repeatability of the SLR, but the interest of working with a more complete set of papers ﬁnally prevailed. Twenty-four contributions were therefore eventually selected. Table 1 Data collection form. Objective results extraction Study identiﬁcation Full bibliographical reference Study origin The source/s from which the study has been selected, the paper from which the study is drawn, or (prev.SLR) if the paper belongs to the baseline of papers previous to the SLR Validation and study methodology The validation procedures and the research method(s) used to achieve the results, if reported Study results The results of the study related to the research questions of the review, including the rationale on which the authors based their work, the initial models, the target requirements, the RMDB tool, and the method used in the proposal, if reported Study problems and limitations Those problems and limitations reported by the study authors Subjective results extraction General impressions and abstractions Here the reviewers raise their own conclusions after reading the study Table 2 Number of found, candidate and selected studies, by source. Identical studies in different sources have not yet been eliminated. Source Studies found Candidate studies Selected studies IEEE Digital Library 50 2 2 ACM Digital Library 214 8 7 Science@Direct 45 2 2 Meta-Press 87 2 2 Wiley Interscience 5 0 0 Google Scholar 394 12 10 Total 795 26 23 1294 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', '3.2. Synthesis of the proposals The proposals selected in this SLR are arranged inTable 4, which shows the summarized data from each selected study. These data are analyzed in this section. A more detailed analysis of each paper can be found in Section4.2. A taxonomy is proposed for the proposals selected in this SLR, based on the establishment of two dimensions:combination mode and scope (columns C. Mode and Scope in Table 4), our intention being to provide a synthesized vision of the ﬁeld of knowledge ad- dressed by research questions RQ1 and RQ2. With regard to thecombination mode, we have identiﬁed what we refer to asgenerative and integrative approaches: /C15Combination mode: Generative. These approaches propose algo- rithms, rules or patterns to generate textual requirements start- ing from models. These proposals are presented with or without automatic support and the text generated can be written in nat- ural or formal language: – Generative (natural language). These approaches generate candidate natural language requirements which must be val- idated. In this group we should stress the work of Maiden et al. on the generation of requirements from i * models [45] (S3 in Table 4); the research of Meziane et al. [47] (S13), to generate English speciﬁcations that paraphrase UML class diagrams; the proposal of Firesmith[28] (S19) which derives textual requirements from use cases, scenar- ios and user stories; and a proposal by Berenbach[10] (S17) which does not, strictly speaking, generate natural lan- guage text but rather a hierarchy of requirements from use case diagrams. – Generative (formal language). These approaches generate requirements written in a formal notation. Formal methods bring beneﬁts such as the mechanical analysis of a system to check for deadlock and livelock freedom, but the adoption of formal methods in industry is challenged by the cost and complexity involved in the formal speciﬁcation of the system. Hence some approaches in literature have investi- gated the derivation of formal speciﬁcations from require- ments models. In this group we should highlight the work on goals models by van Lamsweerde et al., which addresses the automatic generation of operational requirements described by means of pre- and post-conditions and triggers [40] (S5), and the generation of requirements described by means of the tabular technique SCR[21] (S7). In addition, Cabral and Sampaio [15] (S20) research the generation of operational requirements in CSP process algebra from use case speciﬁcations. /C15Combination mode: Integrative. These studies do not provide algorithms, rules or patterns to generate requirements from models, but rather a kind ofopen-ended guidesto relate models and textual requirements (or, on occasions, a philosophy of work). The resulting requirements can be written in natural lan- guage or in a formal notation. This SLR concentrates particularly upon generative approaches but integrative approaches may be of interest in the generation of requirements and requirements documents and have thus also been included. Examples of this are Arlow’s proposal [9] (S1) for literate modelling and Fire- smith’s vision of what a modern requirements speciﬁcation should be like[27] (S2). With regard to thescope of the approach, some proposals deal with the generation of single requirements (correspondence m2rs, explained in Section2.1) while others deal with the genera- tion of requirements documents (correspondencem2rd, described in Section 2.1). Obviously, this dimension is not disjointed, since the same study can address requirements and requirements docu- ments generation. With regard to this SLR both approaches are of equal interest: /C15Scope: Requirement. These approaches deal with the generation of requirements or sets of requirements, but do not address the requirements documents in which the requirements should be placed. One example is the approach of Maiden et al.[45] (S3)', 'of requirements or sets of requirements, but do not address the requirements documents in which the requirements should be placed. One example is the approach of Maiden et al.[45] (S3) which deals with deriving requirements from i * SD models. /C15Scope: Documental. These studies concentrate on the manual, automatic or semi-automatic generation of requirements docu- ments. For example, van Lamsweerde’s group [59] (S8) has developed a tool (Objectiver) with which to semi-automatically generate requirements documents structured from a goal spec- iﬁcation. The literate modelling trend as a whole can also be included in the documental approach, including the works of Arlow [9] and Firesmith[27] (studies S1 and S2 inTable 4). Based on the results inTable 4, Fig. 1relates combination mode to scope, distinguishing between results in natural and formal lan- guage. From this ﬁgure we can conclude that more attention has been paid to the generation of single requirements (24 studies) than of documents (10), while generative approaches (23 studies) are more numerous than integrative approaches (11). In relation to the target requirements, i.e. regarding theGen.Statem. column (Generated Statements)i nTable 4, most of the approaches deal with natural language rather than formal language (26 studies to 8). This column also shows whether the studies report on a particular template for natural language requirements speciﬁcation (e.g. Vol- ere) or a formal notation (e.g. SCR, KAOS, Albert). Note that the same study may address both requirements and documents and both natural and formal language. The studies collected inTable 4are applied to a variety of models (see theInitial Modelcolumn), which are summarized inFig. 2. This ﬁgure does not compute literate modelling approaches (S1 and S2 inTable 4) because they are applicable to any visual modelling lan- guage. The number of studies related to use cases and scenarios (10 studies, 7 of them generative) and goals models (7 studies, 5 of them generative) is particularly noteworthy. On the one hand, although use cases and scenarios have convenient, well-known graphical notations, they are techniques in which text traditionally plays a more important role than diagrams. On the other hand, goal Table 3 Candidate studies not selected. Source Study reference Reason for rejection ACM A. van Lamsweerde, Requirements engineering in the year 00: a research perspective, in: 22nd Intl. Conf. on Software Eng. (ICSE’00), ACM Press, Limerick, Ireland, 2000. [59] (S8 inTable 4) is a more recent version of the study Google Scholar B. Jiang, Combining Graphical Scenarios with a Requirements Management System, Master Thesis, University of Ottawa, Ottawa, Ontario, Canada, 2005 RMDB tool prototype Google Scholar N.A.M. Maiden, S. Manning, S. Jones, J. Greenwood, Towards pattern-based generation of requirements from software model, in: Requirements Engineering: Foundation for Software Quality 2004 (REFQS’04), Riga, Latvia, 2004 [45] (S3 inTable 4) is a more recent and complete version of the study J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1295', 'Table 4 Systematic review studies regarding RQ2. ID Author/s [ref.] (source) Date C. Mode Scope Initial Model Gen. Statem. Method Tool Validation Gen. Int. Req. Doc. FL NL Section 4.2.1. literate modelling S1 Arlow and Neustadt [9] (very similar and more recent than[8], which is cited by S12) 2004 h jj j UML or any other visual modelling language h j Not speciﬁc Not reported (ICS) Enterprise Object Models at British Airways S2 Firesmith [27] (prev. SLR) 2003 j h jj Not speciﬁc h j Not speciﬁc Not reported Not reported (vision paper) Sections 4.2.2–4.2.4. Goal-oriented requirements frameworks S3 Maiden et al. [45] (acm, mpress, gs) 2005 j h j h i* goal-oriented model (SD diagram) h j (V) RESCUE REDEPEND (ICS) (A-R) DMAN, air trafﬁc management S4 van Lamsweerde and Willemet [60] (acm, gs) 1998 j h j h Scenarios and use cases j (K) h KAOS Not included in GRAIL (ACS) ATM and Lift systems S5 Letier and van Lamsweerde [40] (cited by S6 and S7) 2002 j h j h KAOS goal- oriented model j (O) h KAOS Validation with SteP verif. system (ACS) Mine pump control system S6 Alrajeh et al. [6] (acm) 2006 j h j h Temporal logic goal-oriented model j (O) h Not speciﬁc Validation with LTSA model checker and Progol5 inductive learning tool (ACS) Preconditions in KAOS models in the mine pump control system S7 De Landtsheer et al. [21] (acm, mpress, gs) 2004 j h j h KAOS goal- oriented model j (S) h KAOS Validation through SMV model checker (ACS) Safety injection system for a nuclear power plant S8 van Lamsweerde [59] (cited by S3) 2004 j h jj KAOS goal- oriented model j (O) j KAOS Commercial CASE Objectiver (previously GRAIL prototype) (IP) About 20 industrial projects; Objectiver is a commercial tool S9 Yu et al. [61] (gs) 1995 h jj h i * goal-oriented model j (A) h Not reported Not reported (ACS) Banking system S10 Antón and Potts[7] (prev. SLR) 1998 h jj j Goal-oriented model h j GBRAM Not reported (ICS) (A-R) CommerceNet Web Section 4.2.5. Business modelling S11 Cox et al.[17] (sd, gs) 2005 h jj h RAD business model h j Not speciﬁc Not reported (ICS) (A-R)e-business system S12 Türetken et al.[58] (gs) 2004 j h j h eEPC business model h j Not speciﬁc ‘‘KAOS” plug-in for the ARIS toolset (ICS) two military applications Section 4.2.6. UML-based approaches S13 Meziane et al.[47] (prev. SLR) 2007 j h jj UML class diagramh j Any UML-based method GeNLangUML Java prototype (ACS) University system Section 4.2.7. Use cases and scenario modelling S14 Maiden et al.[42] (ieee) 1998 j h j h Scenarios and use cases h j CREWS-SAVRE CREWS-SAVRE (ICS) London Ambulance Service S15 Mavin and Maiden[46] (ieee, gs) 2003 j h j h Scenarios and use cases h j CREWS-SAVRE CREWS-SAVRE (ICS) OCD (naval) & CORA-2 (air trafﬁc managem.) S16 Maiden and Robertson[44] (acm) 2005 j h j h Scenarios and use cases h j (V) RESCUE (evolves CREWS-SAVRE) ART-SCENE (evolves CREWS-SAVRE) (ICS) (A-R) DMAN, air trafﬁc management S17 Berenbach [10] (acm, gs) 2003 j h j h Use case diagrams h j Any use case driven process Based on CASE tool scripts (ICS) Use case models used in Siemens S18 Berenbach [11] (prev. SLR) 2004 h jj j Use case diagrams h j Any use case driven process Not reported (ICS) Mail sorting system at Siemens S19 Firesmith [28] (gs) 2004 j h j h Stories, scenarios and use cases h j Any use case driven process Not automated (ACS) ATM example S20 Cabral and Sampaio[15] (sd, acm) 2008 j h j h Use case templates j (C) h Not speciﬁc Ms Word plug-in, CNL/ CSP translator, FDR, CSP model checker (ICS) Research cooperation involving Motorola S21 Daniels et al.[19] (prev. SLR) 2005 h jj j Use cases h j Rational Uniﬁed Process Not automated (ACS) microwave oven SPL example S22 Probasco and Lefﬁngwell[49] (gs) 1999 h j h j Use cases h j Rational Uniﬁed Process Rational Suite Not reported (white paper) Section 4.2.8. User interface modelling S23 Jungmayr and Stumpe[33] (gs) 1998 j h jj Extended usage model h j Not speciﬁc Java prototype (ACS) UNIrech, bibliographic databases query', 'paper) Section 4.2.8. User interface modelling S23 Jungmayr and Stumpe[33] (gs) 1998 j h jj Extended usage model h j Not speciﬁc Java prototype (ACS) UNIrech, bibliographic databases query S24 Smith [55] (prev. SLR) 1982 j h j h User-system interface h j Not speciﬁc Prototype on UNIX Not reported Source: acm, ieee, sd (ScienceDirect), mpress (MetaPress), wi (Wiley Interscience), gs (Google Scholar), prev.SLR (baseline previous to SLR), cited by [ref.];C. Mode(Combination Mode): Gen.: Generative/Int: Integrative;Scope: Req.: Requirement/Doc: DocumentGen.Statem. (Generated Statements): FL: Formal Language/NL: Natural Language; (K) KAOS, (O) Operation Model, (S) SCR, (A) Albert, (C) CSP, (V) Volere;Validation: (ACS) Academic Case Study/(ICS) Industrial Case Study/(IP) Industrial Practice; A-R: Action-Research. 1296 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', 'models beneﬁt from van Lamsweerde et al.’s attention to KAOS (4 generative studies, S4–5 and S7–8 inTable 4). Two approaches con- cern business modelling, but none of them addresses document generation. There are also two studies on interface models. It is worth noting that there is only one generative approach on UML, leaving apart use cases and scenarios. It seems that more effort should be made in this area, particularly in those UML techniques which are more suitable for use in requirements engineering. With regard to theMethod column in Table 4, few studies are part of a software development method. This is hardly surprising since we believe that these approaches can be used in the context of any method in which the initial models are used. The ﬁrst quality assessment criteria established in Section2.5 is Publication place. Only one white paper appears in the selected studies, which is that of Probasco and Lefﬁngwell[49] (S22). The remaining studies do appear to have been published after a referee process with, perhaps, the exception of the two columns of Fire- smith at JOT[27,28] (S2 and S19). Arlow and Neustadt’s book chap- ter on literate modelling[9] (S1) comes from a contribution to a refereed conference[8]. Data regardingTool and Validation can also be found inTable 4: /C15As regards Tool, to the best of our knowledge, REDEPEND and Objectiver are the most mature tools found in this SLR. Objecti- ver is even commercially available. Besides, automation is not reported in some of the studies, notably in Arlow and Neustadt’s proposal [9] (S1) for literate modelling and in Firesmith’svision of modern requirements speciﬁcation[27] (S2). The remaining approaches which report automation show their viability by means of prototypes whose real maturity level is difﬁcult to assess from the information reported in the papers. /C15Fig. 3presents a summary ofValidation, and makes it is clear that real industrial practice is scarce in this ﬁeld. A case study in a com- pany must overcome an important gap if it is to become part of that company’s real practice. The reading of the papers does not allow us to make a precise assessment of the size of this gap in all cases. The number of academic and industrial case studies is similar (9 and 11 studies, respectively). Industrial case studies 0 2 4 6 8 10 12 14 16 18 Scope (NL, Natural Language; FL, Formal Language) Number of studies Integrative 5150 Generative 12 6 4 1 Requirement (NL) Requirement (FL) Document (NL) Document (FL) Fig. 1. Number of studies by scope and combination mode. 0 1 2 3 4 5 6 7 8 9 Initial models Number of studies Requirement 72812 Document 20311 Goal-oriented  models Business  models Use cases &  scenarios UML (other  than use cases  & scenarios) User interface Fig. 2. Number of studies by scope and initial model. J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1297', 'are in principle preferable, although it is often difﬁcult to make a precise assessment of the real scope of an industrial case study by reading the papers. Nevertheless, some papers present rigorous theoretical research and are illustrated by means of an academic case study (e.g. S4, S5 and S7 inTable 4by van Lamsweerde et al.). Finally, but of no less importance, no approach exists to address the issue of maintaining synchronization between the documents or requirements generated and the initial models. In the papers se- lected the generation always takes place in one direction, from mod- els to requirements. No approach exists to permit the changes in the generated requirements to be automatically propagated in the initial models. We believe that this synchronization could be useful in an iterative and incremental software process, especially during valida- tion with customers. Validation could therefore be carried out directly on the widely understandable generated textual require- ments, which could be changed to make the related models evolve automatically through traceability relationships. We are obviously referring to the synchronization of certain predeﬁned changes in the generated textual requirements or documents, since synchroni- zation in general may be difﬁcult. 4. Discussion Having ﬁlled in the data collection forms and analyzed the se- lected contributions, we now present answers to the research questions presented in Section2.2. 4.1. RQ1. What value can be drawn from the literature with regard to the generation of requirements speciﬁcations from software engineering models? An a priori justiﬁcation of the interest of this topic was formulated in Section 1, before the SLR was performed. This question (RQ1) permits us to look for statements found in literature which justify an interest in this topic. In this section we collect some of the statements that we consider to be most representative. Arlow and Neustadt [9] (S1 in Table 4) believe that in many cases not all stakeholders are able to understand the syntax and semantics of a business model that uses UML and other visual models. Furthermore, these authors discuss the following concerns with regard to visual models: /C15In order to access the information embedded in a model it may be necessary to know how to operate a modelling tool. The reports that all modelling tools generate – usually in HTML for- mat – are often difﬁcult to read and navigate and are thus, in Arlow and Neustadt’s experience, of limited practical use. /C15Unless one is familiar with the general ‘‘shape” of a visual model, it can be difﬁcult to determine where to start reading, either when reading the model in a modelling tool or when reading a generated report. /C15It is sometimes difﬁcult, or even impossible, to uncover the busi- ness requirements and the rationale underlying the visual model, as these becomeinvisible when taken out of the business context and expressed in a visual notation. For instance, a highly important requirement can be expressed in such a concise way that the requirement can be easily overlooked during a walk- through. Arlow and Neustadt call this ‘‘the trivialisation of busi- ness requirements by visual modelling”[9]. The same authors show this trivialisation through an example in which a highly important business requirement is ﬁnally expressed in the mod- els as an association multiplicity ‘‘n” rather than ‘‘1”: this requirement could easily pass unnoticed during the validation of the model. Literate modelling is a technique drawn from theliterate pro- gramming proposed by Knuth in the eighties, in which modelling and natural language documentation are seamlessly bound to- gether in a synergetic manner, and the functionality of CASE (Com- puter-Aided Software Engineering ) and RMDB tools are truly integrated. We believe that many software engineers have been using literate modelling intuitively in an ad hoc manner through-', 'puter-Aided Software Engineering ) and RMDB tools are truly integrated. We believe that many software engineers have been using literate modelling intuitively in an ad hoc manner through- out their professional careers, without being aware of this term. In a paper referenced by Türetken et al. [58] (S12), Finkelstein and Emmerich[26] examine the future of RMDB tools and place lit- erate modelling within the long-term future of these tools. These authors claim that there is no suitable manner in which to syner- gistically use models and natural language in RMDB tools. These authors foresee that literate modelling has a role to play in these tools. There is a school of thought that argues that natural language requirements are a vestige of outmoded practice which survives because of the lack of technological transfer to the IT industry of R&D modelling methods. Finkelstein and Emmerich, however, be- lieve that natural language plays a valuable role which is, further- more, unlikely to be supplanted. Natural language is useful to show the correspondences between the components of the models and those of real-world phenomena, and allows the stakeholders to validate the speciﬁcation. Meziane et al.[47] (S13) add another beneﬁt: the automatic generation of natural language require- ments for maintenance purposes. Software implementations are often not consistent with the documentation, since developers do not update analysis and design models when they change the code. Design models can be generated from the evolved implementation by means of a tool allowing reverse engineering. It would therefore be useful if natural language requirements were generated based on that updated design. Maiden et al. [45] (S3) believe that their work is part of an important trend towards the integration of requirements models. These authors state that although many model-based speciﬁcation and analysis approaches with which to specify the requirements of computer-based systems exist, most organizations continue to represent requirements textually. ‘‘Unfortunately”, they write, ‘‘most modelling approaches have not been designed to support the derivation of requirements statements from models, or to be used alongside textual requirements descriptions”. When van Lamsweerde[59] (S8) exposes the lessons learned in the application of goal-oriented requirements engineering, he states that ‘‘the diversity of requirements engineering projects in type, size, and focus call for highly ﬂexible technologies. We felt that a multi-button method and tool that by default supports graphical and textual speciﬁcations, plus formal speciﬁcations only when and where needed for incremental analysis of critical model fragments, is a promising step in that direction”. van Lamsweerde also asserts that requirements documents are generally perceived as being big, complex, outdated, and too far away from the execut- able products customers are paying for. He adds that ‘‘in the end, what bothers customers the most is the quality of project delivera- Academic Case  Studies (ACS); 9;  38% Industrial Case  Studies (ICS); 11;  45% Industrial Practice  (IP); 1; 4% Not reported; 3;  13% Fig. 3. Types of validation (including number of studies and percentage). 1298 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', 'bles. The model-driven requirements document generated with our tool [Objectiver] was perceived as the main success indicator in many projects”. Firesmith [27] (S2) discusses the problems of traditional requirements speciﬁcations and proposes the requirements in an approach to manage those problems in an iterative and incremen- tal software process. Such an approach should enable the automatic generation of audience-speciﬁc requirements speciﬁca- tions. For example, the needs of executive managers or project directors are very different from those of software testers. With regard to iterative process models, the need for requirements documents to have unresolved questions annotated is made expli- cit in Antón and Potts’s approach[7] (S10) towards goal-oriented requirements engineering. In a requirements speciﬁcation the listing of requirements can be as important as the tracking of open issues, which appear and disappear dynamically. Türetken et al. [58] (S12) work on the automatic generation of requirements in natural language from business process models and state that their approach can be used to manage the minimiza- tion of ‘‘non-value-added tasks” such as the rewriting and docu- menting of requirements, thus improving their quality as well as facilitating their modiﬁcation. The functional requirements of the system and the software are generally based on visual models, but the traceability and the interaction between these models and the requirements represented in natural language are gener- ally missed. One of the beneﬁts of their approach is that the stan- dard format for the requirements generated simpliﬁes the understanding, veriﬁcation, validation, and management of these requirements for all stakeholders. Berenbach’s [10] (S17) experiences in Siemens have led him to observe the disconnection between a UML model and the require- ments of the processes modelled. This author believes that this gap tends to widen: as models become more complex the extraction of detailed requirements becomes more difﬁcult. Failure to develop complete requirements sets from UML models may have serious consequences at a later date. For example, the derived test cases might not provide complete coverage. Berenbach[11] (S18) states that the solution arises from the planned integration between models, the text of the use cases, and the requirements. He believes that ‘‘a UML model is a repository, not a set of diagrams”. He also provides a list of best practices, including the suggestion that doc- umentation should be generated ‘‘on demand”. With regard to the integration of use cases and textual require- ments, Daniels et al.[19] (S21) state that use cases provide under- standability, context, and direct traceability to actor needs and interfaces, while shall-statement requirements add the precision necessary to completely and unambiguously specify the system. Despite the usefulness of use cases, shall-statements, diagrams, ta- bles, equations, graphs, pictures, pseudo-code, state machines and other methods must still be used to capture additional require- ments and add richness to provide a sufﬁcient level of detail to characterize a system. These authors conclude that ‘‘use case mod- els and traditional shall-statement requirements are synergistic speciﬁcation techniques that should be employed in a complemen- tary fashion to best communicate and document requirements”. 4.2. RQ2. What techniques have been addressed in this ﬁeld? For reasons of clarity, the selected proposals described in this section have been grouped as they are shown inTable 4. 4.2.1. Literate modelling A detailed introduction to the intention of literate modelling was given in Section4.1. Arlow et al.’s original proposal on this to- pic [8] is later reﬁned in[9] (S1 inTable 4). In this approach a new type of document is deﬁned,Business Context Document (BCD),i n which (1) the diagrams are embedded as ﬁgures in a document', 'pic [8] is later reﬁned in[9] (S1 inTable 4). In this approach a new type of document is deﬁned,Business Context Document (BCD),i n which (1) the diagrams are embedded as ﬁgures in a document written in natural language and the visual model is paraphrased (for example, in relation to certain UML associations it can be writ- ten as ‘‘Each Product has a Price List which contains zero or more Prices”); (2) brief explanations of the relevant UML syntax are in- cluded in footnotes; (3) real, yet simple examples of the notation are presented to illustrate speciﬁc points; and (4) some important questions such as the rationale are emphasized. Literate models are thus UML models that are embedded in a natural language text which explains them. In Arlow et al.’s experience, it is preferably to structure BCDs around the keythings that deliver value to the business rather than the businessprocesses. Things are more stable than processes and tend to naturally form cohesive clusters that provide a suitable fo- cus for the BCD. A simple relationship between the package struc- ture of the UML models and the BCDs can be expected. This proposal does not, however, mention speciﬁc patterns or algo- rithms for generating requirements from UML models: instead, the authors describe the base contents of the BCD and provide guidelines with which to create it. According to Arlow et al., each BCD has the following base structure: (1) business context; (2) compliance to standards; (3) a roadmap UML model, showing all the main things and relationships, with cross-references to the appropriate parts of the BCD; and (4) a number of sections, each describing a thing or related things and comprising: (4.1) narrative, referencing one or more model fragments; (4.2) UML diagrams illustrating the narrative (which, according to the authors, are typ- ically class diagrams, use case diagrams, and sequence diagrams); and (4.3) informal diagrams wherever they enhance the descrip- tion. Arlow et al. also provides guidelines on the writing style to be used in BCDs and, for example, recommend the use of concrete examples to illustrate the models and the development of a busi- ness nomenclature. We believe that, in a broad sense, the proposals collected and analyzed in this SLR could be considered as a part of this trend in literate modelling, although no speciﬁc reference is made to this in the papers themselves. We particularly believe that Firesmith’s proposal with regard to requirements speciﬁcations[27] (S2) could be considered as literate modelling. The goal of this approach is to obtain a correct, complete, consistent, current, and audience- appropriate (i.e., supportive of the role-speciﬁc tasks of its numer- ous audiences) requirements speciﬁcation. To improve the require- ments speciﬁcation, and based on his experience, Firesmith proposes the use of a tool that (1) has a ﬁne-grained repository; (2) enables automatic speciﬁcation generation, as a kind of separa- tion between Model and View in the MVC (Model-View-Control) paradigm; and (3) generates different speciﬁcations for different readerships. Another interesting idea that Firesmith points out is the need to establish some kind of publish-subscribe mechanism which permits stakeholders to be notiﬁed when certain require- ments of interest to them change in an iteration or are added in incremental development. This feature would, for example, be crit- ical to a designer. In conclusion, in the words of Arlow and Neustadt[9]: ‘‘in prac- tice, literate modelling, although a very good idea, didn’t really take off well. This was partly because of its reliance on special text-processing tools that were not widely available and partly be- cause programmers generally prefer to write code rather than nar- rative! In contrast to this, literate modelling has proven to be very popular with those that have tried it. This is because a literate model not only provides a context for a UML model that is other-', 'popular with those that have tried it. This is because a literate model not only provides a context for a UML model that is other- wise lacking but also helps the modeller do his or her work”. We believe that if this proposal of literate modelling is to be successful it should be based on the automatic or closely monitored genera- tion of the BCD or signiﬁcant parts of the BCD. If BCDs have to be created entirely manually, then literate modelling will probably J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1299', 'not be applied in practice: the manual generation of BCDs can be cumbersome, and BCDs can be hard to write because, according to Arlow and Neustadt, they require a very sound and broad over- view of the business, good UML skills, and good writing and com- munications skills. 4.2.2. RESCUE and REDEPEND: generation of candidate natural language requirements from the i* framework In the set of proposals of Neil Maiden et al. regarding the requirements engineering process RESCUE, one of the premises is that it is easier for the stakeholders to identify errors of commis- sion rather than those of omission. In other words, the stakehold- ers ﬁnd it easier to read a possible alternative scenario in a use case or a candidate requirement in natural language and to accept or re- ject it than to recall all the possible scenarios or all the require- ments of the system. In this context, two set of papers can be found in the scope of RESCUE: (1) those generating requirements in natural language from i* SD – Strategic Dependency – context models (these are basically graphical context models that allow the goals of the interactions between the agents of a socio-techni- cal system to be captured); and (2) those attempting to complete the set of alternative scenarios in the use cases (described in Sec- tion 4.2.7). In the ﬁrst set of papers, Maiden et al.[45] (S3) describe 19 pat- terns that have been developed toparaphrase an i* SD model thus making information already included in the model explicit. These patterns identify recurring syntactic and semantic structures in the i* models, from which candidate requirements are generated that must be validated. These requirements are speciﬁed in natural language by using the VOLERE template. This approach was ap- plied manually in an industrial case study, DMAN (DEparture MAN- ager), an air trafﬁc management system. The authors consider this approach to be a success because 214 requirements statements were generated manually in three days, of which 207 were in- cluded in the ﬁnal requirements speciﬁcation (encompassing al- most 900 requirements). Therefore, almost 25% of the requirements in the ﬁnal speciﬁcation were generated in this man- ner. This approach helps to improve the completeness of the requirements speciﬁcation although, as Regnell et al.[51] point out, probably not the elicitation per se since the information must already be included in the i* model. This approach can be useful in any system in which a set of heterogeneous actors interact in order to achieve certain dependent goals. Maiden et al. conclude this study by asking themselves whether an automatic, pattern-based generation of candidate requirements statements would be cost- effective, or whether the number of duplicate and false-positive requirements would be unacceptable. An initial answer to this last question raised by study S3 can be found in a subsequent paper,[41], in which Maiden et al. pres- ent the lessons learned during the application of i* in several industrial case studies, and discuss the use of a new version of REDEPEND which automates the applications of the previous pat- terns. For the purpose of this SLR, REDEPEND enables analysts to construct i* SD models during a requirements workshop, to auto- matically generate candidate requirement statements from this i* model in real time, and to then walkthrough these generated requirements to select and reject them. The 19 previous patterns are represented in an Ms Excel ﬁle in REDEPEND, meaning that eventual new patterns can be added without changing the tool. In the previous DMAN case study, the automatic generation of 287 candidate requirements took REDEPEND only 12 s when running on a standard PC. It should be noted that the number of candidate requirements generated is larger than in [45] because the 19 patterns had been reﬁned, thus leading to more requirements of different types. With regard to requirements gen-', 'of candidate requirements generated is larger than in [45] because the 19 patterns had been reﬁned, thus leading to more requirements of different types. With regard to requirements gen- eration, the initial evaluation of the tool made by Maiden et al. is positive, although they aim to gather and report evaluation data in the future. 4.2.3. Goal-oriented requirements engineering with KAOS and Objectiver van Lamsweerde et al. have produced an authoritative collec- tion of work around KAOS, a goal-oriented framework that in- cludes method and tool support. The KAOS method has been applied in about 20 industrial projects at CEDITI (a university spin-off) [59], covering a variety of domains. Natural language is used in KAOS to describe the system informally, although when necessary a temporal logic can be used to describe the system for- mally. This section addresses the generation of both textual requirements and requirements documents. With regard to textual requirements, the requirements generated are not speciﬁed in nat- ural language but in formal notations (pre/post-conditions & trig- gers and SCR). The starting models are scenarios and goal models, which have convenient graphical diagrams, although they are processed from their textual representation. The ﬁrst work in this section is related to scenarios, which are widely considered to be an effective means to elicit, validate, and document requirements. However, scenario models are partial and may pass over certain properties of the system. Goals, requirements and assumptions related to scenarios are, in addi- tion, only implicitly described. The system’s properties must therefore be explicitly expressed to permit an analysis of consis- tency and completeness to be carried out. A method was thus developed in the realm of KAOS to systematically infer formal, ab- stract declarative speciﬁcations of goals, requirements and assumptions starting from informal, concrete scenarios [60] (S4). The generated speciﬁcation uses temporal logic. This pro- posal is grounded in the experience of the KAOS research group in numerous projects but, according to this paper[60], the putt- ing into practice of this proposal in a real industrial case study is still pending. After this work, research was then conducted in the realm of KAOS to project declarative goals models to operational require- ments in order to achieve the best of both approaches: /C15Operations speciﬁed by pre- and post-conditions and triggers [40] (S5). The functional goals assigned to the software agents must be operationalized in the speciﬁcations of the software services that the agents should provide to meet those goals, by applying operationalization patterns. The authors recognize their limited experience with these patterns, mainly based on their handling of a variety of case studies from literature. Alrajeh et al.[6] (S6) afﬁrm that this reﬁnement of operational require- ments from goal models is a tedious manual task only partially supported by operationalization patterns. These authors address this problem by proposing a semi-automated approach based on model checking and inductive learning. This is initial research which is described through the case of learning preconditions for KAOS models. Alrajeh et al. believe that this method can be tailored to generate other operational requirements such as triggers. /C15Tabular event-based speciﬁcations for control software, written in the SCR language[21] (S7). These speciﬁcations are a well- established method with which to specify operational require- ments for the development of control software, and provide sophisticated techniques and tools for the late analysis of soft- ware models. This proposal is again shown through an example found in literature. In a keynote at RE’04, van Lamsweerde[59] (S8) reﬂected on goal-oriented requirements engineering research and practice. One of these reﬂections concerned ‘‘the need for effective tool sup-', 'In a keynote at RE’04, van Lamsweerde[59] (S8) reﬂected on goal-oriented requirements engineering research and practice. One of these reﬂections concerned ‘‘the need for effective tool sup- 1300 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', 'port”, and Objectiver was presented, together with some related lessons learned and challenges. Objectiver is a commercial toolset (www.objectiver.com) supporting the KAOS method. This is a ma- ture and well-documented environment which evolved from the GRAIL prototype. The Objectiver toolset makes it possible to gener- ate requirements documents in a closely monitored form. The structure of these documents stems from the goal reﬁnement graph and from requirements documents templates which reﬂect company-speciﬁc standards (IEEE 830[31] is also included). The requirements document contains a glossary of terms generated from the object model, textual annotations retrieved from the model, and ﬁgures selected by the user through drag-and-drop from the goal, object, agent and operation sub-models. Objectiver is the only work found in this SLR that we would be sure of catalo- guing as ‘‘industrial practice”. 4.2.4. Other requirements speciﬁcation derivations from goal modelling The joint proposal of the Yu & Mylopoulos and Du Bois & Dubois groups [61] (S9) combines two agent-oriented frameworks for requirements engineering: the speciﬁcation of (primarily func- tional) requirements with Albert starting from goal-based business modelling with i*. Albert is a rigorous language which uses a ﬁrst- order temporal logic. This approach assumes that the requirements process can iterate between the levels ofspeciﬁcation and under- standing towards a requirements speciﬁcation: the level of speciﬁ- cation or modelling prescribeswhat agents should do or know, and the understanding or analysis level describeswhy agents relate to each other in a certain way, and why they might prefer some other conﬁguration of relationships. The objective of this work is not to translate semi-formal i* diagrams into Albert formal speciﬁcations but to use i* as a ﬁrst modelling notation for eliciting high-level goals before converting them into ﬁner formal requirements. This is, therefore, an ‘‘integrative” approach (seeC.Mode in Table 4), but is a preliminary, early work which should elaborate on the method needed to obtain the system requirements. The proposal is shown through an academic example. GBRAM (Goal-Based Requirements Analysis Method) is a require- ments engineering method outlined by Antón and Potts[7] (S10), and is used to infer goals from espoused requirements and to then derive more complete operational requirements from those goals. This study has been included in the SLR since the last activity of GBRAM related to goal reﬁnement is ‘‘Operationalize”, which con- sists of translating goals into operational requirements for the ﬁnal requirements speciﬁcation. These requirements are speciﬁed by means of templates containing a reﬁned goal, pre and post-condi- tions, and associated scenarios. Note that the mode of this study is ‘‘integrative” and not ‘‘generative” (Table 4). GBRAM does not, therefore, propose algorithms, rules or patterns with which to de- rive the operational requirements, but provides a set of heuristics and involves the timely posing of systematic questions, the relax- ation of initial goals by consideringobstacles (anything that can happen to thwart a goal), and the exploration of scenarios. An interesting contribution of this paper is that requirements docu- ments should be ‘‘living documents” in the sense that they keep track of requirements, open issues that appear and disappear dynamically, and organizational requirements that encode certain important information for the requirements engineering process (for example, the person who has a good knowledge of certain requirements, or the person who will ultimately be affected by a decision). The output of the whole process is a requirements doc- ument whose structure is based on the main functional areas with- in the system, each containing the following subsections: Goals, Functional Requirements, Non-functional Requirements, and Orga-', 'ument whose structure is based on the main functional areas with- in the system, each containing the following subsections: Goals, Functional Requirements, Non-functional Requirements, and Orga- nizational Requirements. One limitation of the study is that the link between goals and non-functional requirements is not consid- ered. This work, conducted by means of Action-Research, has been validated in several real case studies including an e-commerce application. 4.2.5. Deriving requirements from business modelling The generation of requirements from business process models includes an article by Cox et al.[17] (S11) which attempts to (1) discover the applicability of Michael Jackson’sproblem frameprop- ositions to complex, industrial projects; (2) link business process models by means of the RAD (Role-Activity Diagram) notation on these problem frames; and ﬁnally (3) derive requirements from the process models. In our view, thisderivation of requirements is not systematic, but is rather one of the steps to obtain a problem frame related to the process model and involves the production of a requirements speciﬁcation in which the requirements engineer applies his or her knowledge to the problem. We believe that, in spite of the title of the paper, the derivation of requirements plays a secondary role in this work. The research is validated through an industrial e-business system and the case study is conducted using Action-Research. The research into the derivation of requirements starting from business process models also includes the contribution of Türetken et al.[58] (S12), who propose a pattern to specify part of the pro- cess models written in eEPC notation in natural language. This pat- tern is automatically applied through a tool and, according to the authors, provides an important productivity gain, although 40% of the requirements generated needed modiﬁcation. The reported case studies are two large military applications. This work, there- fore, is in line with those of Maiden et al. (Section4.2.2), although only one requirements pattern is used and it does not take into ac- count the generation of conditional sentences. 4.2.6. Deriving requirements from UML models Arlow et al.’s proposal for literate modelling (Section4.2.1)i s related to UML but does not provide speciﬁc algorithms, rules or patterns to derive requirements from UML models. Apart from use cases and scenarios (discussed in next Section 4.2.7), only one proposal regarding UML has been found. Meziane et al. [47] (S13) introduce another interesting line of work in the SLR: natural language generation systems. They pro- pose theGeNLangUML (Generating Natural Language from UML) prototype, which generates English speciﬁcations that ‘‘para- phrase” UML 1.5 class diagrams by making use of a linguistic ontol- ogy calledWordNet. Meziane et al.’s aim is twofold: (1) on the one hand they wish to provide users with two different views of the system speciﬁcation at any time: UML and natural language; (2) on the other hand they see the motivation for their tool in a reverse engineering process during the maintenance stage, to enable back- wards transformation allowing the stakeholders to ‘‘visualize” the changes in the system’s implementation in natural language. System evolution is derived from source code, design notation – UML – and system speciﬁcation in natural language. Text genera- tion is exempliﬁed by means of an academic case study on a University system. The speciﬁcation generated includes statements such as ‘‘A person is a professor or a student”, ‘‘A professor has a name, a home address, parking privileges, a date, a seminar and seminar overseen”, ‘‘Zero or many professors instruct zero or many seminars”, ‘‘A person lives at an address”, ‘‘An address has a street, a city, a state and a zip code”, and so on. GeNLangUML extends the ModEx (Model Explainer) system, an earlier work by Lavoie et al. [39].', 'a city, a state and a zip code”, and so on. GeNLangUML extends the ModEx (Model Explainer) system, an earlier work by Lavoie et al. [39]. Meziane et al. identify some weaknesses in their approach, which is purely academic. Firstly, the proposal relies on naming conventions extracted from text books. These conventions might change in industrial practice and the tool would have to be conﬁg- ured accordingly. Secondly, the level of abstraction of the text gen- J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1301', 'erated is close to the level of abstraction of the initial UML model. Thirdly, the authors postulate that an ideal system to generate nat- ural language from object-oriented models would include class diagrams, interaction diagrams (sequence and collaboration), state diagrams, activity diagrams and OCL. Regarding interaction dia- grams, we doubt that it is necessary to include collaboration dia- grams since in our experience they are not concerned with requirements, but with design. Sequence diagrams can, on the con- trary, be used to describe use case scenarios. Finally, the authors of this work state that it can be complemented by Burke and Johan- nisson’s [14] research into the translation of OCL speciﬁcations into natural language. 4.2.7. Use cases, scenarios and user stories Use cases constitute a well-known technique to elicit, analyze, specify and validate requirements that have a simple, widespread graphical representation, although use cases are mainly textual in nature [38]. This section deals with use cases, scenarios, and user stories in combination with textual requirements and docu- ments. We have included a heterogeneous set of proposals whose intent is different but, we believe, complementary in that they can be applied by following a logical sequence of action. (1) We ﬁrst discuss Maiden et al.’s research aimed at improving the complete- ness of requirements by analysing system scenarios [42,44,46]. This process uses the existing use case model as a starting point and derives new scenarios, taking into account situations which have not yet been considered. This derivation of new scenarios leads, in turn, to new requirements. (2) We then examine the works of Berenbach [10,11] and Firesmith [28], which address the generation of natural language requirements from use case models. Berenbach is concerned with generating the hierarchy of requirements while Firesmith proposes a pattern with which to de- rive the requirements’ text. Cabral and Sampaio[15], in contrast, research the generation of formal language requirements from use case models. (3) Finally, Daniels et al.[19] and Probasco and Lefﬁngwell [49] deal with the requirements documents to be developed from use cases and scenarios, integrating traditional shall-statement requirements and use cases. These last two papers do not strictly deal with the generation of textual requirements from models, but have been included in the SLR in order to exem- plify the guidelines concerning the contents of the requirements document to be built when use cases and textual requirements are combined in requirements engineering. Maiden et al. have carried out long-term research to study the development process of scenarios and use cases to identify missing requirements needed to ensure the requirements speciﬁcation is complete. These authors have developed a database containing a hierarchy of abnormal and error conditions that is used to generate candidate, alternative event courses to be validated. These alter- nate courses are raised fromwhat-if questions related to a normal course of events. The database contains 54 classes of abnormal behaviors and states, which can be used either manually as a checklist for each event or automatically through the ART-SCENE tool in the RESCUE process. This general taxonomy has been ex- tended with knowledge from several application domains. An early version of this tool (then called CREWS-SAVRE) is presented in greater detail in Maiden et al.[42] (S14) through the example of a retrospective scenario analysis of part of the London Ambulance Service. Mavin and Maiden[46] (S15) then go on to study what types of scenario and which walkthrough techniques are most effective for discovering requirements through two case studies: (1) a simulator in the naval warfare domain at BAE SYSTEMS; and (2) Conﬂict Resolution Assistant (CORA-2) in the air trafﬁc management system domain at Eurocontrol. Mavin and Maiden', '(1) a simulator in the naval warfare domain at BAE SYSTEMS; and (2) Conﬂict Resolution Assistant (CORA-2) in the air trafﬁc management system domain at Eurocontrol. Mavin and Maiden provide guidelines for scenario-based requirements discovery, and curiously suggest that systematic walkthroughs of simple sce- narios that do not contain excessive domain knowledge are more effective for discovering system requirements. Building on this work, Maiden and Robertson[44] (S16) carry out a retrospective analysis of a previous experience to investigate how use cases and scenarios were developed during the application of the sce- nario-based RESCUE process in DMAN, an air trafﬁc management system for the UK’s National Air Trafﬁc Services. Maiden and Robertson also study the connection between scenarios and requirements in natural language speciﬁed through the VOLERE template. Maiden et al.’s inﬂuential research is still being extended. We shall now report on additional, subsequent work with the aim of complementing the analysis of the previously selected studies. This consists of (1) an extension to ART-SCENE to include rich media scenarios [62], which serves to study the improvement of require- ments discovery by using other scenario forms, such as visual sim- ulations of agents in the domain, and which are presented to the stakeholders alongside text scenarios in ART-SCENE; and (2) the Mobile Scenario Presenter (MSP) tool[54], which is an extension of ART-SCENE and serves to investigate the use of PDAs (Personal Digital Assistants) to undertake ART-SCENE scenario walkthroughs on site. Maiden et al. study then[43] whether visual simulations of scenarios and scenario walkthroughs in the work context can trigger requirements that might not be discovered with ART-SCENE scenario walkthroughs. Berenbach [10] (S17) has designed an algorithm for the auto- matic extraction of requirements from use case diagrams. In our view, this author conceives use case diagrams in a particular way, as conceptual diagrams or rather asfeature diagrams [35] of the domain under study. This algorithm is used to create a require- ments tree: the so-calledabstract use casesare mapped ontofea- tures and sub-features, while the so-called concrete use cases are transformed into detailed requirements for which project tasks may be created and test cases can be generated. This approach con- centrates on the generation of the hierarchy of requirements rather than on the requirements text itself. This work has been tested in several complex models from Siemens operating companies. In [11] (S18), Berenbach builds on these results to report on the syn- thesis of text-based requirements and the so-calledmodel-driven requirements engineering. This synthesis results in a requirements engineering approach that seamlessly integrates use cases, fea- tures and requirements. In this work Berenbach proposes a set of best practices and recommendations, including automated use case and SRS document generation. Berenbach’s industrial experi- ence has led to the discovery that the approach described in this paper [11] takes about one third to one half the time of a tradi- tional approach owing to scalability. Firesmith’s aim[28] (S19) is to derive a set of complete, unam- biguous, and veriﬁable requirements starting from ‘‘incomplete and vague” stories, scenarios and use cases. To that end, Firesmith proposes the derivation of textual requirements from use case path interactions by using the following standard format: ‘‘If atrigger occurs when certainpreconditions hold, then the system shall per- form a required set ofactions and shall be left in a required set of post-conditions”. This template is manually applied to the use case speciﬁcation, producing a requirements speciﬁcation in natural language which the stakeholders are able to understand and vali- date. Firesmith postulates that the extra work needed to build a requirements speciﬁcation is therefore soon recovered owing to', 'language which the stakeholders are able to understand and vali- date. Firesmith postulates that the extra work needed to build a requirements speciﬁcation is therefore soon recovered owing to the effort saved during the other software development activities. The textual requirements generated are speciﬁed in natural lan- guage to enable stakeholders to validate them. However, these requirements are long and complex and their readability may be difﬁcult. This problem, which Firesmith associates with the ‘‘unavoidable complexity of complete requirements”, can be miti- gated by breaking the sentences into their constituent parts. The 1302 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', 'author shows the applicability of the proposal through an aca- demic, classical ATM example. The research of Cabral and Sampaio[15] (S20) introduces the generation of formal speciﬁcations from use case models. These authors propose a strategy through which to automatically trans- late use cases written in a subset of English (CNL,Controlled Natural Language) into a speciﬁcation in CSP process algebra. CNL includes an ontology which describes the speciﬁc entities in the application domain. Use cases are speciﬁed by means of two levels of tem- plates: (1) user view use cases, which design how actors interact with the system; and (2)component view use cases, which specify the system behavior based on user interaction with system compo- nents. This approach can be useful in those domains in which requirements consistency is especially important, such as telecom- munications. This work speciﬁcally arises from a research project in collaboration with Motorola. The resulting formal speciﬁcation can be used to automatically generate test cases. This formal spec- iﬁcation is not legible to stakeholders, but we do not believe that this is a problem since they can validate the initial use case model. The use of a controlled language also helps to avoid ambiguity in use cases templates. This approach is entirely supported by a tool- kit consisting of an Ms Wordplug-in to edit use case speciﬁcations according to the CNL grammar; a translator of CNL use cases to CSP; and FDR, a CSP model checker, to check reﬁnement between use and component views. Daniels et al.[19] (S21) propose a practical method with which to integrate use cases and shall-statement requirements. For these authors use cases are not requirements, but a vehicle to discover requirements. They propose the use of aFunctional Requirements Segment in the Speciﬁc Requirements section in the use case tem- plate in which to contain the functional shall-statement require- ments that the requirements engineer extracts manually from the sentences of the scenario (note that a sentence in a use case can contain multiple functional requirements). The textual requirements retain their context since they are traced to the use case event or sequence of events from which they were derived. When the requirements are documented as a set of shall-state- ments without context it is difﬁcult to comprehend them and fully interpret their intent and dependencies. The Supplementary Requirements Speciﬁcation also contains the requirements that do not ﬁt well within the context of only one use case. If a traditional requirements speciﬁcation is to be developed, then the require- ments engineer manually copies and pastes the requirements doc- umented in each use case’s Speciﬁc Requirements along with the requirements in the Supplementary Requirements Speciﬁcation. The approach is illustrated through an academic example of a microwave oven software product line. A short paper by Probasco and Lefﬁngwell[49] (S22) in a similar vein to that of Daniels et al. shows a Rational Software proposal to combine use cases and traditional requirements speciﬁcations through a simple construct called theSRS Package. This package pulls together the complete set of software requirements for a sys- tem, which may be contained in a single document, multiple doc- uments, a requirements repository (consisting of the requirements’ text, attributes and traceability), use case speciﬁcations, and the use case diagrams. Probasco and Lefﬁngwell’s document[49] is only a white paper and no concrete validation is mentioned. 4.2.8. Deriving requirements from user interface modelling Finally, but of no less importance, a set of papers dealing with the generation of requirements documentation starting from user interface models exists. Jungmayr and Stumpe[33] (S23) propose extended usage models that consist of three sub-models:scenario model, action model, anduser interface model: (1) the scenario mod-', 'interface models exists. Jungmayr and Stumpe[33] (S23) propose extended usage models that consist of three sub-models:scenario model, action model, anduser interface model: (1) the scenario mod- el describes the semantics of the usage model in terms of goals that can be achieved when using the software, tasks that have to be accomplished in order to achieve the goals, and solutions, which are scenarios on how to use the software to solve a particular task; (2) the action model is a state machine that deﬁnes all possible se- quences of user inputs (actions) and covers the information of a conventional usage model; and (3) the user interface model de- scribes the user interface of the software and the interface ele- ments that are themselves subject to user inputs. In this context, an HTML document can be automatically generated which struc- tures the information already introduced in the extended usage models and can serve as user documentation. Extended usage models need a considerable effort to be built, but this is also true of conventional usage models. The structure of the output docu- ments is shown in detail in[33]. Data in the output documents are linked by means of predeﬁned string patterns. This work has been evaluated in a communication application to query commer- cial databases retrieving bibliographic references. The example is simple but well evaluated. An old study by Smith[55] (S24) at MITRE Corporation pre- sents a method to generate functional requirements concerning the user interface by starting from interface checklists. In these checklists the analyst indicates whether a concrete feature of the interface is required, useful,o r not needed. The so-called pat- terned prose is used in order to generate the requirements in nat- ural language by paraphrasing the questionnaires. In essence, patterned prose consists of a hierarchically related set of sen- tences, phrases and words, and their logical connectors, which are organized (and numbered) in correspondence with the struc- ture of the checklists which are used to extract them. In our view, Smith’s approach is very close to the design of the user interface: it seems closer to areal level of speciﬁcation (physical) than to an essential (logical) level. Today applications have a GUI which is far more complex than those illustrated in the examples of this work, and we therefore believe that an extensive domain analysis would be needed to redeﬁne the checklists. However, we found the idea of patterned prose interesting. An implementation in UNIX was available, but no validation procedures were reported in this paper. 4.2.9. Limitations of this SLR As was previously mentioned, the searches in this SLR were de- ﬁned by using certain overloaded software engineering terms, such as model, requirement, and speciﬁcation as a starting point. It was necessary to use these general-use terms to seek studies in a het- erogeneous ﬁeld such as that deﬁned in Section2.1. The terms used to build the query (Section2.2) have more synonyms, and some of these terms are homonyms. We cannot therefore guarantee that all the work related to the scope of this SLR has been found by means of the queries. For example, one important paper in this SLR (that of Meziane et al.[47] (S13)) was directly included in the review as a result of our previous knowledge of the papers of the ﬁeld, and did not appear in the results of the searches. However, after check- ing the references to the papers included in the SLR, some of which have been published in top journals and conferences, we believe that we have analyzed the contents of an illustrative sample of the ﬁeld. As explained in Section3.1, we have introduced two deviations from protocol in order to improve the completeness of the set of selected papers. We therefore believe that the discussion is more comprehensive, although the repeatability of the SLR is challenged. We considerTool and Validation to be part of the quality assess-', 'selected papers. We therefore believe that the discussion is more comprehensive, although the repeatability of the SLR is challenged. We considerTool and Validation to be part of the quality assess- ment criteria (Section2.5), but these items are difﬁcult to quantify with precision. Most of the automated support involved in the se- lected studies is not accessible and it is therefore difﬁcult to pre- cisely assess the tools’ maturity level. What is more, the correctness of the validation procedures cannot be precisely evalu- ated from the reading of the papers. J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1303', '5. Product requirements derivation in software product lines While analyzing the selected papers we concluded that one weakness of this SLR was that no paper speciﬁcally addressed soft- ware product lines (SPLs). We knew of one commercial SPL tool, GEARS [3], which automatically derives a software requirements speciﬁcation that describes in natural language the decisions made in the instantiation of the SPL variation points and the mandatory features of SPL products. We intuitively believed that SPLs consti- tute a domain in which the derivation of requirements from mod- els can play an interesting role: product derivation in SPL implies navigation through adecision space (for example, a feature model [35] or an independent variability model[48]), solving a set of var- iation points to produce the speciﬁcation of a product in the prod- uct line. We believed that the speciﬁcation of this product in natural language in a requirements document could help clients to understand the conﬁguration of the product they purchase. These reﬂections led us to extend the previous general SLR by per- forming a further SLR on product requirements derivation in SPLs. This new SLR is reported in full in this section. Detailed processes for requirements modelling and guidance for requirements derivation are not sufﬁciently studied by the re- search community and are not handled by SPL tools[24]. In the SPL paradigm there is a consensus on the current need to pay greater attention to the product derivation process (see, for exam- ple, Käkölä and Dueñas’ preface in[34]). In this vein Bühne et al. [13] present an overview of the research on the derivation of appli- cation speciﬁcations in SPLs. From a general point of view, they examine the development of an application requirements speciﬁ- cation, paying particular attention to the treatment of stakehold- ers’ new requirements which are not included in the SPL (called deltas), but without addressing the speciﬁc subject of this paper. This Section5, in contrast, is devoted to the approaches that explic- itly consider requirements derivation from the SPL variability mod- els in application requirements engineering. Thus, a new research question arises: RQ3. Which approaches take into account requirements deriva- tion from SPL models? The search strings shown inTable 5 were used to answer this question. The searches were performed in the Google Scholar search engine since it had delivered the most complete results in the general SLR. The inclusion and exclusion criteria were the same as those of the general SLR, but were obviously constrained to the domain of SPLs. The data collection and data analysis procedures were also the same. In this way 5 papers were selected (seeTable 6). By following S25 inTable 6 we have considered interesting to include S26 in the selection (a deviation from protocol), and thus 6 studies were ﬁnally selected which are discussed in the remain- der of this section. The combination mode of the 6 studies is gen- erative and their scope is requirement. There is no study that addresses requirements documents generation. The initial models are feature and variability models (4 and 2 studies, respectively), while the target models are natural language (4 of the approaches) and formal notations (3 studies). Note that S27 addresses both for- mal notation and natural language. Regarding product requirements derivation in SPLs, an early work by Hein et al.[30] (S25 inTable 6) claims that domain knowl- edge must be formalized to enable a partly automated require- ments derivation process starting from a feature model. Hein et al. focus on afeature model (which extends that of FODA[35]), and a so-called requirements model (which encompasses the requirements texts and parameter deﬁnitions of the product or the domain). A requirement addresses variability through parame- ters, and each requirement that contains parameters can be seen as', 'requirements texts and parameter deﬁnitions of the product or the domain). A requirement addresses variability through parame- ters, and each requirement that contains parameters can be seen as a requirement template. Features are modelled as the nodes of a tree. Basically, each node corresponds to a parameter. The feature tree therefore describes the possible values that can be assigned to a parameter. Traditional RMDB tools were designed to support sin- gle products, not product families, and these tools are thus out of the range of applicability to SPLs. Hein et al. report on several prob- lems based on their experience in managing domain analysis work products with RMBD tools, especially with QSS DOORS. Further- more, Hein et al. impose 9 requirements on a RMDB tool for SPLs, including ‘‘automatic generation of speciﬁc requirements texts”. Generation of requirements speciﬁcations can be partially auto- mated by the use of templates instantiated with different variants. This is regarded as very useful in the product line context, where different products and product variants have to be derived from abstract domain descriptions. A textual representation is more suitable for discussion with domain experts than more formal models in which the information is dispersed. These authors state that the application derivation problem can be handled by knowl- edge based systems, and especially conﬁguration systems, such as that of Konwerk[53] (S26). These results are consolidated and val- idated in a real-scale industrial experiment at Bosch, in the Car Periphery Supervision (CPS) domain. The previous work by Hein et al. imposes that the representa- tion of domain knowledge must be formalized to enable a partly automated derivation process starting from the feature model. The authors also discovered that RMDB tools do not provide any customer guidance for product conﬁguration in the SPL derivation process. We believe that a short paper by Rabiser et al.[50] (S27) can also be placed in this group. These authors present a tool suite, called DOPLER (Decision-Oriented Product Line Engineering for effec- tive Reuse), which supports variability modelling and product con- ﬁguration in SPLs. DOPLER includes aConﬁgurationWizard to make decisions during product derivation, allowing product customiza- tion, requirements capture, and conﬁguration generation. In a sim- ilar vein to that of Hein et al., Rabiser et al. claim that to fully exploit the beneﬁts of SPLs it is essential to make product line models accessible to non-technicians such as sales people or even customers who make important decisions during product deriva- tion. Decisions under the responsibility of a customer are listed as questions written in natural language, which are answered by yes/no. A graph and a tree-based view depict dependencies be- tween decisions to support navigation in the decision space pro- vided by the variability model. Newly captured requirements which are not yet covered by the SPL are also considered, and are described using the VOLERE template. Thus the product conﬁgura- tion generated from the variability model consists of a formalized tabular representation of the selected features together with the templates of the new textual requirements. The conﬁgurationWiz- ard can also be used to launch simulator applications based on the selected assets. The references in this paper allow us to determine that this development research is taking place to support a typical scenario in product derivation and sales processes, regarding an SPL whose goal is the automation of continuous casting in SIE- MENS VAI’s steel plants. Tavakoli and Reiser [57] (S28) propose a requirements library based on a new variability model to manage variability and com- monality of product families at the requirements level. This study stems from a process improvement initiative at DaimlerChrysler in the automotive domain, where variability is complex due to the', 'monality of product families at the requirements level. This study stems from a process improvement initiative at DaimlerChrysler in the automotive domain, where variability is complex due to the involvement ofhierarchical product lines: model ranges of vehicles contain electronic control units which are, in turn, product lines. Requirements play a vital role in this industry, since the other soft- ware artefacts such as software architecture or code are developed by suppliers. Efﬁciency when creating speciﬁcations in short devel- opment cycles is of increasing importance. In this approach, deriv- ing product speciﬁcations from the requirements library means a 1304 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', 'stepwise decrease in variability. Requirements reuse within the requirements library is driven by selecting features, which in turn pre-select requirements in the database. Therefore the require- ments engineers’ main task shifts from developing requirements speciﬁcations to deriving requirements speciﬁcations (including the derivation of a variability deﬁnition). Tool support has been developed through the commercial RMDB tool DOORS[2] by using its script language dxl (DOORS eXtension Language). The authors state that the applicability of the approach in a tool is an important step toward the practical implementation of the approach. Although well-grounded in the automotive domain, it would ap- pear that this work has not yet been applied in practice. Djebbi and Salinesi’s ongoing research into RED-PL (Require- ments Elicitation & Derivation for Product Lines)[22] (S29) is aimed at deﬁning a method for product requirements derivation in SPLs. Unlike previous approaches, RED-PL does not consist of selecting a product conﬁguration from a SPL variability model in order to re- trieve the requirements specifying the product to be built. In con- trast, the RED-PL approach consists of (1) eliciting customers’ requirements (customers are free to choose the way in which they specify requirements); (2) matching customers’ requirements with SPL requirements, generating a set of wanted/unwanted require- ments; and (3) deriving a consistent requirements collection that is optimal for a set of customers and company constraints (e.g. rev- enue, cost, resources, time). A constraint solver is proposed to match customers’ needs, which are expressed textually, with the SPL requirements using similarity analysis techniques. SPL require- ments are currently speciﬁed as feature diagrams although this ap- proach is applicable to different SPL modelling languages (e.g. use cases, goals, UML, aspects). In[22], Djebbi and Salinesi use Integer Linear Programming (ILP) for similarity analysis. Ms Excel was used to apply ILP to a case study on blood analyzers developed in the STAGO Instruments Company. Some difﬁculties were ob- served while applying the method: (1) the experiment showed that ILP could not be used properly where complex requirements had to be expressed. Matching was difﬁcult due to a lack of precision in the formulation of customer requirements. Difﬁculties were found not only with regard to terminology, but also conceptual mis- matches between customers’ requirements and SPL requirements (different levels of abstraction, different views). (2) ILP also has scalability problems. Furthermore, Djebbi et al.[23] (S30) is essen- tially a summary of[22] in which another constraint solver tech- nique (Constraint Programming) is used, which shows better results in the case study. The authors report the ongoing develop- ment of a tool prototype via GNU-Prolog Solver. Although the authors do not explicitly deﬁne the term ‘‘requirement”, it seems that a requirement in this context can be assimilated to a state- ment on the conﬁguration of the SPL products. This work does not speciﬁcally address the generation of textual requirements. However, this approach has been analyzed because it introduces a different modus operandi and its output, an instantiated feature model, which we consider a kind of formal notation, can be easily presented in textual form. 6. Conclusions and further work The literature on the generation of textual requirements and requirements documents starting from (business or software) models has been comprehensively reviewed and synthesized in this paper. It could be argued that this is a narrow topic within the mainstream research in requirements engineering. However, although the generation of requirements and documents from models has received relatively little attention in research (30 papers selected in the SLR), sound studies in literature have been found to corroborate a justiﬁcation of interest in this line. Table 5', 'papers selected in the SLR), sound studies in literature have been found to corroborate a justiﬁcation of interest in this line. Table 5 Search of SPL-related approaches (research question RQ3) in Google Scholar. Search strings Studies found Candidate studies Selected studies +‘‘requirements derivation” +‘‘software product lines” 14 3 3 +‘‘deriving requirements” +‘‘software product lines” 6 2 2 +‘‘requirements integration” +‘‘software product lines” 4 0 0 +‘‘integrating requirements” +‘‘software product lines” 14 0 0 Table 6 Systematic review studies regarding RQ3. ID Author/s [ref.] (source) Date C. Mode Scope Initial model Gen. Statem. Method Tool Validation Gen. Int. Req. Doc. FL NL Section 5. Product’s requirements derivation in software product lines S25 Hein et al.[30] (gs) 2000 j h j h Feature model h j Not speciﬁc QSS DOORS (ICS) Car Periphery Supervision at Bosch S26 Schlick and Hein [53] (cited by S25) 2000 j h j h Feature model h j Not speciﬁc Konwerk conﬁguration system (ICS) Car Periphery Supervision at Bosch S27 Rabiser et al. [50] (gs) 2007 j h j h Variability model jj (V) Not speciﬁc DOPLER (ICS) CL2 SPL for continuous casting in steel plants at SIEMENS VAI S28 Tavakoli and Reiser [57] (gs) 2007 j h j h Variability model and requirements library h j Not speciﬁc Based on DOORS through a dxl plug-in (ICS) Electronic control units at DaimlerChrysler S29 Djebbi and Salinesi [22] (gs) 2007 j h j h Feature model (applicable to other SPL modelling lang.) j h RED-PL Ms Excel and Ms Excel Solver (ICS) Blood analyzers at Stago Instruments S30 Djebbi et al. [23] (gs) 2007 j h j h Feature model (applicable to other SPL modelling lang.) j h RED-PL On-going GNU-Prolog Solver-based prototype (ICS) Blood analyzers at Stago Instruments Source: gs (Google Scholar);C. Mode(Combination Mode): Gen.: Generative/Int.: Integrative;Scope: Req.: Requirement/Doc.: DocumentGen. Statem. (Generated Statements): FL: Formal Language/NL: Natural Language; (V) Volere;Validation: (ACS) Academic Case Study/(ICS) Industrial Case Study/(IP) Industrial Practice; A-R: Action-Research. J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1305', 'Moreover, as 17 of the 30 selected papers have been published in the last ﬁve years, we believe that it is reasonable to say that the interest in this topic, although narrow, is moderately increasing, especially in the SPL domain. In our view, literate modelling – which we believe could broadly encompass all the work selected in this SLR – is an excellent idea which seems to have had little relevance in prac- tice. What is more, the essence of this idea can be also found in a closely-related area: knowledge management. For instance, Eriks- son [25] claims that ‘‘unfortunately, there is a surprisingly large gap between knowledge modelled in ontologies and the text doc- umenting the same knowledge” and he proposes an approach to combine documents and ontologies, ‘‘allowing users to access the knowledge in multiple ways”. We believe that there is value in making a stronger effort in the generation of textual require- ments and documentation starting from models, in particular in literate modelling and SPLs, so that: (1) these approaches can be empirically evaluated in more industrial settings; (2) the set of techniques from which requirements and documents can be automatically generated are broadened (particularly covering the entire set of UML techniques related to requirements speciﬁcation); (3) requirements engineering process models are reﬁned to take into account the use of literate modelling; and (4) commercial RMDB tool support is developed. We believe that without proper tool support these approaches are not truly applicable in practice, especially in the context of agile developments. We believe that both researchers and practitioners can beneﬁt from an improvement in the readability of software engineering models, making these models available to a wider spectrum of stakeholders and thus improving their usability and facilitating their validation. Furthermore, regarding practitioners, one of the challenges that the technology roadmap of the industrial consor- tium ITEA has identiﬁed in relation to requirements-driven process management is precisely that special views are required that can give each stakeholder an appropriate vision of the requirements and how they are realised[32]. Moreover, the quality of the docu- mentation generated by means of the literate modelling approach can serve practitioners in improving the vision of requirements documents as a contract between customers and developers. In addition, we believe that CASE developers can add value to their tools by including a literate modelling approach. When examining the evolution of software engineering in the last two decades, Som- merville [56] concludes that when looking beyond technology to the fundamental processes, much has stayed the same in software engineering. One of the reasons mentioned is precisely that CASE tools are still essentially diagram editors with some checking and code-generation functionality. The results of the SLR have led us to propose ﬁve key issues that should be supported by a general-purpose RMDB tool which seam- lessly integrates graphical and textual models of the system. In terms of the taxonomy deﬁned in Section3.2, the tool should be generative for both requirements and documents. In addition: 1. The tool should enable the automatic or closely monitored gen- eration of requirements documents integrating (business and software) models and natural language requirements, ideally ensuring appropriate bidirectional traceability links between models and textual requirements. 2. The requirements documentation structure should follow the structure of the models in order to improve the requirements engineer’s understanding: the section-subsection structure of the requirements documentation should emulate the models’ structure in some way. Redundancy can be used to facilitate the understanding and/or modiﬁcation of the documentation, but must be tracked by the tool. 3. Requirements documentation should be produced in a format', 'the understanding and/or modiﬁcation of the documentation, but must be tracked by the tool. 3. Requirements documentation should be produced in a format that enables its modiﬁcation. Requirements documents are liv- ing documents which should reﬂect open issues that may appear, change, and disappear. Therefore, formats that do not enable change, such as PDF, should not be exclusively used to browse textual requirements documents. These formats can obviously be used to create snapshots of the requirements spec- iﬁcation under development, but we believe that the tool should manage the textual requirements in a manner which promotes change. 4. Once generated, the requirements documentation should be maintained insynchronization with the models, so that if an ele- ment changes in any of the twoviews of the system (themodels’ view or the text view), the tool will propagate the necessary changes to the related elements of the other view. Moreover, the requirements generation should be linked to an iterative and incremental development in such a way that the require- ments generation will not necessarily affect the whole model, nor will further generations overwrite the changes directly effected in that documentation. A version control mechanism is therefore needed. If a change in the textual view does not cor- respond to any element in the models adelta must be registered and controlled. 5. In addition to key issue 2, the tool should enable the tailoring of the documentation according to its target readership or its intended use. The textual view, in particular, is not necessarily unique: there could be aclient contract view,a nanalyst view,a developer view, etc. There are a number of commercial tools that have links with the ﬁeld reviewed in this paper. For instance, well-known commercial RMDB tools such as Requisite Pro[5], DOORS [2], and Caliber-RM [1] are integrated with the related UML-based tools of the suite to allow the synchronization of use case models and diagrams. Fur- ther work could therefore be carried out to review the tool market in order to analyze the combination between models and require- ments in commercial RMDB tools. Acknowledgements Partially ﬁnanced by the CICYT (Science and Technology Joint Committee), Spanish Ministry of Science and Technology (DEDALO Project, TIN2006-15175-C05-03), and the Science and Technology regional ministry of the Junta de Comunidades de Castilla-La Man- cha (MELISA Project, PAC08-0142-335). References [1] Caliber-RM, Borland, 2009, <http://www.borland.com/caliber/index.html>. [2] Doors, Telelogic (IBM Company), 2009, <http://www.telelogic.com/doors>. [3] GEARS, BigLever Software, 2009, <www.biglever.com/overview.html>. [4] OMG’s MetaObject Facility (MOF), Object Management Group, 2009, <http:// www.omg.org/mof/>. [5] RequisitePro, IBM Rational Software, 2009, < http://www-306.ibm.com/ software/rational/>. [6] D. Alrajeh, A. Russo, S. Uchitel, Inferring operational requirements from scenarios and goal models using inductive learning, in: Intl. Workshop on Scenarios and State Machines: Models, Algorithms, and Tools, ACM, Shanghai, China, 2006. [7] A.I. Antón, C. Potts, The use of goals to surface requirements for evolving systems, in: 20th Intl. Conf. on Software Eng. (ICSE’98), IEEE Computer Society, Kyoto, Japan, 1998. [8] J. Arlow, W. Emmerich, J. Quinn, Literate modelling – capturing business knowledge with the UML, in: «UML»’98: Beyond the Notation, First Intl. Workshop, Springer LNCS 1618, Mulhouse, France, 1998. [9] J. Arlow, I. Neustadt, Enterprise Patterns and MDA: Building Better Software with Archetype Patterns and UML, The Addison-Wesley Object Technology Series, Addison-Wesley, Boston, 2004. [10] B. Berenbach, The automated extraction of requirements from UML models, in: 11th Intl. Conf. on Requirements Eng. (RE’03), IEEE Computer Society, Monterey, CA, USA, 2003. 1306 J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307', '[11] B. Berenbach, Comparison of UML and text based requirements engineering, in: Companion to the 19th Conf. on OO Programming, Sys., Lang., and App. (OOPSLA’04), ACM Press, Vancouver, BC, Canada, 2004. [12] J. Biolchini, P. Gomes Mian, A.C. Cruz Natali, G. Horta Travassos, Systematic Review in Software Engineering, TR-ES 679/05, Sys. Eng. and Computer Sci. Rio de Janeiro, Dep. COPPE/UFRJ, 2005. [13] S. Bühne, G. Halmans, K. Lauenroth, K. Pohl, Scenario-based application requirements engineering, in: Software Product Lines: Research Issues in Engineering and Management, Springer, Berlin, 2006, pp. 161–194. [14] D. Burke, K. Johannisson, Translating formal software speciﬁcations to natural language: a grammar-based approach, in: Logical Aspects of Computational Linguistics Conf., Bordeaux, France, 2005. [15] G. Cabral, A. Sampaio, Formal speciﬁcation generation from requirement documents, Electron. Notes Theor. Comput. Sci. 195 (2008) 171–188. [16] CMMI, CMMI, Capability Maturity Model Integration, v.1.2, 2006, <http:// www.sei.cmu.edu/cmmi/general/index.html>. [17] K. Cox, K.T. Phalp, S.J. Bleistein, J.M. Verner, Deriving requirements from process models via the problem frames approach, Inform Software Technol. 47 (5) (2005) 319–337. [18] B.H.C. Cheng, J.M. Atlee, Research directions in requirements engineering, in: Future of Software Eng. (FOSE’07), Minneapolis, USA, 2007. [19] J. Daniels, R. Botta, T. Bahill, A hybrid requirements capture process, in: INCOSE 15th Annual Intl. Symposium on Sys. Eng., Rochester, NY, 2005. [20] A.M. Davis, in: Just Enough Requirements Management: Where Software Development Meets Marketing, Dorset House, New York, NY, 2005. [21] R. De Landtsheer, E. Letier, A. van Lamsweerde, Deriving tabular event-based speciﬁcations from goal-oriented requirements models, Requirements Eng. 9 (2) (2004) 104–120. [22] O. Djebbi, C. Salinesi. RED-PL, a method for deriving product requirements from a product line requirements model, in: 19th Intl. Conf. on Advanced Inf. Sys. Eng. (CAiSE’07), Trondheim, Norway, 2007. [23] O. Djebbi, C. Salinesi, D. Diaz, Deriving product line requirements: the RED-PL guidance approach, in: Asia–Paciﬁc Software Eng. Conf. (APSEC 2007), Nagoya, Japan, 2007. [24] O. Djebbi, C. Salinesi, G. Fanmuy, Industry survey of product lines management tools: requirements, qualities and open issues, in: 15th IEEE Intl. Requirements Eng. Conf. (RE ‘07), Delhi, India, 2007. [25] H. Eriksson, The semantic-document approach to combining documents and ontologies, Int. J. Human–Comput. Stud. 65 (7) (2007) 624–639. [26] A. Finkelstein, W. Emmerich, The future of requirements management tools, in: Inf. Sys. in Public Administration and Law, 2000. [27] D. Firesmith, Modern requirements speciﬁcations, J. Object Tech. 2 (1) (2003) 53–64. [28] D. Firesmith, Generating complete, unambigous, and veriﬁable requirements from stories, scenarios, and use cases, J. Object Tech. 3 (10) (2004) 27–39. [29] R.F. Goldsmith, Discovering Real Business Requirements for Software Project Success, Artech House Publishers, Boston, London, 2004. [30] A. Hein, J. MacGregor, M. Schlick, Requirements and feature management for software product lines. in: Deutscher Software-Produktlinien Workshop (DSPL-1), Kaiserslautern, Germany, 2000. [31] IEEE, Std 830-1998, Guide to Software Requirements Speciﬁcations, Resource and Technique Standards, vol. 4, The Institute of Electrical and Electronics Engineers, Inc., IEEE Software Eng. Stds. Collection, 1999. [32] ITEA-Ofﬁce, ITEA Technology Roadmap for Software Intensive Systems, second ed., May 2004, <http://www.itea-ofﬁce.org>. [33] S. Jungmayr, J. Stumpe, Another motivation for usage models: generation of user documentation, in: CONQUEST’98, Nüremberg, Germany, 1998. [34] T. Käkölä, J.C. Dueñas (Eds.), Software Product Lines. Research Issues in Engineering and Management, Springer, Berlin Heidelberg, 2006. [35] K. Kang, S. Cohen, J. Hess, W. Novak, A. Peterson, Feature-Oriented Domain', 'Engineering and Management, Springer, Berlin Heidelberg, 2006. [35] K. Kang, S. Cohen, J. Hess, W. Novak, A. Peterson, Feature-Oriented Domain Analysis (FODA) Feasibility Study, SEI (Software Eng. Inst.), Carnegie Mellon Univ., Pittsburgh, PA, 1990. [36] B.A. Kitchenham, Guidelines for performing Systematic Literature Reviews in Software Engineering, EBSE Tech. Report, EBSE-2007-01, Software Eng. Group, School of Computer Sci. and Math., Keele Univ. (UK), Dep. of Computer Sci., Univ. of Durham (UK), 2007. [37] B.A. Kitchenham, O.P. Brereton, D. Budgen, M. Turner, J. Bailey, S. Linkman, Systematic literature reviews in software engineering – a systematic literature review, Inform Software Technol. 51 (1) (2009) 7–15. [38] C. Larman, Applying UML and Patterns, third ed., Prentice Hall, Upper Saddle River, NJ, 2005. [39] B. Lavoie, O. Rambow, E. Reiter, The ModelExplainer, in: 8th Intl. Workshop on Natural Lang. Generation (INLG’96), Herstmonceux Castle, England, 1996. [40] E. Letier, A. van Lamsweerde, Deriving operational software speciﬁcations from system goals, in: 10th Symposium on Foundations of Software Eng. 2002 (FSE’02), ACM Press, Charleston, South Carolina, USA, 2002. [41] N. Maiden, S. Jones, C. Ncube, J. Lockerbie, Using i * in requirements projects: some experiences and lessons, in: E. Yu (Ed.), Social Modeling for Requirements Engineering, MIT Press, 2007. [42] N. Maiden, S. Minocha, K. Manning, M. Ryan, CREWS-SAVRE: systematic scenario generation and use, in: 3rd Intl. Conf. on Requirements Eng. (ICRE’98), IEEE Computer Society, Colorado Springs, CO, USA, 1998. [43] N. Maiden, C. Ncube, S. Kamali, N. Seyff, P. Grünbacher, Exploring scenario forms and ways of use to discover requirements on airports that minimize environmental impact, in: 15th Intl. Req. Eng. Conf. (RE’07), New Delhi, India, 2007. [44] N. Maiden, S. Robertson, Developing use cases and scenarios in the requirements process, in: 27th Intl. Conf. on Software Eng. (ICSE ‘05), ACM Press, St. Louis, MO, USA, 2005. [45] N.A.M. Maiden, S. Manning, S. Jones, J. Greenwood, Generating requirements from systems models using patterns: a case study, Requirements Eng. 10 (4) (2005) 276–288. [46] A. Mavin, N. Maiden, Determining socio-technical systems requirements: experiences with generating and walking through scenarios, in: 11th Intl. Conf. on Requirements Eng. (RE’03), IEEE Computer Society, Monterey, CA, USA, 2003. [47] F. Meziane, N. Athanasakis, S. Ananiadou, Generating natural language speciﬁcations from UML class diagrams, Requirements Eng. 13 (1) (2008) 1– 18. [48] K. Pohl, G. Böckle, F. van der Linden, Software Product Line Engineering, Foundations, Principles and Techniques, Springer, Berlin Heidelberg, 2005. [49] L. Probasco, D. Lefﬁngwell, Combining software requirements speciﬁcations with use case modeling, in: Rational White Paper, 1999. [50] R. Rabiser, D. Dhungana, P. Grunbacher, K. Lehner, C. Federspiel, involving non- technicians in product derivation and requirements engineering: a tool suite for product line engineering, in: 15th IEEE Intl. Requirements Eng. Conf. (RE ‘07), Delhi, India, 2007. [51] B. Regnell, E. Kamsties, V. Gervasi, Summary of the 10th anniversary workshop on requirements engineering: foundation for software quality, in: Requirements Eng.: Foundation for Software Quality 2004 (REFSQ’04), Riga, Latvia, 2004. [52] S. Robertson, J. Robertson, Mastering the Requirements Process, second ed., Addison-Wesley, New York, NY, 2006. [53] M. Schlick, A. Hein, Knowledge engineering in software product lines, in: European Conf. on Artiﬁcial Intelligence (ECAI 2000), Workshop on Knowledge-Based Sys. for Model-Based Eng., Berlin, Germany, 2000. [54] N. Seyff, F. Graf, P. Grünbacher, N. Maiden, The mobile scenario presenter: a tool for in situ requirements discovery with scenarios, in: 15th Intl. Requirements Eng. Conf. (RE’07), New Delhi, India, 2007. [55] S.L. Smith, Patterned prose for automatic speciﬁcation generation, in: Conf. on', 'Requirements Eng. Conf. (RE’07), New Delhi, India, 2007. [55] S.L. Smith, Patterned prose for automatic speciﬁcation generation, in: Conf. on Human Factors in Computing Sys., ACM Press, Gaithersburg, Maryland, USA, 1982. [56] I. Sommerville, Software Engineering, seventh ed., Pearson Education Limited, Boston, 2004. [57] R. Tavakoli, M.O. Reiser, Reusing requirements: the need for extended variability models, in: Intl. Symposium on Fundamentals of Software Eng. (FSEN 2007), Tehran, Iran, 2007. [58] O. Türetken, O. Su, O. Demirörs, Automating software requirements generation from business process models, in: 1st Conf. on the Principles of Software Eng. (PRISE’04), Buenos Aires, Argentina, 2004. [59] A. van Lamsweerde, Goal-oriented requirements engineering: a roundtrip from research to practice, in: 12th Requirements Eng. Conf. 2004 (RE’04), IEEE Publishers, Kyoto, Japan. [60] A. van Lamsweerde, L. Willemet, Inferring declarative requirements speciﬁcations from operational scenarios, IEEE Trans. Software Eng. 24 (12) (1998) 1089–1114. [61] E. Yu, P. Du Bois, E. Dubois, J. Mylopoulos, From organization models to system requirements. A cooperating agents approach, in: 3rd Intl. Conf. on Cooperative Inf. Sys. (CoopIS-95), Vienna, Austria, 1995. [62] K. Zachos, N. Maiden, A. Tosar, Rich Media Scenarios for Discovering Requirements, IEEE Software 22 (5) (2005) 89–97. J. Nicolás, A. Toval / Information and Software Technology 51 (2009) 1291–1307 1307']","['GENERATION   OF   REQUIREMENTS   SPECIFICATIONS  FROM   SOFTWARE   ENGINEERING   MODELS Following are the techniques used to genera- te requirements from software engineering  models: • Literate modelling: In this approach a  new type of document is defined, Busi - ness Context Document (BCD), in which  (1) the diagrams are embedded as fi - gures in a document written in natural  language and the visual model is para - phrased; (2) brief explanations of the re- levant UML syntax are included in foot - notes; (3) real, yet  simple  examples  of   the  notation  are  presented  to  illustra- te  specific  points; and (4) some impor- tant questions such as the rationale are  emphasized. Literate models  are  thus   UML  models  that  are  embedded  in   a  natural  language  text which explains  them.  • Deriving requirements from business  modelling: (1) discover the applicabili - ty of Michael Jackson’s problem frame  propositions to complex, industrial pro- jects; (2) link  business  process  models   by  means  of  the  RAD  (Role Activity   Diagram) notation on these problem fra- mes; and finally (3) derive requirements  from the process  models.  The  resear - ch  is  validated  through  an  industrial   e business system and the case study is  conducted using action research. • Deriving  requirements  from  UML  mo- dels:  The  GeNLangUML  (Generating  Natural Language from UML) generates  English specifications that “paraphrase”  UML 1.5 class diagrams by making use  of a linguistic ontology called WordNet.  (1) on the one hand they wish to pro - vide users with two different  views of  the system  specification  at  any  time:   UML  and  natural  language;  (2)  on  the   other hand  they  see  the  motivation   for  their  tool  in  a  reverse  engineering   process during the maintenance stage,  to enable backwards transformation  allowing the stakeholders to “visualize”  the changes in the system’s implemen - tation in natural language.  • Use  cases,  scenarios  and  user  sto - ries:  An  approach  proposes  a  process   that uses the existing use case model  as a starting point and derives new sce - narios, taking  into  account  situations   which  have  not  yet  been  considered.   This derivation  of  new  scenarios  le - ads,  in  turn,  to  new  requirements.   Another approach is concerned with ge- nerating the hierarchy of requirements  while other proposes  a  pattern  with   which  to  derive  the  requirements’   text.  In  contrast, another research pro- poses the generation of formal langua - ge requirements from use  case  models.   This  work  has  been  tested  in  several   complex  models  from Siemens opera - ting companies.  • Deriving  requirements  from  user  in - terface  modelling:  A  research  propose  extended usage models that consist of  three sub models: scenario model, ac - tion model, and user interface model:  (1) the scenario model describes the se- mantics of  the  usage  model  in  terms   of  goals  that  can  be  achieved  when   using  the software, tasks that have to  be accomplished in order to achieve the  goals, and solutions, which are scena - rios on how to use the software to solve  a particular task; (2) the action model is  a state machine that defines all possib - le sequences of  user  inputs  (actions)   and  covers  the  information  of  a  con- ventional  usage model;  and  (3)  the   user  interface  model  describes  the   user  interface  of  the software and the  interface elements that are themselves  subject to user inputs. In  this  context,   an  HTML  document  can  be  automa - tically  generated  which structures the  information already introduced in the  extended usage models and can serve  as user documentation.  A  proposed  taxonomy  with  two  dimensions   of  generation  methods  is  composed  by  “combination mode” and “scope”.  With regard to the combination mode, there  are two approaches, the generative and the  integrative:', 'of  generation  methods  is  composed  by  “combination mode” and “scope”.  With regard to the combination mode, there  are two approaches, the generative and the  integrative: • Generative: These approaches propose  algorithms, rules or patterns to gene - rate textual requirements starting from  models. These proposals are presented  with or without automatic support and  the text generated can be written in na- tural or formal language:  Natural  language:  These  approaches   generate  candidate natural language  requirements which must be valida - ted. In this group there are the genera- tion of requirements from i* models;  the  generation  of  English  specifi - cations  that  paraphrase  UML class  diagrams; the derivation of textual re- quirements from use cases,  scenarios   and  user  stories;  and  the  generation   a hierarchy of requirements from use  case diagrams.  Formal  language:  These  approa - ches  generate  requirements written  in a formal notation. Formal methods  bring benefits such as  the  mechanical   analysis  of  a  system  to  check  for   deadlock and  livelock  freedom,  but   the  adoption  of  formal  methods  in  industry is challenged by the cost and  complexity involved in the formal  spe- cification  of  the  system. Hence  some   approaches  in literature  have  investi- gated  the  derivation  of  formal speci- fications  from  requirements  models.   In  this  group  we should highlight  the work on goals models, which ad - dresses the automatic  generation  of   operational requirements  described   by means  of  pre   and  post conditions   and  triggers,  and  the generation  of   requirements  described  by  means  of   the  tabular technique.  In  addition, - there  is  the  generation  of  opera - tional requirements  in  CSP  process   algebra  from  use  case specifications.  • Integrative:  These  approaches  do  not   provide  algorithms,  rules  or  patterns   to generate  requirements  from  mo - dels,  but  rather  open ended  guides   to  relate models  and  textual require - ments. The resulting requirements  can   be written  in natural language or in a  formal notation. With regard to the scope of the approach,  some proposals deal with the generation of  single requirements while others deal with  the generation of requirements documents.  • Requirement:  These  approaches  deal   with  the  generation  of  requirements   or sets of requirements, but do not ad - dress the requirements documents in  which the requirements should be pla - ced. One example is the approach that  deals with deriving requirements from  i* SD models.  • Documental: These approaches concen- trate on the manual, automatic or semi- -automatic  generation  of  requirements   documents.  For  example,  the  tool  Objectiver  which  semi automatically   generate  requirements  documents  structured from a goal specification.  The literate modelling trend as a whole  can also be included in the documental  approach. MAIN FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Joaquín Nicolás, Ambrosio Toval, On the generation of requirements specifications from software engineering models: A systematic literature re- view, Information and Software Technology, Volume 51, Issue 9, September 2009, Pages 1291 1307, ISSN 0950 5849, http://dx.doi.org/10.1016/j. infsof.2009.04.001. Keywords: Specification generation from  software engineering model Textual requirements generation  from software engineering model Requirements document generation  from software engineering model Systematic literature review Who is this briefing for? Software engineers practitioners  who want to make decisions  about software requirements generation from models based on  scientific evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic re- view conducted by Nicolás et al. What is systematic reviews? cin.ufpe.br/eseg/systematic-reviews What is included in this brie- fing?', 'extracted from the systematic re- view conducted by Nicolás et al. What is systematic reviews? cin.ufpe.br/eseg/systematic-reviews What is included in this brie- fing? The main findings of the original  systematic review. Evidence characteristics through  a brief description about the ori- ginal systematic review and the  studies it analized. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. For additional information  about this briefing: cin.ufpe.br/eseg/briefings This briefing reports evidence on methods  and techniques dealing with the generation,  of requirements from models based on  scientific evidence from a systematic review.']","**Title: Enhancing Requirements Specification Through Model Integration**

**Introduction:**
This Evidence Briefing summarizes findings from a systematic literature review conducted by Nicolás and Toval, which explores the generation of textual requirements specifications from software engineering models. The goal is to highlight the benefits of integrating graphical models and textual requirements, providing insights for practitioners in software engineering.

**Core Findings:**
1. **Importance of Integration**: Combining software engineering models (like UML diagrams) with textual requirements enhances clarity and understanding for stakeholders. This dual approach allows for precise communication of requirements while ensuring they are accessible to clients.

2. **Automation Benefits**: The automatic or closely monitored generation of requirements from models can significantly reduce the effort involved in writing specifications. This leads to improved completeness of specifications, as stakeholders find it easier to review generated requirements than to recall them from memory.

3. **Traceability Enhancement**: Utilizing models to derive textual requirements automates traceability, which is crucial for requirements management. This aligns with best practices outlined in the Capability Maturity Model Integration (CMMI), promoting bidirectional traceability between requirements and development products.

4. **Techniques Identified**: The review identifies various techniques for generating requirements, including:
   - **Literate Modelling**: This approach integrates natural language documentation with UML models, making it easier for stakeholders to understand the system's requirements.
   - **Goal-Oriented Requirements Engineering**: Techniques such as KAOS and Objectiver facilitate the generation of both textual requirements and structured requirements documents from goal models.
   - **Use Cases and Scenarios**: The review highlights the effectiveness of using scenarios to derive new requirements, enhancing the completeness of specifications.

5. **Challenges and Limitations**: Despite the advantages, the review notes that many existing approaches do not fully support the synchronization of generated requirements with evolving models. As models change, maintaining alignment with textual requirements remains a challenge.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, including requirements engineers, project managers, and developers, who seek to improve the efficiency and effectiveness of requirements specification processes.

**Where the findings come from?**
The findings are derived from a systematic literature review conducted by Joaquín Nicolás and Ambrosio Toval, focusing on the integration of software engineering models and textual requirements in the requirements specification process.

**What is included in this briefing?**
The briefing includes an overview of the benefits of integrating models and textual requirements, key techniques identified in the literature, and the implications for practice in software engineering.

**What is NOT included in this briefing?**
This briefing does not provide detailed statistical analyses or exhaustive descriptions of individual studies. It focuses on summarizing core findings and practical implications rather than presenting raw data.

**To access other evidence briefings on software engineering:**
Visit [Evidence Briefings Repository](http://ease2017.bth.se/)

**Original Research Reference:**
Nicolás, J., & Toval, A. (2009). On the generation of requirements specifications from software engineering models: A systematic literature review. Information and Software Technology, 51(6), 1291-1307. https://doi.org/10.1016/j.infsof.2009.04.001"
"['13th International Conference on Evaluation and Assessment in Software Engineering    1  Harmfulness of Code Duplication -   A Structured Review of the Evidence    Wiebe Hordijk, María Laura Ponisio1, Roel Wieringa  University of Twente, The Netherlands  hordijkwtb|m.l.ponisio|roelw@ewi.utwente.nl    Duplication of code has long been thought to decre ase changeabili- ty of systems, but recently doubts have been expressed whether  this is true in general. This is a problem for researchers because it  makes the value of research aimed against clones uncertain, and for  practitioners as they cannot be sure whe ther their effort in redu cing  duplication is well-spent. In this paper we try to shed light on this i s- sue by collecting empirical evidence in favor and against the neg a- tive effects of duplication on changeability. We go beyond the flat  yes/no-question of h armfulness and present an explanatory model  to show the mechanisms through which duplication is su spected to  affect quality. We aggregate the evidence for each of the causal links  in the model. This sheds light on the current state of duplication r e- search and helps practitioners choose between the available mitig a- tion strategies.   Keywords: Duplication, clones, changeability, maintainability, structured review  1. INTRODUCTION  Duplication of source code is an important factor that is suspected to affect the quality of systems in terms of cha n- geability and the number of errors. We want to investigate how duplication affects quality. There is a vast body of  research about code duplication, and in this review we aggregate the current knowledge about the effects of dupli- cation on changeability and error levels.  1.1 Problems  There is a lot of literature about code duplication, but only a few studies have attempted to investigate if and how  duplication actually has a negative effect on changeability and error levels . Therefore surprisingly little is known  empirically about the harmfulness of duplication . This is a problem for researchers because many investigations  are based upon the a ssumption that clones are harmful, and if this assumption is false, the value of th e research  would be called into doubt. Fortunately we have found positive though inconclusive evidence for this assum ption.  For practitioners this lack of knowledge about harmfulness of duplication is a problem because they do not know if  they should inves t effort in avoiding or removing clones, and if so, how to prioritize those efforts between different  kinds of clones. Based on practitioners‟ reports and our own experience, we see that little use is made of clone d e- tectors in practice. We think that soli d knowledge about the harmfulness of clones would make such tools more  attractive to practitioners.  1.2 Contributions   This study is a structured review of the evidence in code clone literature for harmfulness of duplication. We have  reviewed all the relevant clone duplication literature about the effects of duplic ation on system quality that we found                                                              1 Supported by the Netherlands Organisation for Scientific Research, project nr 638 .004.609 (QuadRead).', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 2  using the systematic review method outlined in section 2. We integrate the results in a causal model, showing i n- termediate variables between duplication and q uality attributes, and hypothesized causal links. We aggregate  claims from the literature in section 3 and present the aggregated knowledge per hypothesis in section 4. We co n- clude that the main questions about the effect of duplication on quality are not yet answered, but that co nsiderable  progress has been made which positively supports sub -hypotheses of those questions, and that there is a rea ssur- ing amount of co rrelation between findings of different studies. For example, findings about how many clones are  co-changed are quite consistent. There are however also contradictions among findings, which require further i n- vestigation, such as the question “what demands more change effort, cloned code or non -cloned code?”. For r e- searchers our review suggests topics for further empirical research.  For practitioners we include a table which lists the conclusions from the literature along with strategies to mitigate  the negative effects of duplication on system quality. When there is more support for a conclusion, t here will be  more need for its corresponding mitigation strategy. We hope that this table will help practitioners select proper a c- tions to reduce negative effects of duplication in their own projects.    2. METHODOLOGY  We gathered information only from prima ry research, not from empirical observations. We followed a method d e- scribed by Kitchenham‟s general procedure for performing systematic reviews (Kitchenham, 2007) . Though the  entire investigation is not completely repeatable, as human judgment is involved in interpreting articles, Kitche n- ham‟s method makes steps of the process as repeatable as possible. The following sections summarize our steps.  2.1 Identification of research  We searched a number of literature sources with several search criteria, aimed at finding a set of articles with the  most complete possible coverage of the field of code clones. We chose the criteria to reflect our main  research  question: “What effects does duplication of code have on the quality attributes of a software system?”  We searched  the following dat abases: DBLP, AC M Portal, CiteSeer and Scopus with the following search terms: “code clone”,  “clones”, “code | duplication” ( “duplication” yields too many false hits), on December 17, 2008. We discarded a r- ticles that were not about code clones; examples include compiler optimization, set theory and DNA research. After  our extensive searches we have validated the completeness of the search actions by looking for re ferences in the  selected papers to other papers that were not present in our sample but that would pass our se arch criteria. We  found only 2 such references, which were workshop papers. Altogether, this yielded 153 p apers. A full list is in  (Hordijk et al., 2009); to our knowledge we have thus exhausted all available evidence in the period under review.  2.2 Selection of primary studies  We applied the following criteria to the found sources for inclusion in this review.   - The article must be published in a journal or conference proceedings. This excludes drafts of article s and technical r e- ports found on web sites of research groups.  - The article should present evidence for a relation between duplication and a quality attribute of the system, or between  intermediate variables, e.g. between duplication and co-change. We judged this by reading the entire papers, not just the  titles and abstracts, because sometimes evidence is stated in a case study which is used as an illustration of, for exa m- ple, a clone detector, on which the paper focuses.', 'titles and abstracts, because sometimes evidence is stated in a case study which is used as an illustration of, for exa m- ple, a clone detector, on which the paper focuses.  - The article should not be published before 1990. This boundary is chosen arbitrarily to limit the search for sources.  We have not applied quality criteria to the primary sources, because so few papers passed the selection criteria that no add i- tional selection was needed. The resulting set contains 18 papers, which are discussed in section 3.  2.3 Aggregation of evidence  We analyzed the evidence in the included papers. When a claim was made, we analyzed the external validity, that  is, for which situations the claim would hold. For example, if a paper draws conclusions from an experiment with  one system, then those conclusions may not be valid in another system because of difference s between those sys- tems. However if conclusions are based on five different open source Java systems, and another p aper draws the  same conclusions from two other open source Java systems, we may generalize the conclusions to the class of  open source Java systems. An overview of the conclusions is in section 4. Since we are interested in the circu ms- tances under which du plication is harmful, we also list what is known about the context factors under which these  conclusions hold.    3. CAUSAL MODEL  Figure 1 contains our hypothesized causal model of the effects of duplication on system quality. The model has  been constructed after analysis of the reviewed literature; for space reasons in this paper we only summarize the', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 3  evidence for and against each of the hypotheses in the model. The names in the figure are variables and the a r- rows are causal effects. Each arrow repr esents a separate hypothesis such as, “duplication increases co -change”.  This should be read thus: in general, all else being equal, a system with more duplication will exhibit more co - change than an equivalent system with less duplication.       FIGURE 1: Causal Model    We now discuss the concept of change and then review the evidence concerning the role of each of the four inte r- mediate variables in our model.  3.1 General Model of Change  To be able to speak about effects of duplication on changeability we introduce a general model of change  processes. A change is a set of activities that starts when a change request is presented by the customer to the  maintenance team and that ends when the adapted software product is accepted by the customer. To see the po s- sibly negative influence of duplication, we need to compare the situation where the part of the software that needs  to be changed has some form of duplication with the situation in which this particular duplication is not present, all  else being equal. It is possible  that the only way of avoiding the duplication would be to make the software more  complex, which would have its own negative effect on changeability.  Without disregard for the differences in sof t- ware development and maintenance metho dologies we generalize the change process as consisting of three  phases.   - Analysis: the maintainers analyze the change request and the code, determining which parts of the code need to be  changed.  - Coding: the maintainers change the code.  - Test: the maintainers test whether the ch anged system meets the requirements of the change request, and whether no  new bugs have been introduced. When errors are found the process loops back to coding or analysis, depending on the  type of error.    This distinction makes it easier to explain the me chanisms through which duplication is thought to affect chang ea- bility. Suppose a change request reaches the maintenance team. After analysis the team concludes which parts of  the code need to be changed. Now several conditions can hold.   - Two or more parts that are clones of each other must be changed in the same way, so that after the change they are still  clones of each other.  o The programmers change them in the same way. This is co -change and costs extra effort because the same  editing operations are performed multiple times.  o The programmers change them in different ways, or change only one. This is inconsistent change. This leads to  errors during test which must be fixed, incurring extra effort during the change process, or when the errors are  not found it can lead to errors later on.  - There are no such same changes in clones. Duplication in other parts of the system may still affect the effort required for  analysis. Simply put, the fact that the total size of the code is larger makes it harder to find the part one is looking for.   –  Duplication  Inconsistent  change  Co-change  Code  comprehension  Changeability  Errors  –  +  +  – +  System quality  +  –  Code  size  +  –', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 4  3.2 Co-change  We define co-change as the situation where two or more clone occurrences are changed in the same way because  of one change request, so that after the change they are still clone occurrences of each other.   Co-change can be o bserved after the fact by detecting clones and changes in multiple consecutive versions of a  system. When in two clone occurrences in version n some change is performed, and in version n+1 they are still  clone occurrences of each other, they have bee n co-changed. This operationalisation leaves open the possibility  that the occurrences have by coincidence been changed in the same way due to completely different change r e- quests. This seems unlikely enough to disregard the quantity of such „accidental co -changes‟.  Code clone genealogies  (Kim et al., 2005) examined clone genealogies in 2 open source Java programs of 20 to 25 KLoC each. A genea lo- gy is a clone set  (a set of code fragments which are all clones of each other)  traced over multiple versions of the  system. In this study CCFinder was used to detect clones of at least  30 tokens. They found that 36% to 38% of  code clone genealogies contained consistently changing pa tterns. Even though clone genealogies are not the  same as clone pairs, we can conclude that co -change was observed in a significant portion of clone pairs. T his is  strong evidence that at least some clones negatively affect changeability through co -change. A smaller portion of  genealogies – 26% to 34% of all genealogies which were abandoned in later versions – dissolved because occu r- rences were modified in dif ferent ways. The r esearchers have investigated these genealogies by hand, so we can  assume that these were not changed inconsistently while they should have been co -changed. This is thus a group  of clones which did not harm changeability. It‟s not clear wh at happened to the rest of the abandoned genealogies.  Presumably, they were refa ctored. That must have cost effort, which is another way in which duplication increases  change effort. The researchers also manually examined the gen ealogies that lived in more  than half of the history  of their systems to see if the clones could be refactored. The researchers judged that about two thirds of these g e- nealogies contain unrefacto rable clones, and most genealogies containing unrefactorable clones exhibited co - change. It has not been invest igated if there are any clone properties that predict whether a clone will be co - changed or evolve differently. Conclusions (with supporting evidence):  - A significant portion of clones are co-changed (2 small open source Java programs).  - Of the clones that are removed in later versions, a minority is removed because of different changes, while the rest is  presumably removed through refactoring (2 small open source Java programs).  - Clones that are not refactorable tend to live longer than  refactorable clones and are very likely to exhibit co -change (2  small open source Java programs).  How clones are maintained  (Aversano et al., 2007)  have investigated how clones are maintained in two Java systems of 25 and 200 KLoC r e- spectively. They used SimScan, an AST -based clone detector, with a minimum clone size of 6 LOC. In one system  they found that, from one version to the next, 45% of clones were co -changed; 32% were changed in different  ways; 2% were refactored; 5% were removed otherwise; and 16% were changed inconsiste ntly (see paragraph 3.3  about inconsistent change). The latter group can be identified by late propagation. Late propagation is the situation  where clone occu rrences are changed in different ways, and are later changed so they are the same again; this', 'where clone occu rrences are changed in different ways, and are later changed so they are the same again; this  can be detected automatically. Not every inconsistent change leads to late propag ation, because not all inconsi s- tencies lead to e rrors. Not every late propagation is preceded by inconsistent change: it may be caused by similar  change requests to different modu les at different times.  The researchers have manually identified many causes of  late propagation, and argue that the majority of cases of late propagation are not actually i nstances of inconsistent  change, but rather they are cases where similar change req uests are performed for different subsystems at diffe r- ent times. This seems more like co -change than inconsistent change. In another system they found 74% co - change, 13% independent change and 13% inconsistent change, though in a very small total number of  clones. We  can conclude that these numbers are roughly consistent with those of Kim et al. The authors have observed an  interesting relation between the granularity (block, method or class) of clones and how they are mai ntained: most  class-level clones we re co -changed, while method -level clones most often evolve independently. This indicates  that clones with larger granularity have more chance of being harmful (inducing co -change) than smaller ones.  Conclusions (with support):  - A significant portion of clones are co-changed (1 small and 1 large open source Java program).  - Clones with class-level granularity have more co-change than clones with smaller granularities (1 small and 1 large open  source Java program).  Visualizing maintenance patterns of clones  (Balint et al., 2006) present a tool, integrated in the Moose reengineering suite that visualizes maintenance pa tterns  of clones. This tool can show lines of cloned code that are maintained consistently or inconsistently and m ade con- sistent later. Without giving numbers they report that many clones are changed consistently, and a few are', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 5  changed inconsistently but made consistent later. This agrees with the findings of Kim et al. and more sp ecifically  with those of Aversano et al.  Quantification of co-change  (Krinke, 2007) investigated co-change of clone groups in five systems (of 10, 90, 118, 193 and 228 KLoC respe c- tively) written in Java, C and C++. They used Simian, a text -based clone detector, with a minimum clone size of 11  lines. They found that 45% to 55% of the clone sets are co -changed all along the investigation period of 200  weeks. This is higher than the range reported by Kim et al. (36% - 38%) and narrower than that reported by Ave r- sano et al. (45% - 75%). A likely explanation for these differences is the normalization of source code which is done  before detecting changes: comments are r emoved and the code is reformatted so that changes to whitespace are  disregarded. The paper reports that without this normalization, the ratio of co -changed clones is much lower, more  towards that reported by Kim et al. Neither Kim nor Aversano report that  they use normalization of the code, so this  explanation is plausible. Also, Krinke uses more and larger systems. We conclude therefore that the ratio of co - changed clone sets is likely to lie in the 45% -55% range. Krinke et al. have also investigated how the ratio of co - change depends on minimum clone size: this has only small and inconsistent impact. Conclusions:  - 45% to 55% of clone sets are co -changed during a long part of the life cycle of systems (5 systems: Java, C, C++; var i- ous sizes).  - Clone size has little impact on the chance of co-change (5 systems: Java, C, C++; various sizes).  Correlation between cloning ratio and change coupling  (Geiger et al., 2006)  have investigated the correlation between cloning and co -change on file level. As a measure  for cloning between files they use the ratio of lines that are duplicated between those files over the total number of  lines, and as a measure for change coupling they use the ratio of the number of times the two files are changed at  the same time (same check -in) over the total number of changes for the files. Clones are detected u sing CCFinder  on a number of releases of Mozilla, in the C programming language, using a minimum clone size of 30 tokens.  They show a dot plot which indicates that, on average, files with more cloning b etween them have more change  coupling. The correlation is not statistically significant, but for files with very high cloning ratios the change co upling  is also very high. It has not been investigated how often the changes are in cloned code, or whether th e same  changes are made in different files. Therefore this amounts to rather weak evidence for duplication‟s negative e f- fect on changeability through co-change. Conclusions:  - Files between which there is a larger ratio of cloning seem to be changed in the same release more often than other files  (weak correlation; 1 large C system).  - Files between which there is a very high ratio of cloning are very often changed in the same release (1 large C system).  Comparing co-change on method level  (Lozano et al., 2007)  have investigated methods that have contained clones at some point in their life. For those  methods they have compared the periods during which the me thods contained clones with the periods during  which they did not. They measured the number of changes to the methods themselves and the number of co - changes between pairs of methods between which there is a cloning relation. The definition of co -change here is  that the methods were both changed within a very short time interval; the changes need not be the same. The first  finding is that in periods with clones , the methods were changed more often than in periods without clones. This', 'finding is that in periods with clones , the methods were changed more often than in periods without clones. This  result does not have a clear explanation. It is seconded by Mo nden but contradicted by (Krinke, 2008) (see section  3.6) in which cloned code was found to be changed less often than non -cloned code. The second finding is that in  periods in which a method pair has  a cloning relation, the pair is co-changed less often than in periods with out a  cloning relation. This second result is very surprising, and the authors offer no explanation, nor can we think of one.  The second result contradicts the conclusion by (Geiger et al., 2006). Conclusions:  - Methods are changed more often during periods in which they contain clones than during periods in which they do not (1  small Java system).  - Method pairs have less co-change during periods in which they have a cloning rela tion than during periods in which they  have not (1 small Java system).  Discussion  If we want to investigate the harmfulness of duplication, it is important to compare co -change between duplicated  code with co-change between non-duplicated code fragments. O f these studies, only (Geiger et al., 2006) and (Lo- zano et al., 2007) have done so. The results are contradictory, so more research is needed in which cloned code is  compared to non-cloned code with respect to co -change. Also we need investigations with more systems and with  more attention to which circumstances may explain the different findings. Perhaps the differences can be explained  from the fact that they investigate d different systems; perhaps because they measured on different levels of gran u- larity; perhaps other clone properties or context factors play a role.', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 6  3.3 Inconsistent change  Inconsistent change is the situation where two or more occurrences of the same clo ne set should be changed in  the same way b ecause of a change request, but at least one of them is changed differently from the others. This  may lead to an error in the system when the inconsistency leads to different behavior, decreasing the reliability of   the system. In that case the bug will have to be fixed, increasing change effort. It is also possible that the inconsi s- tency does not lead to incorrect behavior, but then when the clone occurrences need to be co -changed in a later  version the inconsistenc y will make the change more difficult. In that case too, inconsistencies increase change  effort. It is hard to detect inconsistent change automatically, because it may be quite legitimate that clone occu r- rences evolve independently. Some researchers have i nvestigated changes to clones manually to see whether di f- ferent changes were inconsistencies or not. Others have used late propagation  (see §3.2, „How clones are mai n- tained‟) as an indicator.   Late propagation  Aversano et al. have found that between 13% an d 16% of clones were involved in late propagation in two sy stems,  of which only a minority however could be attributed to inconsistent change upon manual inspection. This indicates  that inconsistent change does not occur very often. Conclusions:  - A small portion of clones are changed inconsistently which is repaired in later versions. However only a part of the cases  in which a change is applied to other occurrences later on (late propagation) stem from inconsistent change (1 small and  1 large open source Java program).  Copy-paste related bugs  (Li et al., 2004, Li et al., 2006)  have built a tool to detect copy -paste related bugs in code. They search for clones  which differ only in the names of identifiers used between occurrences. When i n one occurrence of a clone, iden- tifier A is used exactly in the places where in another occurrence  of the clone identifier B is used, there is no pro b- lem; but when one instance of A is changed into C, that may indicate a bug. Such bugs can easily be intro duced by  inconsistent changes. Li et al. have found a number of bugs in Linux, FreeBSD, Apache and PostgreSQL using  their tool, CPMiner. To give an idea of numbers: the total number of clone occurrences in Linux is 122,282 (without  allowing gaps in the clo nes); CPMiner reports 421 potential errors due to inconsistencies between identif iers; after  manual inspection, 28 of these are real bugs and 21 are „careless programming‟, meaning they should be fixed in  the interest of maintainability although they will not lead to erroneous behavior. These findings indicate that dupl ica- tion can indeed lead to errors in software, but only in very small numbers, which agrees with the findings of Ave r- sano et al. Conclusions:  - In a small number of cases it was observed that the act of copying code led to errors (4 large open source C programs).    (Jürgens et al., 2008) are working on an approach to detect bugs caused by inconsistent change. Their work is o n- going. So far their results seem to be in line w ith the other sources mentioned in this paragraph. They found a rel a- tively small number of bugs which stem from inconsistent change. Conclusions:   - In a small number of cases it was observed that the act of copying code led to errors (1 large industrial C# program).  Context differences  (Jiang et al., 2007)  take a different approach to identifying clone -related bugs. Their intuition is that sim ilar code  should be used in the same way, so they look for differences in the context of clone occurrences. This will yield', 'should be used in the same way, so they look for differences in the context of clone occurrences. This will yield  different errors than the approaches by Li and Jürgens, but quantitatively the results are similar: a minority of  clones with context differences were confirmed to contain bugs. Conclusions:  - In a small number of cases it was observed that copying code led to errors (1 large C and 1 large Java program).  Discussion  When clones are changed inconsistently, that is clearly a source of error s. However, had the code not been dupl i- cated, it might have been more complex, and other errors might have been made when changing it. To know  whether duplication causes an increase of errors through inconsistent change, we need to compare the amount of  errors introduced into duplicated code with that in non -duplicated code. If the extra errors introduced through i n- consistent changes do not lift the amount of errors in duplicated code above the normal amount of errors in a co m- parable body of code in the same system, then we cannot hold the conclusion that duplication causes more errors.  3.4 Code comprehension  Code comprehension is inherently hard to study. Many papers assume duplicated code is harder to understand.   Maintaining mental models  (LaToza et al., 2006) have done a survey in which a majority of programmers inte rviewed for a case study reported  that finding duplicated code was a problem for them, especially when clones were created inadve rtently. The study', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 7  shows that programmers put much effort in creating rich mental models of the system they are working on, whi ch  are rarely documented, and that when trying to understand new code the biggest problem is to understand the r a- tionale of its design. 59% of the programmers indicated that „Finding all the places code has been d uplicated‟ is a  problem for them. When inte rviewed about why duplication is a problem, programmers referred to „making the  same change in different places‟, which seems to be the same as co -change; however the intervie wees used this  notion in a broad way, including cases where similar functionality  was implemented in different programming la n- guages, where similar functionality was implemented using different algorithms, or different branches of the same  code base. Latoza et al. make a case for including such cases in the definition of co -change and in code duplic a- tion research. Conclusions:  - Programmers experience that duplication hinders code comprehension.  - Researchers should be aware of types of duplication that go beyond syntactical clones.  - Clones created by different programmers are harder to unde rstand and harder to refactor than clones created by one  programmer.  - Issues with code ownership may introduce duplication.  - Changes that involve aspects (in the AOP sense) are especially hard to analyze.  Complex refactorings  (Balazinska et al., 1999) present a technique to automatically refactor certain kinds of clones in Java, repla cing the  cloned code with an application of the Strategy design pattern. Though this may help for large clones with many  occurrences, the paper contains a necessarily smaller example. Consequently, the paper exa mple contains more  classes and more lines of code after refactoring than before, which arguably increases its co mplexity. This serves  as a counter -example to the claim that clone d code is harder to comprehend than its non -cloned equivalent. For  the same reason (Fowler, 1999) argues that when the same code appears only twice, that is not yet enough reason  to refactor it; the duplicated code may be more understandable. Conclusions:   - Duplicated code is not necessarily more complex or harder to understand than the equivalent non -duplicated code (1 Ja- va example).  Discussion  We cannot draw conclusions about the relation between dupl ication and code comprehension. We can conclude  that co-change is a problem for programmers, and that co -change goes farther than the syntactic clones that can  be detected with clone  detectors. Co-change occurs between diffe rent systems, different programming languages  and different branches of the same system. This calls for more research.   3.5 Code size   (Baker, 1992) assumes that when a clone set has n occurrences, n -1 of them could have been avoided, so the  increase is n-1 times the size of the cloned fragment. However, we shoul d compare the increase in code size due  to duplication to code size required by the alternative. (Kamiya et al., 2002) takes into account that the occurrences  have to be replaced with a fun ction call. (Balazinska et al., 1999) show that refactoring clones can be more difficult  because of dependences and diff erences in the occu rrences, so in their example, after refactoring the resulting  code is actually bigger than the cloned code was before refactoring. Conclusions:  - It is simple to measure the total size of code involved in a clone set.   - It is difficult to estimate the size of code needed in an alternative solution, for example after refactoring. Replacing clones  by an alternative does not always result in reducing the code size.  Discussion  It is easy to compute how much code is involved in duplication once t he clones have been detected, but it is hard to', 'by an alternative does not always result in reducing the code size.  Discussion  It is easy to compute how much code is involved in duplication once t he clones have been detected, but it is hard to  measure how much code can be saved by replacing clones by an alternative. When clone occurrences have no  differences between them and no dependencies on their context, they can easily be replaced by functions  or me- thods. For more complex situ ations, Balazinska et al. have presented a method to automatically replace clones by  a Strategy pattern, which works for certain well -defined clones; other clones can be refactored with other mech an- isms or by hand. It cannot be stated in general that duplication increases code size compared to altern atives.  3.6 Direct relationships  Some studies have attempted to measure the impact of duplication on changeability directly.   Revision number  (Monden et al., 2002)  tried to show a direct effect of duplication on changeability. They correlated the amount of  duplication in modules with th e modules‟ revision numbers: a higher revision number would indicate lower cha n- geability. They found a clear positive correlation between the maximum size of clones in a module and its revision  number. The number of revisions of a module however is not a g ood indicator of changeability. When more change', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 8  requests have impacted a module, that only says something about the customers needs, or perhaps about the d e- sign of a system when a module is changed for very dissimilar change requests. Also the causality m ay be r e- versed: when a module is changed more often, its design may degrade and more clones may be intr oduced. As the  authors remark, the nature of this very interesting correlation has yet to be investigated.   Number of errors  Monden et al. also have inves tigated the number of errors found in modules, and compared mo dules that contain  clones with modules that do not. They found that modules without clones have 1.7 times as many errors per line of  code than modules with clones. Their explanation is that when  one copies code that works, one will not generally  introduce bugs; this could imply that the practice of duplication reduces the number of errors in a system . However,  with clones, more lines of code are needed to implement the same functionality, and tho se extra lines dilute the  number of e rrors per line. The interesting question is whether the system as a whole would contain more or less  errors if the clones were removed; the authors do not discuss this question.   Stability  (Krinke, 2008) has investigated the stability of cloned code versus that of non -cloned code. It turns out that ove rall,  cloned code is changed less often than non -cloned code. The reason for this phenomenon has not been invest i- gated. A possible explanation is that programmers try to avoid changing cloned code because it is more difficult to  change, which would strengthen the hypothesis that duplication decreases changeability. Ho wever, it could also be  that code contains fewer clones because it  is changed more often, when those changes include refactorings to r e- move clones. Also the programmers may have been careful enough to only clone code which was not likely to be  changed often. Yet another explanation is that  most code clones are templates,  company standards etc. that are  never changed. So Krinke‟s observation is very interesting, but more research is needed before we can e xplain the  observation.  Discussion  A direct link between duplication and changeability has not been proven. It is very h ard to do so, because as yet no  good indicators for changeability are known, and it is very cumbersome to measure change effort. Also, there are  many factors that affect changeability. It seems more likely that we will advance by further investigating the me- chanisms through which duplication affects changeability than by trying to measure the effect directly.    4. CONCLUSIONS AND FUTURE RESEARCH  The goal of our research was to find out under which circumstances code duplication harms system quality. Is it  easier to change a system if we have removed the clones first? Our literature survey su ggests that a direct link be- tween duplication and changeability has not been proven yet, but not rejected either. We have refined the existing  theory by bringing together the various mechanisms through which d uplication is thought to affect quality, and we  have aggregated the evidence for and against each of these hypoth esized mechanisms. In Table 1 we list the h y- potheses from the cause/effect grap h and the evidence for and against each of them. For the first two h ypotheses,  we give two related statements which have been proven to some e xtent; those are indented. The status of the most  interesting hypotheses is that „more research is needed‟. Though  there is a large volume of research in code dupl i- cation, there are but few studies aimed at the question whether clones really are harmful, which clones are more  harmful than others, and under which circumstances.     TABLE 1: Hypotheses and their evidence  Hypothesis Evidence for Evidence', 'harmful than others, and under which circumstances.     TABLE 1: Hypotheses and their evidence  Hypothesis Evidence for Evidence  against  Status  There is more co-change between clone  pairs than between other code fragments  Geiger2006 (weak) Lozano2007 Proven nor rejected: open for investigation (see  §3.2)  Co-change occurs in 30% to 50%  of clone sets  Kim2005, Aversano2007,  Krinke2007   Proven for various open source systems in C,  C++, Java of various sizes  Changes to duplicated code result in more  errors than changes to non-duplicated code    Proven nor rejected: open for investigation (see  §3.3)  Inconsistent change, resulting in  errors, happens to a minority of  clones  Aversano2007, Li2004,  Jiang2007, Juergens2008   Proven for various open source and industrial  systems in C, C#, Java of various sizes  Duplicated code is generally harder to un- derstand than its non-duplicated equivalent  LaToza2006 Balazinska1999    Likely, considering the consensus in LaToza‟s  study, but not generally valid for each clone  (see §3.4).  Duplicated code is generally longer than its  non-duplicated equivalent   Balazinska1999 Can be proven analytically for many clones, but  there are counter-examples. It depends on the  alternative (see §3.5).', 'Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 9    In Table 2 we list all the conclusions that we have distilled from the literature, along w ith their supporters. We also  give strategies, based on our own experience and reasoning, for practitioners to mitigate the harmful effects that  clones have on system quality. We hope to make it easier for practitioners to use the knowledge present in the vast  body of literature on this topic. When there is more support for a conclusion, there will be more need for its corr es- ponding mitigation strategy. The effectiveness of the mitigation strategies themselves has not been invest igated, as  there is too little evaluation research available about mitigation strategies.    TABLE 2: Conclusions, support and mitigation strategies  Conclusion Supported by Mitigation  Co-change  A significant portion of clones are co-changed. Kim2005, Aversa- no2007, Balint2006,  Krinke2007  For co-changing clones that cannot be  refactored, consider using change propa- gating tools, such as Linked Editing  (Toomim et al., 2004).  Of the clones that are removed in later versions, a mino ri- ty is removed because of different changes, while the rest  is presumably removed through refactoring.  Kim2005   Clones that are not refactorable tend to live longer and are  very likely to exhibit co-change. 45% to 55% of clone sets  are co-changed during a long part of the life cycle of sys- tems.  Kim2005, Krinke2007 We cannot mitigate the negative effect of  these clones by refactoring, so simulta- neous editing seems like the only solu- tion.  Clones with class -level granularity have more co -change  than clones with smaller granularities.  Aversano2007 These also cause more increase in code  size. These should be the first to refactor.  Files b etween which there is a very high ratio of cloning  are very often changed in the same release.  Geiger2006 When removing clones, give priority to  files between which a lot of clones exist.  Clone size has little impact on the chance of co-change. Krinke2007   Inconsistent change  A small portion of clones are changed inconsistently which  is r epaired in later versions. However only a part of the  cases in which a change is applied to other occu rrences  later on (late propagation) stem from inconsistent change.  Aversano2007   In a small number of cases it was observed that the act of  copying code led to errors.  Li2004, Jiang2007,  Juergens2008  Consider using the approaches men- tioned as an additional means to search  for potential errors in the code, especially  when a lot of copy-paste has happened.  Code comprehension  Programmers experience that duplication hinders code  comprehension.  LaToza2006 Consider adding clone detection results to  system documentation to aid comprehen- sion.  Researchers should be aware of typ es of duplic ation that  go beyond syntactical clones.  LaToza2006   Clones created by different programmers are harder to  understand and harder to refactor than clones created by  one programmer.  LaToza2006   Issues with code ownership may introduce duplication. LaToza2006   Changes that involve aspects (in the AOP sense) are e s- pecially hard to analyze.  LaToza2006 When aspect-related code is cloned and  changes to those aspects are expected in  the future, use AOP techniques to avoid  co-change.  Duplicated code is not necessarily more complex or har d- er to understand than the equivalent non-duplicated code.  Balazinska1999 Be careful when refactoring clones to not  increase the complexity.  Code size  It is simple to measure the total size of code i nvolved in a  clone set. However it is difficult to est imate the size of  code needed in an alternative solution, for e xample after  refactoring. Removal of clones does not always result in  reducing the code size.  Baker1992, Ka- miya2002, Balazins- ka1999', 'code needed in an alternative solution, for e xample after  refactoring. Removal of clones does not always result in  reducing the code size.  Baker1992, Ka- miya2002, Balazins- ka1999  Be careful when refactoring clones to not  increase the code size.    Below we list the open questions that we have encountered in the discussions above.   - Does duplicated code get involved in co -change more often than non -duplicated code? Which kinds of clones get i n- volved in co -change and which do not? What context factors influence co -change? For example, can the different fin d-', ""Harmfulness of Code Duplication – A Structured Review of the Evidence  13th International Conference on Evaluation and Assessment in Software Engineering 10  ings of Geiger and Lozano be explained by differences in the granularity of their measurements? Comparisons between  duplicated and non-duplicated code have been performed (see 3.2) but with contradicting results.   - Are more errors introduced in cloned code than in non -cloned code? Under which circumstances? It has been shown  that the presence of clones causes errors during maintenance (see 3.3), but this effect  has not been compared to the er- rors that could have been introduced if the code had not contained these clones.  - Under which circumstances is cloned code more difficult to understand than its non-cloned equivalent?    We thank the anonymous reviewers for their helpful suggestions for improving this paper.    REFERENCES.  AVERSANO, L., CERULO, L. & DI PENTA, M. (2007) How clones are maintained: An empirical study. European  Conference on Software Maintenance and Reengineering. Amsterdam.  BAKER, B. S. (1992) A Program for Identifying Duplicated Code. Computing Science and Statistics, 24, 49-57.  BALAZINSKA, M., MERLO, E., DAGENAIS, M., LAGUE, B. & KONTOGIANNIS, K. (1999) Partial redesign of Java  software systems based on clone analysis. 6th Working Conference on Reverse Engineering. Atlanta, GA,  USA, IEEE.  BALINT, M., GÎRBA, T. & MARINESCU, R. (2006) How developers copy. 14th IEEE International Conference on  Program Comprehension. Athens.  FOWLER, M. (1999) Refactoring - Improving the Design of Existing Code, Addison-Wesley.  GEIGER, R., FLURI, B., GALL, H. & PINZGER, M. (2006) Relation of Code Clones and Change Couplings. Fun- damental Approaches to Software Engineering.  HORDIJK, W., PONISIO, M. L. & WIERINGA, R. (2009) Structured Review of the Evidenc e for Effects of Code  Duplication on Software Quality. University of Twente, The Netherlands.   JIANG, L., SU, Z. & CHIU, E. (2007) Context-based detection of clone-related bugs. ESEC-FSE '07: Proceedings  of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT sym- posium on The foundations of software engineering, 55-64.  JÜRGENS, E., HUMMEL, B., DEISSENBOECK, F. & FEILKAS, M. (2008) Static Bug Detection Through Analysis  of Inconsistent Clones. Software Engineering (Workshops), 443-446.  KAMIYA, T., KUSUMOTO, S. & INOUE, K. (2002) CCFinder: A multilinguistic token-based code clone detection  system for large scale source code. IEEE Transactions on Software Engineering, 28, 654-670.  KIM, M., SAZAWAL, V., NOTKIN, D. & MURPHY, G. C. (2005) An empirical study of code clone genealogies. 10th  European Software Engineering Conference.  KITCHENHAM, B. (2007) Procedures for Performing Systematic Reviews. University of Durham, UK.   KRINKE, J. (2007) A Study of Consistent and Inconsistent Changes to Code Clones. WCRE '07: Proceedings of  the 14th Working Conference on Reverse Engineering, 170-178.  KRINKE, J. (2008) Is Cloned Code More Stable than Non-cloned Code? Source Code Analysis and Manipulation,  2008 Eighth IEEE International Working Conference on, 57-66.  LATOZA, T. D., VENOLIA, G. & DELINE, R. (2006) Maintaining mental models: A study of developer work habits.  International Conference on Software Engineering. Shanghai.  LI, Z., LU, S., MYAGMAR, S. & ZHOU, Y. (2004) CP-Miner: a tool for finding copy-paste and related bugs in oper- ating system code. OSDI'04: Proceedings of the 6th conference on Symposium on Opearting Systems D e- sign & Implementation, 20-20.  LI, Z., LU, S., MYAGMAR, S. & ZHOU, Y. (2006) CP-Miner: Finding copy-paste and related bugs in large-scale  software code. IEEE Transactions on Software Engineering, 32, 176-192.  LOZANO, A., WERMELINGER, M. & NUSEIBEH, B. (2007) Evaluating the Harmfulness of Cloning: A Change  Based Experiment. Fourth International Workshop on Mining Software Repositories. IEEE Computer So- ciety."", 'Based Experiment. Fourth International Workshop on Mining Software Repositories. IEEE Computer So- ciety.  MONDEN, A., NAKAE, D., KAMIYA, T., SATO, S. A. & MATSUMOTO, K. A. (2002) Software quality analysis by  code clones in industrial legacy software. IN NAKAE, D. (Ed.) Eighth IEEE Symposium on Software Me- trics.  TOOMIM, M., BEGEL, A. & GRAHAM, S. L. (2004) Managing Duplicated Code with Linked Editing. Symposium on  Visual Languages - Human Centric Computing, VLHCC. IEEE Computer Society.']","['HARMFULNESS   OF   CODE   DUPLICATION Figure  1  contains  a  causal  model  of  the  ef- fects  of  duplication  on  system  quality.  The  model has been constructed after analysis  of the reviewed literature. The names in the  figure are variables and the arrows are cau - sal effects. Each arrow represents a separate  hypothesis  such  as,  “duplication  increases   co change”.  This  should  be  read  thus:  in ge- neral, all else being equal, a system with more  duplication will exhibit more co change than  an equivalent system with less duplication. Figure 1: Causal model for code duplication Co- Change • A significant portion of clones are co- changed. For co changing clones that  cannot be refactored, consider using  change propagating tools, such as  Linked Editing.  • Of the clonesthat are removed in later  versions, a minority is removed becau - se of different changes, while the rest is  presumably removed through refacto - ring.  • Clones that are not refactorable tend to  live longer and are very likely to exhibit  co change. 45% to 55% of clone sets are  co changed during a long part of the life  cycle of systems.  • Clones with class level  granularity have  more co change than clones with smal - ler granularities. These also cause more  increase in code size. These  should be  the first to refactor. • Files between which there is a very high  ratio of cloning are very often changed  in the same release.  INCONSISTENT CHANGE • A small portion of clones are changed  inconsistently which is repaired in later  versions. However only a part of the ca- ses in which a change is applied to other  occurrences later on (late propagation)  stem from inconsistent change. • In a small number of cases it was obser- ved that the act of copying code led to   errors. Consider using the approaches  mentioned as errors in the code, espe - cially when a lot of copy paste has ha - ppened. CODE COMPREHENSION • Programmers experience that dupli - cation hinders code comprehension.  Consider adding clone detection results  to system documentation to aid com - prehension.  • Issues with code ownership may intro - duce duplication.  • Changes that involve aspects (in the  AOP sense) are especially hard to analy- ze. When aspect related code is cloned  and changes to those aspects are expec- ted in the future, use AOP techniques to  avoid co change.  • Duplicated code is not necessarily more  complex or harder to understand than  the equivalent non duplicated code. Be  careful when refactoring clones to not  increase the complexity. Code Size • It is simple to measure the total size  of code involved in a clone set. Howe - ver it is difficult to estimate the size of  code needed in an alternative solution,  for example after refactoring. Removal  of clones does not always result in re - ducing the code size. Be careful when  refactoring clones to not increase the  code size. • Researchers should be aware of types  of duplication that go beyond syntacti - cal clones.  • Clones created by different program - mers are harder to understand and har- der to refactor than clones created by  one programmer. FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Wiebe Hordijk, María Laura Ponisio, and Roel Wieringa. 2009. Harmfulness of code duplication: a structured review of the evidence. In Proce- edings of the 13th international conference on Evaluation nd Assessment in Software Engineering (EASE’09), David Budgen, Mark Turner, and  Mahmood Niazi (Eds.). British Computer Society, Swinton, UK, UK, 88 97. Keywords: Code duplication Code clone Software Maintainance Who is this briefing for? Software engineers practitioners  who want to make decisions  about code duplication based on  scientific evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic re- view conducted by Hordjik et al. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing?', 'from? All findings of this briefing were  extracted from the systematic re- view conducted by Hordjik et al. What is a systematic review? cin.ufpe.br/eseg/slrs What is included in this brie- fing? The main findings of the original  systematic review. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. To access other evidence  briefings on software engine- ering: cin.ufpe.br/eseg/briefings For additional information  about ESEG: cin.ufpe.br/eseg This briefing reports evidence on circuns- tances under which code duplication harms  system quality based on scientific evidence  from a systematic review.']","**Title:** Understanding the Impact of Code Duplication on Software Quality

**Introduction:**  
This Evidence Briefing summarizes findings from a structured review of empirical evidence regarding the harmfulness of code duplication, particularly its effects on changeability and system quality. As code duplication is a common practice in software development, understanding its implications is crucial for researchers and practitioners alike. This briefing aims to clarify whether efforts to reduce code duplication are justified and to provide insights into practical strategies for managing duplicates.

**Main Findings:**  
1. **Changeability Concerns:** Duplication is suspected to negatively impact changeability in software systems. The review indicates that while there is inconclusive evidence regarding the extent of this impact, a significant portion of code clones are co-changed, meaning they require simultaneous updates. This can lead to increased maintenance efforts and potential inconsistencies if not managed properly.

2. **Inconsistent Changes:** Inconsistent changes occur when cloned code segments are modified differently, leading to errors and increased debugging efforts. The review found that while inconsistent changes are relatively rare, they do contribute to maintenance challenges, indicating that cloned code can introduce risks.

3. **Code Comprehension Issues:** Many developers report that duplicated code complicates their understanding of the system. This complexity can hinder effective maintenance and lead to errors during updates. However, some studies suggest that not all duplication is detrimental; in some cases, it may not significantly impact comprehension.

4. **Code Size Implications:** Duplication generally leads to an increase in code size, as multiple instances of similar code are maintained. However, the review highlights that removing duplicates does not always result in a decrease in overall code size, especially if refactoring leads to more complex structures.

5. **Mitigation Strategies:** The review provides practical strategies for practitioners, such as using change-propagating tools for managing co-changed clones and prioritizing the refactoring of larger, class-level clones that exhibit higher co-change rates. Additionally, documenting clone detection results can aid in code comprehension.

6. **Future Research Directions:** The review identifies several open questions, including the need for further empirical studies to clarify the relationship between code duplication and error rates, as well as the contexts in which duplication is more harmful.

**Who is this briefing for?**  
This briefing is intended for software engineers, project managers, and researchers interested in understanding the implications of code duplication for software quality and maintenance. Practitioners looking for evidence-based strategies to manage code duplication will find this information particularly useful.

**Where the findings come from:**  
The findings in this briefing are based on a structured review conducted by Wiebe Hordijk, María Laura Ponisio, and Roel Wieringa, which analyzed 18 primary studies regarding the effects of code duplication on software quality.

**What is included in this briefing?**  
This briefing includes a summary of the core findings related to code duplication, its impact on changeability and comprehension, and actionable mitigation strategies based on empirical evidence.

**To access other evidence briefings on software engineering:**  
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research group:**  
University of Twente, The Netherlands.

**Original Research Reference:**  
Hordijk, W., Ponisio, M. L., & Wieringa, R. (2013). Harmfulness of Code Duplication – A Structured Review of the Evidence. 13th International Conference on Evaluation and Assessment in Software Engineering."
"['The effectiveness of pair programming: A meta-analysis Jo E. Hannaya,b,*, Tore Dybåa,c, Erik Arisholma,b, Dag I.K. Sjøberga,b a Simula Research Laboratory, Department of Software Engineering, Pb. 134, NO-1325 Lysaker, Norway b University of Oslo, Department of Informatics, Pb. 1080 Blindern, 0316 Oslo, Norway c SINTEF Information and Communication Technology, NO-7465 Trondheim, Norway article info Article history: Received 20 October 2008 Received in revised form 9 February 2009 Accepted 9 February 2009 Available online 20 February 2009 Keywords: Pair programming Evidence-based software engineering Systematic review Meta-analysis Fixed effects Random effects abstract Several experiments on the effects of pair versus solo programming have been reported in the literature. We present a meta-analysis of these studies. The analysis shows a small signiﬁcant positive overall effect of pair programming on quality, a medium signiﬁcant positive overall effect on duration, and a medium signiﬁcant negative overall effect on effort. However, between-study variance is signiﬁcant, and there are signs of publication bias among published studies on pair programming. A more detailed examination of the evidence suggests that pair programming is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. The higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that greater attention should be given to moderating factors on the effects of pair programming. /C2112009 Elsevier B.V. All rights reserved. Contents 1. Introduction ........................................................................................................ 1111 2. Method . . . . ........................................................................................................ 1111 2.1. Inclusion and exclusion criteria . . . . . . . ............................................................................ 1111 2.2. Data sources and search strategy . . . . . . ............................................................................ 1111 2.3. Study identification and selection . . . . . ............................................................................ 1111 2.4. Data extraction and checking . . . . . . . . . ............................................................................ 1111 2.5. Statistical analysis .............................................................................................. 1112 2.6. Synthesis . . . . . . ............................................................................................... 1112 2.7. Assumption of parametric normality . . . ............................................................................ 1113 3. Results. . . . . ........................................................................................................ 1113 3.1. Description of studies . . . . . . . . . . . . . . . ............................................................................ 1113 3.2. Effects of pair programming . . . . . . . . . . ............................................................................ 1114 3.2.1. Effects of pair programming on Quality . . . . ................................................................. 1114 3.2.2. Effects of pair programming on Duration . . . ................................................................. 1114 3.2.3. Effects of pair programming on Effort. . . . . . ................................................................. 1115 3.3. Subgroup analyses .............................................................................................. 1115 3.4. Publication bias . . .............................................................................................. 1116', '3.4. Publication bias . . .............................................................................................. 1116 3.5. Moderating effects of task complexity and expertise . . . . . . ............................................................ 1118 4. Discussion. . ........................................................................................................ 1120 5. Conclusion . ........................................................................................................ 1120 Acknowledgements . . . . . . . . . ......................................................................................... 1121 Appendix . ......................................................................................................... 1121 References . ........................................................................................................ 1121 0950-5849/$ - see front matter/C2112009 Elsevier B.V. All rights reserved. doi:10.1016/j.infsof.2009.02.001 * Corresponding author. Address: Simula Research Laboratory, Department of Software Engineering, Pb. 134, NO-1325 Lysaker, Norway. E-mail address: johannay@simula.no (J.E. Hannay). Information and Software Technology 51 (2009) 1110–1122 Contents lists available atScienceDirect Information and Software Technology journal homepag e: www.elsevier.c om/locate/infsof', '1. Introduction Much of the current interest in pair programming is likely due to the popularity of extreme programming (XP), of which pair programming is one of the core practices [2]. Common-sense, but scientiﬁcally unwarranted claims as to both the beneﬁts and the adversities of pair programming abound. Advocates of pair programming claim that it has many beneﬁts over individual programming when applied to new code development or when used to maintain and enhance existing code. Stated beneﬁts in- clude higher-quality code in about half the time as individuals, happier programmers, improved teamwork, improved knowledge transfer, and enhanced learning[53]. There are also expectations with respect to the beneﬁts and drawbacks of various kinds of pairing, e.g., that ‘‘expert–expert pairings seem to be especially accelerated” (ibid., p. 102) and that ‘‘novice–novice is the most problematic” (ibid., p. 120). Stephens and Rosenberg [49] claim that pair programming is clearly the least beneﬁcial of XP prac- tices and also that novice–novice pairing is obviously an undesir- able combination, because it is akin to ‘‘the blind leading the blind”. With the considerable interest in pair programming by the software industry and academia, it is important to determine sci- entiﬁcally, whether, and if so when, pairing two programmers is beneﬁcial in terms of important cost drivers such as duration, ef- fort, and quality. The ﬁnding across decades of small group re- search is that groups usually fall short of reasonable expectations to improved performance [48,35,33]. An important question, therefore, is whether the claims regarding pair pro- gramming can be substantiated by empirical evidence and how pair programming relates to such group research. Several empir- ical studies have now been conducted that set out to examine the effects of pair programming in a systematic manner. This article provides a systematic review in terms of a meta-analysis of all published experiments on the effectiveness of pair programming and subsequently offers recommendations for evidence-based practice [18]. This review extends the intermediate analysis re- ported in [15], which brieﬂy summarized pair programming experiments published up and until 2006. In the present full-scale analysis, we take into account between-study variance, subgroup differences and publication bias. We also take into account stud- ies published in 2007. Section 2 presents the methods for systematic review and sta- tistical meta-analysis. Section3 presents the results from the anal- ysis, and Section4 discusses implications for theory and practice. Section 5 concludes. 2. Method Informed by the general procedures for performing systematic reviews [34] and the established methods of meta-analysis [39,45], we undertook the meta-analysis in distinct stages: identi- ﬁcation of inclusion and exclusion criteria, search for relevant studies, identiﬁcation and selection of studies, data extraction, and statistical analysis and synthesis, see details below. The meta-analysis focused on combining quantitative effects on three central outcome constructs that were investigated in the included studies. We did not assess the quality of the in- cluded studies in terms of, e.g., the appropriateness of the chosen effect size measures [30], the appropriateness of randomization procedures [31], subject/task selection and validity issues [46], statistical power[17], the use of theory[25], the approach to real- ism [24], etc. Future meta-analyses might incorporate study qual- ity, but at present, it is not clear how to aggregate this multifaceted concept into a single measure to be used in a meta-analysis. 2.1. Inclusion and exclusion criteria Studies were eligible for inclusion in the meta-analysis if they presented quantitative data on the effectiveness of pair program- ming in which a comparison was made between pairs and individ- uals, possibly in a team context. The subjects could be either', 'presented quantitative data on the effectiveness of pair program- ming in which a comparison was made between pairs and individ- uals, possibly in a team context. The subjects could be either students or professional software developers. Included studies had to report one of the primary outcomesQuality, Duration,o rEf- fort. We did not want to put any restrictions on the operationaliza- tion of these outcome constructs. Furthermore, the studies had to be reported in English. 2.2. Data sources and search strategy The search strategy included electronic databases and hand searches of conference proceedings. We searched in the following electronic databases: ACM Digital Library, Compendex, IEEE Xplore, and ISI Web of Science. We did not perform separate searches in the SE-speciﬁc databases Kluwer Online, ScienceDirect, SpringerLink, and Wiley Inter Science Journal Finder, because pre- vious experience with systematic search strategies has shown that articles retrieved from these databases are also returned by either ISI Web of Science or Compendex[16]. In addition to the electronic databases, we hand-searched all volumes of the following thematic conference proceedings: XP, XP/Agile Universe, Agile, and Agile Development Conference. We used the basic search string ‘‘‘pair programming’ OR ‘collaborative programming’” to conduct the searches. 2.3. Study identiﬁcation and selection The identiﬁcation and selection process consisted of three stages. At Stage 1, the second author applied the search string to the titles, abstracts, and keywords of the articles in the included electronic databases and conference proceedings. All retrieved arti- cles were published, or accepted for publication, before or in 2007. Excluded from the search were editorials, prefaces, article summa- ries, interviews, news items, correspondence, discussions, com- ments, reader’s letters, and summaries of tutorials, workshops, panels, and poster sessions. This search strategy resulted in a total of 236 unique citations. At Stage 2, the ﬁrst and second authors went through the titles and abstracts of all the studies resulting from stage 1 for relevance to the review. If it was unclear from the title, abstract, and key- words whether a study conformed to our inclusion criteria, it was included for a detailed review. This screening process resulted in 57 citations that were passed on to the next stage. At Stage 3, the full text of all 57 citations from Stage 2 were re- trieved and reviewed in detail by the ﬁrst and second authors. This resulted in 23 included articles according to the inclusion criteria. Five of these did not report enough information to compute stan- dardized effect sizes and were excluded. Thus, 18 studies (all experiments) met the inclusion criteria and were included in the review (seeAppendix). 2.4. Data extraction and checking We collected data from the 18 articles, including type of treat- ment, type of system, type of tasks, duration of the study, number of groups, group assignment, type of subjects and their experience with pair programming, number of pairs, number of individuals, outcome variable, means, standard deviations, counts, percentages, and p-values. Every article included in the review was read in de- tail and the data was extracted and cross-checked by the ﬁrst, sec- ond and third authors. Discrepancies were resolved by discussion among all four authors. J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122 1111', '2.5. Statistical analysis We used Comprehensive Meta-Analysis v2 to calculate effect size estimates for all the tests in the 18 articles. 1 In order to be comparable across studies, effect sizes must be standard- ized. In this meta-analysis, we used Hedges’g as the standardized measure of effect size. Like Cohen’sd and Glass’D, Hedges’g is sim- ply the difference between the outcome means of the treatment groups, but standardized with respect to the pooled standard devia- tion, sp, and corrected for small sample bias[37]: Hedges’ g ¼ /C22x1 /C0 /C22x2 sp ð1Þ The pooled standard deviation is based on the standard deviations in both groups,s1, s2: sp ¼ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ðn1 /C0 1Þs2 1 þð n2 /C0 1Þs2 2 ðn1 /C0 1Þþð n2 /C0 1Þ s ð2Þ Hedges’ g, Cohen’s d, and Glass’D have the same properties in large samples (i.e., they are equivalent in the limit ðn1 þ n2Þ!1 ), but Hedges’ g has the best properties for small samples when multiplied by a correction factor that adjusts for small sample bias[26]: Correction factor for Hedges’g ¼ 1 /C0 3 4ðN /C0 2Þ/C0 1 ; ð3Þ where N = total sample size. An effect size of .5 thus indicates that the mean of the treatment group is half a standard deviation larger than the mean of the con- trol group. Effect sizes larger than 1.00 can be assumed to belarge, effect sizes of .38–1.00medium, and effect sizes of 0–.37small [30]. 2.6. Synthesis We conducted separate meta-analyses for the three outcome constructs Quality, Duration, and Effort. Some studies applied sev- eral tests on the same outcome construct. In theses cases, we used the mean of the effect sizes over these tests to give only one effect size per outcome per study. Comprehensive Meta-Analysis calcu- lates these mean effect sizes as an option. Because we expected considerable heterogeneity, we decided to calculate the resulting meta-analytic effect sizes both under the assumption of the random-effects model and under the assump- tion of the ﬁxed-effects model. These models lead to different sig- niﬁcance test and conﬁdence intervals for meta-analytic results [29]. The ﬁxed-effects model assumes an unknown and ﬁxed popula- tion effect size that is estimated by the studies in the meta-analy- sis. All the studies in the meta-analysis are seen as drawn from the same population, and variances in effect sizes between individual studies are viewed as due to subject variability[27]. The random-effects model, on the other hand, assumes an un- known and stochastic population effect-size distribution. That is, the true effect size of pair programming varies around a meanl. This caters for the view that the effect of pair programming de- pends on situational variables and other factors (both known and unknown) that are not taken into consideration in the analysis. Variance in effect sizes are then seen as due to subject variability, and also to inter-study variability, since each study is seen as approximating a different part of the true effect size distribution [27,4]. Both models relate to an unknown population parameter. The approaches are hence referred to asunconditional [37]. The choice of which model to use is made prior to the analysis based on the- ory, past empirical ﬁndings, and on insights as to what the in- cluded studies describe. However, one may also use the analysis techniques associated with the two models merely to characterize the studies relatively to each other without any reference to a population effect size. Which model to use in this case, is determined from the observed data based on heterogeneity measures, which are calculated under the assumption of a ﬁxed-effects model. If heterogeneity is non- signiﬁcant, a ﬁxed-effects model is an appropriate characterization of data. Otherwise, a random-effects model best characterizes the data. The results from suchconditional approaches should, how- ever, not be confounded with statements regarding population parameters.', 'data. The results from suchconditional approaches should, how- ever, not be confounded with statements regarding population parameters. We conducted our analysis from both an unconditional and a conditional perspective. For the unconditional perspective, we chose the random-effects model. Hedges and Vevea state that ‘‘In the case of random-effects models, for example, some individ- ual effect-size parameters may be negative even thoughl is po- sitive. That corresponds to the substantive idea that some realizations of the treatment may actually be harmful even if the average effect is beneﬁcial”[27]. Results in [1] suggest that the effects of pair programming may be positive or negative dependent on other factors (e.g., expertise and task complexity). Also, the outcome constructs Quality, Duration, and Effort are not yet well-deﬁned in software engineering. These constructs are operationalized in very diverse ways (Section3.1), and for the time being, it is reasonable to view these diverse operational- izations as different aspects of a construct (a so-calledformative measurement model [3,6,12,40]). Under these circumstances, and until the constructs are better understood, it is reasonable to relate to a random-effects model. In the conditional approach, we tested whether there were gen- uine differences underlying the results of the studies (heterogene- ity), or whether the variation in the ﬁndings was compatible with chance alone. In the following, we give an overview of the technical details of the meta-analysis. For further elaborations, see e.g.,[27,4,39]. Let k be the number of studies in the meta-analysis. LetTi be the standardized effect size estimate (in our case, Hedges’g)o f study i. In the ﬁxed-effects model, the estimate T/C15 of the as- sumed ﬁxed population effect size, and the estimate’s variance v/C15, are T/C15¼ Pk i¼1wiTi Pk i¼1wi v/C15¼ 1 Pk i¼1wi ð4Þ where wi ¼ 1=vi is the weight assigned to studyi, i.e., the reciprocal of the variancevi for studyi. Thus,T/C15is a weighted mean over the effect sizes of the individual studies, where studies with less vari- ance are given greater weight. In the random-effects model, the weights are based on between-study variance in addition to with- in-study variancevi. Speciﬁcally, the estimateT/C3 /C15of the meanl of the assumed population effect size distribution, and the estimate’s variance v/C3 /C15, are T/C3 /C15¼ Pk i¼1w/C3 i Ti Pk i¼1w/C3 i v/C3 /C15¼ 1 Pk i¼1w/C3 i ð5Þ where w/C3 i ¼ 1=v/C3 i , for v/C3 i ¼ vi þ s2. Here, s2 is the additional be- tween-study variance: s2 ¼ Q /C0 df C if Q > df 0i f Q 6 df 8 < : ð6Þ where the degrees of freedomdf ¼ k /C0 1, andQ represents the total variance:1 Comprehensive Meta-Analysis is a trademark of Biostat Inc. 1112 J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122', 'Q ¼ Xk i¼1 wiðTi /C0 T/C15Þ2 ð7Þ In Eq.(6), C is simply a scaling factor that ensures thats2 has the same denomination as within-study variance, i.e., C ¼ Pwi /C0 Pw2 i = Pwi. In fact,Q is a statistic that indicates heterogeneity, and one that we used for this purpose. A signiﬁcantQ rejects the null hypothesis of homogeneity and indicates that the variability among the effect sizes is greater than what is likely to have resulted from subject-le- vel variability alone[27]. We also calculated theI2-statistic, which indicates heterogeneity in percentages: I2 ¼ 100%ðQ /C0 df Þ=Q ð8Þ A value of 0 indicates no observed heterogeneity, 25% low, 50% moderate, and 75% high heterogeneity[28]. 2.7. Assumption of parametric normality Hedges’ g is a parametric effect size measure that is based, ﬁrstly, on the assumption that one wishes to relate to a population distribution, and, secondly, that each sub-population (solo, pair) is normally distributed on the response variable. This assumption also underlies the meta-analytic procedures and estimates that we used. It is well-known thatDuration and Effort measures have Gamma distributions, and the various Quality measures in this study may or may not have normal distributions. Thus, one may question whether other standardized effect-size measures should have been used for the meta-analysis. Non-parametric effect-size measures would merely allow one to characterize data and not al- low one to relate to a population. On the other hand, specifying other distributions (e.g., Gamma) would demand that we had ac- cess to the individual studies’ raw data. Since means are meaning- ful measures for all three outcome constructs, we therefore decided to use Hedges’ g as a best compromise for conducting the meta-analysis. Note also that even if the assumed population distribution is not evident in a (small) sample, this reason alone should not lead one to abandon the model (unless the sole aim is to characterize data). Small samples will of course, lead to less con- ﬁdent parameter estimates but these conﬁdence estimates will be calculated correctly according the assumed population distribution. 3. Results We ﬁrst present characteristics of the 18 studies that were in- cluded in the review and meta-analysis. We then give the meta- analytic results in terms of overall effects, subgroup effects and publication bias. 3.1. Description of studies Characteristics of the 18 studies included in the meta-analysis are summarized inTable 1 in alphabetical order, while full cita- tions are provided in theAppendix. Of the 18 studies, 11 were from Europe and seven from North America. The number of subjects in the studies varied from 12 to 295. Thirteen of the studies used students as subjects, while four used professionals. One used both professionals and students. Five studies made the comparison within a team context, that is, teams of pairs versus teams of individuals (marked with an asterisk in the ﬁrst column). The studies often administered several tests and the number of data points may have varied across tests (numbers in parentheses inTable 1). All studies used programming tasks as the basis for comparison. In addition, Madeyski (2006) and Madeyski (2007) included test- Table 1 Characteristics of the included studies. Study Subjects NTot NPair NInd Study setting Arisholm et al. (2007) Professionals 295 98 99 10 sessions with individuals over 3 months and 17 sessions with pairs over 5 months (each of 1 day duration, with different subjects). Modiﬁed 2 systems of about 200–300 Java LOC each *Baheti et al. (2002) Students 134 16 9 Teams had 5 weeks to complete a curricular OO programming project. Distinct projects per team Canfora et al. (2005) Students 24 12 24 2 applications each with 2 tasks (run1 and run2) Canfora et al. (2007) Professionals 18 5(4) 8(10) Study session and 2 runs (totalling 390 min) involving 4 maintenance tasks (grouped in 2', 'Canfora et al. (2007) Professionals 18 5(4) 8(10) Study session and 2 runs (totalling 390 min) involving 4 maintenance tasks (grouped in 2 assignments) to modify design documents (use case and class diagrams) Domino et al. (2007) Professionals Students 88 28 32 Run as several sessions during a period of two months. Pseudocode on ‘‘Create-Design” tasks Test-driven development *Heiberg et al. (2003) Students 84(66) 23(16) 19(17) 4 sessions over 4 weeks involving 2 programming tasks to implement a component for a larger ‘‘gamer” system Madeyski (2006) Students 188 28 31(35) 8 laboratory sessions involving 1 initial programming task in a ﬁnance accounting system (27 user stories) Madeyski (2007) Students 98 35 28 Java course project of developing a 27 user story accounting system over 8 laboratory sessions of 90 min each. Test-driven development Müller (2005) Students 38 19 23 2 runs of 1 programming session each on 2 initial programming tasks (Polynomial and Shufﬂe- Puzzle) producing about 150 LOC Müller (2006) Students 18(16) 4(5) 6 1 session involving initial design + programming tasks on an elevator system Nawrocki & Wojciechowski (2001) Students 15 5 5 4 lab sessions over a winter semester, as part of a University course. Wrote four C/C++ programs ranging from 150–400 LOC Nosek (1998) Professionals 15 5 5 45 min to solve 1 programming task (database consistency check script) *Phongpaibul & Boehm (2006) Students 95 7 7 12 weeks to complete 4 phases of development + inspection *Phongpaibul & Boehm (2007) Students 36 5 4 Part of a team project to extend a system. 13 weeks to complete 4 phases of development + inspection Rostaher & Hericko (2002) Professionals 16 6 4 6 small user stories ﬁlling 1 day *Vanhanen & Lassenius (2005) Students 20 4 8 9-week student project in which each subject spent a total of 100 h (400 h per team). A total of 1500–4000 LOC was written Williams et al. (2000) Students 41 14 13 6-week course where the students had to deliver 4 programming assignments Xu & Rajlich (2006) Students 12 4 4 2 sessions with pairs and 1 session with individuals. 1 initial programming task producing around 200–300 LOC J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122 1113', 'driven development; Müller (2005), Phongpaibul and Boehm (2006), and Phongpaibul and Boehm (2007) included inspections; and Müller (2006) included design tasks. Quality was typically reported as the number of test cases passed or number of correct solutions of programming tasks, but student grades, delivered functionality, and metrics for code com- plexity were also used as measures ofQuality. Duration was re- ported mainly in two modes: as the total time taken to complete all tasks considered (all solutions), or as the total time taken to complete the tasks that had been assessed as having passed a cer- tain quality standard (checked solutions). For comparisons be- tween pairs and solo programmers, pairEffort was reported as twice the duration of each individual in the pair. For team-based comparisons, e.g., teams of individuals versus teams of pairs,Effort was reported as the total effort spent by the respective groups. Thus, the studies included in the meta-analysis do not all apply the same measures or have similar context variables. Rather, they investigate the effectiveness of pair programming with respect to different aspects of the constructsQuality, Duration, and Effort.A s such, the studies may be seen as differentiated replications[36], and any measure from a particular study in the meta-analysis is but one indicator of, perhaps, many indicators regarding one of these constructs. 3.2. Effects of pair programming Table 2summarizes the meta-analysis. Theg column shows the meta-analytic estimates T/C15 and T/C3 /C15, in terms of Hedges’g, of the population effect size parameter in the ﬁxed-effects model and random-effects model, respectively, along with 95% conﬁdence intervals andp-values. Also given are heterogeneity measures cal- culated under the assumption of a ﬁxed-effects model. TheOverall effects analysis will be described in detail in this section, and the Subgroup analysis will be given in detail in Section3.3. Overall, 14 studies used Quality as an outcome construct, 11 used Duration, and 11 used Effort. Fig. 1 shows Forest plots2 of the standardized effects for each of the three outcome constructs. The studies are sorted according to the relative weight that a study’s effect size receives in the meta-analysis. Relative weights are nor- malized versions of the weights wi or w/C3 i used in calculating the meta-analytic estimates T/C15 or T/C3 /C15. The rightmost columns inFig. 1 show these weights according to the ﬁxed-effects and random-ef- fects models. Estimates from larger studies will usually be more pre- cise than the estimates from smaller studies; hence, larger studies will generally be given greater weight. The squares indicate the effect size estimate for each study. The size of each square is proportional to the relative weight of the study according to the random-effects model. The relative weights of a random-effects model are generally more uniform than those of ﬁxed-effects models, due to the incorporation of between-study variance into all the studies’ weights. The horizontal lines indicate the 95% conﬁdence intervals for each study’s effect size estimate according to the random-effects model. The diamonds at the bottom give the meta-analytic effect size estimate according to the ﬁxed-effects and the random-effects model, i.e., T/C15 and T/C3 /C15, respectively. The diamonds’ centers and widths indicate the estimates and their 95% conﬁdence interval, respectively. Fig. 2 shows one-study-removed analyses for each of the three outcome constructs. The plots show the meta-analytic effect size estimate when each study is removed from the meta-analysis. The resulting deviation from the full analysis indicates thesensitiv- ity of the full analysis with respect to each study, that is, how much difference a given study makes to the meta-analysis. 3.2.1. Effects of pair programming on Quality Fourteen studies compared the effects of pair programming on', 'difference a given study makes to the meta-analysis. 3.2.1. Effects of pair programming on Quality Fourteen studies compared the effects of pair programming on Quality in a total of 38 tests. These studies used a total of 1160 sub- jects, although in some studies, not all subjects were used in these particular tests. The subjects were distributed to study units (pairs, solo, teams of pairs, teams of solo) in various ways (Section3.1). The meta-analytic effect size estimate is .23 in the ﬁxed-effects model and .33 in the random-effects model. Both the ﬁxed-effects model and the random-effects model sug- gest that there is a small3 positive overall effect of pair program- ming on Quality compared with solo programming. Only one study showed a negative effect of pair programming onQuality (Domino et al., 2007). All the other studies showed mostly small to medium positive effects. The three studies by Domino et al. (2007), Arisholm et al. (2007), and Madeyski (2006) contribute more than 50% of the total weight in the meta-analysis for Quality. The one-study-re- moved analysis shows that the meta-analysis is most sensitive to the inclusion/exclusion of Williams et al. (2000). Heterogeneity is signiﬁcant at a medium level (Q ¼ 35:97; p <: 01; I2 ¼ 63:86%). 3.2.2. Effects of pair programming on Duration Eleven studies reported effects onDuration in a total of 21 tests. These studies used a total of 669 subjects. Both the ﬁxed-effects model and the random-effects model suggest that there is a med- ium positive overall effect of pair programming onDuration. Table 2 Summary of meta-analysis. Analysis k Model Effect Size Heterogeneity g 95% CI pQ I 2 s2 p Overall effects Quality14 ﬁxed .23 .09 .37 .00 35.97 63.86 .14 .00 random .33 .07 .60 .01 Duration 11 ﬁxed .40 .21 .59 .00 33.57 70.21 .28 .00 random .54 .13 .94 .01 Effort 11 ﬁxed /C0 .73 /C0 .94 /C0 .51 .00 66.36 84.93 .87 .00 random /C0 .52 /C0 1.18 .13 .12 Subgroups Students Quality11 ﬁxed .22 .06 .38 .01 32.97 69.66 .18 .00 random .32 /C0 .01 .65 .06 Duration 7 ﬁxed .58 .33 .84 .00 12.28 51.13 .13 .06 random .63 .24 1.02 .00 Effort 8 ﬁxed /C0 .59 /C0 .88 /C0 .30 .00 48.85 85.67 1.17 .00 random .04 /C0 .82 .91 .92 Professionals Quality3 ﬁxed .26 /C0 .05 .57 .10 2.97 32.56 .07 .23 random .37 /C0 .10 .85 .12 Duration 4 ﬁxed .16 /C0 .13 .46 .28 16.87 82.22 .83 .00 random .50 /C0 .55 1.54 .35 Effort 3 ﬁxed /C0 .90 /C0 1.22 /C0 .58 .00 15.48 87.08 1.54 .00 random /C0 1.99 /C0 3.56 /C0 .41 .01 Teams Quality3 ﬁxed .19 /C0 .22 .60 .36 .23 .00 .00 .89 random .19 /C0 .22 .60 .36 Duration 2 ﬁxed .34 /C0 .13 .81 .16 1.46 31.55 .06 .23 random .31 /C0 .27 .89 .30 Effort 2 ﬁxed .74 /C0 .13 1.61 .09 11.25 91.11 4.14 .00 random .99 /C0 1.96 3.94 .51 No teams Quality11 ﬁxed .24 .09 .39 .00 35.70 71.99 .18 .00 random .38 .05 .70 .02 Duration 9 ﬁxed .41 .20 .63 .00 32.02 75.02 .37 .00 random .63 .13 1.13 .01 Effort 9 ﬁxed /C0 .82 /C0 1.05 /C0 .60 .00 43.36 81.55 .61 .00 random /C0 .85 /C0 1.48 /C0 .23 .01 2 The plots were generated with PSTricks postscript macros in LaTeX within an Excel spreadsheet using data produced by Comprehensive Meta-Analysis. 3 The effect size is small compared with the effect sizes reported in other software engineering experiments[30]. 1114 J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122', 'Compared with Quality, the studies onDuration show a more mixed picture; two of the 11 studies show a negative effect, while the remaining nine show a positive effect. In addition, the smaller studies show medium to large contradictory effects. The three studies by Arisholm et al. (2007), Canfora et al. (2005), and Naw- rocki and Wojciechowski (2001) contribute more than 60% of the total weight in the meta-analysis ofDuration. The one-study-re- moved analysis shows that the meta-analysis is most sensitive to the inclusion/exclusion of Nosek (1998). Heterogeneity is signiﬁ- cant at a medium level (Q¼ 33:57; p <: 01; I2 ¼ 70:21%). 3.2.3. Effects of pair programming on Effort Eleven studies reported effects onEffort in a total of 18 tests. These studies used a total of 586 subjects. Both the ﬁxed-effects model and the random-effects model suggest that there is a med- ium negative overall effect of pair programming onEffort com- pared with solo programming. All the included studies show a negative effect onEffort, apart from the two studies by Phongpaibul and Boehm (2006,2007). However, the results of those studies are not directly comparable, because the researchers compared pair programming teams with teams of individuals who also performed inspections. The three studies by Arisholm et al. (2007), Canfora et al. (2005), and Müller (2005) contribute almost 70% of the total weight in the meta-analysis ofEffort. The one-study-removed anal- ysis shows that the meta-analysis is most sensitive to the inclu- sion/exclusion of either of Phongpaibul and Boehm (2006,2007). Heterogeneity is signiﬁcant and high (Q ¼ 66:36; p <: 01; I 2 ¼ 84:93%). 3.3. Subgroup analyses Because of medium to high heterogeneity, we decided to con- duct subgroup analyses as a step to identify possible immediate sources of heterogeneity. Two subgroup types stand out due to surface dissimilarities: the type of developers (students or profes- sionals) and the type of comparison (isolated pairs vs. individuals, or teams of pairs vs. teams of individuals). The results of these analyses are summarized inTable 2. The most dramatic result of the subgroup analysis is the rever- sal of effect onEffort for the Teams subgroup (from /C0 .52 in the Fig. 1. Forest plots for meta-analysis ofQuality, Duration and Effort. J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122 1115', 'overall analysis to .99 (non-signiﬁcant) in the subgroup analysis, for the random-effects model). This is due to the two studies Phongpaibul and Boehm (2006, 2007), which we remarked upon above. Without these two studies (No teamssubgroup), the effect size increases to from ./C0 52 to /C0 .85 in the random-effects model. Apart from this reversal, effect sizes in the subgroups remain in the same order of magnitude as in the overall analysis, except for the following cases: ForQuality, the No teams subgroup reaches a medium level in the random model’s estimate (.38). ForDuration, the Professionals subgroup declines to a small level in the ﬁxed model’s estimate (.16). ForEffort, the Students subgroup declines to small (.04) (but non-signiﬁcant,p ¼ :92), and theProfessionals subgroup increases to a large effect size (/C01.99) in their respective random-effects models. Heterogeneity levels remain medium to high.Professionals and No teams increase to high heterogeneity forDuration (82.22) and Teams decreases to small (.00) (but non-signiﬁcant,p ¼ :89) for Quality. Signiﬁcance levels for both effect size estimates and heteroge- neity decrease dramatically in several instances in the subgroups, probably due to smallk. This is particularly the case in theProfes- sionals and Teams subgroups. Note however, that the effect onEf- fort actually turns signiﬁcant in theProfessionals subgroup. Note that the Professionals subgroup excludes Phongpaibul and Boehm (2006, 2007) since they have student subjects. 3.4. Publication bias Publication bias captures the idea that studies that report rela- tively large treatment effects are the ones that were most likely to be initiated in the ﬁrst place, and/or submitted and accepted for publication. The effect size estimated from a biased collection of Fig. 2. Forest plots for one-study-removed meta-analysis ofQuality, Duration and Effort. 1116 J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122', 'studies will tend to overestimate the true effect. We used Compre- hensive Meta-Analysis to assess the likely extent of publication bias, and its potential impact on the conclusions. Figs. 3–5 show funnel plots for Quality, Duration and Effort, respectively. These graphs plot each study according to a measure of study size calledprecision (which is simply 1=StdError) on the vertical axis, and according to effect size (Hedges’g) on the hori- zontal axis. Hence, large studies tend to appear toward the top of the graph, and smaller studies tend toward the bottom of the graph. Since there is more inter-study variance in effect size estimates among the smaller studies, these studies will be dispersed wider, while larger studies will tend to cluster near the meta-analytic estimate of effect size. In the absence of publication bias, studies should be distributed symmetrically on either side of the meta-analytic estimate of effect size. In the presence of bias, the bottom part of the plot should show a higher concentration of studies on one side of the meta- analytic estimate than the other. This would reﬂect the fact that smaller studies are more likely to be published if they have larger than average effects, which makes them more likely to meet the criterion for statistical signiﬁcance. Various statistical measures can be used to complement the picture given by the funnel plots. Duval and Tweedie’strim and ﬁllmethod [13,14], takes the basic idea behind the funnel plot one step further and imputes (com- putes and inserts) assumed missing studies to obtain symmetry if the funnel plot is asymmetric and then recomputes the meta- analytic estimate of effect size. The method initially trims the asymmetric studies from the biased side to locate the unbiased ef- fect (in an iterative procedure), and then ﬁlls the plot by re-insert- ing the trimmed studies on the biased side as well as their imputed counterparts on the opposite side of the meta-analytic estimate of effect size.Figs. 6–8show the plots ofFigs. 3–5with imputed stud- ies (black dots) and adjusted meta-analytic estimates of effect size (black diamonds). These analyses suggest that there is indeed some publication bias in our sample. For example, six studies are imputed to obtain symmetry forQuality. This might be taken as an incentive to pub- lish more high-quality studies that, perhaps, exhibit low effects or opposite effects to those that are expected. This might include pro- moting gray literature to be readily accessible in journals and conferences. However, it is important to note that publication bias may not be the only cause of funnel plot asymmetry. For instance, asymme- try may be caused by between-study heterogeneity, study quality, or other factors[44]. In the presence of heterogeneity (which we are assuming), there are three meaningful ways to impute the assumed missing studies. (1) A random-effects model is used to trim and ﬁll, and then the adjusted meta-analytic estimate of effect size is calculated from the ﬁlled data using a random-effects model (a so-calledran- dom–random approach) [13,14]. However, in a random-effects model meta-analysis, smaller studies are given added weight in the synthesis. Thus, if publication bias exists, the meta-analytic estimate is likely to be more biased than that obtained from a ﬁxed-effects model meta-analysis [44]. Thus, an alternative is: (2) a ﬁxed-effects model is used to trim and ﬁll, and then the ad- justed meta-analytic estimate of effect size is calculated from the ﬁlled data using a ﬁxed-effects model (ﬁxed–ﬁxed). It is recom- mended to report both ﬁxed–ﬁxed and random–random analyses [51]. The ﬁxed–ﬁxed approach may be unsatisfying if one wishes to view adjustments relative to the original random-effects model meta-analysis. (3) A third approach is to use a ﬁxed-effects model to trim and ﬁll the meta-analysis, but a random-effects model to calculate the meta-analytic effect size estimate from the ﬁlled data', 'to trim and ﬁll the meta-analysis, but a random-effects model to calculate the meta-analytic effect size estimate from the ﬁlled data (ﬁxed–random). The ﬁxed-effects trim and ﬁll process is less likely to be inﬂuenced by any publication bias than the random-effects model approach, but the resulting estimate from the random-ef- fects model is likely to be more conservative than if a ﬁxed-effects model is used[44]. The plots inFigs. 6–8were generated using the ﬁxed–random approach. Fig. 3. Funnel plot quality. Fig. 4. Funnel plot duration. Fig. 5. Funnel plot effort. J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122 1117', 'Table 3presents the results from all perspectives. For example, for Quality a ﬁxed-effects model approach to trim and ﬁll yields six imputed studies and adjusts the estimate in the ﬁxed-effects mod- el from .23 to .03 (ﬁxed–ﬁxed), while it adjusts the estimate in the random-effects model from .33 to .07 (ﬁxed–random). The trim and ﬁll procedure searches for missing studies on one side at a time. ForQualtiy and Duration missing studies were found on the left (Table 3), i.e., in the opposite direction of the estimated effect, which is consistent with the assumption of publication bias. For Effort, the ﬁxed approach found a missing study on the left, while the random approach found missing studies on the right. The missing study on the left is not consistent with the assumption of publication bias and is due to the overly large effort estimate of one of the studies in the opposite direction of the estimate. Sterne and Egger[50] note that trim and ﬁll plots merely detect a relationship between sample size and effect size, not a causal mechanism between the two. The effect size may be larger in small studies due to publication bias. However, it is also possible that small studies estimate a different part of the population effect size distribution than do large studies, e.g., because smaller studies use different operationalizations of the outcome constructs, or have different situational variables than large studies. Because one does not know for certain whether funnel plot asymmetry is really caused by publication bias, it is recommended to use the trim and ﬁll method mainly as a form of sensitivity analysis[44]. 3.5. Moderating effects of task complexity and expertise Due to the interdisciplinary and complex nature of industrial software engineering, it is usually not reasonable to test an hypothesis that considers only one independent (predictor) vari- able and one dependent (criterion) variable. Hypotheses related to software engineering should typically include additional vari- ables and test more complex relationships in order to provide a more accurate description of reality. Indeed, the relatively small overall effects and large between-study variance (heterogeneity) indicate that one or more moderator variables might play a signif- icant role. Only two of the studies in the review, Vanhanen and Lassenius (2005) and Arisholm et al. (2007), tested explicitly for moderator effects, while only one of the other studies discussed the potential inﬂuence of such effects; Williams et al. (2000) suggested that the relative improvement by pairs after the ﬁrst programming task in their experiment was due to ‘‘pair jelling”. The literature on group dynamics (e.g.,[5,21]) suggests that the extent to which group performance exceeds that of individuals, and the mechanisms by which such gains in performance may be achieved, depend upon the composition of the group and the char- acteristics of the tasks. Vanhanen and Lassenius (2005) found that task complexity did not affect the differences in effort between Fig. 7. Funnel plot duration trim and ﬁll ﬁxed–random. Fig. 8. Funnel plot effort trim and ﬁll ﬁxed–random. Fig. 6. Funnel plot quality trim and ﬁll ﬁxed–random. Table 3 Trim and ﬁll analysis. Imputed Fixed Random Q g 95% C.I. g 95% C.I. Quality Observed .23 .09 .37 .33 .07 .60 35.97 ﬁxed 6 .03 /C0 .09 .16 .07 /C0 .23 .37 81.42 Left random 0 .23 .09 .37 .33 .07 .60 35.97 Left Duration Observed .40 .21 .59 .53 .13 .94 33.57 ﬁxed 2 .34 .15 .54 .35 /C0 .10 .80 49.77 Left random 1 .37 .18 .56 .44 /C0 .01 .88 45.01 Left Effort Observed /C0 .73 /C0 .94 /C0 .51 /C0 .52 /C0 1.18 .13 66.36 ﬁxed 1 /C0 .75 /C0 .97 /C0 .54 /C0 .74 /C0 1.46 /C0 .01 86.01 Left random 2 /C0 .58 /C0 .78 /C0 .37 /C0 .07 /C0 .79 .65 104.46 Right 1118 J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122', 'solo and pair programming; the Pearson correlation between task complexity and effort difference was as low asr ¼/C0 :02. On the other hand, Arisholm et al. (2007) found moderating ef- fects of both task complexity and expertise. The results are shown in Fig. 9. Overall, the results showed that the pairs had an 8% de- crease in duration (g ¼ :21) with a corresponding 84% increase in effort ( g ¼/C0 :68) and a 7% increase in correctness (g ¼ :11) (Fig. 9a). However, the main effects of pair programming were masked by the moderating effect of system complexity, in that simpler designs had shorter duration, while more complex designs had increased correctness (Fig. 9e). Furthermore, when considering the moderating effect of pro- grammer expertise, junior pairs had a small (5%) increase in dura- tion and thus a large increase in effort (111%), and a 73% increase in correctness (Fig. 9b). Intermediate pairs had a 28% decrease in duration (43% increase in effort) and a negligible (4%) increase in correctness (Fig. 9c). Senior pairs had a 9% decrease in duration (83% increase in effort) and an 8% decrease in correctness (Fig. 9d). Thus, the juniors beneﬁted from pair programming in terms of increased correctness, the intermediates in terms of de- creased duration, while there were no overall beneﬁts of pair pro- gramming for seniors. When considering the combined moderating effect of system complexity and programmer expertise on pair programming, there appears to be an interaction effect: Among the different treatment combinations, junior pairs assigned to the complex design had a remarkable 149% increase on correct- Total Effect of PP 84 % 7 % -8 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 %Difference from individuals Moderating Effect of System Complexity on PP 60 % -16 % 6 % 112 % 48 % -20 %-40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 % Difference from individuals CC (easy) DC (complex) Moderating Effect of System Complexity for Juniors 4 % 109 % 32 % 6 % 112 % 149 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 % Difference from individuals CC (easy) DC (complex) Moderating Effect of System Complexity for Seniors 55 % -13 % 8 % 115 % -23 % -2 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 % Difference from individuals CC (easy) DC (complex) Moderating Effect of System Complexity for Intermediates 22 % -16 % 68 % 92 % -39 % -29 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 % Difference from individuals CC (easy) DC (complex) Effect of PP for Juniors 5 % 111 % 73 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 %Difference from individuals Effect of PP for Intermediates 43 % 4 % -28 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 %Difference from individuals Effect of PP for Seniors -9 % 83 % -8 % -40 % -20 % 0 % 20 % 40 % 60 % 80 % 100 % 120 % 140 % 160 % Duration Effort Correctness Duration Effort Correctness Duration Effort Correctness Duration Effort CorrectnessDuration Effort Correctness Duration Effort Correctness Duration Effort Correctness Duration Effort Correctness Difference from individuals a b c d e f g h Fig. 9. The moderating effects of programmer expertise (a)–(d) and system complexity (e)–(h) on the relation of pair programming on duration, effort, and correctness (Arisholm et al., 2007). J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122 1119', 'ness compared with individuals (Fig. 9f). Furthermore, intermedi- ates and seniors experienced an effect of pair programming on duration on the simpler design, with a 39% (Fig. 9g) and 23% (Fig. 9h) decrease, respectively. However, the cost of this shorter duration was a corresponding decrease in correct solutions by 29% and 13%, respectively. 4. Discussion The unconditional part of the meta-analysis suggests that the population effect size meanl for each of the outcome constructs are small to medium. More interestingly, the conditional part of the meta-analysis showed large and partly contradictory differences in the reported overall effects of pair programming, speciﬁcally with respect to Duration and Effort. Our subgroup analyses do not suggest that these differences are due to differences in samples (e.g., students or professionals). Differences in organization (e.g., teams or no teams) were apparent from the subgroup analysis, but these differ- ences were due to two particular studies that, when removed, did not decrease overall heterogeneity. At the outset, we anticipated between-study variance (hetero- geneity) due to moderating factors such as expertise and task com- plexity and to the fact that the outcome constructs of the meta- analysis are operationalized by indicators that are different aspects of the constructs. Such heterogeneity was taken into account by using a random-effects model for the unconditional interpretation. Nevertheless, contradictory differences still manifested them- selves. Thus, it seems clear that moderating factors play an impor- tant role and should be investigated further. The study in [1] corroborates this conclusion for expertise and task complexity, but other moderating factors, such as amount of training in pair programming, motivation, team climate, etc., are likely to be rele- vant as well. Consequently, the question of whether pair programming is better than solo programming is not precise enough to be mean- ingful, since the answer to that question in the present context is both ‘‘yes” and ‘‘no”. On the basis of the evidence from this review, the answer is that ‘‘it depends”: It depends on other factors, for example, the expertise of the programmers and on the complexity of the system and tasks to be solved. In the literature, expertise and task complexity are perhaps the most central situation-independent predictors of performance. (Situation-dependent factors, on the other hand, include more dy- namic factors such as motivation, team climate, organizational is- sues, etc.). Theory predicts that experts perform better on complex tasks than do novices because experts’ level of understanding cor- responds to the deep structure[10,8,9,19] of a complex task. That is, experts perceive (objectively) complex tasks as subjectively less complex, i.e., lessvariable and moreanalyzable [43,38]. Conversely, experts perceive (objectively) less complex tasks as morevariable and less analyzable since such tasks do not match the expert’s deep-level understanding of the problem. Novices, on the other hand, do have an understanding that matches the surface structure of a non-complex task, and are expected to do better on non-com- plex tasks than on complex tasks. Moreover, they may be expected to even outperform experts on non-complex tasks[23]. These effects are evident in[1], in which the levels of correct- ness for individual juniors, intermediates, and seniors on the non-complex system were 63%, 87%, and 86%, respectively, whereas the levels on the complex system were 34%, 41%, and 81%, respectively. Thus, the performance drop was much higher for juniors than for intermediates and seniors, when moving from the non-complex to the complex system, although juniors did not outperform higher expertise groups on the non-complex system, see also[23]. In our context, one of the most interesting observations is that the pairing up of individuals seems to elevate the junior pairs up to', 'see also[23]. In our context, one of the most interesting observations is that the pairing up of individuals seems to elevate the junior pairs up to near senior pair performance. Thus, pair collaboration might com- pensate for juniors’ lack of deep understanding, for example, by inducing an expert-like strategy. Change tasks rely heavily on program comprehension. To com- prehend code, experts use a top–down model[47] that short-cuts the available information by only investigating details as dictated by domain knowledge. This approach to comprehension is more efﬁcient than bottom–up comprehension, which builds under- standing from details, and which is the approach found to be used by programmers encountering totally unfamiliar code[41,42]. By forcing junior peers to rationalize their ideas to each other, junior pairs might adopt a top–down strategy to comprehension, rather than getting lost in code on their own. The mere act of think- ing aloud whilst solving problems has been shown to increase performance, when the verbalization is intended to reason or explain action, e.g.,[7] (Type 3 verbalization, in Ericsson and Si- mon’s terminology) [20]. Several studies have concluded that apparent successes of pair programming are not due to the partic- ularities of pair programming (such as the speciﬁc roles of driver and navigato r), but rather to the shear amount of verbalization that the pair programming situation necessitates[11,22]. Group performance not only relies on task complexity but also on the collaborative nature of a task. In fact, the appropriateness of each of the familiar adages ‘‘two heads are better than one”, ‘‘many hands make light work”, and ‘‘a chain is only as strong as its weak- est link” depends on whether a task is additive, compensatory, dis- junctive or conjunctive [48]. For example, the chain analogy is appropriate for conjunctive tasks, where all group members must contribute to the solution, but is inappropriate for disjunctive tasks for which it sufﬁces that one group member has the ability to com- plete the task. It is not obvious what sort of task pair programming is in this respect. The precise collaborative nature of pair programming also inﬂuences what social mechanisms (social loaﬁng, social labour- ing, social facilitation, social inhibition, social compensation, etc.) are applicable. However, these social mechanisms also depend on a host of other factors. In a meta-analysis of social loaﬁng (the phenomenon that individuals tend to expend less effort when working collectively than when working individually), Karau and Williams [32] identiﬁed several conditions in which such loaﬁng is eliminated (e.g., by high group cohesion) and some in which the opposite phenomenon, social laboring[5], could be observed (i.e., greater effort on group tasks). Social laboring seems to occur when complex or highly involving tasks are performed, or when the group is considered important for its members, or if the prevailing values favor collectivism rather than individualism [5]. 5. Conclusion Our meta-analysis suggests that pair programming is not uni- formly beneﬁcial or effective, that inter-study variance is high, and that perhaps publication bias is an issue. Hence, if further investigations are to be undertaken on pair programming, then the focus should be on untangling the moderating factors of the ef- fect of pair programming. However, with respect to the central factors expertise and task complexity, the current state of knowledge suggest that pair pro- gramming is beneﬁcial for achieving correctness on highly com- plex programming tasks. Pair programming may also have a time gain on simpler tasks. By cooperating, programmers may complete tasks and attain goals that would be difﬁcult or impossible if they worked individually. Junior pair programmers, for example, seem able to achieve approximately the same level of correctness in', 'tasks and attain goals that would be difﬁcult or impossible if they worked individually. Junior pair programmers, for example, seem able to achieve approximately the same level of correctness in 1120 J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122', 'about the same amount of time (duration) as senior individuals. However, the higher quality for complex tasks comes at a price of a considerably higher effort (cost), while the reduced completion time for the simpler tasks comes at a price of a noticeably lower quality. This fact conﬁrms Voas’[52] contention that you cannot expect faster and better and cheaper. These relationships give rise to a few evidence-based guidelines for the use of pair program- ming for professional software developers. If you do not know the seniority or skill levels of your programmers, but do have a feeling for task complexity, then employ pair programming either when task complexity is low and time is of the essence, or when task complexity is high and correctness is important. In the future, we intend to investigate deeper into the theoret- ical and empirical underpinnings of collaboration in pairs, e.g., by studying group dynamics and analyzing pair dialogues to obtain insights into subjects’ learning and reasoning processes. Only by understanding what makes pairs work, and what makes pairs less efﬁcient, can steps be taken to provide beneﬁcial conditions for work and to avoid detrimental conditions; or to avoid pairing alto- gether when beneﬁcial conditions cannot be provided. Acknowledgement The authors are grateful to the anonymous referees for insight- ful remarks. Appendix The following articles were included in the meta-analysis: Arisholm et al. (2007). E. Arisholm, H. Gallis, T. Dybå, D.I.K. Sjøberg, Evaluating pair programming with respect to system complex- ity and programmer expertise, IEEE Trans. Software Eng. 33 (2007) 65–86 (Feb.) Baheti et al. (2002). P. Baheti, E. Gehringer, D. Stotts, Exploring the efﬁcacy of distributed pair programming, in: Proc. XP/Agile Uni- verse 2002, ser. Lecture Notes in Computer Science, vol. 2418, Springer Verlag, 2002, pp. 208–220. Canfora et al. (2007). G. Canfora, A. Cimitlie, F. Garcia, M. Piattini, C.A. Visaggio, Evaluating performances of pair designing in industry,J. Systems and Software80(8) (2007) 1317–1327 (Aug.) Canfora et al. (2005). G. Canfora, A. Cimitlie, C.A. Visaggio, Empiri- cal study on the productivity of the pair programming, in: Proc. XP 2005, ser. Lecture Notes in Computer Science,vol. 3556, Springer Verlag, 2005, pp. 92–99. Domino et al. (2007). M.A. Domino, R. Webb Collins, A.R. Hevner, Controlled experimentation on adaptations of pair program- ming, Information Technology and Management 8(4) (2007) 297–312 (Dec.) Heiberg et al. (2003). S. Heiberg, U. Puus, P. Salumaa, A. Seeba, Pair- programming effect on developers productivity, in: Proc. XP 2003, ser. Lecture Notes in Computer Science, vol. 2675. Springer Verlag, 2003, pp. 215–224. Madeyski (2006). L. Madeyski, The impact of pair programming and test-driven development on package dependencies in object-oriented design – an experiment, in: Proc. PROFES 2006, ser. Lecture Notes in Computer Science, vol. 4034, Springer Verlag, 2006, pp. 278–289. Madeyski (2007). L. Madeyski, On the effects of pair programming on thoroughness and fault-ﬁnding effectiveness of unit tests, in: Proc. PROFES 2007, ser. Lecture Notes in Computer Science, vol. 4589. Springer Verlag, 2007, pp. 207–221. Müller (2005). M. Müller, Two controlled experiments concerning the comparison of pair programming to peer review,J. Systems and Software78(2) (2005) 169–179 Müller (2006). M. Müller, A preliminary study on the impact of a pair design phase on pair programming and solo programming, Information and Software Technology48(5) (2006) 335–344 (May) Nawrocki and Wojciechowski (2001). J. Nawrocki, A. Wojciechow- ski, Experimental evaluation of pair programming, in: Proc. European Software Control and Metrics Conference (ESCOM’01), 2001, pp. 269–276. Nosek (1998). J. Nosek, The case for collaborative programming, Comm. ACM 41(3) (1998) 105–108 (Mar.) Phongpaibul and Boehm (2006). M. Phongpaibul, B. Boehm, An', '(ESCOM’01), 2001, pp. 269–276. Nosek (1998). J. Nosek, The case for collaborative programming, Comm. ACM 41(3) (1998) 105–108 (Mar.) Phongpaibul and Boehm (2006). M. Phongpaibul, B. Boehm, An empirical comparison between pair development and software inspection in Thailand, in: Proc. Int’l Symposium on Empirical Software Engineering (ISESE’06), 2006, pp. 85–94. Phongpaibul and Boehm (2007). M. Phongpaibul, B. Boehm, A rep- licate empirical comparison between pair development and software development with inspection, in: Proc. 1st Int’l Symp. Empirical Software Engineering and Measurement (ESEM’07), IEEE Computer Society, 2007, pp. 265–274. Rostaher and Herick (2002). M. Rostaher, M. Hericko, Tracking test ﬁrst pair programming – an experiment, in: Proc. XP/Agile Uni- verse 2002, ser. Lecture Notes in Computer Science, vol. 2418. Springer Verlag, 2002, pp. 174–184. Vanhanen and Lassenius (2005). J. Vanhanen, C. Lassenius, Effects of pair programming at the development team level: an exper- iment, in: Proc. Int’l Symposium on Empirical Software Engi- neering (ISESE’05), 2005, pp. 336–345. Williams et al. (2000). L. Williams, R.R. Kessler, W. Cunningham, R. Jeffries, Strengthening the case for pair programming, IEEE Soft- ware 17(4) (2000) 19–25 Xu and Rajlich (2006). S. Xu, V. Rajlich, Empirical validation of test- driven pair programming in game development, in:Proc. Int’l Conference on Computer and Information Science and Int’l Workshop on Component-Based Software Engineering, Soft- ware Architecture and Reuse (ICIS-COMSAR’06), 2006, pp. 500–505. References [1] E. Arisholm, H. Gallis, T. Dybå, D.I.K. Sjøberg, Evaluating pair programming with respect to system complexity and programmer expertise, IEEE Trans. Software Eng. 33 (Feb.) (2007) 65–86. [2] K. Beck, C. Andres, Extreme Programming Explained: Embrace Change, second ed., Addison-Wesley, 2003. [3] K. Bollen, R. Lennox, Conventional wisdom on measurement: a structural equation perspective, Psychol. Bull. 110 (2) (1991) 305–314. [4] M. Borenstein, L.V. Hedges, H. Rothstein, Meta-analysis – ﬁxed effects versus random effects, Biostat Inc., Tech. Rep., 2007, <meta-analysis.com/downloads/ Meta-analysis_ﬁxed_effect_vs_random_effects.pdf>. [5] R. Brown, Group Processes: Dynamics within and between Groups, second ed., Blackwell, 2000. [6] C. Burke jarvis, S.B. Mackenzie, P.M. Podsakoff, A critical review of construct indicators and measurement model misspeciﬁcation in marketing and consumer research, J. Consum. Res. 30 (Sept.) (2003) 199–218. [7] M.T.H. Chi, N. de Leeuw, M.H. Chiu, C. LaVancher, Eliciting self-explanations improves understanding, Cognitive Sci. 18 (1994) 439–477. [8] M.T.H. Chi, P.J. Feltovich, R. Glaser, Categorization and representation of physics problems by experts and novices, Cognitive Sci. (1981) 121–152. [9] M.T.H. Chi, R. Glaser, E. Rees, Expertise in problem solving, in: R. Sternberg (Ed.), Advances in the Psychology of Human Intelligence, Lawrence Erlbaum Associates Inc., 1982, pp. 17–76. [10] N. Chomsky, Syntactic Structures, second ed., Mouton, 2002. [11] J. Chong, T. Hurlbutt, The social dynamics of pair programming, in: Proc. 29th Int’l Conf. Software Engineering, 2007. [12] A. Diamantopoulos, J.A. Siguaw, Formative versus reﬂective indicators in organizational measure development: a comparison and empirical illustration, Brit. J. Manag. 17 (2006) 263–282. [13] S. Duval, R. Tweedie, A nonparametric ‘‘trim and ﬁll” method of accounting for publication bias in meta-analysis, J. Am. Stat. Assoc. 95 (445) (2000) 89–98. [14] S. Duval, R. Tweedie, Trim and ﬁll: a simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis, Biometrics 56 (2) (2000) 455–463. J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122 1121', '[15] T. Dybå, E. Arisholm, D.I.K. Sjøberg, J.E. Hannay, F. Shull, Are two heads better than one? on the effectiveness of pair programming, IEEE Software, November/ December 2007, pp. 12–15. [16] T. Dybå, T. Dingsøyr, G.K.S. Hanssen, Applying systematic reviews to diverse study types: an experience report, in: Proc. 1st Int’l Symp. Empirical Software Engineering and Measurement (ESEM’07), IEEE Computer Society, 2007, pp. 225–234. [17] T. Dybå, V.B. Kampenes, D.I.K. Sjøberg, A systematic review of statistical power in software engineering experiments, J. Inform. Software Technol. 48 (2006). [18] T. Dybå, B.A. Kitchenham, M. Jørgensen, Evidence-based software engineering for practitioners, IEEE Software 22 (2005) 58–65. [19] K.A. Ericsson, N. Charness, Expert performance – its structure and acquisition, Am. Psychol. 49 (1994) 725–747. [20] K.A. Ericsson, H.A. Simon, Protocol Analysis, revised ed., The MIT Press, 1993. [21] D.R. Forsyth, Group Dynamics, fourth ed., Thomson Wadsworth, 2006. [22] S. Freudenberg (née Bryant), P. Romero, B. du Boulay, ‘Talking the talk’: is intermediate-level conversation the key to the pair programming success story? in: Proc. AGILE 2007, 2007. [23] T. H/C26rem, D. Rau, The inﬂuence of degree of expertise and objective task complexity on perceived task complexity and performance, J. Appl. Psychol. 92 (5) (2007) 1320–1331. [24] J.E. Hannay, M. Jørgensen, The role of deliberate artiﬁcial design elements in software engineering experiments, IEEE Trans. Software Eng. 34 (Mar/Apr) (2008) 242–259. [25] J.E. Hannay, D.I.K. Sjøberg, T. Dybå, A systematic review of theory use in software engineering experiments, IEEE Trans. Software Eng. 33 (Feb.) (2007) 87–107. [26] L.V. Hedges, I. Olkin, Statistical Methods for Meta-Analysis, Academic Press Inc., 1985. [27] L.V. Hedges, J.L. Vevea, Fixed- and random-effects models in meta-analysis, Psychol. Meth. 3 (4) (1998) 486–504. [28] J.P. Higgins, S.G. Thompson, J.J. Deeks, D.G. Altman, Measuring inconsistency in meta-analyses, Brit. Med. J. 327 (4) (2003) 557–560. [29] J.E. Hunter, F.L. Schmidt, Fixed effects vs. random effects meta-analysis models: implications for cumulative research knowledge, Int. J. Select. Assess. 327 (8) (2000) 272–292. [30] V.B. Kampenes, T. Dybå, J.E. Hannay, D.I.K. Sjøberg, A systematic review of effect size in software engineering experiments, Inform. Software Technol. 49 (11–12) (2007) 1073–1086. [31] V.B. Kampenes, T. Dybå, J.E. Hannay, D.I.K. Sjøberg, A systematic review of quasi-experiments in software engineering, Inform. Software Technol. 49 (11– 12) (2007) 1073–1086. [32] S.J. Karau, K.D. Williams, Social loaﬁng: a meta-analytic review and theoretical integration, J. Pers. Soc. Psychol. 65 (4) (1993) 681–706. [33] N.L. Kerr, R.S. Tindale, Group performance and decision making, Ann. Rev. Psychol. 55 (2004) 623–655. [34] B.A. Kitchenham, S. Charters, Guidelines for Performing Systematic Literature Reviews in Software Engineering, Keele University, EBSE Technical Report, EBSE-2007-01, Tech. Rep., 2007. [35] J.M. Levine, R.L. Moreland, Progress in small group research, Ann. Rev. Psychol. 41 (1990) 585–634. [36] R.M. Lindsay, A.S.C. Ehrenberg, The design of replicated studies, Am. Statistician 47 (3) (1993) 217–228. [37] M.W. Lipsey, D.B. Wilson, Practical Meta-Analysis, Sage, 2001. [38] J.G. March, H.A. Simon, Organizations, second ed., Wiley-Blackwell, 1993. [39] J. Miller, Applying meta-analytical procedures to software engineering experiments, J. Syst. Software 54 (1–2) (2000) 29–39. [40] J.C. Nunnally, I. Bernstein, Psychometric Theory, third ed., McGraw-Hill Inc., 1994. [41] N. Pennington, Comprehension strategies in programming, in: Proc. Second Workshop Empirical Studies of Programmers, 1987, pp. 100–113. [42] N. Pennington, Stimulus structures and mental representations in expert comprehension of computer programs, Cognitive Psychol. 19 (1987) 295–341.', '[42] N. Pennington, Stimulus structures and mental representations in expert comprehension of computer programs, Cognitive Psychol. 19 (1987) 295–341. [43] C. Perrow, A framework for the comparative analysis of organizations, Am. Soc. Rev. 32 (1967) 194–208. [44] J.L. Peters, A.J. Sutton, D.R. Jones, K.R. Abrams, L. Rushton, Performance of the trim and ﬁll method in the presence of publication bias and between-study heterogeneity, Stat. Med. 26 (2007) 4544–4562. [45] R. Rosenthal, M.R. DiMatteo, Meta-analysis: recent developments in quantitative methods for literature reviews, Ann. Rev. Psychol. 52 (2001) 59–82. [46] D.I.K. Sjøberg, J.E. Hannay, O. Hansen, V.B. Kampenes, A. Karahasanovic´, N.K. Liborg, A.C. Rekdal, A survey of controlled experiments in software engineering, IEEE Trans. Software Eng. 31 (9) (2005) 733–753. [47] E. Soloway, B. Adelson, K. Ehrlich, Knowledge and processes in the comprehension of computer programs, in: M. Chi, R. Glaser, M. Farr (Eds.), The Nature of Expertise, Lawrence Erlbaum Assoc., 1988, pp. 129–152. [48] I.D. Steiner, Group Process and Productivity, Academic Press, New York, London, 1972. [49] M. Stephens, D. Rosenberg, Extreme Programming Refactored: The Case Against XP, APress, 2003. [50] J.A.C. Sterne, M. Egger, Regression methods to detect publication and other bias in meta-analysis, in: H.R. Rothstein, A.J. Sutton, M. Borenstein (Eds.), Publication Bias in Meta-analysis: Prevention, Assessment and Adjustments, Wiley, 2005, pp. 99–110 (Chapter 6). [51] A.J. Sutton, Evidence concerning the consequences of publication and related biases, in: H.R. Rothstein, A.J. Sutton, M. Borenstein (Eds.), Publication Bias in Meta-analysis: Prevention, Assessment and Adjustments, Wiley, 2005, pp. 175–192 (Chapter 10). [52] J. Voas, Faster, better, and cheaper, IEEE Software 18 (3) (2001) 96–99. [53] L. Williams, R.R. Kessler, Pair Programming Illuminated, Addison-Wesley, 2002. 1122 J.E. Hannay et al. / Information and Software Technology 51 (2009) 1110–1122']","['THE EFFECTIVENESS OF PAIR PROGRAMMING • The findings presented in this briefing  consider quality as the number of test  cases passed or number of correct solu - tions of programming tasks; duration as  the total time taken to complete all tasks  considered (all solutions); and effort was  reported as twice the duration of each in- dividual in the pair.  • Studies present a small significant positi - ve overall effect of pair programming on  quality, a medium significant positive ove- rall effect on duration, and a medium sig- nificant negative overall effect on effort. • Evidence suggests that pair programming  is faster than solo programming when  programming task complexity is low and  also yields code solutions of  • higher quality when task complexity is  high. • The higher quality for complex tasks co - mes at a price of considerably greater ef - fort, while the reduced completion time  for the simpler tasks comes at a price of  noticeably lower quality. • Research results show that the question  of whether pair programming is better  than solo programming depends on other  factors, for example, the expertise of the  programmers and on the complexity of  the system and tasks to be solved. • One of the most interesting observations  is that the pairing up of individuals seems  to elevate the junior pairs up to near se - nior pair performance. Thus, pair collabo- ration might compensate for juniors’ lack  of deep understanding, for example, by  inducing an expert like strategy. • If you do not know the seniority or skill  levels of your programmers, but do have  a feeling for task complexity, then employ  pair programming either when task com - plexity is low and time is of the essence,  or when task complexity is high and cor - rectness is important. • When considering the moderating effect  of programmer expertise, junior pairs had  a small (5%) increase in duration and thus  a large increase in effort (111%), and a  73% increase in correctness. • Intermediate pairs had a 28% decrease in  duration (43% increase in effort) and a ne- gligible (4%) increase in correctness. • Senior pairs had a 9% decrease in dura - tion (83% increase in effort) and an 8%  decrease in correctness. • The juniors benefited from pair program - ming in terms of increased correctness,  the intermediates in terms of decreased  duration, while there were no overall be - nefits of pair programming for seniors. • When considering the combined modera- ting effect of system complexity and pro - grammer expertise on pair programming,  there appears to be an interaction effect:  Among the different treatment combina - tions, junior pairs assigned to the complex  design had a remarkable 149% increase  on correctness compared with individu - als. • Intermediates and seniors experienced an  effect of pair programming on duration on  the simpler design, with a 39% and 23%  decrease, respectively. • However, the cost of this shorter duration  was a corresponding decrease in correct  solutions by 29% and 13%, respectively. MAIN FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Jo E. Hannay, Tore Dybå, Erik Arisholm, Dag I.K. Sjøberg, The effectiveness of pair programming: A meta analysis, Information and Software Tech- nology, Volume 51, Issue 7, July 2009, Pages 1110 1122, ISSN 0950 5849, http://dx.doi.org/10.1016/j.infsof.2009.02.001. Keywords: Pair programming Meta-analysis Who is this briefing for? Software engineers practitioners  who want to make decisions  about pair programming based  on scientific evidence. Where the findings come  from? All findings of this briefing were  extracted from the systematic re- view conducted by Hannay et al. What is systematic reviews? cin.ufpe.br/eseg/systematic-reviews What is included in this brie- fing? The main findings of the original  systematic review. Evidence characteristics through  a brief description about the ori- ginal systematic review and the  studies it analized. What is not included in this  briefing?', 'systematic review. Evidence characteristics through  a brief description about the ori- ginal systematic review and the  studies it analized. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. For additional information  about this briefing: cin.ufpe.br/eseg/briefings This briefing reports evidence on the effecti- veness of pair programming around quality  duration and effort based on scientific evi - dence from a systematic review.']","**Title: Understanding the Impact of Pair Programming on Software Development**

**Introduction:**
This Evidence Briefing summarizes findings from a comprehensive meta-analysis examining the effectiveness of pair programming compared to solo programming. The goal is to provide actionable insights for software engineering practitioners regarding when and how to implement pair programming effectively in their projects.

**Main Findings:**
The meta-analysis reveals several key insights about pair programming:

1. **Quality of Code**: Pair programming tends to produce higher quality code, particularly for complex tasks. The analysis indicates a small positive effect on code quality, with an effect size of approximately 0.23 to 0.33, suggesting that pairs generally produce better outcomes than individuals.

2. **Duration of Tasks**: Pair programming can lead to faster completion of simpler tasks, with a medium positive effect size of around 0.40 to 0.54. However, this speed comes at a cost: for complex tasks, while pairs may take longer, they produce higher quality results.

3. **Effort Required**: The analysis shows a medium negative effect on effort, indicating that pair programming generally requires more effort than solo programming, with effect sizes ranging from -0.73 to -0.52. This suggests that while pair programming can enhance quality, it also demands more resources, especially for complex tasks.

4. **Task Complexity and Expertise**: The benefits of pair programming are influenced significantly by the complexity of the programming tasks and the expertise of the programmers involved. For simpler tasks, pair programming can be more efficient, while for complex tasks, it can lead to higher quality but requires significantly more effort. Junior programmers benefit notably from pair programming, achieving performance levels comparable to more experienced individuals.

5. **Publication Bias**: There is evidence of publication bias in the existing studies, indicating that the reported benefits of pair programming may be overstated. This highlights the need for further research, particularly studies that explore the effects of pair programming under varied conditions and contexts.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, team leads, and educators who are considering implementing pair programming in their development processes. It provides insights into the conditions under which pair programming can be most effective.

**Where the findings come from?**
All findings in this briefing are derived from a systematic review and meta-analysis conducted by Hannay et al. (2009), which compiled and analyzed data from multiple studies on pair programming.

**What is included in this briefing?**
The briefing includes a summary of the core findings regarding the effects of pair programming on code quality, task duration, and effort, along with insights about moderating factors such as task complexity and programmer expertise.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research group:**
[Simula Research Laboratory](http://www.simula.no)

**Original Research Reference:**
Hannay, J. E., Dybå, T., Arisholm, E., & Sjøberg, D. I. K. (2009). The effectiveness of pair programming: A meta-analysis. Information and Software Technology, 51(11), 1110-1122. https://doi.org/10.1016/j.infsof.2009.02.001"
"['Using Scrum in Global Software Development: A Systematic Literature Review  Emam Hossain                          CSE, The University of New South    Wales and National ICT Australia  Sydney, Australia  Emam.Hossain@nicta.com.au  Muhammad Ali Babar          Lero, University of Limerick  Limerick, Ireland  Muhammad.AliBabar@lero.ie  Hye-young Paik                       CSE,The University of New South  Wales, UNSW                      Sydney, Australia  hpaik@cse.unsw.edu.au        Abstract—  There is a growing interest in applying agile pract ices  in Global Software Development (GSD) projects. The literature  on using Scrum, one of the most popular agile appro aches, in  distributed development projects has steadily been growing.  However, there has not been any effort to systemati cally select,  review, and synthesize the literature on this topic . We have  conducted a systematic literature review of the pri mary studies  that report using Scrum practices in GSD projects. Our search  strategy identified 366 papers, of which 20 were id entified as  primary papers relevant to our research. We extract ed data from  these papers to identify various challenges of usin g Scrum in  GSD. Current strategies to deal with the identified  challenges  have also been extracted. This paper presents the r eview’s  findings that are expected to help researchers and practitioners  to understand the challenges involved in using Scru m for GSD  projects and the strategies available to deal with them.         Keywords- Global software development, agile approa ches,  Scrum, systematic literature reviews  I. INTRODUCTION   The trend in the recent software development indust ry is  to move towards Global Software Development (GSD).  This is driven by a number of factors such as impro ved  network infrastructure, move towards component-base d  architecture and increased time-to-market pressure[ 1].  Despite its popularity, the question of “which agil e practices  are effective for GSD under which circumstances?” h as not  been closely researched yet [2].     Agile Software Development (ASD) paradigm has  gained significant attention due to its flexible ap proach to  managing the requirement volatility and emphasis on   extensive collaboration between customers and devel opers  [3]. Recently, we have observed that an increased n umber of  GSD project managers are seriously considering intr oducing  agile practices [4]. Given the increased interest i n applying  agile practices in GSD projects, it appears worthwh ile for  the practitioners and researchers to investigate th e relevant  experiences reported in the literature to learn how  agile  practices can be effectively used in GSD projects.  Due to  the fact that agile practices are based on the phil osophy of  close, frequent and collocated collaborations, the  geographical distance in GSD alone can present a ch allenge.  Through a number of reports by GSD practitioners in  the  literature, we have found that, despite the obvious   difficulties, there are some instances of success o f using  agile practices with distributed teams [S1-S5]. But  other  researchers [5] still argue that the fundamental qu estion on  whether agile practices can be used in a distribute d setting is  still open to debate.   As the interest in using agile approaches in GSD pr ojects  is growing; so is the research literature on variou s  mechanisms, challenges and strategies of deploying agile  practices for GSD projects. However, there has not been any  significant effort to systematically identify, synt hesize, and  report the literature on using agile in GSD project s. To  address this research gap, this systematic literatu re review  seeks to identify, synthesize, and present the find ings  reported about using Scrum practices in GSD to date . In this  review, we only investigate agile practices that pe rtain to  software project management. We chose “Scrum” as it  has a', 'reported about using Scrum practices in GSD to date . In this  review, we only investigate agile practices that pe rtain to  software project management. We chose “Scrum” as it  has a  focus on day to day project management and is the m ost  widely adopted agile project management method. Rec ently,  an increasing number of GSD project managers are al so  seriously considering the use of Scrum practices in  their  development environment [6].  The next section gives an overview of Scrum method and  discusses the motivation of this research. Section 3 describes  the research methods used. The results of this stud y are  presented in Section IV. Section V discusses the fi ndings to  draw some conclusions. The limitations of the study  are  mentioned in Section VI. Section VII closes the paper with a  brief discussion of the researchable issues on this topic.  II. BACKGROUND AND MOTIVATION  In this section, we first introduce the Scrum metho d,  place the Scrum in the context of GSD and more conc retely  justify the need for this review.  A. Scrum  Scrum is an iterative and incremental project  management approach that provides a simple “inspect  and  adapt” framework. In Scrum, software is delivered i n  increments called “Sprints” (usually 2-4 weeks iter ations)  [6]. Each sprint starts with planning and ends with  a review.  A sprint planning by a Scrum team is a time-boxed m eeting,  which could last up to 4 hours. It is dedicated to developing  2009 Fourth IEEE International Conference on Global Software Engineering 978-0-7695-3710-8/09 $25.00 © 2009 IEEE DOI 10.1109/ICGSE.2009.25 175 2009 Fourth IEEE International Conference on Global Software Engineering 978-0-7695-3710-8/09 $25.00 © 2009 IEEE DOI 10.1109/ICGSE.2009.25 175 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'detailed plans for the sprint. The Stakeholders of a project  attend sprint review meetings to review the state o f the  business, the market and technology. These meetings  could  also last up to 4 hours. A retrospective meeting ma y be  scheduled to assess the teamwork in the completed sprints. A  daily Scrum meeting by a Scrum team is a 15-minute long  and each team member addresses three questions: wha t did I  do yesterday, what will I do today and what impediments are  in my way? Scrum produces three artefacts, namely: product  backlogs, sprint backlogs and burn-down charts. Bac klogs  contain customer requirements and daily burn down c harts  show the cumulative work remaining.  B. Scrum in Global Software Development  Agile approaches are usually considered effective f or the  projects with high uncertainty [3]. Paasivaara et a l [S1]  reported that distributed software development proj ects with  volatile requirements and uncertain implementation  technologies can use various agile practices for ef fectively  organizing and managing projects. Scrum has been al ready  found an effective approach to managing projects wi th  many small, collocated development teams [3]. Suthe rland  and Schwaber [6] argue that Scrum can also be used for  large and distributed teams. Indeed, from the paper s  reviewed in this review, we have found some distrib uted  projects in which Scrum has been successfully used.  C. Objective of this Review  Scrum teams are self organized, are facilitated by rich  communication and a collaborative environment and a re  usually considered effective for co-located project s with a  small team size [3]. Thus, it is apparently difficu lt to apply  Scrum practices in GSD projects because of the phys ical  separation of the development team members [4]. The re can  be other GSD project contextual factors (e.g., numb er of  distributed sites, collaboration modes, i.e., inter   organizational or intra organizational, number of t eams,  project personnel or team size, socio-cultural dist ance and  so on) that may also impact on Scrum team collabora tion  processes. A recent survey about agile practice ado ption rate  [7], reported that agile practices can be successfu lly used by  significantly distributed team members. Another sur vey  concludes that among the various agile practices, p roject  management practices such as Scrum practices have a   higher adoption rate [8]. Thus, we can argue that S crum, as  an agile method, is becoming increasingly popular a nd may  also be used for globally distributed teams. But th e actual  process of using Scrum’s collaborative practices in stead of  project stakeholder’s distribution is not clearly u nderstood  [4]. For this reason we have decided to explore, in vestigate  and explain various challenging factors that restri ct the use  of Scrum practices due to the global project. Curre nt  strategies to reduce these challenging factors are also be  explored.   III. RESEARCH METHOD  This research has been carried out by following  Kitchenham and Charters [9] guidelines for conducti ng  Systematic Literature Review (SLR) or Systematic Re view  (SR), which involves several activities such as the   development of review protocol, the identification and  selection of primary studies, the data extraction a nd  synthesis, and reporting the results. We followed a ll these  steps for the reported study as described in the fo llowing  sections of this paper.     The broad objective of this study is to answer the  following research question.  RQ.  What is currently known about the use of the S crum  practices in GSD projects?  More specifically, this study focuses on the follow ing two  questions:  RQ1. What challenging or risk factors restrict the use of  Scrum practices in globally distributed projects?  RQ2. What strategies or practices are being commonl y used  to deal with these challenging factors to support t he use of  Scrum practices in globally distributed projects?', 'RQ2. What strategies or practices are being commonl y used  to deal with these challenging factors to support t he use of  Scrum practices in globally distributed projects?  A. Data Sources and Search Strategies  We only searched for papers that are written in Eng lish  and available online. The search strategy included electronic  databases and manual searches of conference proceed ings.  The following electronic databases were used.  •  IEEEXplore (www.ieeexplore.ieee.org/Xplore/)  •  ACM Digital library (www.portal.acm.org/dl.cfm)  •  Google Scholar (http://scholar.google.com.au/)  •  Compendex EI (www.engineeringvillage2.org/)  •  Wiley InterSciene (www.interscience.wiley.com/)  •  Elsevier Science Direct (www.sceincedirect.com/)  •  AIS eLibrary (www.aisel.aisnet.org/)  •  SpringerLink (www.springerlink.com/)      We also searched the following conference proce edings  for papers on the use of the Scrum practice(s) in G SD  context.   •  Agile Processes in Software Engineering and Extreme   Programming(XP/Agile Universe)  •  Agile Conference  The types of papers ranged from industry experience   reports, theoretical, empirical and experimental ac ademic  papers.  Figure 1 shows the review process and the number  of papers identified at each stage. In stage 1, we searched  the databases using the search terms listed in Tabl e I.  Category 1 has more keywords and shows many variati ons  of the same term “Global Software Development”. All  these  search items were combined by using the Boolean “AN D”  operator, which entails that an article that focuse s on both  Agile and Global Software Development, will be retr ieved.  That is, we searched every possible combination of one item  from Category Type 1 AND Category Type 2. The searc h  excluded articles that address editorials, prefaces , article,  reviews, discussion comments, news, summaries of  176 176 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'tutorials, workshops, panels and poster sessions. T his search  strategy resulted in a total of 583 “hits’ that inc luded 366  unduplicated papers.   TABLE I.  SEARCH TERMS USED IN THIS REVIEW    B. Managing Studies and Inclusion Decisions  Our study followed the citation management procedur e  reported by Dyba and Dingsoyr [10]. We used EndNote  for  storing relevant citations from stage 1 (n=366). Th e  citations were then imported into a spreadsheet whe re we  recorded the sources of each citation and subsequen t  inclusion / exclusion decision. We maintained separ ate  Endnote library and spreadsheet for each stage. In the  second stage, two of the authors sat together and w ent  through the titles of all the 366 studies that resu lted from  stage 1, to determine their relevance to the system atic  review. At this stage, articles with titles that in dicated  clearly that the articles were outside the scope of  the SLR  boundary were excluded and identified 123 relevant studies.  However, a paper’s title may not always represent t he  content of the paper. During the next stage, we div ided 123  abstracts among three researchers in such a way so that each  abstract was reviewed by two researchers independen tly.  We found 109 abstract agreements among 123 assessme nts.  All the disagreements were resolved by three resear cher’s  discussions. At the end of stage 3, we were left wi th 77  papers for stage 4 of the selection process.  C. Final Selection  We used the following screening criteria to ensure the  papers address our research topic.  1. Does a paper address the use of any Scrum practices  in  distributed projects?   2. Does a paper discuss any real life experience of us ing  Scrum practices in distributed projects?         As there is a lack of existing empirical rese arch, we also  consider “lesson learned” report based on expert op inion  that address the use of Scrum practice in GSD proje cts. For  additional quality assessment, we included followin g two  criteria related to the quality of each paper’s des cription.  3. Does the objective of the paper is clearly mentioned?  4. Does the paper discuss GSD project contextual facto rs  adequately?   The adequacy of project contextual factors discussi on  was measured based on the GSE background informatio n as  shown in Appendix B. These 4 points provided a meas ure of  the extent to which we are confident that a selecte d paper  could make a valuable contribution to understand th e  current use of Scrum practices in distributed setti ng. Each of  the 4 criteria was graded on a dichotomous (“yes” o r “no”)  scale.       Figure 1. The selection process of primary papers.    We selected 21 papers out of the 77 articles by car rying  out the quality assessment based on these four scre ening  criteria. We accepted a paper that has satisfied 4 criteria and  graded as all “yes”. For example, we excluded a num ber of  papers that discussed some other agile methods and  practices (e.g. XP, pair programming). Among the 21   papers, we found that one journal paper [S1] was an   extended version of previously published conference  paper  [S1a]. We also found that two papers [S3] and [S3a]   published in two different conferences were based o n the  same empirical study. In both cases, we included th e  comprehensive recently published papers as mentione d in  appendix A. In addition, one researcher went throug h the  reference list of every selected paper of this fina l stage. This  helped us to identify any relevant paper that was n ot  extracted by our search strategy. In this process, we  identified one journal paper [S8] that was not retr ieved  through our search of electronic databases but was cited by  some of the selected papers [S1, S4]. The abstract was  reviewed by two researchers independently and agree d that  the paper [S8] appeared to be within the scope of t he  research. Finally we selected 20 papers (excluding two  177 177', 'reviewed by two researchers independently and agree d that  the paper [S8] appeared to be within the scope of t he  research. Finally we selected 20 papers (excluding two  177 177 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'repeated papers S1a and S3a and including one journ al  paper S8 from initially selected 21 papers) for dat a  extraction and synthesis phases. We have enlisted t he  selected primary studies in Appendix A.   D. Data Extraction and Synthesis  From the final selected studies, we extracted data using a  pre-defined data extraction form as shown in Append ix B.  The detail description of the data extraction form can be  obtained in the technical report [11]. During data extraction,  we found it quite difficult to extract relevant and  meaningful  information that can answer the research questions.  This is  because the primary studies included in this SLR ar e mainly  based on industry based experience reports and most  of  them are not described in a commonly used research paper  structure. As usually a standard research report di scusses  research problem, related research work, research m ethod,  data analysis technique and conclusion adequately [ 12]. For  this reason, two researchers performed data extract ion  independently. Extracted data from each researcher were  compared and disagreements were discussed and resol ved  by consensus in meetings. For further disagreement,  we  consulted with a third independent researcher who h as  extensive experience in SLR. We used a qualitative data  analysis tool (NVivo) to store textual data that ar e able to  address our research questions.  We synthesized the data by identifying themes emana ting  from the findings reported in each of the paper rev iewed in  this study. In the following section, we present fr equencies  of the number of times each theme is identified in different  studies. The respective frequencies reflect the num ber of  times a particular challenge has been mentioned in different  papers.  IV. RESULTS  A. Overview of Studies  Table II shows that the number of papers on the iss ue of  using Scrum practices in GSD context are increasing  over  the last few years. It can be argued that the publi cation trend  may be an indicator of practitioners and researcher s’  growing interest in using and reporting Scrum pract ices for  GSD projects.   TABLE II.  SELECTED PAPERS BY YEAR INTERVAL  Year 2003 2004 2005 2006 2007 2008 2009  papers 1 1 1 3 4 9 1  % 5% 5% 5% 15% 20% 45% 5%        Table III shows that only 4 studies (20%) included in this  SLR are empirical studies and all of them are indus trial case  studies. Rest of the 16 studies (80%) are classifie d as  “lesson learned” or industrial experience reports. Hence, we  conclude that there is a little empirical evidence based  reported on the use of Scrum practices in GSD context.   TABLE III.  TYPES OF STUDIES REVIEWED   Study Focus Number  of Papers  percentage Reference  Empirical Study 4 20% [S1-4]  Industrial  Experience  Reports  16 80% [S5-20]         Table IV presents project frequencies that are   categorized according to few distributed project co ntextual  factors. We have found that most of the studies rep ort the  use of Scrum practices in GSD projects from intra- organizational, multi-national companies. Our findi ngs also  reveal that a limited number of distributed sites a re involved  while Scrum practices are used in distributed sites .  However, some researchers claim that a distributed project  with multiple teams can also use Scrum in their  development [6]. Scrum can also be used in a distri buted  project with large number of project personnel or t eam size.  In this case a number of Scrum teams are involved w ithin  the project. Some of the distributed projects can a lso use  Scrum by minimizing the challenge of no overlap tim e  between distributed sites. We have also found that a wide  range of project domains ranging from simple web  application to mission critical projects have been undertaken  using Scrum in distributed development environment.   TABLE IV.  PROJECT CATOGORIZATION ACCORDING TO FEW PROJECT  CONTEXT FACTORS       178 178', 'using Scrum in distributed development environment.   TABLE IV.  PROJECT CATOGORIZATION ACCORDING TO FEW PROJECT  CONTEXT FACTORS       178 178 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'B. Findings about Research Questions  This section discusses how the data extracted from the  reviewed studied address our research questions. By   investigating the two research questions, we aim to provide a  synthesized overview of the literature on using Scr um  practices in different distributed projects.   1) RQ1-Challenges of Using Scrum Due to Project  Distribution  We have identified sixteen papers that can help us to  answer the research question 1 (RQ1), “What are the   challenges of using Scrum practices in distributed  development?”  Our analysis of the extracted data has revealed tha t the  temporal, geographical and socio-cultural distance of GSD  projects impact on using various Scrum practices in   distributed settings. We have found that communicat ion  related issues are the major challenges when using Scrum in  distributed settings. Cultural differences among di stributed  team members may also impact on team collaboration and  communication processes. Managing a large team can also  be considered as one of the key challenges. A lack of  dedicated meeting room for each site and Scrum team   distribution at multiple sites also appear to be ch allenging  factors that restrict the team communication and  collaboration processes. Table V summarizes our fin dings  about the key challenges of using the Scrum practic es in  GSD projects. Usually sprint planning or retrospect ive  sessions can last up to four hours or sometimes eve n more  [6]. Thus, it is very difficult to conduct such a l ong meeting  if the distributed teams experience significant tim e zone  differences. For this reason, lack of synchronous  communication is considered as one of the most vita l  challenges for using Scrum in GSD context.   TABLE V.  CHALLENGING FACTORS DUE TO PROJECT  GLOBAL  DISTRIBUTION     Challenging factors Paper references Frequency   (# of  studies)  Synchronous  communication  [S1-2,S6-7,S9- 10,S16-17,S19]  9  Collaboration  difficulties  [S1-3,S15-16,S19] 6  Communication  Bandwidth  [S5-7,S15-16,S19- 20]  6  Tool support [S4, S10-11,S15- 18]  6  Large Team [S2,S5,S7,S10,S16] 5  Office Space [S15-17] 2  Multiple sites [S9] 1      A distributed project usually involves people with  cultural and linguistic diversity, which may discou rage  offshore team members from voicing their opinions o r  views fully and completely [S1]. This situation usu ally  results in miscommunication, misunderstandings or  confusion among team members. This SLR has found th at  some Scrum teams could not conduct effective retros pective  meetings due to the socio-cultural distance involve d in the  distributed project [S1, S7]. Communication network s can  also be slow and unreliable with poor transmission quality  hampering communication standards when using variou s  communication tools (e.g. video conferencing) [S15- 16,  S19]. Providing better communication bandwidth and right  tool in a distributed project that use distributed Scrum  meeting practices is vital [S17].   Lack of effective collaborative tools, global task boards,  suitable bug and issue trackers, globally accessibl e backlog  tool are also reported to be challenging factors [S 10-11,  S15]. Managing a project with a team of large numbe r of  members distributed at multiple sites is considered  a  challenging undertaking [S2, S5, S7]. The need of a   dedicated meeting room with necessary infrastructur e and  tool support is also considered necessary in a numb er of  reviewed studies [S15-17]. Using Scrum in a team th at is  distributed in more than two sites with different t ime zone  differences is also observed quite difficult [S9].  2) RQ2- Used Strategiesto deal with these challenging  factors  Our SLR has found that Scrum teams use various  practices or strategies to reduce these challenging  factors to  support the use of Scrum practices in globally dist ributed  projects. This review has identified and categorize d these  practices as follows.', 'support the use of Scrum practices in globally dist ributed  projects. This review has identified and categorize d these  practices as follows.   Synchronous communication:  Our SLR found that Scrum  teams used some strategies to provide synchronous  communication when distributed team has no overlap time.   From the reviewed papers, we found ten projects had   distributed sites without any overlapping working h ours.  Thus we can argue that Scrum can be used within a  distributed project that has even no overlap time b etween  distributed sites. To address the lack of synchrono us  communication following practices were widely used.   Synchronized work hours:  This practice is widely used by  Scrum teams to ensure synchronous communication amo ng  distributed sites can be arranged. This is done by adjusting  working hours, working from home, working long hour s  and so on [S1-2, S6, S9, S13-14, S16-17, S19-20]. S ome  Scrum teams used strategies to avoid the need of in creased  overlap time. For example, a Scrum team used strict  time- boxed meeting (e.g. two hours planning meeting) to avoid  late night meeting at some sites [S6]. To make the meetings  short and effective, team members post their three daily  Scrum questions or develop backlog (feature list) b efore  attending the distributed meetings [S8, S10, S12, S15].   Local Scrum team:  Due to the lack of overlap time, Scrum  teams are formed locally and each site conducts the ir own  scrum [S6-9, S10-11, S18]. The meeting practice Scr um of  Scrums is attended by a key touch point member for each  team to ensure inter-team communication. To form su ch a  179 179 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'Scrum team, the local team should be autonomous and   should also allocate independent architectural subs ystems  with well defined interfaces to each team to reduce  inter site  communication [S6- 9, S13].   To establish multiple   communication lines, Scrum team allows additional  distributed meetings along with Scrum master meetin g  attended by technical lead or design architect of e ach local  Scrum team [S9].     Modified practices: In some cases, Scrum team modifies or  extends Scrum practices to address the communicatio n  challenges. For example, Berczuk reports that havin g a local  “mini-scrum” in the morning after a distributed scr um  meeting can be very effective to reinforce the valu e of the  Scrum within a local team [S17]. Scrum teams also u se  strict communication policy (e.g. E-mail reply with in 12  hours) to avoid delay due to the temporal distance of a  distributed team [S9]. Instead of whole team presen ce in the  late night (or early morning) Scrum meetings, only key  members of the team attend the meetings with distri buted  teams [S5, S7, S13]. Moreover, the distributed dail y Scrum  meetings are usually cut down to twice-a-week meeti ngs  [S16]. We also found other modified practices such as  asynchronous retrospective meetings (e.g., posting  comments and results on Wikis, emailing the minutes  of  local Scrum meeting to the onshore team), conductin g sprint  demo by onshore team only (later onshore team brief s  offshore team) [S1-3, S9, S13, S16].   Team Collaboration: Our SLR revealed that socio-cultural  distance in GSD projects substantially impacts team   collaboration processes and may cause ineffective S crum  meeting practices (e.g. daily Scrum meeting). GSD p roject  managers use a number of practices that facilitate better  team collaboration while using Scrum practices.   Team Gathering: To increase a project’s domain knowledge  and reduce the cultural distance, a Scrum team gath ers and  performs few initial sprints at one site before dis tributed  development starts [S13, S15-19]. The members of a  distributed Scrum team are also gathered quarterly or  annually for few days [S1, S6, S10, S18]. During th is  gathering, a Scrum team can perform scrum planning,   review meeting, retrospectives, sprint and various  socializing activities, which can help to reduce cu ltural  distance [S18].  Visit: To reduce the cultural distance and increase proje ct  vision, a Scrum team adopts the practice of exchang e visits  for example Product owners regularly visit offshore  team  throughout the development. [S15-16, S19]. Cultural   exchange is also performed by maintaining planned r otation  among offshore and onshore teams and cross-location  visits  [S14-15]. Practices like product owners organizing quarterly  product roadmap meetings were also proven effective  for  helping team’s members to fully understand a projec t’s  vision [S16].  Unofficial distributed meetings:   For increased team  collaboration, along with formal meetings, distribu ted  Scrum team members may also use frequent informal  meetings for clarifying various issues [S1]. These unofficial  meetings may involve leadership meetings, testing, and  architectural meetings, distributed team lead meeti ngs, peer  meetings, and socializing meetings (for example, vi rtual  party or games) or even “coffee talks” for the coll ocated  team members [S14].   Training: Our SLR also found that Scrum teams use some  practices that can be categorized as “training”.   Practices  for example “initial Scrum training,” “technical Sc rum” to  clarify new technology issues, reinforce the value of Scrum  and improve team collaboration while using Scrum practices  in GSD projects [S9, S16].   Key documentation:  Maintaining valuable documentation  may also improve GSD team collaboration processes w hile  using Scrum practices [S7, S9, S16, S19]. For examp le,  supplementing user stories with Use Case diagrams i n', 'may also improve GSD team collaboration processes w hile  using Scrum practices [S7, S9, S16, S19]. For examp le,  supplementing user stories with Use Case diagrams i n  globally accessible backlogs helps reduce  misunderstandings and improves team collaboration  processes [S16]. Scrum teams use a number of tools,  for  example, issue tracker (e.g. Jira), enterprise wiki s (e.g.  Confluence), and project management tool (e.g. Scru m  works) to maintain better documentation and project   transparency [S9, S16].   Mandatory participation:  To reduce “offshore silence”  challenge, Scrum team can assign each site a thirty -minute  mandatory demo presentation during retrospective se ssions  [S18]. The participation in these sessions helps ma ke an  empowered distributed team [S16]. To reduce cultura l  impediments, offshore teams are also encouraged to provide  useful information during daily Scrum meetings [S1].   Gradual team distribution:  Scrum teams may move from a  collocated project to a distributed project gradual ly through  several stages (i.e., evaluation, inception, transi tion and  steady state) [S13]. The gradual transition helps d eal with  the challenges caused by cultural distances and als o helps to  increase project domain knowledge. Our SLR reveals that in  one specific Scrum project, during initial three st ages of  gradual team distribution (i.e. evaluation, incepti on and  transition phase) only a representative of an offsh ore team  participated with onshore team in Scrum meeting pra ctices.  However, in steady state stage, all the Scrum team members  located in onshore and offshore teams participated in the  distributed Scrum meetings [S13]. In another projec t, one  onshore Scrum master facilitated offshore Scrum mee tings  for few initial sprints and came back to onshore wh en the  offshore team became familiar with Scrum practices [S15].  Communication bandwidth:  To provide a rich  communication environment and also to avoid slow,  unreliable, and poor transmission, Scrum teams use the  practice “multiple communication modes” . The practice  ensures that a Scrum team with distributed project  stakeholders is supported with various options of  communication tools such as phone, web camera,  teleconference, video conference, web conference, n et  meeting, email, shared mailing list, Instant Messag e (IM),  Short Message Service (SMS), and Internet Relay cha t  180 180 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', '(IRC) [S1]. Hence, Scrum team can choose appropriat e tool  from a wide range of communication tools suitable t o the  communication bandwidth. For example, if a Scrum te am  found videoconferencing is not supported by the exi sting  communication bandwidth, they may choose a  teleconference in their distributed meeting sessions.    Tool Support:  GSD projects that consider using Scrum  need a wide range of tool support. Tools may includ e  communication, collaborative, project management, i ssue  tracking, bug tracking, globally accessible backlog , and  burn down chart etc. We found the practice “proactive  resource management”  helps ensure that a Scrum team has  the necessary tools and skills to support their Scr um  practices in distributed settings. Our SLR revealed  that  along with communication tools, Scrum teams also us e a  number of collaborative tools including Wikis, Blog s, social  book marking, expertise finders, whiteboards, elect ronic  work space, desktop and application sharing, photo charts,  knowledge bases, experience databases, lesson learn ed  repositories, while using Scrum practices [S1-20]. An  enterprise wiki (e.g. Confluence) has been found to  be very  effective while using Scrum practices [S20].  Distr ibuted  team members can communicate and publish the result s of  various Scrum meetings minutes in wiki [S20]. To in crease  project transparency and visibility and to support the Scrum  practice “Backlog”, our SLR has also revealed that  distributed Scrum teams use a number of tools inclu ding  globally accessible project management tools (e.g. “Rally”),  issue tracker, bug tracker (e.g. “Jira”), backlog m anagement  tools (e.g. “Scrum works”), and tools for supportin g the  Scrum artifacts “Burn down charts” [S1- 4, S7, S10,  S17,  S19-20].   Team management:  we have also found that a commonly  used strategy for managing a large distributed team  that  considered using Scrum is to split into small manag eable  sub-teams [S1-2, S5]. Thus, a large GSD project may   contain a number of Scrum teams (or sub teams) and some  of the Scrum teams may also be geographically distr ibuted  [S1]. Scrum teams use a number of strategies to for m sub  teams. An autonomous sub team can be built and allo cated  based on features, functions and so on that ensure each sub  team is allocated independent architectural subsyst ems with  well defined interfaces [S6-8, S9, S13]. For exampl e, highly  volatile features need frequent interaction with bu siness  users and such features can be developed with a sub -team  close to the customer [S3, S13].  In some cases, a sub-team  has its own product owner and Scrum master and cond ucts  their own Scrum [S1, S3, S5].       We also observed that GSD projects used followi ng  Scrum team models suitable to their development  environments while considering Scrum [6].  Isolated Scrum team: GSD project teams are geographically  isolated; in most cases offshore teams are not cros s- functional and may not use Scrum processes. There i s less  empirical evidence of using this type of team model  while  using Scrum.  Distributed Scrum of Scrums team:  In this team model,  Scrum teams (or sub-teams) are formed based on loca l site  and each team perform their site based own independ ent  Scrum. The meeting practice, Scrum of Scrums that i s  attended by the key touch points (e.g. Scrum master ) from  each site based sub-team ensures effective inter-te am  communication [S1]. If the number of sub-teams incr eases,  in some cases, a nested Scrum of Scrums meeting pra ctice  (e.g. Scrum of Scrum of Scrums) ensures effective s ub-team  coordination [6].   Fully Integrated Scrum team:  In this team model, Scrum  teams are cross-functional with team members distri buted  across geographical locations. This type of Scrum t eam  should consider the risks due to geographical, temp oral and  socio-cultural distances. In this model, all team m embers', 'across geographical locations. This type of Scrum t eam  should consider the risks due to geographical, temp oral and  socio-cultural distances. In this model, all team m embers  should attend and participate in every Scrum meetin g  practice. We found in some cases, a GSD project tha t has  several fully integrated Scrum teams follows the pr actice  “centrally located management team”  in which  management persons of each Scrum team are located i n a  central site (e.g. onshore) [S1-2]. In this case fr equent  meetings among a centrally located product owner te am, a  team of Scrum masters, and architects from the sub- teams  ensures effective multiple sub-team communication a nd  collaboration [S2].  Office space:  Our SLR has revealed that to support a better  communication and collaborative work and meeting  environment, Scrum teams use following practices:  Single room:  This practice ensures each Scrum team is  allocated to a single room so that they can communi cate  with each other [S1, S9, S11]. In this case if a pe rson  switches teams, he or she is also relocated to the new team’s  room [S1]. If the Scrum team is divided into multip le sub- teams, then all co-located sub-teams are able to wo rk in a  single room should be ensured [S1].   Dedicated meeting room:  This practice also ensures each  site has a separate meeting room with all necessary  network  connectivity and tools while attending a distribute d meeting  [S1, S3]. To make Scrum meetings visible to everyon e, each  site can use a video projector [S15]. In some cases , a virtual  conference room can also be used as a dedicated mee ting  room for Scrum meetings sessions [S5].  Multi sites:  It has been reported that Scrum teams usually  use the following strategies while using Scrum prac tices in  GSD projects with multi sites development.  Local Scrum team:  GSD project managers build  autonomous site-based local Scrum teams and allocat e tasks  with independent architectural subsystems and well defined  interfaces to each local team [S6-8, S9, S13]. The practice  Scrum of Scrums attended by a key touch point (e.g.  Scrum  master) of each site provides inter-team coordination [S14].   Restricted team distribution:  In this practice, a fully  integrated Scrum team is restricted within a limite d number  of sites distributions. For example, one of the stu dies  reported on a project that was distributed over mul tiple sites  181 181 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'but each Scrum team was distributed between two sit es only  [S9].   V. DISCUSSION  In this section, we review various findings of our  Systematic Literature Review (SLR) and draw followi ng  conclusions     Conclusion 1.  There is a growing interest and literature  demands more empirical study to understand the use of  Scrum practices in globally distributed projects.        It is still an open debate whether or not the S crum  practices can successfully be used in distributed s ettings  [[5]. However, the increasing number of publication s on this  topic, as shown in table 1, appears to be an indica tion that  there is an increasing interest in using Scrum prac tices in  GSD projects. We have found that most of the papers  and  all the empirical studies have been published after  2007.  Among the reviewed twenty studies, all of the four  empirical studies and few experience reports have r eported  some degree of success in using Scrum practices in GSD.  Despite these successes, the mechanics of combining  Scrum  practices and GSD are not well understood [4]. Thes e  findings highlight a vital research gap that needs immediate  attention of GSD and agile communities. Hence, ther e is a  clear need of building empirically founded knowledg e about  using agile practices  in general and Scrum practic es in  particular in the context of GSD.    Conclusion  2.  The use of Scrum practices may be limited  by various GSD project’s contextual factors.          Our review has revealed that there can be seve ral  contextual factors of a project that may impact the  use of  Scrum practices in GSD. Some of the factors identif ied in  the reviewed studies are shown in Table 2. Our find ings also  reveal that most of the distributed projects were w ithin the  same company and the team distribution was limited by the  number of distributed sites. We also found that the re is a  limited evidence of using Scrum for safety critical   applications. Though our findings reveal that the S crum  practices can be used in a distributed project that  has  multiple numbers of teams, very large project perso nnel or  even no overlap time between distributed sites, but  the  actual process of using Scrum is not clearly unders tood yet.  We did not consider the impact of other project con textual  factors (for example: budget, complexity, criticali ty, team  experience, time constraints, contract nature and s o on) on  using Scrum in GSD projects. Thus, we conclude that  the  use of Scrum practices may be limited by various co ntextual  factors of a GSD project.     Conclusion 3.  Globally distributed Scrum teams usually  face a number of challenges as project distribution  impact  on communication, coordination and collaboration  processes.      Our review findings reveal that the temporal,  geographical and socio-cultural distances due to th e project  stakeholder’s distribution cause a number of challe nging  factors that impact GSD communication, coordination  and  collaboration processes. The communication related  challenges are identified as vital. Any cultural di fferences  involved in a distributed team can substantially im pact on  the team’s collaboration process. Managing a large team  distributed at multiple sites is quite challenging as well.  Lack of tools and insufficient infrastructure suppo rt may  also make use of Scrum practices in GSD difficult.     Conclusion  4.  Scrum practices need to be extended or  modified in order to support globally distributed s oftware  development teams.    Our findings reveal that to support the use of Scru m  practices in various distributed projects, Scrum te ams need  to add a number of strategies suitable to their dev elopment  environments. A distributed Scrum team can choose  different Scrum team models to reduce its project  distribution challenges. A distributed team usually  needs  some overlap time between them to carry out various  Scrum', 'different Scrum team models to reduce its project  distribution challenges. A distributed team usually  needs  some overlap time between them to carry out various  Scrum  meeting practices. To support a distributed team th at has no  overlap time, Scrum teams may use some supporting  distributed practices including synchronized work h ours,  local Scrum, additional local team meetings, strict   communication policy, key persons attending all dis tributed  meetings, reducing number of Scrum meetings,  asynchronous retrospective and so on. To increase t he team  collaboration processes, Scrum team can also use so me  practices including team gathering, exchange visits ,  informal meetings of distributed team members, mand atory  presentations, maintaining key documentation, and g radual  team distribution which also help to reduce team cu ltural  differences. A Scrum team can also use different pr actices  such as multiple modes of communication to address the  challenges caused by the lack of communication band width  and tools. A distributed Scrum team also needs to b e  supported by various tools for project management, backlog  management, tracking issues, and so on.  VI. LIMITATION  Like any empirical study, this study also has certa in  limitations that should be kept in mind while consi dering  the reported findings. With the increasing number o f studies  in this area, this review may have missed some pape rs that  address the use of Scrum practices in GSD. However,  we  are confident that it would not have been a systema tic  omission.        The papers included in this review have underg one a  thorough selection process and involved two researc hers  cross checking the completeness of searchers and va lidating  the suitability of each paper for inclusion. Howeve r, the  182 182 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', 'findings of this review may have been affected by t he  systematic bias in describing the use of Scrum prac tices in  various primary studies as some of the selected stu dies  describe the use of various Scrum practices along w ith other  agile practices (e.g. XP practices).          During the data extraction process, we found that several  papers lacked sufficient details about the reported  projects’  contextual factors and the challenges faced and str ategies  used while using Scrum practices in GDS projects. W e  synthesized our data by identifying and categorizin g the  themes from the papers included in this review. Sin ce some  of the selected papers do not provide detailed info rmation,  there is a possibility that the extraction process may have  resulted in some inaccuracies.  VII. CONCLUSIONS AND FUTURE RESEARCH  We have conducted a systematic review of the litera ture  on the use of Scrum practices in GSD projects. The aim of  this review was to identify various challenging fac tors that  restrict the use of Scrum practices in projects tha t are  globally distributed. Exploring potential strategie s to deal  with those challenging factors were also another re search  focus. We have presented our findings in two stages : initial  quantitative data presentation about the number of published  papers in each year starting from 2003, the types o f studies  reported in the reviewed papers and the contextual factors of  the reported projects. In the second stage, we have  analyzed  and interpreted the data extracted from the primary  studies  included in this review in order to find the answer s to our  research questions. Our analysis and interpretation  of the  data have enabled us to draw some general conclusio ns in  Section 5 about the current state of practice of us ing Scrum  practices in GSD projects.   The results of this review provide information that  can be  useful for GSD practitioners’ understanding the var ious  challenging factors that may impact on GSD  communication, collaboration and coordination proce sses  and restrict the use of Scrum practices.  Moreover,  the GSD  project managers can also benefit from the synthesi zed  knowledge about the strategies that are being used to deal  with the identified challenges. However, the streng th of  evidence found in the literature about the identifi ed  strategies is very low. That is why it is difficult  to offer any  specific advice to practitioners solely based on th is review.  This review has also identified several interesting  research  challenges that need to be addressed in the future research  efforts by GSD and agile researchers. A clear findi ng of this  review is that there is an immediate need of increa sing the  quantity and quality of empirical studies to descri be,  evaluate, explore and explain the use of various Sc rum  practices in GSD projects.   To enhance the findings of this review, we intend t o  conduct a comprehensive survey of practitioners to identify  the key challenges involved in and the strategies t o reduce  these challenges to support the use of Scrum practi ces in  GSD projects. In addition to this survey, we will a lso  conduct multiple in depth industry based case studi es to  provide an empirically supported body of knowledge about  the use of Scrum in GSD projects considering variou s  contextual factors.    Appendix A. Papers included in the review     [S1] M. Paasivaara, S. Durasiewicz, C. Lassenius,  “Distributed Agile Development: Using Scrum in a La rge  Project,” Software Process Improvement and Practice , Vol.  13, Issue 6, pp. 527-544, 2008.  [S1a] (Excluded) M. Paasivaara, S. Durasiewicz, C.  Lassenius, “Distributed Agile Development: Using Sc rum  in a Large Project,” in Proceedings of ICGSE 2008, pp. 87- 95, 2008.  [S2] J. Sutherland, A. Viktorov, J. Blount, N. Punt ikov,  “Distributed Scrum: Agile Project management with  Outsourced Development Teams” in Proceedings of the', '95, 2008.  [S2] J. Sutherland, A. Viktorov, J. Blount, N. Punt ikov,  “Distributed Scrum: Agile Project management with  Outsourced Development Teams” in Proceedings of the   Conference on HICSS’40, pp. 274, 2007.  [S3]  J. Sutherland, G. Schoonheim, M. Rijk, “Fully   distributed Scrum: Replacing Local Productivity and   Quality with Offshore Teams,” in proceedings of the   Conference on HICSS’42, pp. 1-8, 2009.   [S3a] (Excluded) J. Sutherland, G. Schoonheim, E.  Rustenburg, M. Rijk, “Fully distributed Scrum: The secret  sauce for Hyperproductive Outsourced Development  Teams” in Proceedings of the Conference on Agile 20 08,  pp. 339-344, 2008.  [S4] J. Cho, “Distributed Scrum for Large-Scale and   Mission-Critical Projects,” in Proceedings of the C onference  on AMCIS 2007, paper 235, 2007.  [S5] W. Williams, M. Stout, “Colossal, Scattered, a nd  Chaotic (Planning with a Large Distributed Team),” in  Proceedings of the Conference on Agile 2008, pp. 35 6-361,  2008.  [S6] B. Drummond, J. F. Unson, “Yahoo! Distributed Agile:  Notes from the World Over,” in Proceedings of the  Conference on Agile 2008, pp. 315-321, 2008.  [S7] M. Cristal, D. Wildt, R. Prikladnicki, “Usage of  SCRUM Practices within a Global Company,” in  Proceedings of ICGSE 2008,  pp. 22-226, 2008.  [S8] H. Holmstrom, B. Fitzgerald, P. J. Agerfalk, E . O.  Conchuir, “Agile Practices Reduce Distance in Globa l  Software Development” Information Systems Management ,  Summer, pp. 7-26, 2006.  [S9] M. Vax, S. Michaud, “Distributed Agile: Growin g a  Practice Together” in Proceedings of the Conference  on  Agile 2008.pp.310, 2008.  [S10] H. Smits, “Implementing Scrum in a Distribute d  Software Development Organization,” in proceedings of the  Conference on AGILE 2007, pp.371-375, 2007.  [S11] B. Jensen, A. Zilmer, “Cross- continent Devel opment  using Scrum and XP” in Proceedings of   the Confere nce on  XP 2003, pp.146-153, 2003.  183 183 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.', '[S12] C. Kussmaul, R. Jack, B. Sponsler, “Outsourci ng and  Off shoring with Agility: A Case Study” in Proceedi ngs of  the Conference on XP/Agile Universe, pp. 147-154, 2004.  [S13] K. Sureshchandra, J. Shrinivasavadhani,  “Ado pting  Agile in Distributed Development” in Proceedings of   ICGSE’08,pp. 217-221, 2008.  [S14] A. Danait, “Agile offshore techniques- A case  Study”  in proceedings of the Conference on Agile Developme nt  Conference, pp.214-217, 2005.  [S15] M. Summers, “Insights into an Agile Adventure  with  Offshore Partners,” in Proceedings of the Conferenc e on  Agile 2008, pp. 333-338, 2008.  [S16] E. Therrien, “Overcoming the Challenges of Bu ilding  a Distributed Agile Organization,” in Proceedings o f the  Conference on Agile 2008, pp. 368-372, 2008.  [S17] S. Berczuk, “Back to Basics: The Role of agil e  Principles in Success with a Distributed Scrum Team ,” in  Proceedings of AGILE 2007, pp. 382-388, 2007.  [S18] P. Karsten, F. Cannizzo, “The Creation of a  distributed Agile Team,” in proceedings of XP 2007,   pp.235-239, 2007.  [S19] M. Cottmeyer, “The Good and Bad of Agile Offs hore  Development,” in proceedings of the Conference on A GILE  2008, pp.362-367, 2008.  [S20] M. Paasivaara, C. Lassenius, “Could Global So ftware  Development Benefit from Agile Method?” in proceedi ngs  of  ICGSE 2006, pp. 109-113, 2006.    Appendix B. Data Extraction form    Paper description:    1. Paper identifier: Unique id for the paper  2. Date of data extraction:  3. Bibliographic reference: Author, year, title, source  4. Type of article: Journal article/conference paper/  workshop paper/unclear  5. Paper aims: what were the aims of this paper?  6. Paper Evidence: empirical study/experience  report/unclear    GSD Background:    1.  Collaboration mode: inter organizational/intra  organizational/unclear  2. Number of Sites: ……./unclear  3. Number of Teams:……/unclear  4. Project personnel:……/unclear  5. Time zone differences:……/unclear  6. Application Domain: ……./unclear    Study Findings:    1. Scrum team scenario (model): Isolated Scrum  team/Scrum of Scrum meeting used for site based tea m  coordination/Fully integrated ( e.g. Scrum team con tain  onshore and offshore personnel)/unclear  2. Challenges:  challenging factors that impact GSD  communication, coordination and collaboration  processes and restrict the use of Scrum practices.  3. Strategies: Used various strategies to reduce proje ct  stakeholder’s distribution challenges to support th e use  of Scrum practices.  4. Subjective evaluation: a small summary of the findi ngs  from the paper.     ACKNOWLEDGMENT        M. Ali Babar’s research is partially supporte d by  Science foundation Ireland under grant number  03/CE2/I303-1.    REFERENCES    [1] E. Carmel, Global software teams: collaborating across  borders and time zones: Prentice-Hall, 1999.   [2] J. D. Herbsleb “Global Software Engineering: Th e Future of  Socio- technical Coordination” in proceeding of Fut ure of  Software Engineering, FOSE, pp.188-298, 2007.  [3]  P. Abrahamsson, O. Salo, J. Ronkainen, and J. Warsta, Agile  software development methods - Review and analysis , VTT  Electronics ed.: VTT Publications, 2002.   [4] P. J. Ågerfalk and B. Fitzgerald, ""Flexible an d distributed  software processes: Old petunias in new bowls?"" Communications  of the ACM, vol. 49, Oct 2006, pp. 26-34.  [5] F. Abbattista, F. Calefato, D. Gendarmi, F. Lan ubile,  “Incorporating Social Software into Distributed Agi le  Development Environments” in proceedings of the 23 rd ASE  workshop, pp.46-51,2008.    [6] J. Sutherland, K. Schwaber, “The Scrum Papers:  Nuts, Bolts,  and Origin of an Agile Process,”  http://scrumtraininginstitute.com/home/stream_download/scrumpa pers, Last accessed 11 Apr 2009.  [7] Ambler, S.: Agile Adoption Rate Survey: Februar y 2008  http://www.ambysoft.com/surveys/agileFebruary2008.html Last  accessed 11 Apr 2009.  [8] Ambler, S.: Agile Practice and Principles Surve y: July', 'http://www.ambysoft.com/surveys/agileFebruary2008.html Last  accessed 11 Apr 2009.  [8] Ambler, S.: Agile Practice and Principles Surve y: July  2008,http://www.ambysoft.com/surveys/practicesPrinciples2008.ht ml Last accessed 11 Apr 2009.  [9] B. Kitchenham, S. Charters, “Guidelines for Per forming  Systematic Literature Reviews in Software Engineeri ng” EBSE  Technical Report, EBSE-2007-01, 2007.   [10] T. Dyba and T. Dingsoyr, ""Empirical Studies o f Agile  Software Development: A Systematic Review,"" Information and  Software Technology, Vol. 50, Issue 9-10, 2008, pp-833-859.   [11]E., Hossain, M., A., Babar, H., Paik, “Using Ag ile Practices in  Global Software Development: A Systematic Review,” UNSW  CSE Technical Report, TR 904, (2009).   [12] R. K., Yin, “Case Study Research,” Thousand Oa ks, Ca. Sage  publications, (2003).      184 184 Authorized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on May 07,2025 at 20:24:48 UTC from IEEE Xplore.  Restrictions apply.']","['USING  SCRUM  IN  GLOBAL  SOFTWARE  DEVELOPMENT Findings are grouped in terms of challenges  and strategies. Each challenge has a list of  strategies to deal with it, and each strategy  has a brief description together with the re - search findings. Synchronous communication • Synchronized work hours: This practice  is widely used by Scrum teams to ensu - re synchronous communication among  distributed sites can be arranged. This is  done by adjusting working from home,  or working long hours. • Local Scrum team : Due to the lack of  overlap time, Scrum teams are formed  locally and each site conducts their own  scrum. The meeting practice Scrum of  Scrums is attended by a key touch point  member for each team to ensure inter - team communication. • Modified practices : In some cases,  Scrum team modifies or extends Scrum  practices to address the communication  challenges. For example, studies repor - ted that having a local “miniscrum” in  the morning after a distributed scrum  meeting can be very effective to reinfor- ce the value of the Scrum within a local  team. Other researches reported that  the Scrum teams also use strict commu- nication policy (e.g. Email reply within  12 hours) to avoid delay due to the tem- poral distance of a distributed team. Collaboration difficulties • Team Gathering: To increase a project’s  domain knowledge and reduce the cul - tural distance, a Scrum team gathers  and performs few initial sprints at one  site before distributed development  starts. • Visit: To reduce the cultural distance  and increase project vision, some teams  reported that they exchanged visits, for  example Product owners regularly visit  offshore team throughout the develop - ment. • Unofficial distributed meetings : For in - creased team collaboration, along with  formal meetings, distributed Scrum  team members may also use frequent  informal meetings for clarifying various  issues. • Training: Studies shows that Scrum te - ams use some practices that can be ca - tegorized as “training”, for example: ini- tial Scrum training and technical Scrum  to clarify new technology issues, rein - force the value of Scrum and improve  team collaboration. • Key documentation:  Maintaining  valuable documentation may also im - prove GSD team collaboration. • Mandatory participation: To reduce “of- fshore silence” challenge, Scrum team  can assign each site a thirtyminute  mandatory demo presentation during  retrospective sessions. • Gradual team distribution: Scrum teams  may move from a collocated project to  a distributed project gradually through  several stages (i.e., evaluation, incep - tion, transition and steady state). Communication Bandwidth: • Researches show that to provide a rich  communication environment and also  to avoid slow, unreliable, and poor  transmission, Scrum teams use the  practice “multiple communication mo - des”. The practice ensures that a Scrum  team with distributed project stakehol - ders is supported with various options  of communication tools such as phone,  web camera, teleconference, video con- ference, web conference, net meeting,  email, shared mailing list, Instant Mes - sage (IM), Short Message Service (SMS),  and Internet Relay chat (IRC). Tool support: • Studies revealed that along with commu- nication tools, Scrum teams also use a  number of collaborative tools including:  Wikis, Blogs, social bookmarking, ex - pertise finders, whiteboards, electronic  workspace, desktop and application  sharing, photo charts, knowledge bases,  experience databases, lesson learned  repositories, while using Scrum practi - ces. Large Team: • Isolated Scrum team: GSD project teams  are geographically isolated; in most ca - ses offshore teams are not crossfunctio- nal and may not use Scrum processes. • Distributed Scrum of Scrums team : In  this team model, Scrum teams (or sub - teams) are formed based on local site  and each team perform their site based  own independent Scrum.', '• Distributed Scrum of Scrums team : In  this team model, Scrum teams (or sub - teams) are formed based on local site  and each team perform their site based  own independent Scrum. • Fully Integrated Scrum team:  In this  team model, Scrum teams are cross - functional with team members distribu- ted across geographical locations. This  type of Scrum team should consider the  risks due to geographical, temporal and  sociocultural distances. Office Space: • This practice ensures each Scrum team  is allocated to a single room so that they  can communicate with each other. • Dedicated meeting room: This practice  also ensures each site has a tools while  attending a distributed meeting. Multiple sites: • Local Scrum team : GSD project mana - gers build autonomous site-based local  Scrum teams and allocate tasks with in- dependent team. • Restricted team distribution : In this  practice, a fully integrated Scrum team  is restricted within a limited number of  sites distributions. MAIN FINDINGS ORIGINAL SYSTEMATIC REVIEW REFERENCE Hossain, E.; Babar, M.A.; Hyeyoung Paik, “Using Scrum in Global Software Development: A Systematic Literature Review,” in Global Software Engi- neering, 2009. ICGSE 2009. Fourth IEEE International Conference on , vol., no., pp.175184, 1316 July 2009 doi: 10.1109/ICGSE.2009.25. Keywords: Global software development  Agile approaches  Scrum  Who is this briefing for? Software engineers practitioners  who want to make decisions  about the use of Scrum on global  software development  projects based on scientific evi- dence. Where the findings come  from? All findings of this briefing were  extracted from the systematic re- view conducted by Hossain et al. What is systematic reviews? cin.ufpe.br/eseg/systematic-reviews What is included in this brie- fing? The main findings of the original  systematic review. Evidence characteristics through  a brief description about the ori- ginal systematic review and the  studies it analized. What is not included in this  briefing? Additional information not pre- sented in the original systematic  review.  Detailed descriptions about the  studies analised in the original  systematic review. For additional information  about this briefing: cin.ufpe.br/eseg/briefings This briefing reports evidence on findings  about challenges involved in using Scrum  for global software development (GSD) pro - jects and the strategies available to deal with  them based on scientific evidence from a  systematic review.']","**Title: Enhancing Scrum Practices in Global Software Development**

**Introduction:**
This briefing aims to present the key findings from a systematic literature review on the application of Scrum practices in Global Software Development (GSD). As GSD becomes increasingly prevalent, understanding the challenges and strategies associated with implementing Scrum in distributed environments is essential for researchers and practitioners alike.

**Main Findings:**
1. **Challenges in Using Scrum for GSD:**
   - The review identified several significant challenges faced by teams applying Scrum in GSD contexts, including:
     - **Communication Barriers:** Synchronous communication is often hindered by time zone differences, making it difficult to conduct lengthy meetings such as sprint planning or retrospectives.
     - **Cultural Differences:** Variations in cultural backgrounds can lead to misunderstandings and reduced collaboration among team members.
     - **Coordination Issues:** Managing large, distributed teams complicates coordination and can lead to ineffective Scrum practices.
     - **Tool Limitations:** Insufficient or inadequate tools can restrict the effectiveness of Scrum practices, impacting collaboration and communication.

2. **Strategies to Overcome Challenges:**
   - The review highlighted various strategies that Scrum teams have employed to mitigate the identified challenges:
     - **Synchronized Work Hours:** Adjusting working hours to create overlapping time for meetings can facilitate better communication.
     - **Local Scrum Teams:** Forming local teams that conduct their own Scrum practices can enhance collaboration while maintaining some level of autonomy.
     - **Modified Scrum Practices:** Adapting Scrum methodologies, such as conducting shorter meetings or asynchronous retrospectives, can help teams navigate time zone challenges.
     - **Team Gatherings:** Bringing team members together periodically can build rapport and improve understanding of project goals, reducing cultural distance.
     - **Use of Diverse Communication Tools:** Employing a range of communication tools (e.g., video conferencing, instant messaging) can help address bandwidth issues and improve overall communication.

3. **Empirical Evidence and Limitations:**
   - The review found a growing body of literature on Scrum in GSD, but most studies are based on industry experience reports rather than empirical research. This indicates a need for more rigorous studies to better understand the application of Scrum in distributed settings.

**Who is this briefing for?**
This briefing is intended for software engineering practitioners, project managers, and researchers interested in implementing or studying Scrum practices in globally distributed software development environments.

**Where the findings come from?**
The findings are derived from a systematic literature review conducted by Emam Hossain, Muhammad Ali Babar, and Hye-young Paik, which analyzed 20 primary studies on the use of Scrum in GSD projects.

**What is included in this briefing?**
This briefing summarizes the core challenges of using Scrum in GSD, alongside strategies employed by teams to address these challenges. It reflects the current state of literature and identifies gaps that need further exploration.

**To access other evidence briefings on software engineering:**
[http://ease2017.bth.se/](http://ease2017.bth.se/)

**For additional information about the research:**
Emam Hossain, CSE, The University of New South Wales, Emam.Hossain@nicta.com.au  
Muhammad Ali Babar, Lero, University of Limerick, Muhammad.AliBabar@lero.ie  
Hye-young Paik, CSE, The University of New South Wales, hpaik@cse.unsw.edu.au  

**Original Research Reference:**
Hossain, E., Babar, M. A., & Paik, H. (2009). Using Scrum in Global Software Development: A Systematic Literature Review. Proceedings of the 2009 Fourth IEEE International Conference on Global Software Engineering, 175-182. DOI: 10.1109/ICGSE.2009.25"
